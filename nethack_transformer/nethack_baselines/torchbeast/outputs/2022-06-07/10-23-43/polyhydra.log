[2022-06-07 10:23:43,246][root][INFO] - name: null
wandb: false
project: nethack_challenge
entity: user1
group: group1
mock: false
single_ttyrec: true
num_seeds: 0
write_profiler_trace: false
fn_penalty_step: constant
penalty_time: 0.0
penalty_step: -0.01
reward_lose: 0
reward_win: 100
state_counter: none
character: mon-hum-neu-mal
mode: train
env: challenge
num_actors: 256
total_steps: 1000000000.0
batch_size: 32
unroll_length: 80
num_learner_threads: 1
num_inference_threads: 1
disable_cuda: false
learner_device: cuda:0
actor_device: cuda:0
learning_rate: 0.0002
grad_norm_clipping: 40
alpha: 0.99
momentum: 0
epsilon: 1.0e-06
entropy_cost: 0.001
baseline_cost: 0.5
discounting: 0.999
normalize_reward: true
model: baseline
use_lstm: true
hidden_dim: 256
embedding_dim: 64
layers: 5
crop_dim: 9
use_index_select: true
restrict_action_space: true
msg:
  hidden_dim: 64
  embedding_dim: 32
load_dir: null

[2022-06-07 10:23:43,256][root][INFO] - Symlinked log directory: /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/latest
[2022-06-07 10:23:43,259][root][INFO] - Logging results to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43
[2022-06-07 10:23:43,259][root][INFO] - Creating archive directory: /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/archives
[2022-06-07 10:23:43,321][palaas/out][INFO] - Found log directory: /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43
[2022-06-07 10:23:43,322][palaas/out][INFO] - Saving arguments to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/meta.json
[2022-06-07 10:23:43,323][palaas/out][INFO] - Saving messages to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/out.log
[2022-06-07 10:23:43,323][palaas/out][INFO] - Saving logs data to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/logs.csv
[2022-06-07 10:23:43,324][palaas/out][INFO] - Saving logs' fields to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/fields.csv
[2022-06-07 10:23:43,408][root][INFO] - Using CUDA.
[2022-06-07 10:23:43,430][root][INFO] - Using model baseline
[2022-06-07 10:23:43,430][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:46,884][root][INFO] - Number of model parameters: 12330098
[2022-06-07 10:23:46,884][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,011][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,013][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,013][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,012][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,013][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,013][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,013][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,016][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,017][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,017][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,017][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,020][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,023][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,018][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,024][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,032][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,033][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,034][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,034][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,035][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,035][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,035][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,036][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,036][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,037][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,037][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,037][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,037][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,038][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,038][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,038][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,039][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,039][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,051][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,051][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,055][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,072][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,072][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,073][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,073][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,073][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,073][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,076][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,076][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,077][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,077][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,077][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,078][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,078][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,079][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,079][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,079][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,079][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,079][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,074][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,081][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,081][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,082][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,083][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,081][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,082][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,077][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,075][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,083][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,084][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,084][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,077][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,084][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,084][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,077][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,083][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,085][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,085][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,086][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,086][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,086][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,086][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,080][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,086][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,088][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,088][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,089][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,092][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,092][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,111][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,115][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,115][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,127][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,128][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,128][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,128][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,129][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,129][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,129][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,130][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,120][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,122][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,121][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,124][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,124][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,123][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,124][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,124][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,124][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,124][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,125][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,126][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:47,179][nle.env.base][INFO] - Not saving any NLE data.
[2022-06-07 10:23:52,006][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint_0.tar
[2022-06-07 10:23:52,142][root][INFO] - Step 2560 @ 511.9 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 2560, mean_episode_return = None, mean_episode_step = 40.5, total_loss = -356.96, pg_loss = -372.37, baseline_loss = 26.441, entropy_loss = -11.037, learner_queue_size = 32, train_seconds = 5.0)
[2022-06-07 10:23:52,813][palaas/out][INFO] - Updated log fields: ['_tick', '_time', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'pg_loss', 'baseline_loss', 'entropy_loss', 'learner_queue_size', 'train_seconds']
[2022-06-07 10:23:57,148][root][INFO] - Step 12800 @ 1991.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 12800, mean_episode_return = -0.02, mean_episode_step = 45.41, total_loss = 80.823, pg_loss = 58.144, baseline_loss = 31.522, entropy_loss = -8.8428, learner_queue_size = 32, train_seconds = 10.1, _tick = 2, _time = 1.6546e+09)
[2022-06-07 10:24:02,154][root][INFO] - Step 20480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20480, mean_episode_return = -1.07, mean_episode_step = 98.329, total_loss = 6.1375, pg_loss = 7.555, baseline_loss = 6.0918, entropy_loss = -7.5093, learner_queue_size = 32, train_seconds = 15.1, _tick = 4, _time = 1.6546e+09)
[2022-06-07 10:24:07,158][root][INFO] - Step 30720 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 30720, mean_episode_return = None, mean_episode_step = 120.09, total_loss = -30.243, pg_loss = -23.148, baseline_loss = 1.9593, entropy_loss = -9.0547, learner_queue_size = 32, train_seconds = 20.2, _tick = 5, _time = 1.6546e+09)
[2022-06-07 10:24:12,162][root][INFO] - Step 38400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38400, mean_episode_return = None, mean_episode_step = 150.5, total_loss = -9.387, pg_loss = -1.4008, baseline_loss = 1.7476, entropy_loss = -9.7337, learner_queue_size = 32, train_seconds = 25.2, _tick = 5, _time = 1.6546e+09)
[2022-06-07 10:24:17,166][root][INFO] - Step 48640 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 48640, mean_episode_return = None, mean_episode_step = 190.0, total_loss = -11.5, pg_loss = -3.5195, baseline_loss = 1.4349, entropy_loss = -9.4149, learner_queue_size = 32, train_seconds = 30.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:22,170][root][INFO] - Step 56320 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 56320, mean_episode_return = None, mean_episode_step = 222.22, total_loss = -10.203, pg_loss = -1.6739, baseline_loss = 1.3827, entropy_loss = -9.9122, learner_queue_size = 32, train_seconds = 35.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:27,174][root][INFO] - Step 66560 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 66560, mean_episode_return = None, mean_episode_step = 257.91, total_loss = 0.84215, pg_loss = 10.215, baseline_loss = 1.1378, entropy_loss = -10.511, learner_queue_size = 32, train_seconds = 40.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:32,180][root][INFO] - Step 74240 @ 1534.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 74240, mean_episode_return = None, mean_episode_step = 291.69, total_loss = 4.9275, pg_loss = 14.536, baseline_loss = 0.96374, entropy_loss = -10.572, learner_queue_size = 32, train_seconds = 45.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:37,186][root][INFO] - Step 84480 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 84480, mean_episode_return = None, mean_episode_step = 329.72, total_loss = -2.2845, pg_loss = 7.4299, baseline_loss = 1.0438, entropy_loss = -10.758, learner_queue_size = 32, train_seconds = 50.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:42,190][root][INFO] - Step 92160 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 92160, mean_episode_return = None, mean_episode_step = 382.0, total_loss = 5.6779, pg_loss = 15.353, baseline_loss = 0.89116, entropy_loss = -10.566, learner_queue_size = 32, train_seconds = 55.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:47,196][root][INFO] - Step 102400 @ 2045.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 102400, mean_episode_return = None, mean_episode_step = 366.66, total_loss = -14.653, pg_loss = -4.461, baseline_loss = 0.67933, entropy_loss = -10.871, learner_queue_size = 32, train_seconds = 60.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:52,198][root][INFO] - Step 110080 @ 1535.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 110080, mean_episode_return = None, mean_episode_step = 427.59, total_loss = 0.26196, pg_loss = 10.246, baseline_loss = 0.6572, entropy_loss = -10.641, learner_queue_size = 32, train_seconds = 65.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:24:57,202][root][INFO] - Step 120320 @ 2046.3 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 120320, mean_episode_return = None, mean_episode_step = 490.41, total_loss = -13.106, pg_loss = -2.9159, baseline_loss = 0.59217, entropy_loss = -10.783, learner_queue_size = 32, train_seconds = 70.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:02,206][root][INFO] - Step 128000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 128000, mean_episode_return = None, mean_episode_step = 478.75, total_loss = -0.8496, pg_loss = 9.3554, baseline_loss = 0.60558, entropy_loss = -10.811, learner_queue_size = 32, train_seconds = 75.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:07,210][root][INFO] - Step 138240 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 138240, mean_episode_return = None, mean_episode_step = 543.0, total_loss = -5.6142, pg_loss = 4.7482, baseline_loss = 0.52098, entropy_loss = -10.883, learner_queue_size = 32, train_seconds = 80.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:12,214][root][INFO] - Step 145920 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 145920, mean_episode_return = None, mean_episode_step = 605.5, total_loss = 20.603, pg_loss = 29.838, baseline_loss = 1.7087, entropy_loss = -10.944, learner_queue_size = 32, train_seconds = 85.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:17,232][root][INFO] - Step 156160 @ 2040.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 156160, mean_episode_return = None, mean_episode_step = 612.59, total_loss = 23.925, pg_loss = 29.248, baseline_loss = 5.6043, entropy_loss = -10.928, learner_queue_size = 32, train_seconds = 90.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:22,234][root][INFO] - Step 163840 @ 1535.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 163840, mean_episode_return = None, mean_episode_step = 614.72, total_loss = -50.571, pg_loss = -40.274, baseline_loss = 0.56829, entropy_loss = -10.865, learner_queue_size = 32, train_seconds = 95.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:27,238][root][INFO] - Step 174080 @ 2046.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 174080, mean_episode_return = None, mean_episode_step = 685.5, total_loss = -31.299, pg_loss = -20.991, baseline_loss = 0.51297, entropy_loss = -10.821, learner_queue_size = 32, train_seconds = 100.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:32,244][root][INFO] - Step 184320 @ 2045.5 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 184320, mean_episode_return = None, mean_episode_step = 758.0, total_loss = 47.04, pg_loss = 52.623, baseline_loss = 5.1127, entropy_loss = -10.695, learner_queue_size = 32, train_seconds = 105.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:37,250][root][INFO] - Step 192000 @ 1534.2 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 192000, mean_episode_return = None, mean_episode_step = 726.78, total_loss = -29.816, pg_loss = -19.755, baseline_loss = 0.52208, entropy_loss = -10.583, learner_queue_size = 32, train_seconds = 110.2, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:42,256][root][INFO] - Step 202240 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 202240, mean_episode_return = None, mean_episode_step = 775.34, total_loss = -42.634, pg_loss = -32.808, baseline_loss = 0.55858, entropy_loss = -10.384, learner_queue_size = 32, train_seconds = 115.3, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:47,262][root][INFO] - Step 209920 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 209920, mean_episode_return = None, mean_episode_step = 803.12, total_loss = -37.432, pg_loss = -27.63, baseline_loss = 0.47192, entropy_loss = -10.273, learner_queue_size = 32, train_seconds = 120.3, _tick = 6, _time = 1.6546e+09)
[2022-06-07 10:25:52,265][root][INFO] - Step 220160 @ 2047.0 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 220160, mean_episode_return = None, mean_episode_step = 877.84, total_loss = -19.049, pg_loss = -9.2029, baseline_loss = 0.65774, entropy_loss = -10.503, learner_queue_size = 32, train_seconds = 125.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:25:57,266][root][INFO] - Step 227840 @ 1535.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 227840, mean_episode_return = None, mean_episode_step = 860.5, total_loss = 72.387, pg_loss = 80.552, baseline_loss = 2.2444, entropy_loss = -10.41, learner_queue_size = 32, train_seconds = 130.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:02,270][root][INFO] - Step 238080 @ 2046.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 238080, mean_episode_return = None, mean_episode_step = 906.66, total_loss = 166.77, pg_loss = 162.59, baseline_loss = 14.806, entropy_loss = -10.627, learner_queue_size = 32, train_seconds = 135.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:07,274][root][INFO] - Step 245760 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 245760, mean_episode_return = None, mean_episode_step = 921.78, total_loss = -14.873, pg_loss = -5.5313, baseline_loss = 1.1403, entropy_loss = -10.482, learner_queue_size = 32, train_seconds = 140.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:12,278][root][INFO] - Step 256000 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 256000, mean_episode_return = None, mean_episode_step = 1016.6, total_loss = -8.1601, pg_loss = 1.8531, baseline_loss = 0.48931, entropy_loss = -10.502, learner_queue_size = 32, train_seconds = 145.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:17,282][root][INFO] - Step 266240 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 266240, mean_episode_return = None, mean_episode_step = 1013.6, total_loss = 226.73, pg_loss = 186.15, baseline_loss = 51.177, entropy_loss = -10.597, learner_queue_size = 32, train_seconds = 150.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:22,288][root][INFO] - Step 273920 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 273920, mean_episode_return = None, mean_episode_step = 1061.9, total_loss = -21.4, pg_loss = -12.253, baseline_loss = 1.1321, entropy_loss = -10.28, learner_queue_size = 32, train_seconds = 155.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:27,294][root][INFO] - Step 281600 @ 1534.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 281600, mean_episode_return = None, mean_episode_step = 1095.5, total_loss = 143.62, pg_loss = 135.9, baseline_loss = 17.976, entropy_loss = -10.259, learner_queue_size = 32, train_seconds = 160.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:32,300][root][INFO] - Step 291840 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 291840, mean_episode_return = None, mean_episode_step = 1167.6, total_loss = 9.7312, pg_loss = 16.758, baseline_loss = 3.0551, entropy_loss = -10.082, learner_queue_size = 32, train_seconds = 165.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:37,306][root][INFO] - Step 299520 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 299520, mean_episode_return = None, mean_episode_step = 1191.7, total_loss = -26.77, pg_loss = -18.528, baseline_loss = 1.3825, entropy_loss = -9.6247, learner_queue_size = 32, train_seconds = 170.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:42,310][root][INFO] - Step 309760 @ 2046.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 309760, mean_episode_return = None, mean_episode_step = 1203.0, total_loss = 33.564, pg_loss = 40.172, baseline_loss = 3.5543, entropy_loss = -10.163, learner_queue_size = 32, train_seconds = 175.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:47,314][root][INFO] - Step 320000 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 320000, mean_episode_return = None, mean_episode_step = 1244.2, total_loss = 42.526, pg_loss = 47.127, baseline_loss = 5.4172, entropy_loss = -10.018, learner_queue_size = 32, train_seconds = 180.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:52,318][root][INFO] - Step 327680 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 327680, mean_episode_return = None, mean_episode_step = 1240.5, total_loss = -22.278, pg_loss = -14.431, baseline_loss = 2.0037, entropy_loss = -9.8509, learner_queue_size = 32, train_seconds = 185.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:26:57,322][root][INFO] - Step 337920 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 337920, mean_episode_return = None, mean_episode_step = 1293.2, total_loss = -5.8447, pg_loss = 1.0859, baseline_loss = 3.0119, entropy_loss = -9.9425, learner_queue_size = 32, train_seconds = 190.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:27:02,326][root][INFO] - Step 345600 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 345600, mean_episode_return = None, mean_episode_step = 1340.5, total_loss = 21.723, pg_loss = 28.562, baseline_loss = 2.9727, entropy_loss = -9.8124, learner_queue_size = 32, train_seconds = 195.3, _tick = 7, _time = 1.6546e+09)
[2022-06-07 10:27:07,332][root][INFO] - Step 353280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 353280, mean_episode_return = None, mean_episode_step = 1371.8, total_loss = -42.563, pg_loss = -34.111, baseline_loss = 1.3965, entropy_loss = -9.8483, learner_queue_size = 32, train_seconds = 200.3, _tick = 8, _time = 1.6546e+09)
[2022-06-07 10:27:12,338][root][INFO] - Step 363520 @ 2045.7 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 363520, mean_episode_return = None, mean_episode_step = 1365.1, total_loss = -34.174, pg_loss = -25.254, baseline_loss = 0.62817, entropy_loss = -9.5488, learner_queue_size = 32, train_seconds = 205.3, _tick = 9, _time = 1.6546e+09)
[2022-06-07 10:27:17,344][root][INFO] - Step 371200 @ 1534.1 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 371200, mean_episode_return = None, mean_episode_step = 1477.9, total_loss = 21.076, pg_loss = 23.966, baseline_loss = 6.7403, entropy_loss = -9.6308, learner_queue_size = 32, train_seconds = 210.3, _tick = 9, _time = 1.6546e+09)
[2022-06-07 10:27:22,350][root][INFO] - Step 381440 @ 2045.5 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 381440, mean_episode_return = None, mean_episode_step = 1457.7, total_loss = -38.673, pg_loss = -30.533, baseline_loss = 1.1097, entropy_loss = -9.2499, learner_queue_size = 32, train_seconds = 215.3, _tick = 9, _time = 1.6546e+09)
[2022-06-07 10:27:27,356][root][INFO] - Step 389120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 389120, mean_episode_return = 29.62, mean_episode_step = 1533.0, total_loss = 370.24, pg_loss = 269.87, baseline_loss = 109.81, entropy_loss = -9.4374, learner_queue_size = 32, train_seconds = 220.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:27:32,362][root][INFO] - Step 399360 @ 2045.5 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 399360, mean_episode_return = None, mean_episode_step = 1545.2, total_loss = -25.414, pg_loss = -25.313, baseline_loss = 9.0859, entropy_loss = -9.1864, learner_queue_size = 32, train_seconds = 225.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:27:37,368][root][INFO] - Step 407040 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 407040, mean_episode_return = None, mean_episode_step = 1599.0, total_loss = 81.774, pg_loss = 68.901, baseline_loss = 22.1, entropy_loss = -9.2273, learner_queue_size = 32, train_seconds = 230.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:27:42,374][root][INFO] - Step 417280 @ 2045.6 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 417280, mean_episode_return = None, mean_episode_step = 1665.6, total_loss = 40.037, pg_loss = 37.981, baseline_loss = 10.897, entropy_loss = -8.8416, learner_queue_size = 32, train_seconds = 235.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:27:47,378][root][INFO] - Step 424960 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 424960, mean_episode_return = None, mean_episode_step = 1615.7, total_loss = -26.983, pg_loss = -21.514, baseline_loss = 3.3212, entropy_loss = -8.7907, learner_queue_size = 32, train_seconds = 240.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:27:52,384][root][INFO] - Step 435200 @ 2045.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 435200, mean_episode_return = None, mean_episode_step = 1598.7, total_loss = -56.637, pg_loss = -48.341, baseline_loss = 0.67629, entropy_loss = -8.9721, learner_queue_size = 32, train_seconds = 245.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:27:57,390][root][INFO] - Step 442880 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 442880, mean_episode_return = None, mean_episode_step = 1761.7, total_loss = 226.0, pg_loss = 186.66, baseline_loss = 48.525, entropy_loss = -9.1779, learner_queue_size = 32, train_seconds = 250.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:28:02,394][root][INFO] - Step 453120 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 453120, mean_episode_return = None, mean_episode_step = 1687.5, total_loss = 64.602, pg_loss = 64.581, baseline_loss = 9.2764, entropy_loss = -9.2563, learner_queue_size = 32, train_seconds = 255.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:28:07,400][root][INFO] - Step 460800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 460800, mean_episode_return = None, mean_episode_step = 1814.5, total_loss = -0.21827, pg_loss = 4.1151, baseline_loss = 4.9023, entropy_loss = -9.2357, learner_queue_size = 32, train_seconds = 260.4, _tick = 10, _time = 1.6546e+09)
[2022-06-07 10:28:12,406][root][INFO] - Step 471040 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 471040, mean_episode_return = -21.33, mean_episode_step = 1743.7, total_loss = 83.766, pg_loss = 79.23, baseline_loss = 13.934, entropy_loss = -9.3979, learner_queue_size = 32, train_seconds = 265.4, _tick = 11, _time = 1.6546e+09)
[2022-06-07 10:28:17,410][root][INFO] - Step 481280 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 481280, mean_episode_return = None, mean_episode_step = 1837.5, total_loss = -0.080173, pg_loss = 6.214, baseline_loss = 3.0552, entropy_loss = -9.3494, learner_queue_size = 32, train_seconds = 270.4, _tick = 11, _time = 1.6546e+09)
[2022-06-07 10:28:22,414][root][INFO] - Step 488960 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 488960, mean_episode_return = None, mean_episode_step = 1790.6, total_loss = 3.5865, pg_loss = 9.7946, baseline_loss = 3.0709, entropy_loss = -9.279, learner_queue_size = 32, train_seconds = 275.4, _tick = 11, _time = 1.6546e+09)
[2022-06-07 10:28:27,440][root][INFO] - Step 499200 @ 2037.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 499200, mean_episode_return = None, mean_episode_step = 2000.1, total_loss = -54.842, pg_loss = -46.352, baseline_loss = 0.53201, entropy_loss = -9.0226, learner_queue_size = 32, train_seconds = 280.4, _tick = 12, _time = 1.6546e+09)
[2022-06-07 10:28:32,446][root][INFO] - Step 506880 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 506880, mean_episode_return = None, mean_episode_step = 1990.6, total_loss = 0.62526, pg_loss = 4.0097, baseline_loss = 5.7048, entropy_loss = -9.0893, learner_queue_size = 32, train_seconds = 285.4, _tick = 12, _time = 1.6546e+09)
[2022-06-07 10:28:37,452][root][INFO] - Step 517120 @ 2045.5 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 517120, mean_episode_return = None, mean_episode_step = 1971.6, total_loss = 70.753, pg_loss = 50.26, baseline_loss = 28.718, entropy_loss = -8.2257, learner_queue_size = 32, train_seconds = 290.4, _tick = 13, _time = 1.6546e+09)
[2022-06-07 10:28:42,458][root][INFO] - Step 524800 @ 1534.2 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 524800, mean_episode_return = 4.7398, mean_episode_step = 1959.2, total_loss = 160.92, pg_loss = 148.54, baseline_loss = 21.154, entropy_loss = -8.771, learner_queue_size = 32, train_seconds = 295.5, _tick = 15, _time = 1.6546e+09)
[2022-06-07 10:28:47,464][root][INFO] - Step 535040 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 535040, mean_episode_return = None, mean_episode_step = 2091.9, total_loss = -50.792, pg_loss = -47.776, baseline_loss = 5.749, entropy_loss = -8.7651, learner_queue_size = 32, train_seconds = 300.5, _tick = 15, _time = 1.6546e+09)
[2022-06-07 10:28:52,470][root][INFO] - Step 542720 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 542720, mean_episode_return = None, mean_episode_step = 2160.6, total_loss = 513.92, pg_loss = 367.28, baseline_loss = 155.11, entropy_loss = -8.477, learner_queue_size = 32, train_seconds = 305.5, _tick = 15, _time = 1.6546e+09)
[2022-06-07 10:28:57,474][root][INFO] - Step 552960 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 552960, mean_episode_return = -14.77, mean_episode_step = 2078.1, total_loss = 3.7464, pg_loss = 5.3492, baseline_loss = 6.373, entropy_loss = -7.9758, learner_queue_size = 32, train_seconds = 310.5, _tick = 16, _time = 1.6546e+09)
[2022-06-07 10:31:03,340][root][INFO] - Step 775680 @ 1769.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 775680, mean_episode_return = None, mean_episode_step = 2806.1, total_loss = 108.13, pg_loss = 77.617, baseline_loss = 38.239, entropy_loss = -7.7223, learner_queue_size = 32, train_seconds = 436.3, _tick = 30, _time = 1.6546e+09)
[2022-06-07 10:31:08,346][root][INFO] - Step 785920 @ 2045.8 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 785920, mean_episode_return = None, mean_episode_step = 2776.7, total_loss = -38.423, pg_loss = -33.374, baseline_loss = 2.2198, entropy_loss = -7.2687, learner_queue_size = 32, train_seconds = 441.3, _tick = 30, _time = 1.6546e+09)
[2022-06-07 10:31:13,352][root][INFO] - Step 796160 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 796160, mean_episode_return = nan, mean_episode_step = 3127.8, total_loss = 351.48, pg_loss = 244.58, baseline_loss = 114.46, entropy_loss = -7.57, learner_queue_size = 32, train_seconds = 446.3, _tick = 30, _time = 1.6546e+09)
[2022-06-07 10:31:18,358][root][INFO] - Step 803840 @ 1534.1 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 803840, mean_episode_return = None, mean_episode_step = 2856.0, total_loss = 86.07, pg_loss = 32.947, baseline_loss = 60.287, entropy_loss = -7.1638, learner_queue_size = 32, train_seconds = 451.4, _tick = 30, _time = 1.6546e+09)
[2022-06-07 10:31:23,362][root][INFO] - Step 814080 @ 2046.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 814080, mean_episode_return = -14.391, mean_episode_step = 2961.7, total_loss = 379.36, pg_loss = 218.27, baseline_loss = 167.21, entropy_loss = -6.1167, learner_queue_size = 32, train_seconds = 456.4, _tick = 32, _time = 1.6546e+09)
[2022-06-07 10:31:28,366][root][INFO] - Step 821760 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 821760, mean_episode_return = None, mean_episode_step = 2839.2, total_loss = 510.89, pg_loss = 307.25, baseline_loss = 209.95, entropy_loss = -6.2997, learner_queue_size = 32, train_seconds = 461.4, _tick = 32, _time = 1.6546e+09)
[2022-06-07 10:31:33,369][root][INFO] - Step 832000 @ 2046.8 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 832000, mean_episode_return = -7.3104, mean_episode_step = 2960.9, total_loss = -81.402, pg_loss = -85.298, baseline_loss = 11.076, entropy_loss = -7.1799, learner_queue_size = 32, train_seconds = 466.4, _tick = 33, _time = 1.6546e+09)
[2022-06-07 10:31:38,374][root][INFO] - Step 839680 @ 1534.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 839680, mean_episode_return = None, mean_episode_step = 2995.2, total_loss = 77.533, pg_loss = 30.137, baseline_loss = 54.284, entropy_loss = -6.8889, learner_queue_size = 32, train_seconds = 471.4, _tick = 33, _time = 1.6546e+09)
[2022-06-07 10:31:43,378][root][INFO] - Step 849920 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 849920, mean_episode_return = None, mean_episode_step = 2974.9, total_loss = 107.5, pg_loss = 69.224, baseline_loss = 44.972, entropy_loss = -6.6929, learner_queue_size = 32, train_seconds = 476.4, _tick = 33, _time = 1.6546e+09)
[2022-06-07 10:31:48,384][root][INFO] - Step 857600 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 857600, mean_episode_return = -26.801, mean_episode_step = 3054.5, total_loss = 42.31, pg_loss = -4.4629, baseline_loss = 53.125, entropy_loss = -6.3521, learner_queue_size = 32, train_seconds = 481.4, _tick = 34, _time = 1.6546e+09)
[2022-06-07 10:31:53,386][root][INFO] - Step 867840 @ 2047.1 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 867840, mean_episode_return = None, mean_episode_step = 2878.2, total_loss = -46.497, pg_loss = -66.17, baseline_loss = 26.206, entropy_loss = -6.5332, learner_queue_size = 32, train_seconds = 486.4, _tick = 34, _time = 1.6546e+09)
[2022-06-07 10:31:58,388][root][INFO] - Step 875520 @ 1535.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 875520, mean_episode_return = None, mean_episode_step = 3046.0, total_loss = 159.48, pg_loss = 77.866, baseline_loss = 87.695, entropy_loss = -6.0816, learner_queue_size = 32, train_seconds = 491.4, _tick = 34, _time = 1.6546e+09)
[2022-06-07 10:32:03,394][root][INFO] - Step 885760 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 885760, mean_episode_return = None, mean_episode_step = 3265.8, total_loss = 68.221, pg_loss = -25.542, baseline_loss = 100.04, entropy_loss = -6.2798, learner_queue_size = 32, train_seconds = 496.4, _tick = 34, _time = 1.6546e+09)
[2022-06-07 10:32:08,400][root][INFO] - Step 893440 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 893440, mean_episode_return = None, mean_episode_step = 3088.4, total_loss = 125.54, pg_loss = 78.003, baseline_loss = 53.568, entropy_loss = -6.0355, learner_queue_size = 32, train_seconds = 501.4, _tick = 34, _time = 1.6546e+09)
[2022-06-07 10:32:13,406][root][INFO] - Step 903680 @ 2045.7 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 903680, mean_episode_return = None, mean_episode_step = 3339.9, total_loss = -106.76, pg_loss = -102.7, baseline_loss = 2.1878, entropy_loss = -6.2436, learner_queue_size = 32, train_seconds = 506.4, _tick = 34, _time = 1.6546e+09)
[2022-06-07 10:32:18,410][root][INFO] - Step 911360 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 911360, mean_episode_return = 5.4494, mean_episode_step = 3471.6, total_loss = 273.23, pg_loss = 151.99, baseline_loss = 126.97, entropy_loss = -5.7267, learner_queue_size = 32, train_seconds = 511.4, _tick = 35, _time = 1.6546e+09)
[2022-06-07 10:32:23,414][root][INFO] - Step 921600 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 921600, mean_episode_return = None, mean_episode_step = 3458.0, total_loss = 103.7, pg_loss = 53.682, baseline_loss = 55.636, entropy_loss = -5.6218, learner_queue_size = 32, train_seconds = 516.4, _tick = 36, _time = 1.6546e+09)
[2022-06-07 10:32:28,418][root][INFO] - Step 929280 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 929280, mean_episode_return = None, mean_episode_step = 3582.1, total_loss = 368.73, pg_loss = 100.28, baseline_loss = 274.45, entropy_loss = -5.9925, learner_queue_size = 32, train_seconds = 521.4, _tick = 36, _time = 1.6546e+09)
[2022-06-07 10:32:33,424][root][INFO] - Step 939520 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 939520, mean_episode_return = None, mean_episode_step = 3085.0, total_loss = 47.657, pg_loss = 12.992, baseline_loss = 39.896, entropy_loss = -5.2309, learner_queue_size = 32, train_seconds = 526.4, _tick = 36, _time = 1.6546e+09)
[2022-06-07 10:32:38,430][root][INFO] - Step 949760 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 949760, mean_episode_return = None, mean_episode_step = 3616.6, total_loss = 87.926, pg_loss = 11.094, baseline_loss = 81.473, entropy_loss = -4.6407, learner_queue_size = 32, train_seconds = 531.4, _tick = 38, _time = 1.6546e+09)
[2022-06-07 10:32:43,436][root][INFO] - Step 957440 @ 1534.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 957440, mean_episode_return = None, mean_episode_step = 3234.2, total_loss = 114.22, pg_loss = 44.985, baseline_loss = 73.336, entropy_loss = -4.1022, learner_queue_size = 32, train_seconds = 536.4, _tick = 39, _time = 1.6546e+09)
[2022-06-07 10:32:48,442][root][INFO] - Step 965120 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 965120, mean_episode_return = -4.3406, mean_episode_step = 3380.9, total_loss = 44.154, pg_loss = 1.7586, baseline_loss = 45.474, entropy_loss = -3.078, learner_queue_size = 32, train_seconds = 541.4, _tick = 40, _time = 1.6546e+09)
[2022-06-07 10:32:53,446][root][INFO] - Step 975360 @ 2046.3 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 975360, mean_episode_return = None, mean_episode_step = 3452.9, total_loss = -22.37, pg_loss = -34.906, baseline_loss = 15.164, entropy_loss = -2.6274, learner_queue_size = 32, train_seconds = 546.4, _tick = 41, _time = 1.6546e+09)
[2022-06-07 10:32:58,450][root][INFO] - Step 983040 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 983040, mean_episode_return = None, mean_episode_step = 3501.2, total_loss = -82.237, pg_loss = -109.37, baseline_loss = 30.188, entropy_loss = -3.0586, learner_queue_size = 32, train_seconds = 551.4, _tick = 41, _time = 1.6546e+09)
[2022-06-07 10:33:03,454][root][INFO] - Step 993280 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 993280, mean_episode_return = -7.1306, mean_episode_step = 3422.4, total_loss = -47.595, pg_loss = -49.299, baseline_loss = 6.3981, entropy_loss = -4.6945, learner_queue_size = 32, train_seconds = 556.4, _tick = 43, _time = 1.6546e+09)
[2022-06-07 10:33:08,458][root][INFO] - Step 1000960 @ 1534.7 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 1000960, mean_episode_return = None, mean_episode_step = 3530.2, total_loss = -40.065, pg_loss = -35.603, baseline_loss = 1.1434, entropy_loss = -5.6055, learner_queue_size = 32, train_seconds = 561.5, _tick = 43, _time = 1.6546e+09)
[2022-06-07 10:33:13,462][root][INFO] - Step 1011200 @ 2046.4 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 1011200, mean_episode_return = 20.719, mean_episode_step = 3390.1, total_loss = 171.47, pg_loss = 105.82, baseline_loss = 71.731, entropy_loss = -6.0892, learner_queue_size = 32, train_seconds = 566.5, _tick = 44, _time = 1.6546e+09)
[2022-06-07 10:33:18,468][root][INFO] - Step 1018880 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1018880, mean_episode_return = None, mean_episode_step = 3775.5, total_loss = 0.58608, pg_loss = -16.798, baseline_loss = 23.863, entropy_loss = -6.4786, learner_queue_size = 32, train_seconds = 571.5, _tick = 44, _time = 1.6546e+09)
[2022-06-07 10:33:23,474][root][INFO] - Step 1029120 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 1029120, mean_episode_return = None, mean_episode_step = 3824.2, total_loss = -35.978, pg_loss = -34.022, baseline_loss = 4.251, entropy_loss = -6.2077, learner_queue_size = 32, train_seconds = 576.5, _tick = 45, _time = 1.6546e+09)
[2022-06-07 10:33:28,478][root][INFO] - Step 1036800 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 1036800, mean_episode_return = None, mean_episode_step = 3821.2, total_loss = -75.176, pg_loss = -73.367, baseline_loss = 4.37, entropy_loss = -6.1794, learner_queue_size = 32, train_seconds = 581.5, _tick = 46, _time = 1.6546e+09)
[2022-06-07 10:33:33,484][root][INFO] - Step 1047040 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 1047040, mean_episode_return = None, mean_episode_step = 3561.7, total_loss = 588.09, pg_loss = 366.53, baseline_loss = 227.91, entropy_loss = -6.3509, learner_queue_size = 32, train_seconds = 586.5, _tick = 47, _time = 1.6546e+09)
[2022-06-07 10:33:38,490][root][INFO] - Step 1057280 @ 2045.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 1057280, mean_episode_return = None, mean_episode_step = 3871.0, total_loss = 195.75, pg_loss = 140.61, baseline_loss = 61.722, entropy_loss = -6.5875, learner_queue_size = 32, train_seconds = 591.5, _tick = 49, _time = 1.6546e+09)
[2022-06-07 10:33:43,496][root][INFO] - Step 1064960 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 1064960, mean_episode_return = None, mean_episode_step = 3586.2, total_loss = 70.072, pg_loss = 39.11, baseline_loss = 37.158, entropy_loss = -6.1963, learner_queue_size = 32, train_seconds = 596.5, _tick = 49, _time = 1.6546e+09)
[2022-06-07 10:33:48,502][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 10:33:48,640][root][INFO] - Step 1072640 @ 1534.1 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 1072640, mean_episode_return = 20.569, mean_episode_step = 3853.7, total_loss = -128.85, pg_loss = -127.48, baseline_loss = 4.86, entropy_loss = -6.2308, learner_queue_size = 32, train_seconds = 601.5, _tick = 51, _time = 1.6546e+09)
[2022-06-07 10:33:53,645][root][INFO] - Step 1082880 @ 1991.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 1082880, mean_episode_return = None, mean_episode_step = 3581.0, total_loss = -24.715, pg_loss = -41.572, baseline_loss = 23.005, entropy_loss = -6.1475, learner_queue_size = 32, train_seconds = 606.6, _tick = 51, _time = 1.6546e+09)
[2022-06-07 10:33:58,651][root][INFO] - Step 1090560 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 1090560, mean_episode_return = None, mean_episode_step = 4043.1, total_loss = 87.72, pg_loss = 18.667, baseline_loss = 75.101, entropy_loss = -6.0486, learner_queue_size = 32, train_seconds = 611.6, _tick = 51, _time = 1.6546e+09)
[2022-06-07 10:34:03,654][root][INFO] - Step 1100800 @ 2046.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 1100800, mean_episode_return = None, mean_episode_step = 3913.6, total_loss = -77.855, pg_loss = -99.837, baseline_loss = 27.797, entropy_loss = -5.8156, learner_queue_size = 32, train_seconds = 616.6, _tick = 52, _time = 1.6546e+09)
[2022-06-07 10:34:08,660][root][INFO] - Step 1108480 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 1108480, mean_episode_return = None, mean_episode_step = 3929.5, total_loss = 77.904, pg_loss = -7.8447, baseline_loss = 91.962, entropy_loss = -6.2132, learner_queue_size = 32, train_seconds = 621.7, _tick = 52, _time = 1.6546e+09)
[2022-06-07 10:34:13,666][root][INFO] - Step 1118720 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1118720, mean_episode_return = None, mean_episode_step = 3824.8, total_loss = 35.275, pg_loss = 5.4721, baseline_loss = 35.597, entropy_loss = -5.7943, learner_queue_size = 32, train_seconds = 626.7, _tick = 53, _time = 1.6546e+09)
[2022-06-07 10:34:18,670][root][INFO] - Step 1126400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1126400, mean_episode_return = None, mean_episode_step = 3259.9, total_loss = -148.82, pg_loss = -148.83, baseline_loss = 6.0415, entropy_loss = -6.0394, learner_queue_size = 32, train_seconds = 631.7, _tick = 53, _time = 1.6546e+09)
[2022-06-07 10:34:23,674][root][INFO] - Step 1136640 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1136640, mean_episode_return = -18.431, mean_episode_step = 4014.8, total_loss = 30.494, pg_loss = -20.851, baseline_loss = 57.486, entropy_loss = -6.141, learner_queue_size = 32, train_seconds = 636.7, _tick = 54, _time = 1.6546e+09)
[2022-06-07 10:34:28,678][root][INFO] - Step 1146880 @ 2046.3 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 1146880, mean_episode_return = None, mean_episode_step = 3816.2, total_loss = 64.088, pg_loss = 19.287, baseline_loss = 50.902, entropy_loss = -6.0999, learner_queue_size = 32, train_seconds = 641.7, _tick = 55, _time = 1.6546e+09)
[2022-06-07 10:34:33,682][root][INFO] - Step 1154560 @ 1534.8 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 1154560, mean_episode_return = None, mean_episode_step = 4032.0, total_loss = 142.24, pg_loss = 75.878, baseline_loss = 72.098, entropy_loss = -5.741, learner_queue_size = 32, train_seconds = 646.7, _tick = 55, _time = 1.6546e+09)
[2022-06-07 10:34:38,686][root][INFO] - Step 1164800 @ 2046.4 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 1164800, mean_episode_return = None, mean_episode_step = 3879.7, total_loss = 204.25, pg_loss = 110.8, baseline_loss = 99.43, entropy_loss = -5.9875, learner_queue_size = 32, train_seconds = 651.7, _tick = 55, _time = 1.6546e+09)
[2022-06-07 10:34:43,690][root][INFO] - Step 1172480 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 1172480, mean_episode_return = None, mean_episode_step = 4092.1, total_loss = -62.532, pg_loss = -63.292, baseline_loss = 6.5322, entropy_loss = -5.772, learner_queue_size = 32, train_seconds = 656.7, _tick = 56, _time = 1.6546e+09)
[2022-06-07 10:34:48,702][root][INFO] - Step 1182720 @ 2043.1 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 1182720, mean_episode_return = None, mean_episode_step = 4037.6, total_loss = -18.733, pg_loss = -48.246, baseline_loss = 34.726, entropy_loss = -5.2125, learner_queue_size = 32, train_seconds = 661.7, _tick = 57, _time = 1.6546e+09)
[2022-06-07 10:34:53,706][root][INFO] - Step 1190400 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1190400, mean_episode_return = None, mean_episode_step = 4044.6, total_loss = -51.849, pg_loss = -49.003, baseline_loss = 1.0, entropy_loss = -3.8455, learner_queue_size = 32, train_seconds = 666.7, _tick = 58, _time = 1.6546e+09)
[2022-06-07 10:34:58,712][root][INFO] - Step 1200640 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 1200640, mean_episode_return = 49.479, mean_episode_step = 3341.1, total_loss = 17.403, pg_loss = -13.458, baseline_loss = 34.783, entropy_loss = -3.9228, learner_queue_size = 32, train_seconds = 671.7, _tick = 61, _time = 1.6546e+09)
[2022-06-07 10:35:03,718][root][INFO] - Step 1208320 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 1208320, mean_episode_return = None, mean_episode_step = 4146.6, total_loss = -41.525, pg_loss = -68.268, baseline_loss = 30.207, entropy_loss = -3.464, learner_queue_size = 32, train_seconds = 676.7, _tick = 61, _time = 1.6546e+09)
[2022-06-07 10:35:08,722][root][INFO] - Step 1218560 @ 2046.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 1218560, mean_episode_return = 8.6541, mean_episode_step = 4350.0, total_loss = -61.416, pg_loss = -59.771, baseline_loss = 4.066, entropy_loss = -5.711, learner_queue_size = 32, train_seconds = 681.7, _tick = 63, _time = 1.6546e+09)
[2022-06-07 10:35:13,726][root][INFO] - Step 1226240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1226240, mean_episode_return = None, mean_episode_step = 3976.5, total_loss = -18.329, pg_loss = -13.613, baseline_loss = 1.4116, entropy_loss = -6.1273, learner_queue_size = 32, train_seconds = 686.7, _tick = 64, _time = 1.6546e+09)
[2022-06-07 10:35:18,732][root][INFO] - Step 1233920 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 1233920, mean_episode_return = -10.921, mean_episode_step = 4118.6, total_loss = 94.983, pg_loss = 72.098, baseline_loss = 29.983, entropy_loss = -7.0983, learner_queue_size = 32, train_seconds = 691.7, _tick = 65, _time = 1.6546e+09)
[2022-06-07 10:35:23,738][root][INFO] - Step 1244160 @ 2045.5 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 1244160, mean_episode_return = None, mean_episode_step = 4158.8, total_loss = -44.893, pg_loss = -44.559, baseline_loss = 6.2134, entropy_loss = -6.5472, learner_queue_size = 32, train_seconds = 696.7, _tick = 66, _time = 1.6546e+09)
[2022-06-07 10:35:28,740][root][INFO] - Step 1251840 @ 1535.3 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 1251840, mean_episode_return = 20.919, mean_episode_step = 3768.6, total_loss = 579.3, pg_loss = 397.08, baseline_loss = 189.2, entropy_loss = -6.9835, learner_queue_size = 32, train_seconds = 701.7, _tick = 67, _time = 1.6546e+09)
[2022-06-07 10:35:33,746][root][INFO] - Step 1262080 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 1262080, mean_episode_return = None, mean_episode_step = 4001.9, total_loss = -13.624, pg_loss = -36.497, baseline_loss = 29.874, entropy_loss = -7.0014, learner_queue_size = 32, train_seconds = 706.7, _tick = 67, _time = 1.6546e+09)
[2022-06-07 10:35:38,750][root][INFO] - Step 1269760 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 1269760, mean_episode_return = None, mean_episode_step = 3508.2, total_loss = 378.76, pg_loss = 241.28, baseline_loss = 144.62, entropy_loss = -7.1383, learner_queue_size = 32, train_seconds = 711.7, _tick = 68, _time = 1.6546e+09)
[2022-06-07 10:35:43,754][root][INFO] - Step 1280000 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 1280000, mean_episode_return = None, mean_episode_step = 4141.8, total_loss = 132.58, pg_loss = 74.642, baseline_loss = 64.953, entropy_loss = -7.017, learner_queue_size = 32, train_seconds = 716.7, _tick = 69, _time = 1.6546e+09)
[2022-06-07 10:35:48,758][root][INFO] - Step 1287680 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 1287680, mean_episode_return = None, mean_episode_step = 3653.4, total_loss = 320.21, pg_loss = 209.56, baseline_loss = 117.46, entropy_loss = -6.8114, learner_queue_size = 32, train_seconds = 721.8, _tick = 70, _time = 1.6546e+09)
[2022-06-07 10:35:53,762][root][INFO] - Step 1297920 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1297920, mean_episode_return = None, mean_episode_step = 3650.7, total_loss = 2.5337, pg_loss = -18.376, baseline_loss = 27.746, entropy_loss = -6.8356, learner_queue_size = 32, train_seconds = 726.8, _tick = 73, _time = 1.6546e+09)
[2022-06-07 10:35:58,768][root][INFO] - Step 1305600 @ 1534.1 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 1305600, mean_episode_return = None, mean_episode_step = 3714.9, total_loss = -39.196, pg_loss = -38.289, baseline_loss = 6.0834, entropy_loss = -6.99, learner_queue_size = 32, train_seconds = 731.8, _tick = 73, _time = 1.6546e+09)
[2022-06-07 10:36:03,774][root][INFO] - Step 1315840 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 1315840, mean_episode_return = None, mean_episode_step = 4300.9, total_loss = -76.936, pg_loss = -75.208, baseline_loss = 4.5906, entropy_loss = -6.3187, learner_queue_size = 32, train_seconds = 736.8, _tick = 75, _time = 1.6546e+09)
[2022-06-07 10:36:08,778][root][INFO] - Step 1323520 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1323520, mean_episode_return = None, mean_episode_step = 3441.2, total_loss = 41.124, pg_loss = -12.326, baseline_loss = 60.002, entropy_loss = -6.5522, learner_queue_size = 32, train_seconds = 741.8, _tick = 76, _time = 1.6546e+09)
[2022-06-07 10:36:13,782][root][INFO] - Step 1333760 @ 2046.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 1333760, mean_episode_return = 56.087, mean_episode_step = 3874.2, total_loss = -0.60605, pg_loss = -29.008, baseline_loss = 34.629, entropy_loss = -6.2261, learner_queue_size = 32, train_seconds = 746.8, _tick = 79, _time = 1.6546e+09)
[2022-06-07 10:36:18,786][root][INFO] - Step 1344000 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 1344000, mean_episode_return = 42.879, mean_episode_step = 3945.5, total_loss = 17.52, pg_loss = 5.9373, baseline_loss = 17.56, entropy_loss = -5.9781, learner_queue_size = 32, train_seconds = 751.8, _tick = 80, _time = 1.6546e+09)
[2022-06-07 10:36:23,792][root][INFO] - Step 1351680 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 1351680, mean_episode_return = 14.739, mean_episode_step = 3948.3, total_loss = 222.02, pg_loss = 161.23, baseline_loss = 66.438, entropy_loss = -5.6553, learner_queue_size = 32, train_seconds = 756.8, _tick = 82, _time = 1.6546e+09)
[2022-06-07 10:36:28,798][root][INFO] - Step 1361920 @ 2045.6 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 1361920, mean_episode_return = 44.18, mean_episode_step = 4136.0, total_loss = -62.669, pg_loss = -80.816, baseline_loss = 24.274, entropy_loss = -6.1274, learner_queue_size = 32, train_seconds = 761.8, _tick = 84, _time = 1.6546e+09)
[2022-06-07 10:36:33,804][root][INFO] - Step 1369600 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1369600, mean_episode_return = 0.99694, mean_episode_step = 4031.1, total_loss = 347.05, pg_loss = 192.59, baseline_loss = 160.53, entropy_loss = -6.076, learner_queue_size = 32, train_seconds = 766.8, _tick = 87, _time = 1.6546e+09)
[2022-06-07 10:36:38,810][root][INFO] - Step 1379840 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1379840, mean_episode_return = None, mean_episode_step = 3144.4, total_loss = 293.69, pg_loss = 156.15, baseline_loss = 144.02, entropy_loss = -6.485, learner_queue_size = 32, train_seconds = 771.8, _tick = 90, _time = 1.6546e+09)
[2022-06-07 10:36:43,814][root][INFO] - Step 1387520 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 1387520, mean_episode_return = 49.851, mean_episode_step = 4152.1, total_loss = -18.289, pg_loss = -55.096, baseline_loss = 43.424, entropy_loss = -6.6162, learner_queue_size = 32, train_seconds = 776.8, _tick = 93, _time = 1.6546e+09)
[2022-06-07 10:36:48,817][root][INFO] - Step 1397760 @ 2046.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 1397760, mean_episode_return = None, mean_episode_step = 3600.6, total_loss = 228.9, pg_loss = 172.8, baseline_loss = 62.759, entropy_loss = -6.6568, learner_queue_size = 32, train_seconds = 781.8, _tick = 96, _time = 1.6546e+09)
[2022-06-07 10:36:53,822][root][INFO] - Step 1405440 @ 1534.6 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 1405440, mean_episode_return = 35.137, mean_episode_step = 3254.3, total_loss = 376.05, pg_loss = 222.9, baseline_loss = 159.8, entropy_loss = -6.6594, learner_queue_size = 32, train_seconds = 786.8, _tick = 98, _time = 1.6546e+09)
[2022-06-07 10:36:58,826][root][INFO] - Step 1415680 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 1415680, mean_episode_return = None, mean_episode_step = 4260.3, total_loss = 611.69, pg_loss = 411.37, baseline_loss = 206.72, entropy_loss = -6.4061, learner_queue_size = 32, train_seconds = 791.8, _tick = 100, _time = 1.6546e+09)
[2022-06-07 10:37:03,830][root][INFO] - Step 1423360 @ 1534.8 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 1423360, mean_episode_return = None, mean_episode_step = 4301.0, total_loss = 134.29, pg_loss = 75.654, baseline_loss = 65.046, entropy_loss = -6.4131, learner_queue_size = 32, train_seconds = 796.8, _tick = 102, _time = 1.6546e+09)
[2022-06-07 10:37:08,836][root][INFO] - Step 1433600 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 1433600, mean_episode_return = None, mean_episode_step = 3494.1, total_loss = 244.5, pg_loss = 157.15, baseline_loss = 93.747, entropy_loss = -6.4015, learner_queue_size = 32, train_seconds = 801.8, _tick = 104, _time = 1.6546e+09)
[2022-06-07 10:37:13,842][root][INFO] - Step 1441280 @ 1534.2 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 1441280, mean_episode_return = 75.328, mean_episode_step = 4469.1, total_loss = -7.4268, pg_loss = -48.923, baseline_loss = 47.667, entropy_loss = -6.1713, learner_queue_size = 32, train_seconds = 806.8, _tick = 107, _time = 1.6546e+09)
[2022-06-07 10:37:18,846][root][INFO] - Step 1448960 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1448960, mean_episode_return = 8.4193, mean_episode_step = 3180.8, total_loss = -24.935, pg_loss = -79.794, baseline_loss = 60.854, entropy_loss = -5.9948, learner_queue_size = 32, train_seconds = 811.8, _tick = 109, _time = 1.6546e+09)
[2022-06-07 10:37:23,850][root][INFO] - Step 1459200 @ 2046.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 1459200, mean_episode_return = -10.181, mean_episode_step = 3065.5, total_loss = 117.74, pg_loss = 70.584, baseline_loss = 52.294, entropy_loss = -5.1348, learner_queue_size = 32, train_seconds = 816.8, _tick = 113, _time = 1.6546e+09)
[2022-06-07 10:37:28,854][root][INFO] - Step 1466880 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 1466880, mean_episode_return = 35.479, mean_episode_step = 3425.9, total_loss = -148.3, pg_loss = -147.76, baseline_loss = 4.6615, entropy_loss = -5.2036, learner_queue_size = 32, train_seconds = 821.8, _tick = 115, _time = 1.6546e+09)
[2022-06-07 10:37:33,871][root][INFO] - Step 1477120 @ 2041.1 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 1477120, mean_episode_return = -6.8809, mean_episode_step = 4137.2, total_loss = -117.04, pg_loss = -133.93, baseline_loss = 22.438, entropy_loss = -5.5425, learner_queue_size = 32, train_seconds = 826.9, _tick = 117, _time = 1.6546e+09)
[2022-06-07 10:37:38,877][root][INFO] - Step 1484800 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 1484800, mean_episode_return = 67.754, mean_episode_step = 3178.0, total_loss = 86.056, pg_loss = 29.841, baseline_loss = 61.175, entropy_loss = -4.9601, learner_queue_size = 32, train_seconds = 831.9, _tick = 118, _time = 1.6546e+09)
[2022-06-07 10:37:43,883][root][INFO] - Step 1495040 @ 2045.5 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 1495040, mean_episode_return = 9.0696, mean_episode_step = 3572.0, total_loss = -22.683, pg_loss = -94.435, baseline_loss = 76.75, entropy_loss = -4.9974, learner_queue_size = 32, train_seconds = 836.9, _tick = 122, _time = 1.6546e+09)
[2022-06-07 10:37:48,886][root][INFO] - Step 1505280 @ 2046.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1505280, mean_episode_return = None, mean_episode_step = 3768.9, total_loss = -61.849, pg_loss = -58.523, baseline_loss = 1.1854, entropy_loss = -4.5121, learner_queue_size = 32, train_seconds = 841.9, _tick = 125, _time = 1.6546e+09)
[2022-06-07 10:37:53,892][root][INFO] - Step 1512960 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 1512960, mean_episode_return = None, mean_episode_step = 3678.8, total_loss = 76.186, pg_loss = 52.042, baseline_loss = 27.465, entropy_loss = -3.3214, learner_queue_size = 32, train_seconds = 846.9, _tick = 126, _time = 1.6546e+09)
[2022-06-07 10:37:58,898][root][INFO] - Step 1520640 @ 1534.2 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 1520640, mean_episode_return = None, mean_episode_step = 3515.8, total_loss = -65.21, pg_loss = -69.217, baseline_loss = 8.9226, entropy_loss = -4.9165, learner_queue_size = 32, train_seconds = 851.9, _tick = 127, _time = 1.6546e+09)
[2022-06-07 10:38:03,902][root][INFO] - Step 1530880 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1530880, mean_episode_return = None, mean_episode_step = 3706.8, total_loss = -49.223, pg_loss = -46.192, baseline_loss = 2.4896, entropy_loss = -5.521, learner_queue_size = 32, train_seconds = 856.9, _tick = 127, _time = 1.6546e+09)
[2022-06-07 10:38:08,906][root][INFO] - Step 1538560 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 1538560, mean_episode_return = None, mean_episode_step = 2710.2, total_loss = -10.347, pg_loss = -14.877, baseline_loss = 9.7698, entropy_loss = -5.2404, learner_queue_size = 32, train_seconds = 861.9, _tick = 128, _time = 1.6546e+09)
[2022-06-07 10:38:13,912][root][INFO] - Step 1548800 @ 2045.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 1548800, mean_episode_return = None, mean_episode_step = 4002.3, total_loss = 125.79, pg_loss = 70.426, baseline_loss = 60.397, entropy_loss = -5.0352, learner_queue_size = 32, train_seconds = 866.9, _tick = 131, _time = 1.6546e+09)
[2022-06-07 10:38:18,918][root][INFO] - Step 1556480 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1556480, mean_episode_return = 213.46, mean_episode_step = 3350.9, total_loss = -82.798, pg_loss = -88.101, baseline_loss = 10.753, entropy_loss = -5.4509, learner_queue_size = 32, train_seconds = 871.9, _tick = 132, _time = 1.6546e+09)
[2022-06-07 10:38:23,922][root][INFO] - Step 1566720 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1566720, mean_episode_return = 26.621, mean_episode_step = 2736.5, total_loss = -51.176, pg_loss = -76.275, baseline_loss = 30.142, entropy_loss = -5.0429, learner_queue_size = 32, train_seconds = 876.9, _tick = 135, _time = 1.6546e+09)
[2022-06-07 10:38:28,927][root][INFO] - Step 1574400 @ 1534.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1574400, mean_episode_return = None, mean_episode_step = 2883.8, total_loss = -82.054, pg_loss = -79.92, baseline_loss = 2.6125, entropy_loss = -4.7466, learner_queue_size = 32, train_seconds = 881.9, _tick = 136, _time = 1.6546e+09)
[2022-06-07 10:38:33,932][root][INFO] - Step 1584640 @ 2046.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1584640, mean_episode_return = None, mean_episode_step = 3186.8, total_loss = -5.9495, pg_loss = -24.414, baseline_loss = 23.732, entropy_loss = -5.2678, learner_queue_size = 32, train_seconds = 886.9, _tick = 137, _time = 1.6546e+09)
[2022-06-07 10:38:38,938][root][INFO] - Step 1594880 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 1594880, mean_episode_return = -3.2354, mean_episode_step = 2591.3, total_loss = -138.7, pg_loss = -146.97, baseline_loss = 13.573, entropy_loss = -5.2968, learner_queue_size = 32, train_seconds = 891.9, _tick = 139, _time = 1.6546e+09)
[2022-06-07 10:38:43,942][root][INFO] - Step 1602560 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 1602560, mean_episode_return = 81.361, mean_episode_step = 3099.4, total_loss = 31.594, pg_loss = 19.226, baseline_loss = 17.53, entropy_loss = -5.1622, learner_queue_size = 32, train_seconds = 896.9, _tick = 142, _time = 1.6546e+09)
[2022-06-07 10:38:48,945][root][INFO] - Step 1612800 @ 2046.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 1612800, mean_episode_return = None, mean_episode_step = 2655.1, total_loss = 91.354, pg_loss = 71.586, baseline_loss = 24.458, entropy_loss = -4.6895, learner_queue_size = 32, train_seconds = 901.9, _tick = 144, _time = 1.6546e+09)
[2022-06-07 10:38:53,950][root][INFO] - Step 1620480 @ 1534.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1620480, mean_episode_return = None, mean_episode_step = 3905.7, total_loss = 61.577, pg_loss = 27.624, baseline_loss = 38.305, entropy_loss = -4.3521, learner_queue_size = 32, train_seconds = 906.9, _tick = 144, _time = 1.6546e+09)
[2022-06-07 10:38:58,956][root][INFO] - Step 1630720 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 1630720, mean_episode_return = None, mean_episode_step = 2956.6, total_loss = 104.05, pg_loss = 47.775, baseline_loss = 60.866, entropy_loss = -4.5891, learner_queue_size = 32, train_seconds = 912.0, _tick = 147, _time = 1.6546e+09)
[2022-06-07 10:39:03,962][root][INFO] - Step 1638400 @ 1534.1 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 1638400, mean_episode_return = -23.091, mean_episode_step = 4156.3, total_loss = 66.266, pg_loss = 37.306, baseline_loss = 33.979, entropy_loss = -5.019, learner_queue_size = 32, train_seconds = 917.0, _tick = 150, _time = 1.6546e+09)
[2022-06-07 10:39:08,968][root][INFO] - Step 1648640 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 1648640, mean_episode_return = None, mean_episode_step = 3402.2, total_loss = -114.21, pg_loss = -114.09, baseline_loss = 4.8671, entropy_loss = -4.9873, learner_queue_size = 32, train_seconds = 922.0, _tick = 151, _time = 1.6546e+09)
[2022-06-07 10:39:13,974][root][INFO] - Step 1656320 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1656320, mean_episode_return = 25.881, mean_episode_step = 3228.8, total_loss = 23.549, pg_loss = -2.2783, baseline_loss = 31.23, entropy_loss = -5.4028, learner_queue_size = 32, train_seconds = 927.0, _tick = 154, _time = 1.6546e+09)
[2022-06-07 10:39:18,980][root][INFO] - Step 1666560 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 1666560, mean_episode_return = None, mean_episode_step = 3008.2, total_loss = 85.445, pg_loss = 68.149, baseline_loss = 22.58, entropy_loss = -5.2831, learner_queue_size = 32, train_seconds = 932.0, _tick = 156, _time = 1.6546e+09)
[2022-06-07 10:39:23,986][root][INFO] - Step 1674240 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 1674240, mean_episode_return = None, mean_episode_step = 2373.2, total_loss = 42.939, pg_loss = 4.5441, baseline_loss = 43.47, entropy_loss = -5.0757, learner_queue_size = 32, train_seconds = 937.0, _tick = 157, _time = 1.6546e+09)
[2022-06-07 10:39:28,990][root][INFO] - Step 1684480 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 1684480, mean_episode_return = None, mean_episode_step = 3253.9, total_loss = 190.31, pg_loss = 132.66, baseline_loss = 63.045, entropy_loss = -5.3937, learner_queue_size = 32, train_seconds = 942.0, _tick = 159, _time = 1.6546e+09)
[2022-06-07 10:39:33,994][root][INFO] - Step 1692160 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1692160, mean_episode_return = 12.29, mean_episode_step = 2733.4, total_loss = -66.494, pg_loss = -68.121, baseline_loss = 7.0529, entropy_loss = -5.4256, learner_queue_size = 32, train_seconds = 947.0, _tick = 162, _time = 1.6546e+09)
[2022-06-07 10:39:38,998][root][INFO] - Step 1702400 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1702400, mean_episode_return = 64.573, mean_episode_step = 2052.2, total_loss = 428.23, pg_loss = 313.44, baseline_loss = 120.26, entropy_loss = -5.4681, learner_queue_size = 32, train_seconds = 952.0, _tick = 165, _time = 1.6546e+09)
[2022-06-07 10:39:44,002][root][INFO] - Step 1710080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1710080, mean_episode_return = -34.263, mean_episode_step = 2499.9, total_loss = 102.27, pg_loss = 67.936, baseline_loss = 39.853, entropy_loss = -5.5241, learner_queue_size = 32, train_seconds = 957.0, _tick = 168, _time = 1.6546e+09)
[2022-06-07 10:39:49,006][root][INFO] - Step 1720320 @ 2046.3 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 1720320, mean_episode_return = -20.158, mean_episode_step = 2289.2, total_loss = 607.44, pg_loss = 442.24, baseline_loss = 170.53, entropy_loss = -5.3289, learner_queue_size = 32, train_seconds = 962.0, _tick = 170, _time = 1.6546e+09)
[2022-06-07 10:39:54,010][root][INFO] - Step 1728000 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 1728000, mean_episode_return = -35.151, mean_episode_step = 3079.7, total_loss = -40.758, pg_loss = -61.343, baseline_loss = 26.173, entropy_loss = -5.5887, learner_queue_size = 32, train_seconds = 967.0, _tick = 173, _time = 1.6546e+09)
[2022-06-07 10:39:59,016][root][INFO] - Step 1738240 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 1738240, mean_episode_return = 15.18, mean_episode_step = 2849.6, total_loss = -18.498, pg_loss = -38.09, baseline_loss = 25.093, entropy_loss = -5.5006, learner_queue_size = 32, train_seconds = 972.0, _tick = 175, _time = 1.6546e+09)
[2022-06-07 10:40:04,020][root][INFO] - Step 1745920 @ 1534.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1745920, mean_episode_return = 2.9211, mean_episode_step = 2853.5, total_loss = -47.269, pg_loss = -57.741, baseline_loss = 15.969, entropy_loss = -5.4967, learner_queue_size = 32, train_seconds = 977.0, _tick = 177, _time = 1.6546e+09)
[2022-06-07 10:40:09,026][root][INFO] - Step 1756160 @ 2045.8 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 1756160, mean_episode_return = 32.21, mean_episode_step = 2813.9, total_loss = 123.43, pg_loss = 88.541, baseline_loss = 40.236, entropy_loss = -5.3482, learner_queue_size = 32, train_seconds = 982.0, _tick = 179, _time = 1.6546e+09)
[2022-06-07 10:40:14,032][root][INFO] - Step 1763840 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 1763840, mean_episode_return = 72.371, mean_episode_step = 2265.6, total_loss = 34.799, pg_loss = 5.6552, baseline_loss = 34.407, entropy_loss = -5.2629, learner_queue_size = 32, train_seconds = 987.0, _tick = 181, _time = 1.6546e+09)
[2022-06-07 10:40:19,038][root][INFO] - Step 1774080 @ 2045.5 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 1774080, mean_episode_return = None, mean_episode_step = 2606.4, total_loss = -96.118, pg_loss = -92.389, baseline_loss = 1.5391, entropy_loss = -5.268, learner_queue_size = 32, train_seconds = 992.0, _tick = 181, _time = 1.6546e+09)
[2022-06-07 10:40:24,044][root][INFO] - Step 1781760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1781760, mean_episode_return = None, mean_episode_step = 2103.0, total_loss = 70.943, pg_loss = 35.095, baseline_loss = 41.201, entropy_loss = -5.3524, learner_queue_size = 32, train_seconds = 997.0, _tick = 182, _time = 1.6546e+09)
[2022-06-07 10:40:29,050][root][INFO] - Step 1792000 @ 2045.5 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 1792000, mean_episode_return = -37.999, mean_episode_step = 2947.7, total_loss = -57.459, pg_loss = -100.81, baseline_loss = 48.748, entropy_loss = -5.3985, learner_queue_size = 32, train_seconds = 1002.0, _tick = 184, _time = 1.6546e+09)
[2022-06-07 10:40:34,054][root][INFO] - Step 1799680 @ 1534.9 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 1799680, mean_episode_return = 70.119, mean_episode_step = 3012.2, total_loss = -7.3195, pg_loss = -32.694, baseline_loss = 30.694, entropy_loss = -5.3191, learner_queue_size = 32, train_seconds = 1007.0, _tick = 186, _time = 1.6546e+09)
[2022-06-07 10:40:39,058][root][INFO] - Step 1809920 @ 2046.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 1809920, mean_episode_return = 39.944, mean_episode_step = 2720.3, total_loss = 52.014, pg_loss = 17.288, baseline_loss = 39.528, entropy_loss = -4.8024, learner_queue_size = 32, train_seconds = 1012.1, _tick = 188, _time = 1.6546e+09)
[2022-06-07 10:40:44,062][root][INFO] - Step 1817600 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1817600, mean_episode_return = -7.3308, mean_episode_step = 2516.1, total_loss = 141.16, pg_loss = 49.352, baseline_loss = 97.016, entropy_loss = -5.2095, learner_queue_size = 32, train_seconds = 1017.1, _tick = 189, _time = 1.6546e+09)
[2022-06-07 10:40:49,066][root][INFO] - Step 1827840 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 1827840, mean_episode_return = -1.1456, mean_episode_step = 2329.7, total_loss = -12.402, pg_loss = -67.872, baseline_loss = 60.959, entropy_loss = -5.4892, learner_queue_size = 32, train_seconds = 1022.1, _tick = 192, _time = 1.6546e+09)
[2022-06-07 10:40:54,070][root][INFO] - Step 1835520 @ 1534.7 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 1835520, mean_episode_return = None, mean_episode_step = 3080.2, total_loss = 203.19, pg_loss = 130.27, baseline_loss = 77.945, entropy_loss = -5.0247, learner_queue_size = 32, train_seconds = 1027.1, _tick = 193, _time = 1.6546e+09)
[2022-06-07 10:40:59,074][root][INFO] - Step 1845760 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 1845760, mean_episode_return = 66.275, mean_episode_step = 2032.1, total_loss = 85.032, pg_loss = -11.349, baseline_loss = 101.59, entropy_loss = -5.2132, learner_queue_size = 32, train_seconds = 1032.1, _tick = 196, _time = 1.6546e+09)
[2022-06-07 10:41:04,078][root][INFO] - Step 1853440 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 1853440, mean_episode_return = 14.795, mean_episode_step = 2153.0, total_loss = 25.195, pg_loss = -20.057, baseline_loss = 50.203, entropy_loss = -4.9514, learner_queue_size = 32, train_seconds = 1037.1, _tick = 198, _time = 1.6546e+09)
[2022-06-07 10:41:09,082][root][INFO] - Step 1863680 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1863680, mean_episode_return = None, mean_episode_step = 2194.9, total_loss = -86.91, pg_loss = -85.025, baseline_loss = 2.4119, entropy_loss = -4.297, learner_queue_size = 32, train_seconds = 1042.1, _tick = 200, _time = 1.6546e+09)
[2022-06-07 10:41:14,086][root][INFO] - Step 1871360 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 1871360, mean_episode_return = None, mean_episode_step = 2236.1, total_loss = 56.941, pg_loss = 21.394, baseline_loss = 39.827, entropy_loss = -4.2801, learner_queue_size = 32, train_seconds = 1047.1, _tick = 202, _time = 1.6546e+09)
[2022-06-07 10:41:19,090][root][INFO] - Step 1881600 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1881600, mean_episode_return = None, mean_episode_step = 2040.8, total_loss = -46.888, pg_loss = -44.285, baseline_loss = 1.3027, entropy_loss = -3.9057, learner_queue_size = 32, train_seconds = 1052.1, _tick = 204, _time = 1.6546e+09)
[2022-06-07 10:41:24,094][root][INFO] - Step 1889280 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 1889280, mean_episode_return = -3.7953, mean_episode_step = 2635.0, total_loss = -48.876, pg_loss = -62.646, baseline_loss = 17.818, entropy_loss = -4.0483, learner_queue_size = 32, train_seconds = 1057.1, _tick = 205, _time = 1.6546e+09)
[2022-06-07 10:41:29,098][root][INFO] - Step 1899520 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 1899520, mean_episode_return = None, mean_episode_step = 2240.5, total_loss = -88.49, pg_loss = -89.662, baseline_loss = 5.1052, entropy_loss = -3.9339, learner_queue_size = 32, train_seconds = 1062.1, _tick = 205, _time = 1.6546e+09)
[2022-06-07 10:41:34,102][root][INFO] - Step 1907200 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 1907200, mean_episode_return = None, mean_episode_step = 2269.4, total_loss = -35.85, pg_loss = -38.828, baseline_loss = 6.2678, entropy_loss = -3.2897, learner_queue_size = 32, train_seconds = 1067.1, _tick = 206, _time = 1.6546e+09)
[2022-06-07 10:41:39,106][root][INFO] - Step 1914880 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 1914880, mean_episode_return = 18.86, mean_episode_step = 2768.7, total_loss = 7.6966, pg_loss = -25.146, baseline_loss = 36.507, entropy_loss = -3.6647, learner_queue_size = 32, train_seconds = 1072.1, _tick = 207, _time = 1.6546e+09)
[2022-06-07 10:41:44,110][root][INFO] - Step 1925120 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 1925120, mean_episode_return = None, mean_episode_step = 2345.1, total_loss = 91.706, pg_loss = 34.58, baseline_loss = 61.49, entropy_loss = -4.3634, learner_queue_size = 32, train_seconds = 1077.1, _tick = 208, _time = 1.6546e+09)
[2022-06-07 10:41:49,112][root][INFO] - Step 1932800 @ 1535.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 1932800, mean_episode_return = None, mean_episode_step = 1983.5, total_loss = -46.309, pg_loss = -45.74, baseline_loss = 4.0853, entropy_loss = -4.6548, learner_queue_size = 32, train_seconds = 1082.1, _tick = 209, _time = 1.6546e+09)
[2022-06-07 10:41:54,119][root][INFO] - Step 1943040 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1943040, mean_episode_return = None, mean_episode_step = 2438.6, total_loss = 3.4715, pg_loss = -12.761, baseline_loss = 20.716, entropy_loss = -4.483, learner_queue_size = 32, train_seconds = 1087.1, _tick = 209, _time = 1.6546e+09)
[2022-06-07 10:41:59,125][root][INFO] - Step 1950720 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 1950720, mean_episode_return = 233.62, mean_episode_step = 2189.4, total_loss = 157.88, pg_loss = 90.645, baseline_loss = 71.457, entropy_loss = -4.2202, learner_queue_size = 32, train_seconds = 1092.1, _tick = 210, _time = 1.6546e+09)
[2022-06-07 10:42:04,130][root][INFO] - Step 1960960 @ 2045.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 1960960, mean_episode_return = None, mean_episode_step = 2838.8, total_loss = 19.309, pg_loss = -6.9139, baseline_loss = 29.83, entropy_loss = -3.607, learner_queue_size = 32, train_seconds = 1097.1, _tick = 213, _time = 1.6546e+09)
[2022-06-07 10:42:09,134][root][INFO] - Step 1968640 @ 1534.8 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 1968640, mean_episode_return = None, mean_episode_step = 2376.9, total_loss = 21.59, pg_loss = 6.0127, baseline_loss = 19.316, entropy_loss = -3.7385, learner_queue_size = 32, train_seconds = 1102.1, _tick = 213, _time = 1.6546e+09)
[2022-06-07 10:42:14,138][root][INFO] - Step 1978880 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 1978880, mean_episode_return = None, mean_episode_step = 2569.2, total_loss = 13.096, pg_loss = -0.70461, baseline_loss = 19.107, entropy_loss = -5.3071, learner_queue_size = 32, train_seconds = 1107.1, _tick = 215, _time = 1.6546e+09)
[2022-06-07 10:42:19,144][root][INFO] - Step 1986560 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 1986560, mean_episode_return = None, mean_episode_step = 2680.8, total_loss = 151.06, pg_loss = 98.734, baseline_loss = 57.884, entropy_loss = -5.5593, learner_queue_size = 32, train_seconds = 1112.1, _tick = 216, _time = 1.6546e+09)
[2022-06-07 10:42:24,146][root][INFO] - Step 1996800 @ 2047.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 1996800, mean_episode_return = -14.289, mean_episode_step = 2873.5, total_loss = -85.167, pg_loss = -97.205, baseline_loss = 17.723, entropy_loss = -5.6842, learner_queue_size = 32, train_seconds = 1117.1, _tick = 219, _time = 1.6546e+09)
[2022-06-07 10:42:29,150][root][INFO] - Step 2004480 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 2004480, mean_episode_return = None, mean_episode_step = 2954.6, total_loss = 159.89, pg_loss = 106.03, baseline_loss = 59.413, entropy_loss = -5.5519, learner_queue_size = 32, train_seconds = 1122.1, _tick = 220, _time = 1.6546e+09)
[2022-06-07 10:42:34,154][root][INFO] - Step 2014720 @ 2046.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 2014720, mean_episode_return = None, mean_episode_step = 2830.0, total_loss = 517.17, pg_loss = 364.27, baseline_loss = 158.0, entropy_loss = -5.0934, learner_queue_size = 32, train_seconds = 1127.1, _tick = 221, _time = 1.6546e+09)
[2022-06-07 10:42:39,160][root][INFO] - Step 2024960 @ 2045.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 2024960, mean_episode_return = None, mean_episode_step = 2987.2, total_loss = 327.97, pg_loss = 196.55, baseline_loss = 136.24, entropy_loss = -4.8195, learner_queue_size = 32, train_seconds = 1132.2, _tick = 222, _time = 1.6546e+09)
[2022-06-07 10:42:44,166][root][INFO] - Step 2032640 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 2032640, mean_episode_return = 22.96, mean_episode_step = 2491.3, total_loss = -46.865, pg_loss = -53.454, baseline_loss = 10.626, entropy_loss = -4.0374, learner_queue_size = 32, train_seconds = 1137.2, _tick = 224, _time = 1.6546e+09)
[2022-06-07 10:42:49,172][root][INFO] - Step 2042880 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 2042880, mean_episode_return = None, mean_episode_step = 2691.2, total_loss = 426.01, pg_loss = 165.06, baseline_loss = 265.23, entropy_loss = -4.2869, learner_queue_size = 32, train_seconds = 1142.2, _tick = 227, _time = 1.6546e+09)
[2022-06-07 10:42:54,178][root][INFO] - Step 2050560 @ 1534.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 2050560, mean_episode_return = 45.252, mean_episode_step = 3117.3, total_loss = -85.262, pg_loss = -92.343, baseline_loss = 11.859, entropy_loss = -4.7785, learner_queue_size = 32, train_seconds = 1147.2, _tick = 229, _time = 1.6546e+09)
[2022-06-07 10:42:59,184][root][INFO] - Step 2060800 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2060800, mean_episode_return = None, mean_episode_step = 2631.5, total_loss = 65.582, pg_loss = 30.811, baseline_loss = 38.457, entropy_loss = -3.6871, learner_queue_size = 32, train_seconds = 1152.2, _tick = 230, _time = 1.6546e+09)
[2022-06-07 10:43:04,186][root][INFO] - Step 2068480 @ 1535.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2068480, mean_episode_return = None, mean_episode_step = 2518.2, total_loss = -21.519, pg_loss = -19.266, baseline_loss = 1.0183, entropy_loss = -3.2719, learner_queue_size = 32, train_seconds = 1157.2, _tick = 231, _time = 1.6546e+09)
[2022-06-07 10:43:09,190][root][INFO] - Step 2076160 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 2076160, mean_episode_return = None, mean_episode_step = 2839.2, total_loss = -18.99, pg_loss = -24.839, baseline_loss = 9.092, entropy_loss = -3.2432, learner_queue_size = 32, train_seconds = 1162.2, _tick = 232, _time = 1.6546e+09)
[2022-06-07 10:43:14,196][root][INFO] - Step 2086400 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 2086400, mean_episode_return = -50.348, mean_episode_step = 2530.6, total_loss = 712.52, pg_loss = 262.1, baseline_loss = 454.82, entropy_loss = -4.3941, learner_queue_size = 32, train_seconds = 1167.2, _tick = 233, _time = 1.6546e+09)
[2022-06-07 10:43:19,202][root][INFO] - Step 2096640 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2096640, mean_episode_return = None, mean_episode_step = 2406.1, total_loss = -14.194, pg_loss = -19.038, baseline_loss = 9.9784, entropy_loss = -5.1341, learner_queue_size = 32, train_seconds = 1172.2, _tick = 234, _time = 1.6546e+09)
[2022-06-07 10:43:24,208][root][INFO] - Step 2104320 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2104320, mean_episode_return = None, mean_episode_step = 2676.1, total_loss = 33.377, pg_loss = 16.706, baseline_loss = 22.505, entropy_loss = -5.8342, learner_queue_size = 32, train_seconds = 1177.2, _tick = 235, _time = 1.6546e+09)
[2022-06-07 10:43:29,214][root][INFO] - Step 2114560 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2114560, mean_episode_return = None, mean_episode_step = 3274.8, total_loss = 5.5894, pg_loss = 4.2485, baseline_loss = 7.5833, entropy_loss = -6.2425, learner_queue_size = 32, train_seconds = 1182.2, _tick = 235, _time = 1.6546e+09)
[2022-06-07 10:43:34,218][root][INFO] - Step 2122240 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 2122240, mean_episode_return = 10.479, mean_episode_step = 2657.1, total_loss = 1.8434, pg_loss = 3.3964, baseline_loss = 4.723, entropy_loss = -6.2761, learner_queue_size = 32, train_seconds = 1187.2, _tick = 236, _time = 1.6546e+09)
[2022-06-07 10:43:39,222][root][INFO] - Step 2132480 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 2132480, mean_episode_return = 65.224, mean_episode_step = 1959.0, total_loss = 160.61, pg_loss = 100.13, baseline_loss = 66.419, entropy_loss = -5.9315, learner_queue_size = 32, train_seconds = 1192.2, _tick = 240, _time = 1.6546e+09)
[2022-06-07 10:43:44,226][root][INFO] - Step 2140160 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2140160, mean_episode_return = 92.091, mean_episode_step = 2438.6, total_loss = 83.618, pg_loss = 52.845, baseline_loss = 36.701, entropy_loss = -5.9279, learner_queue_size = 32, train_seconds = 1197.2, _tick = 243, _time = 1.6546e+09)
[2022-06-07 10:43:49,230][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 10:43:49,440][root][INFO] - Step 2150400 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 2150400, mean_episode_return = 14.359, mean_episode_step = 2470.3, total_loss = 197.51, pg_loss = 136.46, baseline_loss = 66.949, entropy_loss = -5.898, learner_queue_size = 32, train_seconds = 1202.2, _tick = 247, _time = 1.6546e+09)
[2022-06-07 10:43:54,442][root][INFO] - Step 2158080 @ 1473.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2158080, mean_episode_return = 24.636, mean_episode_step = 2091.0, total_loss = -14.735, pg_loss = -70.4, baseline_loss = 61.678, entropy_loss = -6.0132, learner_queue_size = 32, train_seconds = 1207.4, _tick = 250, _time = 1.6546e+09)
[2022-06-07 10:43:59,446][root][INFO] - Step 2168320 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2168320, mean_episode_return = 73.96, mean_episode_step = 2366.4, total_loss = -82.865, pg_loss = -94.485, baseline_loss = 17.461, entropy_loss = -5.8403, learner_queue_size = 32, train_seconds = 1212.4, _tick = 253, _time = 1.6546e+09)
[2022-06-07 10:44:04,450][root][INFO] - Step 2176000 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 2176000, mean_episode_return = 66.782, mean_episode_step = 2468.5, total_loss = 199.13, pg_loss = 74.681, baseline_loss = 130.27, entropy_loss = -5.8176, learner_queue_size = 32, train_seconds = 1217.4, _tick = 254, _time = 1.6546e+09)
[2022-06-07 10:44:09,454][root][INFO] - Step 2186240 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 2186240, mean_episode_return = 22.861, mean_episode_step = 2721.0, total_loss = 57.652, pg_loss = 16.186, baseline_loss = 46.858, entropy_loss = -5.3921, learner_queue_size = 32, train_seconds = 1222.4, _tick = 257, _time = 1.6546e+09)
[2022-06-07 10:44:14,460][root][INFO] - Step 2193920 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 2193920, mean_episode_return = None, mean_episode_step = 2591.1, total_loss = -27.669, pg_loss = -39.665, baseline_loss = 17.298, entropy_loss = -5.3011, learner_queue_size = 32, train_seconds = 1227.5, _tick = 258, _time = 1.6546e+09)
[2022-06-07 10:44:19,466][root][INFO] - Step 2204160 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2204160, mean_episode_return = 35.57, mean_episode_step = 1856.4, total_loss = -65.704, pg_loss = -73.252, baseline_loss = 12.799, entropy_loss = -5.2509, learner_queue_size = 32, train_seconds = 1232.5, _tick = 262, _time = 1.6546e+09)
[2022-06-07 10:44:24,470][root][INFO] - Step 2211840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2211840, mean_episode_return = None, mean_episode_step = 2153.8, total_loss = 282.98, pg_loss = 179.76, baseline_loss = 108.29, entropy_loss = -5.0737, learner_queue_size = 32, train_seconds = 1237.5, _tick = 263, _time = 1.6546e+09)
[2022-06-07 10:44:29,474][root][INFO] - Step 2222080 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2222080, mean_episode_return = None, mean_episode_step = 2380.7, total_loss = 121.43, pg_loss = 54.838, baseline_loss = 71.998, entropy_loss = -5.4022, learner_queue_size = 32, train_seconds = 1242.5, _tick = 264, _time = 1.6546e+09)
[2022-06-07 10:44:34,478][root][INFO] - Step 2229760 @ 1534.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2229760, mean_episode_return = None, mean_episode_step = 2142.1, total_loss = -40.533, pg_loss = -44.791, baseline_loss = 10.016, entropy_loss = -5.7581, learner_queue_size = 32, train_seconds = 1247.5, _tick = 265, _time = 1.6546e+09)
[2022-06-07 10:44:39,484][root][INFO] - Step 2240000 @ 2045.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 2240000, mean_episode_return = 100.09, mean_episode_step = 2794.9, total_loss = 43.13, pg_loss = 26.726, baseline_loss = 22.158, entropy_loss = -5.7541, learner_queue_size = 32, train_seconds = 1252.5, _tick = 266, _time = 1.6546e+09)
[2022-06-07 10:44:44,490][root][INFO] - Step 2247680 @ 1534.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 2247680, mean_episode_return = None, mean_episode_step = 2331.0, total_loss = 339.67, pg_loss = 208.43, baseline_loss = 137.02, entropy_loss = -5.7744, learner_queue_size = 32, train_seconds = 1257.5, _tick = 267, _time = 1.6546e+09)
[2022-06-07 10:44:49,502][root][INFO] - Step 2257920 @ 2043.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 2257920, mean_episode_return = 19.149, mean_episode_step = 2175.2, total_loss = 8.3666, pg_loss = -19.821, baseline_loss = 33.96, entropy_loss = -5.7724, learner_queue_size = 32, train_seconds = 1262.5, _tick = 270, _time = 1.6546e+09)
[2022-06-07 10:44:54,506][root][INFO] - Step 2268160 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 2268160, mean_episode_return = None, mean_episode_step = 3280.9, total_loss = -143.44, pg_loss = -138.85, baseline_loss = 1.0179, entropy_loss = -5.6074, learner_queue_size = 32, train_seconds = 1267.5, _tick = 272, _time = 1.6546e+09)
[2022-06-07 10:44:59,510][root][INFO] - Step 2275840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2275840, mean_episode_return = None, mean_episode_step = 2712.1, total_loss = 140.68, pg_loss = 92.648, baseline_loss = 53.823, entropy_loss = -5.7885, learner_queue_size = 32, train_seconds = 1272.5, _tick = 273, _time = 1.6546e+09)
[2022-06-07 10:45:04,514][root][INFO] - Step 2283520 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 2283520, mean_episode_return = 219.5, mean_episode_step = 2569.8, total_loss = 271.32, pg_loss = 169.52, baseline_loss = 106.92, entropy_loss = -5.1169, learner_queue_size = 32, train_seconds = 1277.5, _tick = 274, _time = 1.6546e+09)
[2022-06-07 10:45:09,518][root][INFO] - Step 2293760 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 2293760, mean_episode_return = -29.481, mean_episode_step = 2706.2, total_loss = -26.796, pg_loss = -70.614, baseline_loss = 49.247, entropy_loss = -5.4289, learner_queue_size = 32, train_seconds = 1282.5, _tick = 276, _time = 1.6546e+09)
[2022-06-07 10:45:14,524][root][INFO] - Step 2301440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2301440, mean_episode_return = None, mean_episode_step = 2358.3, total_loss = 139.47, pg_loss = 92.337, baseline_loss = 52.606, entropy_loss = -5.4727, learner_queue_size = 32, train_seconds = 1287.5, _tick = 276, _time = 1.6546e+09)
[2022-06-07 10:45:19,530][root][INFO] - Step 2311680 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 2311680, mean_episode_return = None, mean_episode_step = 2535.8, total_loss = -75.058, pg_loss = -71.504, baseline_loss = 1.8013, entropy_loss = -5.3553, learner_queue_size = 32, train_seconds = 1292.5, _tick = 278, _time = 1.6546e+09)
[2022-06-07 10:45:24,542][root][INFO] - Step 2319360 @ 1532.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 2319360, mean_episode_return = None, mean_episode_step = 3054.5, total_loss = -46.963, pg_loss = -43.136, baseline_loss = 1.0379, entropy_loss = -4.8646, learner_queue_size = 32, train_seconds = 1297.5, _tick = 279, _time = 1.6546e+09)
[2022-06-07 10:45:29,546][root][INFO] - Step 2329600 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2329600, mean_episode_return = None, mean_episode_step = 2866.5, total_loss = -4.4318, pg_loss = -19.237, baseline_loss = 20.194, entropy_loss = -5.3882, learner_queue_size = 32, train_seconds = 1302.5, _tick = 280, _time = 1.6546e+09)
[2022-06-07 10:45:34,550][root][INFO] - Step 2337280 @ 1534.8 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 2337280, mean_episode_return = None, mean_episode_step = 2343.0, total_loss = 55.121, pg_loss = 31.831, baseline_loss = 28.56, entropy_loss = -5.2702, learner_queue_size = 32, train_seconds = 1307.5, _tick = 281, _time = 1.6546e+09)
[2022-06-07 10:45:39,556][root][INFO] - Step 2347520 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 2347520, mean_episode_return = 90.708, mean_episode_step = 2697.7, total_loss = -46.404, pg_loss = -65.66, baseline_loss = 24.857, entropy_loss = -5.6004, learner_queue_size = 32, train_seconds = 1312.6, _tick = 282, _time = 1.6546e+09)
[2022-06-07 10:45:44,562][root][INFO] - Step 2355200 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 2355200, mean_episode_return = -23.221, mean_episode_step = 2665.8, total_loss = 110.09, pg_loss = 9.7404, baseline_loss = 105.61, entropy_loss = -5.2663, learner_queue_size = 32, train_seconds = 1317.6, _tick = 283, _time = 1.6546e+09)
[2022-06-07 10:45:49,566][root][INFO] - Step 2365440 @ 2046.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 2365440, mean_episode_return = None, mean_episode_step = 2581.4, total_loss = 482.13, pg_loss = 343.47, baseline_loss = 143.96, entropy_loss = -5.2973, learner_queue_size = 32, train_seconds = 1322.6, _tick = 285, _time = 1.6546e+09)
[2022-06-07 10:45:54,572][root][INFO] - Step 2373120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2373120, mean_episode_return = 90.07, mean_episode_step = 3084.0, total_loss = 74.8, pg_loss = 43.485, baseline_loss = 36.748, entropy_loss = -5.4323, learner_queue_size = 32, train_seconds = 1327.6, _tick = 286, _time = 1.6546e+09)
[2022-06-07 10:45:59,578][root][INFO] - Step 2383360 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2383360, mean_episode_return = None, mean_episode_step = 2759.7, total_loss = 225.45, pg_loss = 156.44, baseline_loss = 74.317, entropy_loss = -5.3135, learner_queue_size = 32, train_seconds = 1332.6, _tick = 289, _time = 1.6546e+09)
[2022-06-07 10:46:04,584][root][INFO] - Step 2391040 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 2391040, mean_episode_return = 128.63, mean_episode_step = 3003.0, total_loss = 91.914, pg_loss = 24.302, baseline_loss = 72.897, entropy_loss = -5.2852, learner_queue_size = 32, train_seconds = 1337.6, _tick = 291, _time = 1.6546e+09)
[2022-06-07 10:46:09,594][root][INFO] - Step 2401280 @ 2043.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 2401280, mean_episode_return = None, mean_episode_step = 2967.1, total_loss = 183.76, pg_loss = 129.35, baseline_loss = 60.041, entropy_loss = -5.6306, learner_queue_size = 32, train_seconds = 1342.6, _tick = 292, _time = 1.6546e+09)
[2022-06-07 10:46:14,598][root][INFO] - Step 2411520 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 2411520, mean_episode_return = 78.981, mean_episode_step = 2552.7, total_loss = -111.68, pg_loss = -111.71, baseline_loss = 5.4233, entropy_loss = -5.3952, learner_queue_size = 32, train_seconds = 1347.6, _tick = 295, _time = 1.6546e+09)
[2022-06-07 10:46:19,602][root][INFO] - Step 2419200 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 2419200, mean_episode_return = None, mean_episode_step = 2976.0, total_loss = -148.5, pg_loss = -144.44, baseline_loss = 1.5235, entropy_loss = -5.5868, learner_queue_size = 32, train_seconds = 1352.6, _tick = 296, _time = 1.6546e+09)
[2022-06-07 10:46:24,608][root][INFO] - Step 2426880 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 2426880, mean_episode_return = None, mean_episode_step = 2300.2, total_loss = 69.659, pg_loss = 45.586, baseline_loss = 29.681, entropy_loss = -5.6083, learner_queue_size = 32, train_seconds = 1357.6, _tick = 297, _time = 1.6546e+09)
[2022-06-07 10:46:29,614][root][INFO] - Step 2437120 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2437120, mean_episode_return = 74.376, mean_episode_step = 2807.9, total_loss = -140.36, pg_loss = -139.04, baseline_loss = 4.3722, entropy_loss = -5.6851, learner_queue_size = 32, train_seconds = 1362.6, _tick = 301, _time = 1.6546e+09)
[2022-06-07 10:46:34,618][root][INFO] - Step 2447360 @ 2046.2 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 2447360, mean_episode_return = 30.29, mean_episode_step = 2065.2, total_loss = -99.0, pg_loss = -97.168, baseline_loss = 3.7477, entropy_loss = -5.5795, learner_queue_size = 32, train_seconds = 1367.6, _tick = 303, _time = 1.6546e+09)
[2022-06-07 10:46:39,624][root][INFO] - Step 2455040 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 2455040, mean_episode_return = 63.609, mean_episode_step = 2695.5, total_loss = 41.245, pg_loss = -3.5724, baseline_loss = 50.668, entropy_loss = -5.8498, learner_queue_size = 32, train_seconds = 1372.6, _tick = 305, _time = 1.6546e+09)
[2022-06-07 10:46:44,626][root][INFO] - Step 2465280 @ 2047.2 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 2465280, mean_episode_return = None, mean_episode_step = 2191.6, total_loss = 65.185, pg_loss = 49.672, baseline_loss = 21.306, entropy_loss = -5.7935, learner_queue_size = 32, train_seconds = 1377.6, _tick = 307, _time = 1.6546e+09)
[2022-06-07 10:46:49,630][root][INFO] - Step 2472960 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2472960, mean_episode_return = 91.481, mean_episode_step = 2665.8, total_loss = -35.319, pg_loss = -91.44, baseline_loss = 61.745, entropy_loss = -5.6233, learner_queue_size = 32, train_seconds = 1382.6, _tick = 309, _time = 1.6546e+09)
[2022-06-07 10:46:54,636][root][INFO] - Step 2483200 @ 2045.5 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 2483200, mean_episode_return = None, mean_episode_step = 2532.4, total_loss = 37.843, pg_loss = 17.219, baseline_loss = 25.855, entropy_loss = -5.2306, learner_queue_size = 32, train_seconds = 1387.6, _tick = 312, _time = 1.6546e+09)
[2022-06-07 10:46:59,642][root][INFO] - Step 2490880 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2490880, mean_episode_return = 3.8693, mean_episode_step = 2141.8, total_loss = -31.509, pg_loss = -42.863, baseline_loss = 16.413, entropy_loss = -5.0599, learner_queue_size = 32, train_seconds = 1392.6, _tick = 314, _time = 1.6546e+09)
[2022-06-07 10:47:04,646][root][INFO] - Step 2501120 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2501120, mean_episode_return = 104.1, mean_episode_step = 2765.5, total_loss = 122.69, pg_loss = 30.475, baseline_loss = 97.296, entropy_loss = -5.0786, learner_queue_size = 32, train_seconds = 1397.6, _tick = 317, _time = 1.6546e+09)
[2022-06-07 10:47:09,650][root][INFO] - Step 2508800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2508800, mean_episode_return = 33.21, mean_episode_step = 2065.4, total_loss = 44.573, pg_loss = 17.978, baseline_loss = 31.721, entropy_loss = -5.1255, learner_queue_size = 32, train_seconds = 1402.6, _tick = 318, _time = 1.6546e+09)
[2022-06-07 10:47:14,654][root][INFO] - Step 2519040 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 2519040, mean_episode_return = None, mean_episode_step = 2406.2, total_loss = 35.156, pg_loss = -8.4191, baseline_loss = 48.905, entropy_loss = -5.3299, learner_queue_size = 32, train_seconds = 1407.6, _tick = 319, _time = 1.6546e+09)
[2022-06-07 10:47:19,660][root][INFO] - Step 2526720 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2526720, mean_episode_return = None, mean_episode_step = 2790.0, total_loss = -6.9409, pg_loss = -16.37, baseline_loss = 14.881, entropy_loss = -5.4522, learner_queue_size = 32, train_seconds = 1412.7, _tick = 319, _time = 1.6546e+09)
[2022-06-07 10:47:24,666][root][INFO] - Step 2536960 @ 2045.6 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 2536960, mean_episode_return = 35.446, mean_episode_step = 2603.8, total_loss = -34.329, pg_loss = -44.679, baseline_loss = 15.694, entropy_loss = -5.3431, learner_queue_size = 32, train_seconds = 1417.7, _tick = 321, _time = 1.6546e+09)
[2022-06-07 10:47:29,672][root][INFO] - Step 2544640 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2544640, mean_episode_return = 36.555, mean_episode_step = 2298.7, total_loss = 41.444, pg_loss = 16.541, baseline_loss = 30.126, entropy_loss = -5.2233, learner_queue_size = 32, train_seconds = 1422.7, _tick = 323, _time = 1.6546e+09)
[2022-06-07 10:47:34,678][root][INFO] - Step 2554880 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 2554880, mean_episode_return = None, mean_episode_step = 2541.6, total_loss = -76.98, pg_loss = -73.61, baseline_loss = 2.0297, entropy_loss = -5.3996, learner_queue_size = 32, train_seconds = 1427.7, _tick = 323, _time = 1.6546e+09)
[2022-06-07 10:47:39,684][root][INFO] - Step 2562560 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 2562560, mean_episode_return = None, mean_episode_step = 2121.3, total_loss = 105.13, pg_loss = 70.997, baseline_loss = 39.465, entropy_loss = -5.3274, learner_queue_size = 32, train_seconds = 1432.7, _tick = 324, _time = 1.6546e+09)
[2022-06-07 10:47:44,690][root][INFO] - Step 2572800 @ 2045.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 2572800, mean_episode_return = None, mean_episode_step = 2443.1, total_loss = 23.091, pg_loss = 0.13324, baseline_loss = 28.335, entropy_loss = -5.3771, learner_queue_size = 32, train_seconds = 1437.7, _tick = 325, _time = 1.6546e+09)
[2022-06-07 10:47:49,696][root][INFO] - Step 2580480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2580480, mean_episode_return = 28.5, mean_episode_step = 2485.5, total_loss = 24.274, pg_loss = -1.7692, baseline_loss = 31.362, entropy_loss = -5.3185, learner_queue_size = 32, train_seconds = 1442.7, _tick = 327, _time = 1.6546e+09)
[2022-06-07 10:47:54,702][root][INFO] - Step 2590720 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 2590720, mean_episode_return = None, mean_episode_step = 2116.4, total_loss = -82.812, pg_loss = -79.942, baseline_loss = 2.3992, entropy_loss = -5.269, learner_queue_size = 32, train_seconds = 1447.7, _tick = 329, _time = 1.6546e+09)
[2022-06-07 10:47:59,706][root][INFO] - Step 2598400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2598400, mean_episode_return = 72.26, mean_episode_step = 2486.4, total_loss = 169.85, pg_loss = 75.121, baseline_loss = 100.03, entropy_loss = -5.309, learner_queue_size = 32, train_seconds = 1452.7, _tick = 332, _time = 1.6546e+09)
[2022-06-07 10:48:04,712][root][INFO] - Step 2608640 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 2608640, mean_episode_return = None, mean_episode_step = 2532.7, total_loss = 188.4, pg_loss = 138.72, baseline_loss = 54.906, entropy_loss = -5.2234, learner_queue_size = 32, train_seconds = 1457.7, _tick = 333, _time = 1.6546e+09)
[2022-06-07 10:48:09,718][root][INFO] - Step 2616320 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2616320, mean_episode_return = None, mean_episode_step = 2908.6, total_loss = 856.57, pg_loss = 459.49, baseline_loss = 402.5, entropy_loss = -5.4169, learner_queue_size = 32, train_seconds = 1462.7, _tick = 335, _time = 1.6546e+09)
[2022-06-07 10:48:14,724][root][INFO] - Step 2626560 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2626560, mean_episode_return = 61.752, mean_episode_step = 2818.8, total_loss = 1.0649, pg_loss = -55.262, baseline_loss = 62.032, entropy_loss = -5.7047, learner_queue_size = 32, train_seconds = 1467.7, _tick = 338, _time = 1.6546e+09)
[2022-06-07 10:48:19,726][root][INFO] - Step 2636800 @ 2047.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2636800, mean_episode_return = None, mean_episode_step = 2558.9, total_loss = 140.2, pg_loss = 70.826, baseline_loss = 74.564, entropy_loss = -5.193, learner_queue_size = 32, train_seconds = 1472.7, _tick = 340, _time = 1.6546e+09)
[2022-06-07 10:48:24,730][root][INFO] - Step 2644480 @ 1534.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2644480, mean_episode_return = 54.691, mean_episode_step = 3102.5, total_loss = 6.0893, pg_loss = -6.7699, baseline_loss = 18.5, entropy_loss = -5.6413, learner_queue_size = 32, train_seconds = 1477.7, _tick = 342, _time = 1.6546e+09)
[2022-06-07 10:48:29,734][root][INFO] - Step 2654720 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 2654720, mean_episode_return = 30.08, mean_episode_step = 1806.2, total_loss = 143.99, pg_loss = 92.848, baseline_loss = 56.372, entropy_loss = -5.2352, learner_queue_size = 32, train_seconds = 1482.7, _tick = 345, _time = 1.6546e+09)
[2022-06-07 10:48:34,740][root][INFO] - Step 2662400 @ 1534.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 2662400, mean_episode_return = 63.871, mean_episode_step = 2109.9, total_loss = -16.686, pg_loss = -33.366, baseline_loss = 21.715, entropy_loss = -5.0356, learner_queue_size = 32, train_seconds = 1487.7, _tick = 346, _time = 1.6546e+09)
[2022-06-07 10:48:39,747][root][INFO] - Step 2672640 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 2672640, mean_episode_return = 95.269, mean_episode_step = 2083.1, total_loss = 255.96, pg_loss = 134.54, baseline_loss = 126.49, entropy_loss = -5.076, learner_queue_size = 32, train_seconds = 1492.7, _tick = 349, _time = 1.6546e+09)
[2022-06-07 10:48:44,753][root][INFO] - Step 2680320 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2680320, mean_episode_return = 0.98969, mean_episode_step = 2749.9, total_loss = -3.6492, pg_loss = -27.974, baseline_loss = 29.583, entropy_loss = -5.2591, learner_queue_size = 32, train_seconds = 1497.7, _tick = 351, _time = 1.6546e+09)
[2022-06-07 10:48:49,758][root][INFO] - Step 2690560 @ 2045.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2690560, mean_episode_return = -11.411, mean_episode_step = 2220.3, total_loss = -59.57, pg_loss = -70.727, baseline_loss = 15.975, entropy_loss = -4.8188, learner_queue_size = 32, train_seconds = 1502.8, _tick = 354, _time = 1.6546e+09)
[2022-06-07 10:48:54,759][root][INFO] - Step 2698240 @ 1535.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2698240, mean_episode_return = 45.913, mean_episode_step = 1676.2, total_loss = 14.923, pg_loss = -3.7491, baseline_loss = 23.297, entropy_loss = -4.6244, learner_queue_size = 32, train_seconds = 1507.8, _tick = 355, _time = 1.6546e+09)
[2022-06-07 10:48:59,762][root][INFO] - Step 2708480 @ 2046.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2708480, mean_episode_return = 49.002, mean_episode_step = 2851.3, total_loss = -76.642, pg_loss = -75.96, baseline_loss = 3.6584, entropy_loss = -4.3396, learner_queue_size = 32, train_seconds = 1512.8, _tick = 359, _time = 1.6546e+09)
[2022-06-07 10:49:04,765][root][INFO] - Step 2716160 @ 1535.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2716160, mean_episode_return = None, mean_episode_step = 2305.7, total_loss = 54.913, pg_loss = 33.418, baseline_loss = 25.811, entropy_loss = -4.3162, learner_queue_size = 32, train_seconds = 1517.8, _tick = 361, _time = 1.6546e+09)
[2022-06-07 10:49:09,770][root][INFO] - Step 2726400 @ 2045.9 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 2726400, mean_episode_return = None, mean_episode_step = 3035.0, total_loss = -112.7, pg_loss = -115.48, baseline_loss = 7.15, entropy_loss = -4.3712, learner_queue_size = 32, train_seconds = 1522.8, _tick = 363, _time = 1.6546e+09)
[2022-06-07 10:49:14,774][root][INFO] - Step 2734080 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 2734080, mean_episode_return = -13.93, mean_episode_step = 2840.6, total_loss = -97.69, pg_loss = -151.11, baseline_loss = 57.962, entropy_loss = -4.5445, learner_queue_size = 32, train_seconds = 1527.8, _tick = 364, _time = 1.6546e+09)
[2022-06-07 10:49:19,778][root][INFO] - Step 2744320 @ 2046.3 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 2744320, mean_episode_return = None, mean_episode_step = 2393.1, total_loss = -26.665, pg_loss = -49.974, baseline_loss = 27.91, entropy_loss = -4.6005, learner_queue_size = 32, train_seconds = 1532.8, _tick = 364, _time = 1.6546e+09)
[2022-06-07 10:49:24,782][root][INFO] - Step 2752000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2752000, mean_episode_return = None, mean_episode_step = 2226.9, total_loss = 25.276, pg_loss = 2.9919, baseline_loss = 27.488, entropy_loss = -5.2046, learner_queue_size = 32, train_seconds = 1537.8, _tick = 365, _time = 1.6546e+09)
[2022-06-07 10:49:29,786][root][INFO] - Step 2762240 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 2762240, mean_episode_return = None, mean_episode_step = 2481.1, total_loss = -8.4648, pg_loss = -14.564, baseline_loss = 10.927, entropy_loss = -4.828, learner_queue_size = 32, train_seconds = 1542.8, _tick = 367, _time = 1.6546e+09)
[2022-06-07 10:49:34,792][root][INFO] - Step 2769920 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 2769920, mean_episode_return = None, mean_episode_step = 1960.4, total_loss = 42.08, pg_loss = 17.344, baseline_loss = 29.467, entropy_loss = -4.7309, learner_queue_size = 32, train_seconds = 1547.8, _tick = 367, _time = 1.6546e+09)
[2022-06-07 10:49:39,798][root][INFO] - Step 2780160 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 2780160, mean_episode_return = None, mean_episode_step = 2797.6, total_loss = -69.671, pg_loss = -66.15, baseline_loss = 0.83918, entropy_loss = -4.36, learner_queue_size = 32, train_seconds = 1552.8, _tick = 368, _time = 1.6546e+09)
[2022-06-07 10:49:44,802][root][INFO] - Step 2787840 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 2787840, mean_episode_return = None, mean_episode_step = 2394.3, total_loss = -24.63, pg_loss = -35.12, baseline_loss = 15.363, entropy_loss = -4.8732, learner_queue_size = 32, train_seconds = 1557.8, _tick = 369, _time = 1.6546e+09)
[2022-06-07 10:49:49,806][root][INFO] - Step 2798080 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 2798080, mean_episode_return = None, mean_episode_step = 2025.2, total_loss = 19.294, pg_loss = 11.729, baseline_loss = 12.64, entropy_loss = -5.0755, learner_queue_size = 32, train_seconds = 1562.8, _tick = 371, _time = 1.6546e+09)
[2022-06-07 10:49:54,810][root][INFO] - Step 2805760 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 2805760, mean_episode_return = -6.29, mean_episode_step = 2965.2, total_loss = -34.375, pg_loss = -42.402, baseline_loss = 13.133, entropy_loss = -5.1055, learner_queue_size = 32, train_seconds = 1567.8, _tick = 373, _time = 1.6546e+09)
[2022-06-07 10:49:59,814][root][INFO] - Step 2816000 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 2816000, mean_episode_return = None, mean_episode_step = 2215.8, total_loss = 146.76, pg_loss = 95.585, baseline_loss = 56.055, entropy_loss = -4.881, learner_queue_size = 32, train_seconds = 1572.8, _tick = 375, _time = 1.6546e+09)
[2022-06-07 10:50:04,818][root][INFO] - Step 2823680 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 2823680, mean_episode_return = 103.0, mean_episode_step = 2907.0, total_loss = 21.48, pg_loss = -16.457, baseline_loss = 43.127, entropy_loss = -5.1904, learner_queue_size = 32, train_seconds = 1577.8, _tick = 377, _time = 1.6546e+09)
[2022-06-07 10:50:09,822][root][INFO] - Step 2833920 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 2833920, mean_episode_return = None, mean_episode_step = 1915.3, total_loss = -86.525, pg_loss = -86.49, baseline_loss = 5.4082, entropy_loss = -5.4432, learner_queue_size = 32, train_seconds = 1582.8, _tick = 377, _time = 1.6546e+09)
[2022-06-07 10:50:14,826][root][INFO] - Step 2841600 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2841600, mean_episode_return = 108.12, mean_episode_step = 2794.8, total_loss = -50.85, pg_loss = -118.55, baseline_loss = 73.138, entropy_loss = -5.4394, learner_queue_size = 32, train_seconds = 1587.8, _tick = 380, _time = 1.6546e+09)
[2022-06-07 10:50:19,830][root][INFO] - Step 2851840 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 2851840, mean_episode_return = None, mean_episode_step = 2403.8, total_loss = 166.89, pg_loss = 94.508, baseline_loss = 78.126, entropy_loss = -5.7466, learner_queue_size = 32, train_seconds = 1592.8, _tick = 382, _time = 1.6546e+09)
[2022-06-07 10:50:24,834][root][INFO] - Step 2859520 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 2859520, mean_episode_return = 137.1, mean_episode_step = 2388.1, total_loss = 145.55, pg_loss = 94.432, baseline_loss = 56.648, entropy_loss = -5.5316, learner_queue_size = 32, train_seconds = 1597.8, _tick = 385, _time = 1.6546e+09)
[2022-06-07 10:50:29,840][root][INFO] - Step 2867200 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2867200, mean_episode_return = None, mean_episode_step = 3010.3, total_loss = 33.244, pg_loss = 12.203, baseline_loss = 26.261, entropy_loss = -5.2195, learner_queue_size = 32, train_seconds = 1602.8, _tick = 387, _time = 1.6546e+09)
[2022-06-07 10:50:34,842][root][INFO] - Step 2877440 @ 2047.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 2877440, mean_episode_return = 30.112, mean_episode_step = 2343.5, total_loss = 139.68, pg_loss = 94.162, baseline_loss = 51.075, entropy_loss = -5.5621, learner_queue_size = 32, train_seconds = 1607.8, _tick = 389, _time = 1.6546e+09)
[2022-06-07 10:50:39,846][root][INFO] - Step 2885120 @ 1534.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 2885120, mean_episode_return = 72.061, mean_episode_step = 3088.7, total_loss = -133.16, pg_loss = -153.81, baseline_loss = 26.149, entropy_loss = -5.4984, learner_queue_size = 32, train_seconds = 1612.8, _tick = 391, _time = 1.6546e+09)
[2022-06-07 10:50:44,850][root][INFO] - Step 2895360 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 2895360, mean_episode_return = -7.3501, mean_episode_step = 2666.9, total_loss = 70.919, pg_loss = 14.293, baseline_loss = 62.057, entropy_loss = -5.4314, learner_queue_size = 32, train_seconds = 1617.8, _tick = 394, _time = 1.6546e+09)
[2022-06-07 10:50:49,856][root][INFO] - Step 2905600 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 2905600, mean_episode_return = -16.991, mean_episode_step = 2268.2, total_loss = 199.25, pg_loss = 117.12, baseline_loss = 87.607, entropy_loss = -5.4802, learner_queue_size = 32, train_seconds = 1622.9, _tick = 398, _time = 1.6546e+09)
[2022-06-07 10:50:54,862][root][INFO] - Step 2913280 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2913280, mean_episode_return = None, mean_episode_step = 2137.1, total_loss = 41.382, pg_loss = 0.40653, baseline_loss = 46.533, entropy_loss = -5.5582, learner_queue_size = 32, train_seconds = 1627.9, _tick = 400, _time = 1.6546e+09)
[2022-06-07 10:50:59,868][root][INFO] - Step 2923520 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 2923520, mean_episode_return = None, mean_episode_step = 2345.7, total_loss = 243.94, pg_loss = 175.35, baseline_loss = 73.773, entropy_loss = -5.1849, learner_queue_size = 32, train_seconds = 1632.9, _tick = 402, _time = 1.6546e+09)
[2022-06-07 10:51:04,874][root][INFO] - Step 2931200 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 2931200, mean_episode_return = None, mean_episode_step = 2847.6, total_loss = 34.593, pg_loss = 6.0469, baseline_loss = 33.781, entropy_loss = -5.2354, learner_queue_size = 32, train_seconds = 1637.9, _tick = 402, _time = 1.6546e+09)
[2022-06-07 10:51:09,879][root][INFO] - Step 2941440 @ 2046.1 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 2941440, mean_episode_return = None, mean_episode_step = 2422.4, total_loss = 98.057, pg_loss = 64.64, baseline_loss = 38.499, entropy_loss = -5.0815, learner_queue_size = 32, train_seconds = 1642.9, _tick = 402, _time = 1.6546e+09)
[2022-06-07 10:51:14,885][root][INFO] - Step 2949120 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2949120, mean_episode_return = None, mean_episode_step = 2191.2, total_loss = -29.124, pg_loss = -50.193, baseline_loss = 26.021, entropy_loss = -4.9519, learner_queue_size = 32, train_seconds = 1647.9, _tick = 403, _time = 1.6546e+09)
[2022-06-07 10:51:19,891][root][INFO] - Step 2959360 @ 2045.5 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 2959360, mean_episode_return = None, mean_episode_step = 2964.3, total_loss = -161.26, pg_loss = -158.02, baseline_loss = 1.8218, entropy_loss = -5.0628, learner_queue_size = 32, train_seconds = 1652.9, _tick = 404, _time = 1.6546e+09)
[2022-06-07 10:51:24,897][root][INFO] - Step 2967040 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 2967040, mean_episode_return = 45.575, mean_episode_step = 2712.7, total_loss = 47.386, pg_loss = -11.144, baseline_loss = 63.733, entropy_loss = -5.2028, learner_queue_size = 32, train_seconds = 1657.9, _tick = 407, _time = 1.6546e+09)
[2022-06-07 10:51:29,902][root][INFO] - Step 2977280 @ 2046.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 2977280, mean_episode_return = 98.182, mean_episode_step = 2672.6, total_loss = 93.172, pg_loss = 39.886, baseline_loss = 58.578, entropy_loss = -5.2929, learner_queue_size = 32, train_seconds = 1662.9, _tick = 408, _time = 1.6546e+09)
[2022-06-07 10:51:34,908][root][INFO] - Step 2984960 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 2984960, mean_episode_return = 43.88, mean_episode_step = 3034.3, total_loss = 118.87, pg_loss = 59.17, baseline_loss = 65.03, entropy_loss = -5.3254, learner_queue_size = 32, train_seconds = 1667.9, _tick = 411, _time = 1.6546e+09)
[2022-06-07 10:51:39,911][root][INFO] - Step 2995200 @ 2046.7 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 2995200, mean_episode_return = 26.62, mean_episode_step = 1999.1, total_loss = 33.778, pg_loss = 14.745, baseline_loss = 24.543, entropy_loss = -5.5097, learner_queue_size = 32, train_seconds = 1672.9, _tick = 412, _time = 1.6546e+09)
[2022-06-07 10:51:44,914][root][INFO] - Step 3002880 @ 1535.2 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 3002880, mean_episode_return = None, mean_episode_step = 2628.8, total_loss = 70.161, pg_loss = 57.144, baseline_loss = 18.179, entropy_loss = -5.1622, learner_queue_size = 32, train_seconds = 1677.9, _tick = 413, _time = 1.6546e+09)
[2022-06-07 10:51:49,920][root][INFO] - Step 3013120 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 3013120, mean_episode_return = 50.074, mean_episode_step = 2773.4, total_loss = 39.465, pg_loss = 5.085, baseline_loss = 39.661, entropy_loss = -5.2811, learner_queue_size = 32, train_seconds = 1682.9, _tick = 416, _time = 1.6546e+09)
[2022-06-07 10:51:54,926][root][INFO] - Step 3020800 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 3020800, mean_episode_return = 111.64, mean_episode_step = 2523.6, total_loss = 57.935, pg_loss = 28.444, baseline_loss = 35.181, entropy_loss = -5.6903, learner_queue_size = 32, train_seconds = 1687.9, _tick = 418, _time = 1.6546e+09)
[2022-06-07 10:51:59,930][root][INFO] - Step 3031040 @ 2046.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 3031040, mean_episode_return = 130.99, mean_episode_step = 2907.8, total_loss = 114.46, pg_loss = 70.992, baseline_loss = 48.973, entropy_loss = -5.5052, learner_queue_size = 32, train_seconds = 1692.9, _tick = 422, _time = 1.6546e+09)
[2022-06-07 10:52:04,936][root][INFO] - Step 3038720 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 3038720, mean_episode_return = 61.562, mean_episode_step = 2396.7, total_loss = 187.24, pg_loss = 123.41, baseline_loss = 69.305, entropy_loss = -5.4817, learner_queue_size = 32, train_seconds = 1697.9, _tick = 424, _time = 1.6546e+09)
[2022-06-07 10:52:09,942][root][INFO] - Step 3048960 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 3048960, mean_episode_return = None, mean_episode_step = 3090.0, total_loss = 36.035, pg_loss = 8.7125, baseline_loss = 32.87, entropy_loss = -5.5478, learner_queue_size = 32, train_seconds = 1702.9, _tick = 426, _time = 1.6546e+09)
[2022-06-07 10:52:14,946][root][INFO] - Step 3056640 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 3056640, mean_episode_return = 137.29, mean_episode_step = 2827.8, total_loss = 197.79, pg_loss = 115.49, baseline_loss = 87.992, entropy_loss = -5.7008, learner_queue_size = 32, train_seconds = 1707.9, _tick = 427, _time = 1.6546e+09)
[2022-06-07 10:52:19,950][root][INFO] - Step 3066880 @ 2046.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 3066880, mean_episode_return = None, mean_episode_step = 2122.5, total_loss = 57.083, pg_loss = 18.53, baseline_loss = 44.121, entropy_loss = -5.5684, learner_queue_size = 32, train_seconds = 1712.9, _tick = 430, _time = 1.6546e+09)
[2022-06-07 10:52:24,956][root][INFO] - Step 3074560 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3074560, mean_episode_return = None, mean_episode_step = 1837.5, total_loss = 89.024, pg_loss = 59.063, baseline_loss = 35.218, entropy_loss = -5.2577, learner_queue_size = 32, train_seconds = 1718.0, _tick = 430, _time = 1.6546e+09)
[2022-06-07 10:52:29,958][root][INFO] - Step 3084800 @ 2047.2 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 3084800, mean_episode_return = 89.449, mean_episode_step = 2236.7, total_loss = -69.349, pg_loss = -72.176, baseline_loss = 7.966, entropy_loss = -5.1386, learner_queue_size = 32, train_seconds = 1723.0, _tick = 434, _time = 1.6546e+09)
[2022-06-07 10:52:34,962][root][INFO] - Step 3092480 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3092480, mean_episode_return = 51.612, mean_episode_step = 2595.6, total_loss = -34.203, pg_loss = -54.591, baseline_loss = 25.609, entropy_loss = -5.2208, learner_queue_size = 32, train_seconds = 1728.0, _tick = 437, _time = 1.6546e+09)
[2022-06-07 10:52:39,968][root][INFO] - Step 3102720 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 3102720, mean_episode_return = -6.4296, mean_episode_step = 2667.8, total_loss = 59.412, pg_loss = 12.315, baseline_loss = 52.117, entropy_loss = -5.0205, learner_queue_size = 32, train_seconds = 1733.0, _tick = 440, _time = 1.6546e+09)
[2022-06-07 10:52:44,974][root][INFO] - Step 3110400 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 3110400, mean_episode_return = None, mean_episode_step = 1904.2, total_loss = 10.73, pg_loss = -21.367, baseline_loss = 37.222, entropy_loss = -5.1244, learner_queue_size = 32, train_seconds = 1738.0, _tick = 440, _time = 1.6546e+09)
[2022-06-07 10:52:49,978][root][INFO] - Step 3120640 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3120640, mean_episode_return = 151.59, mean_episode_step = 2527.7, total_loss = -41.876, pg_loss = -53.027, baseline_loss = 15.848, entropy_loss = -4.6968, learner_queue_size = 32, train_seconds = 1743.0, _tick = 443, _time = 1.6546e+09)
[2022-06-07 10:52:54,982][root][INFO] - Step 3130880 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 3130880, mean_episode_return = 131.63, mean_episode_step = 2327.6, total_loss = -111.73, pg_loss = -109.89, baseline_loss = 2.967, entropy_loss = -4.8083, learner_queue_size = 32, train_seconds = 1748.0, _tick = 444, _time = 1.6546e+09)
[2022-06-07 10:52:59,988][root][INFO] - Step 3138560 @ 1534.1 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 3138560, mean_episode_return = 35.151, mean_episode_step = 2151.8, total_loss = -68.234, pg_loss = -84.225, baseline_loss = 20.701, entropy_loss = -4.7105, learner_queue_size = 32, train_seconds = 1753.0, _tick = 445, _time = 1.6546e+09)
[2022-06-07 10:53:04,994][root][INFO] - Step 3146240 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 3146240, mean_episode_return = -14.121, mean_episode_step = 2399.0, total_loss = -16.649, pg_loss = -17.504, baseline_loss = 4.9927, entropy_loss = -4.1382, learner_queue_size = 32, train_seconds = 1758.0, _tick = 448, _time = 1.6546e+09)
[2022-06-07 10:53:09,998][root][INFO] - Step 3156480 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 3156480, mean_episode_return = None, mean_episode_step = 3004.9, total_loss = 48.221, pg_loss = 21.579, baseline_loss = 30.752, entropy_loss = -4.1104, learner_queue_size = 32, train_seconds = 1763.0, _tick = 451, _time = 1.6546e+09)
[2022-06-07 10:53:15,000][root][INFO] - Step 3166720 @ 2047.1 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 3166720, mean_episode_return = 54.322, mean_episode_step = 2667.6, total_loss = 229.21, pg_loss = 153.92, baseline_loss = 79.483, entropy_loss = -4.1913, learner_queue_size = 32, train_seconds = 1768.0, _tick = 454, _time = 1.6546e+09)
[2022-06-07 10:53:20,001][root][INFO] - Step 3174400 @ 1535.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3174400, mean_episode_return = None, mean_episode_step = 2316.5, total_loss = -24.094, pg_loss = -30.462, baseline_loss = 10.934, entropy_loss = -4.5665, learner_queue_size = 32, train_seconds = 1773.0, _tick = 455, _time = 1.6546e+09)
[2022-06-07 10:53:25,007][root][INFO] - Step 3182080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3182080, mean_episode_return = 79.726, mean_episode_step = 3194.1, total_loss = -51.917, pg_loss = -103.84, baseline_loss = 56.487, entropy_loss = -4.5665, learner_queue_size = 32, train_seconds = 1778.0, _tick = 456, _time = 1.6546e+09)
[2022-06-07 10:53:30,010][root][INFO] - Step 3192320 @ 2046.9 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3192320, mean_episode_return = None, mean_episode_step = 2198.0, total_loss = -99.612, pg_loss = -95.472, baseline_loss = 0.72937, entropy_loss = -4.8693, learner_queue_size = 32, train_seconds = 1783.0, _tick = 457, _time = 1.6546e+09)
[2022-06-07 10:53:35,014][root][INFO] - Step 3200000 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 3200000, mean_episode_return = None, mean_episode_step = 2057.9, total_loss = -50.844, pg_loss = -53.17, baseline_loss = 7.48, entropy_loss = -5.1539, learner_queue_size = 32, train_seconds = 1788.0, _tick = 457, _time = 1.6546e+09)
[2022-06-07 10:53:40,023][root][INFO] - Step 3210240 @ 2044.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 3210240, mean_episode_return = None, mean_episode_step = 3129.9, total_loss = 10.657, pg_loss = 4.1791, baseline_loss = 11.408, entropy_loss = -4.9305, learner_queue_size = 32, train_seconds = 1793.0, _tick = 457, _time = 1.6546e+09)
[2022-06-07 10:53:45,029][root][INFO] - Step 3217920 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3217920, mean_episode_return = 40.41, mean_episode_step = 1800.9, total_loss = 7.4051, pg_loss = -21.44, baseline_loss = 33.418, entropy_loss = -4.5736, learner_queue_size = 32, train_seconds = 1798.0, _tick = 459, _time = 1.6546e+09)
[2022-06-07 10:53:50,034][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 10:53:50,224][root][INFO] - Step 3228160 @ 2045.9 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3228160, mean_episode_return = 121.38, mean_episode_step = 1853.8, total_loss = -12.179, pg_loss = -14.171, baseline_loss = 6.1697, entropy_loss = -4.1784, learner_queue_size = 32, train_seconds = 1803.0, _tick = 463, _time = 1.6546e+09)
[2022-06-07 10:53:55,230][root][INFO] - Step 3238400 @ 1970.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3238400, mean_episode_return = 18.21, mean_episode_step = 2303.2, total_loss = 23.767, pg_loss = 11.137, baseline_loss = 17.636, entropy_loss = -5.0066, learner_queue_size = 32, train_seconds = 1808.2, _tick = 465, _time = 1.6546e+09)
[2022-06-07 10:54:00,234][root][INFO] - Step 3246080 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 3246080, mean_episode_return = -70.297, mean_episode_step = 2514.0, total_loss = -58.799, pg_loss = -55.406, baseline_loss = 1.697, entropy_loss = -5.0899, learner_queue_size = 32, train_seconds = 1813.2, _tick = 467, _time = 1.6546e+09)
[2022-06-07 10:54:05,238][root][INFO] - Step 3256320 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3256320, mean_episode_return = 12.43, mean_episode_step = 2299.0, total_loss = 105.91, pg_loss = 23.728, baseline_loss = 87.317, entropy_loss = -5.1383, learner_queue_size = 32, train_seconds = 1818.2, _tick = 468, _time = 1.6546e+09)
[2022-06-07 10:54:10,242][root][INFO] - Step 3264000 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3264000, mean_episode_return = None, mean_episode_step = 2710.6, total_loss = 106.97, pg_loss = 79.518, baseline_loss = 32.508, entropy_loss = -5.0532, learner_queue_size = 32, train_seconds = 1823.2, _tick = 469, _time = 1.6546e+09)
[2022-06-07 10:54:15,246][root][INFO] - Step 3274240 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3274240, mean_episode_return = None, mean_episode_step = 2279.8, total_loss = -87.654, pg_loss = -84.374, baseline_loss = 1.8097, entropy_loss = -5.0897, learner_queue_size = 32, train_seconds = 1828.2, _tick = 470, _time = 1.6546e+09)
[2022-06-07 10:54:20,250][root][INFO] - Step 3281920 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3281920, mean_episode_return = None, mean_episode_step = 2290.0, total_loss = 94.181, pg_loss = 52.9, baseline_loss = 46.196, entropy_loss = -4.9147, learner_queue_size = 32, train_seconds = 1833.2, _tick = 472, _time = 1.6546e+09)
[2022-06-07 10:54:25,254][root][INFO] - Step 3292160 @ 2046.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3292160, mean_episode_return = None, mean_episode_step = 2866.5, total_loss = 180.97, pg_loss = 122.71, baseline_loss = 63.748, entropy_loss = -5.4883, learner_queue_size = 32, train_seconds = 1838.2, _tick = 473, _time = 1.6546e+09)
[2022-06-07 10:54:30,258][root][INFO] - Step 3299840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 3299840, mean_episode_return = 119.51, mean_episode_step = 2653.7, total_loss = -67.729, pg_loss = -71.97, baseline_loss = 9.6535, entropy_loss = -5.413, learner_queue_size = 32, train_seconds = 1843.3, _tick = 475, _time = 1.6546e+09)
[2022-06-07 10:54:35,264][root][INFO] - Step 3310080 @ 2045.6 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 3310080, mean_episode_return = 97.979, mean_episode_step = 2422.6, total_loss = 64.862, pg_loss = 31.705, baseline_loss = 38.561, entropy_loss = -5.404, learner_queue_size = 32, train_seconds = 1848.3, _tick = 476, _time = 1.6546e+09)
[2022-06-07 10:54:40,270][root][INFO] - Step 3317760 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3317760, mean_episode_return = None, mean_episode_step = 2762.2, total_loss = 147.81, pg_loss = 101.89, baseline_loss = 51.326, entropy_loss = -5.4101, learner_queue_size = 32, train_seconds = 1853.3, _tick = 477, _time = 1.6546e+09)
[2022-06-07 10:54:45,276][root][INFO] - Step 3328000 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 3328000, mean_episode_return = None, mean_episode_step = 2426.1, total_loss = 78.037, pg_loss = 47.699, baseline_loss = 35.998, entropy_loss = -5.66, learner_queue_size = 32, train_seconds = 1858.3, _tick = 478, _time = 1.6546e+09)
[2022-06-07 10:54:50,282][root][INFO] - Step 3335680 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3335680, mean_episode_return = 34.725, mean_episode_step = 2250.7, total_loss = 1598.8, pg_loss = 742.18, baseline_loss = 862.15, entropy_loss = -5.4834, learner_queue_size = 32, train_seconds = 1863.3, _tick = 481, _time = 1.6546e+09)
[2022-06-07 10:54:55,286][root][INFO] - Step 3345920 @ 2046.4 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 3345920, mean_episode_return = None, mean_episode_step = 2163.8, total_loss = -82.104, pg_loss = -79.822, baseline_loss = 3.3098, entropy_loss = -5.5925, learner_queue_size = 32, train_seconds = 1868.3, _tick = 482, _time = 1.6546e+09)
[2022-06-07 10:55:00,290][root][INFO] - Step 3353600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3353600, mean_episode_return = None, mean_episode_step = 2438.6, total_loss = 165.23, pg_loss = 115.08, baseline_loss = 55.908, entropy_loss = -5.7638, learner_queue_size = 32, train_seconds = 1873.3, _tick = 483, _time = 1.6546e+09)
[2022-06-07 10:55:05,294][root][INFO] - Step 3363840 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 3363840, mean_episode_return = 40.784, mean_episode_step = 3092.6, total_loss = 250.49, pg_loss = 174.82, baseline_loss = 81.134, entropy_loss = -5.4643, learner_queue_size = 32, train_seconds = 1878.3, _tick = 485, _time = 1.6546e+09)
[2022-06-07 10:55:10,300][root][INFO] - Step 3371520 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3371520, mean_episode_return = None, mean_episode_step = 2736.4, total_loss = -53.094, pg_loss = -56.503, baseline_loss = 8.9659, entropy_loss = -5.5568, learner_queue_size = 32, train_seconds = 1883.3, _tick = 486, _time = 1.6546e+09)
[2022-06-07 10:55:15,306][root][INFO] - Step 3381760 @ 2045.6 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 3381760, mean_episode_return = 120.73, mean_episode_step = 2269.5, total_loss = -68.963, pg_loss = -75.476, baseline_loss = 11.92, entropy_loss = -5.4061, learner_queue_size = 32, train_seconds = 1888.3, _tick = 489, _time = 1.6546e+09)
[2022-06-07 10:55:20,310][root][INFO] - Step 3389440 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 3389440, mean_episode_return = 20.277, mean_episode_step = 2485.0, total_loss = 39.251, pg_loss = 19.546, baseline_loss = 24.927, entropy_loss = -5.2217, learner_queue_size = 32, train_seconds = 1893.3, _tick = 491, _time = 1.6546e+09)
[2022-06-07 10:55:25,314][root][INFO] - Step 3399680 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 3399680, mean_episode_return = 15.75, mean_episode_step = 1991.6, total_loss = -4.5867, pg_loss = -26.867, baseline_loss = 27.597, entropy_loss = -5.3162, learner_queue_size = 32, train_seconds = 1898.3, _tick = 494, _time = 1.6546e+09)
[2022-06-07 10:55:30,318][root][INFO] - Step 3407360 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3407360, mean_episode_return = 70.1, mean_episode_step = 2209.6, total_loss = 32.313, pg_loss = 12.478, baseline_loss = 24.902, entropy_loss = -5.0665, learner_queue_size = 32, train_seconds = 1903.3, _tick = 496, _time = 1.6546e+09)
[2022-06-07 10:55:35,322][root][INFO] - Step 3417600 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 3417600, mean_episode_return = None, mean_episode_step = 2490.2, total_loss = 16.676, pg_loss = 8.0744, baseline_loss = 13.672, entropy_loss = -5.071, learner_queue_size = 32, train_seconds = 1908.3, _tick = 498, _time = 1.6546e+09)
[2022-06-07 10:55:40,326][root][INFO] - Step 3425280 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 3425280, mean_episode_return = 16.729, mean_episode_step = 2917.7, total_loss = -10.546, pg_loss = -19.662, baseline_loss = 14.417, entropy_loss = -5.3017, learner_queue_size = 32, train_seconds = 1913.3, _tick = 499, _time = 1.6546e+09)
[2022-06-07 10:55:45,330][root][INFO] - Step 3435520 @ 2046.4 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 3435520, mean_episode_return = 18.2, mean_episode_step = 2371.0, total_loss = 9.4025, pg_loss = -17.345, baseline_loss = 32.179, entropy_loss = -5.4315, learner_queue_size = 32, train_seconds = 1918.3, _tick = 500, _time = 1.6546e+09)
[2022-06-07 10:55:50,336][root][INFO] - Step 3443200 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3443200, mean_episode_return = None, mean_episode_step = 2966.2, total_loss = 347.59, pg_loss = 265.44, baseline_loss = 87.384, entropy_loss = -5.2357, learner_queue_size = 32, train_seconds = 1923.3, _tick = 502, _time = 1.6546e+09)
[2022-06-07 10:55:55,342][root][INFO] - Step 3453440 @ 2045.6 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 3453440, mean_episode_return = 26.9, mean_episode_step = 2357.9, total_loss = 101.17, pg_loss = 52.66, baseline_loss = 53.817, entropy_loss = -5.3117, learner_queue_size = 32, train_seconds = 1928.3, _tick = 503, _time = 1.6546e+09)
[2022-06-07 10:56:00,346][root][INFO] - Step 3461120 @ 1534.8 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 3461120, mean_episode_return = 13.54, mean_episode_step = 3511.7, total_loss = 287.22, pg_loss = 211.43, baseline_loss = 81.171, entropy_loss = -5.3802, learner_queue_size = 32, train_seconds = 1933.3, _tick = 504, _time = 1.6546e+09)
[2022-06-07 10:56:05,350][root][INFO] - Step 3468800 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 3468800, mean_episode_return = None, mean_episode_step = 2252.1, total_loss = -72.327, pg_loss = -75.943, baseline_loss = 9.0168, entropy_loss = -5.4014, learner_queue_size = 32, train_seconds = 1938.3, _tick = 505, _time = 1.6546e+09)
[2022-06-07 10:56:10,354][root][INFO] - Step 3479040 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3479040, mean_episode_return = None, mean_episode_step = 2572.1, total_loss = -72.879, pg_loss = -72.534, baseline_loss = 5.1863, entropy_loss = -5.5319, learner_queue_size = 32, train_seconds = 1943.3, _tick = 507, _time = 1.6546e+09)
[2022-06-07 10:56:15,358][root][INFO] - Step 3489280 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3489280, mean_episode_return = 33.78, mean_episode_step = 3375.2, total_loss = -106.33, pg_loss = -127.27, baseline_loss = 26.525, entropy_loss = -5.5842, learner_queue_size = 32, train_seconds = 1948.4, _tick = 509, _time = 1.6546e+09)
[2022-06-07 10:56:20,362][root][INFO] - Step 3496960 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 3496960, mean_episode_return = -44.319, mean_episode_step = 2170.2, total_loss = 2947.9, pg_loss = 898.32, baseline_loss = 2055.2, entropy_loss = -5.6429, learner_queue_size = 32, train_seconds = 1953.4, _tick = 510, _time = 1.6546e+09)
[2022-06-07 10:56:25,366][root][INFO] - Step 3507200 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 3507200, mean_episode_return = 68.665, mean_episode_step = 2658.0, total_loss = -48.403, pg_loss = -93.393, baseline_loss = 50.571, entropy_loss = -5.5809, learner_queue_size = 32, train_seconds = 1958.4, _tick = 512, _time = 1.6546e+09)
[2022-06-07 10:56:30,369][root][INFO] - Step 3514880 @ 1535.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3514880, mean_episode_return = None, mean_episode_step = 3173.2, total_loss = 14.468, pg_loss = 2.1087, baseline_loss = 17.905, entropy_loss = -5.5458, learner_queue_size = 32, train_seconds = 1963.4, _tick = 514, _time = 1.6546e+09)
[2022-06-07 10:56:35,374][root][INFO] - Step 3525120 @ 2045.9 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 3525120, mean_episode_return = None, mean_episode_step = 2310.8, total_loss = -49.507, pg_loss = -46.146, baseline_loss = 2.1129, entropy_loss = -5.4737, learner_queue_size = 32, train_seconds = 1968.4, _tick = 515, _time = 1.6546e+09)
[2022-06-07 10:56:40,381][root][INFO] - Step 3532800 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3532800, mean_episode_return = None, mean_episode_step = 2773.9, total_loss = 152.55, pg_loss = 111.27, baseline_loss = 46.932, entropy_loss = -5.6536, learner_queue_size = 32, train_seconds = 1973.4, _tick = 515, _time = 1.6546e+09)
[2022-06-07 10:56:45,387][root][INFO] - Step 3543040 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3543040, mean_episode_return = 77.167, mean_episode_step = 2528.4, total_loss = -3.0339, pg_loss = -26.565, baseline_loss = 29.096, entropy_loss = -5.5642, learner_queue_size = 32, train_seconds = 1978.4, _tick = 517, _time = 1.6546e+09)
[2022-06-07 10:56:50,390][root][INFO] - Step 3550720 @ 1535.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3550720, mean_episode_return = None, mean_episode_step = 2761.4, total_loss = 28.524, pg_loss = 6.246, baseline_loss = 27.67, entropy_loss = -5.3921, learner_queue_size = 32, train_seconds = 1983.4, _tick = 518, _time = 1.6546e+09)
[2022-06-07 10:56:55,396][root][INFO] - Step 3560960 @ 2045.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3560960, mean_episode_return = None, mean_episode_step = 3036.9, total_loss = -2.2018, pg_loss = -26.264, baseline_loss = 29.618, entropy_loss = -5.556, learner_queue_size = 32, train_seconds = 1988.4, _tick = 521, _time = 1.6546e+09)
[2022-06-07 10:57:00,401][root][INFO] - Step 3568640 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3568640, mean_episode_return = 12.47, mean_episode_step = 2264.4, total_loss = 208.36, pg_loss = 129.92, baseline_loss = 83.976, entropy_loss = -5.5375, learner_queue_size = 32, train_seconds = 1993.4, _tick = 523, _time = 1.6546e+09)
[2022-06-07 10:57:05,406][root][INFO] - Step 3578880 @ 2046.1 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 3578880, mean_episode_return = None, mean_episode_step = 2772.6, total_loss = -95.564, pg_loss = -95.644, baseline_loss = 5.6006, entropy_loss = -5.5202, learner_queue_size = 32, train_seconds = 1998.4, _tick = 525, _time = 1.6546e+09)
[2022-06-07 10:57:10,410][root][INFO] - Step 3586560 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 3586560, mean_episode_return = 33.355, mean_episode_step = 2117.2, total_loss = -205.11, pg_loss = -208.86, baseline_loss = 9.3932, entropy_loss = -5.6452, learner_queue_size = 32, train_seconds = 2003.4, _tick = 528, _time = 1.6546e+09)
[2022-06-07 10:57:15,416][root][INFO] - Step 3594240 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 3594240, mean_episode_return = 80.282, mean_episode_step = 2298.8, total_loss = 575.7, pg_loss = 319.52, baseline_loss = 261.82, entropy_loss = -5.6482, learner_queue_size = 32, train_seconds = 2008.4, _tick = 531, _time = 1.6546e+09)
[2022-06-07 10:57:20,422][root][INFO] - Step 3604480 @ 2045.6 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 3604480, mean_episode_return = None, mean_episode_step = 3417.8, total_loss = -199.61, pg_loss = -195.14, baseline_loss = 1.1907, entropy_loss = -5.661, learner_queue_size = 32, train_seconds = 2013.4, _tick = 532, _time = 1.6546e+09)
[2022-06-07 10:57:25,426][root][INFO] - Step 3612160 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 3612160, mean_episode_return = None, mean_episode_step = 2664.1, total_loss = 392.91, pg_loss = 274.33, baseline_loss = 124.24, entropy_loss = -5.6616, learner_queue_size = 32, train_seconds = 2018.4, _tick = 532, _time = 1.6546e+09)
[2022-06-07 10:57:30,430][root][INFO] - Step 3622400 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3622400, mean_episode_return = 28.65, mean_episode_step = 2093.7, total_loss = -77.354, pg_loss = -97.898, baseline_loss = 26.163, entropy_loss = -5.6185, learner_queue_size = 32, train_seconds = 2023.4, _tick = 534, _time = 1.6546e+09)
[2022-06-07 10:57:35,434][root][INFO] - Step 3630080 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 3630080, mean_episode_return = 25.37, mean_episode_step = 1991.8, total_loss = 176.54, pg_loss = 123.68, baseline_loss = 58.227, entropy_loss = -5.3659, learner_queue_size = 32, train_seconds = 2028.4, _tick = 537, _time = 1.6546e+09)
[2022-06-07 10:57:40,438][root][INFO] - Step 3640320 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 3640320, mean_episode_return = None, mean_episode_step = 2450.0, total_loss = 122.03, pg_loss = 94.242, baseline_loss = 33.295, entropy_loss = -5.5085, learner_queue_size = 32, train_seconds = 2033.4, _tick = 540, _time = 1.6546e+09)
[2022-06-07 10:57:45,442][root][INFO] - Step 3648000 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 3648000, mean_episode_return = None, mean_episode_step = 2203.8, total_loss = 52.161, pg_loss = 31.177, baseline_loss = 26.301, entropy_loss = -5.3171, learner_queue_size = 32, train_seconds = 2038.4, _tick = 541, _time = 1.6546e+09)
[2022-06-07 10:57:50,446][root][INFO] - Step 3658240 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3658240, mean_episode_return = -5.1006, mean_episode_step = 2813.1, total_loss = 22.277, pg_loss = -15.535, baseline_loss = 43.237, entropy_loss = -5.4256, learner_queue_size = 32, train_seconds = 2043.4, _tick = 543, _time = 1.6546e+09)
[2022-06-07 10:57:55,450][root][INFO] - Step 3665920 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 3665920, mean_episode_return = None, mean_episode_step = 2238.5, total_loss = -54.233, pg_loss = -59.87, baseline_loss = 10.845, entropy_loss = -5.2089, learner_queue_size = 32, train_seconds = 2048.4, _tick = 543, _time = 1.6546e+09)
[2022-06-07 10:58:00,454][root][INFO] - Step 3676160 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 3676160, mean_episode_return = None, mean_episode_step = 2484.9, total_loss = -103.34, pg_loss = -101.65, baseline_loss = 3.8244, entropy_loss = -5.5156, learner_queue_size = 32, train_seconds = 2053.4, _tick = 544, _time = 1.6546e+09)
[2022-06-07 10:58:05,458][root][INFO] - Step 3683840 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3683840, mean_episode_return = 32.812, mean_episode_step = 1914.8, total_loss = 45.662, pg_loss = 18.478, baseline_loss = 32.647, entropy_loss = -5.4636, learner_queue_size = 32, train_seconds = 2058.5, _tick = 546, _time = 1.6546e+09)
[2022-06-07 10:58:10,462][root][INFO] - Step 3694080 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3694080, mean_episode_return = None, mean_episode_step = 2156.3, total_loss = 90.119, pg_loss = 54.688, baseline_loss = 40.819, entropy_loss = -5.388, learner_queue_size = 32, train_seconds = 2063.5, _tick = 547, _time = 1.6546e+09)
[2022-06-07 10:58:15,466][root][INFO] - Step 3701760 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3701760, mean_episode_return = None, mean_episode_step = 3875.3, total_loss = -119.24, pg_loss = -114.45, baseline_loss = 1.019, entropy_loss = -5.8113, learner_queue_size = 32, train_seconds = 2068.5, _tick = 549, _time = 1.6546e+09)
[2022-06-07 10:58:20,470][root][INFO] - Step 3712000 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 3712000, mean_episode_return = 83.282, mean_episode_step = 2016.7, total_loss = 118.74, pg_loss = 70.151, baseline_loss = 54.488, entropy_loss = -5.8947, learner_queue_size = 32, train_seconds = 2073.5, _tick = 552, _time = 1.6546e+09)
[2022-06-07 10:58:25,474][root][INFO] - Step 3719680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3719680, mean_episode_return = 28.92, mean_episode_step = 1679.4, total_loss = 49.109, pg_loss = 19.531, baseline_loss = 35.404, entropy_loss = -5.8262, learner_queue_size = 32, train_seconds = 2078.5, _tick = 555, _time = 1.6546e+09)
[2022-06-07 10:58:30,478][root][INFO] - Step 3729920 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3729920, mean_episode_return = 29.082, mean_episode_step = 2281.7, total_loss = 166.32, pg_loss = 93.372, baseline_loss = 78.701, entropy_loss = -5.7542, learner_queue_size = 32, train_seconds = 2083.5, _tick = 559, _time = 1.6546e+09)
[2022-06-07 10:58:35,484][root][INFO] - Step 3737600 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3737600, mean_episode_return = None, mean_episode_step = 1944.0, total_loss = 318.68, pg_loss = 237.91, baseline_loss = 86.439, entropy_loss = -5.669, learner_queue_size = 32, train_seconds = 2088.5, _tick = 560, _time = 1.6546e+09)
[2022-06-07 10:58:40,486][root][INFO] - Step 3747840 @ 2047.1 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 3747840, mean_episode_return = None, mean_episode_step = 2034.1, total_loss = -71.35, pg_loss = -66.931, baseline_loss = 1.228, entropy_loss = -5.6474, learner_queue_size = 32, train_seconds = 2093.5, _tick = 560, _time = 1.6546e+09)
[2022-06-07 10:58:45,492][root][INFO] - Step 3755520 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3755520, mean_episode_return = -16.251, mean_episode_step = 3193.9, total_loss = -67.226, pg_loss = -64.821, baseline_loss = 3.3431, entropy_loss = -5.7482, learner_queue_size = 32, train_seconds = 2098.5, _tick = 561, _time = 1.6546e+09)
[2022-06-07 10:58:50,498][root][INFO] - Step 3765760 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 3765760, mean_episode_return = None, mean_episode_step = 2385.2, total_loss = 232.99, pg_loss = 156.48, baseline_loss = 82.294, entropy_loss = -5.789, learner_queue_size = 32, train_seconds = 2103.5, _tick = 562, _time = 1.6546e+09)
[2022-06-07 10:58:55,504][root][INFO] - Step 3773440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3773440, mean_episode_return = 36.79, mean_episode_step = 2267.8, total_loss = 149.25, pg_loss = 90.039, baseline_loss = 64.867, entropy_loss = -5.6571, learner_queue_size = 32, train_seconds = 2108.5, _tick = 563, _time = 1.6546e+09)
[2022-06-07 10:59:00,510][root][INFO] - Step 3783680 @ 2045.6 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 3783680, mean_episode_return = None, mean_episode_step = 2266.0, total_loss = -146.6, pg_loss = -142.36, baseline_loss = 1.3421, entropy_loss = -5.5836, learner_queue_size = 32, train_seconds = 2113.5, _tick = 565, _time = 1.6546e+09)
[2022-06-07 10:59:05,514][root][INFO] - Step 3793920 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3793920, mean_episode_return = None, mean_episode_step = 1880.2, total_loss = 224.81, pg_loss = 153.87, baseline_loss = 76.551, entropy_loss = -5.6108, learner_queue_size = 32, train_seconds = 2118.5, _tick = 565, _time = 1.6546e+09)
[2022-06-07 10:59:10,518][root][INFO] - Step 3801600 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 3801600, mean_episode_return = 31.48, mean_episode_step = 2035.1, total_loss = -75.196, pg_loss = -80.721, baseline_loss = 11.193, entropy_loss = -5.6679, learner_queue_size = 32, train_seconds = 2123.5, _tick = 567, _time = 1.6546e+09)
[2022-06-07 10:59:15,522][root][INFO] - Step 3811840 @ 2046.3 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 3811840, mean_episode_return = 28.491, mean_episode_step = 2594.3, total_loss = 301.01, pg_loss = 228.69, baseline_loss = 78.234, entropy_loss = -5.9143, learner_queue_size = 32, train_seconds = 2128.5, _tick = 571, _time = 1.6546e+09)
[2022-06-07 10:59:20,526][root][INFO] - Step 3819520 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 3819520, mean_episode_return = -5.7207, mean_episode_step = 1928.7, total_loss = 264.22, pg_loss = 194.3, baseline_loss = 75.991, entropy_loss = -6.0793, learner_queue_size = 32, train_seconds = 2133.5, _tick = 572, _time = 1.6546e+09)
[2022-06-07 10:59:25,530][root][INFO] - Step 3829760 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3829760, mean_episode_return = None, mean_episode_step = 2060.2, total_loss = -53.288, pg_loss = -49.926, baseline_loss = 2.4836, entropy_loss = -5.8456, learner_queue_size = 32, train_seconds = 2138.5, _tick = 573, _time = 1.6546e+09)
[2022-06-07 10:59:30,534][root][INFO] - Step 3837440 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3837440, mean_episode_return = 41.58, mean_episode_step = 2084.5, total_loss = -7.6702, pg_loss = -11.521, baseline_loss = 9.6226, entropy_loss = -5.7721, learner_queue_size = 32, train_seconds = 2143.5, _tick = 576, _time = 1.6546e+09)
[2022-06-07 10:59:35,538][root][INFO] - Step 3847680 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3847680, mean_episode_return = 8.2895, mean_episode_step = 2210.9, total_loss = -95.121, pg_loss = -96.024, baseline_loss = 6.6366, entropy_loss = -5.7339, learner_queue_size = 32, train_seconds = 2148.5, _tick = 578, _time = 1.6546e+09)
[2022-06-07 10:59:40,542][root][INFO] - Step 3855360 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 3855360, mean_episode_return = None, mean_episode_step = 2177.8, total_loss = -8.239, pg_loss = -17.367, baseline_loss = 14.983, entropy_loss = -5.855, learner_queue_size = 32, train_seconds = 2153.5, _tick = 578, _time = 1.6546e+09)
[2022-06-07 10:59:45,546][root][INFO] - Step 3865600 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3865600, mean_episode_return = None, mean_episode_step = 2278.9, total_loss = -82.181, pg_loss = -88.657, baseline_loss = 12.176, entropy_loss = -5.7008, learner_queue_size = 32, train_seconds = 2158.5, _tick = 579, _time = 1.6546e+09)
[2022-06-07 10:59:50,550][root][INFO] - Step 3873280 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 3873280, mean_episode_return = -45.144, mean_episode_step = 2125.4, total_loss = -137.83, pg_loss = -134.55, baseline_loss = 2.6819, entropy_loss = -5.9632, learner_queue_size = 32, train_seconds = 2163.5, _tick = 581, _time = 1.6546e+09)
[2022-06-07 10:59:55,554][root][INFO] - Step 3883520 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3883520, mean_episode_return = None, mean_episode_step = 2607.4, total_loss = -99.865, pg_loss = -97.498, baseline_loss = 3.5796, entropy_loss = -5.9473, learner_queue_size = 32, train_seconds = 2168.5, _tick = 583, _time = 1.6546e+09)
[2022-06-07 11:00:00,558][root][INFO] - Step 3893760 @ 2046.3 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 3893760, mean_episode_return = 68.538, mean_episode_step = 2431.4, total_loss = -13.981, pg_loss = -58.49, baseline_loss = 50.499, entropy_loss = -5.9901, learner_queue_size = 32, train_seconds = 2173.6, _tick = 586, _time = 1.6546e+09)
[2022-06-07 11:00:05,571][root][INFO] - Step 3901440 @ 1532.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 3901440, mean_episode_return = -5.3903, mean_episode_step = 2551.5, total_loss = 140.14, pg_loss = 47.604, baseline_loss = 98.282, entropy_loss = -5.7488, learner_queue_size = 32, train_seconds = 2178.6, _tick = 588, _time = 1.6546e+09)
[2022-06-07 11:00:10,575][root][INFO] - Step 3911680 @ 2046.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 3911680, mean_episode_return = 39.812, mean_episode_step = 1827.3, total_loss = 146.81, pg_loss = 108.57, baseline_loss = 44.093, entropy_loss = -5.8443, learner_queue_size = 32, train_seconds = 2183.6, _tick = 590, _time = 1.6546e+09)
[2022-06-07 11:00:15,581][root][INFO] - Step 3919360 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 3919360, mean_episode_return = None, mean_episode_step = 2557.5, total_loss = -85.075, pg_loss = -80.243, baseline_loss = 1.2026, entropy_loss = -6.0339, learner_queue_size = 32, train_seconds = 2188.6, _tick = 590, _time = 1.6546e+09)
[2022-06-07 11:00:20,586][root][INFO] - Step 3929600 @ 2045.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 3929600, mean_episode_return = 38.843, mean_episode_step = 2215.6, total_loss = 29.393, pg_loss = 16.538, baseline_loss = 18.558, entropy_loss = -5.704, learner_queue_size = 32, train_seconds = 2193.6, _tick = 592, _time = 1.6546e+09)
[2022-06-07 11:00:25,590][root][INFO] - Step 3937280 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 3937280, mean_episode_return = 61.15, mean_episode_step = 2165.8, total_loss = -39.304, pg_loss = -45.436, baseline_loss = 11.984, entropy_loss = -5.8523, learner_queue_size = 32, train_seconds = 2198.6, _tick = 594, _time = 1.6546e+09)
[2022-06-07 11:00:30,596][root][INFO] - Step 3947520 @ 2045.5 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 3947520, mean_episode_return = None, mean_episode_step = 2561.0, total_loss = 239.37, pg_loss = 178.16, baseline_loss = 67.071, entropy_loss = -5.8525, learner_queue_size = 32, train_seconds = 2203.6, _tick = 594, _time = 1.6546e+09)
[2022-06-07 11:00:35,602][root][INFO] - Step 3955200 @ 1534.2 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 3955200, mean_episode_return = -7.6501, mean_episode_step = 2601.3, total_loss = 206.46, pg_loss = 144.03, baseline_loss = 68.148, entropy_loss = -5.7179, learner_queue_size = 32, train_seconds = 2208.6, _tick = 596, _time = 1.6546e+09)
[2022-06-07 11:00:40,606][root][INFO] - Step 3965440 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 3965440, mean_episode_return = 70.06, mean_episode_step = 2478.8, total_loss = -69.051, pg_loss = -74.469, baseline_loss = 11.355, entropy_loss = -5.9367, learner_queue_size = 32, train_seconds = 2213.6, _tick = 599, _time = 1.6546e+09)
[2022-06-07 11:00:45,610][root][INFO] - Step 3973120 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 3973120, mean_episode_return = 52.11, mean_episode_step = 2255.1, total_loss = 173.63, pg_loss = 85.276, baseline_loss = 94.062, entropy_loss = -5.7087, learner_queue_size = 32, train_seconds = 2218.6, _tick = 601, _time = 1.6546e+09)
[2022-06-07 11:00:50,614][root][INFO] - Step 3983360 @ 2046.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 3983360, mean_episode_return = 46.392, mean_episode_step = 2284.4, total_loss = -0.97033, pg_loss = -52.76, baseline_loss = 57.603, entropy_loss = -5.8136, learner_queue_size = 32, train_seconds = 2223.6, _tick = 603, _time = 1.6546e+09)
[2022-06-07 11:00:55,618][root][INFO] - Step 3991040 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 3991040, mean_episode_return = -4.79, mean_episode_step = 2085.4, total_loss = 199.76, pg_loss = 163.2, baseline_loss = 42.263, entropy_loss = -5.6945, learner_queue_size = 32, train_seconds = 2228.6, _tick = 606, _time = 1.6546e+09)
[2022-06-07 11:01:00,622][root][INFO] - Step 4001280 @ 2046.3 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 4001280, mean_episode_return = 68.944, mean_episode_step = 2199.6, total_loss = -0.82674, pg_loss = -33.28, baseline_loss = 38.219, entropy_loss = -5.7662, learner_queue_size = 32, train_seconds = 2233.6, _tick = 607, _time = 1.6546e+09)
[2022-06-07 11:01:05,626][root][INFO] - Step 4008960 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4008960, mean_episode_return = None, mean_episode_step = 2002.6, total_loss = 6.496, pg_loss = -8.5724, baseline_loss = 20.992, entropy_loss = -5.9234, learner_queue_size = 32, train_seconds = 2238.6, _tick = 608, _time = 1.6546e+09)
[2022-06-07 11:01:10,630][root][INFO] - Step 4019200 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4019200, mean_episode_return = 102.04, mean_episode_step = 1850.0, total_loss = -43.32, pg_loss = -59.155, baseline_loss = 21.706, entropy_loss = -5.8709, learner_queue_size = 32, train_seconds = 2243.6, _tick = 612, _time = 1.6546e+09)
[2022-06-07 11:01:15,634][root][INFO] - Step 4026880 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 4026880, mean_episode_return = -24.541, mean_episode_step = 2751.6, total_loss = 22.64, pg_loss = -16.067, baseline_loss = 44.67, entropy_loss = -5.9633, learner_queue_size = 32, train_seconds = 2248.6, _tick = 614, _time = 1.6546e+09)
[2022-06-07 11:01:20,638][root][INFO] - Step 4037120 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 4037120, mean_episode_return = None, mean_episode_step = 2098.2, total_loss = 51.756, pg_loss = 22.701, baseline_loss = 34.999, entropy_loss = -5.9439, learner_queue_size = 32, train_seconds = 2253.6, _tick = 615, _time = 1.6546e+09)
[2022-06-07 11:01:25,642][root][INFO] - Step 4044800 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 4044800, mean_episode_return = None, mean_episode_step = 2119.1, total_loss = 96.771, pg_loss = 70.182, baseline_loss = 32.53, entropy_loss = -5.9417, learner_queue_size = 32, train_seconds = 2258.6, _tick = 616, _time = 1.6546e+09)
[2022-06-07 11:01:30,646][root][INFO] - Step 4055040 @ 2046.3 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 4055040, mean_episode_return = None, mean_episode_step = 1542.7, total_loss = 155.93, pg_loss = 108.13, baseline_loss = 53.603, entropy_loss = -5.8055, learner_queue_size = 32, train_seconds = 2263.6, _tick = 617, _time = 1.6546e+09)
[2022-06-07 11:01:35,650][root][INFO] - Step 4062720 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 4062720, mean_episode_return = None, mean_episode_step = 2332.8, total_loss = 80.734, pg_loss = 51.503, baseline_loss = 34.931, entropy_loss = -5.7002, learner_queue_size = 32, train_seconds = 2268.6, _tick = 619, _time = 1.6546e+09)
[2022-06-07 11:01:40,654][root][INFO] - Step 4072960 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 4072960, mean_episode_return = 74.572, mean_episode_step = 2440.5, total_loss = 20.822, pg_loss = -9.6057, baseline_loss = 35.81, entropy_loss = -5.3826, learner_queue_size = 32, train_seconds = 2273.6, _tick = 622, _time = 1.6546e+09)
[2022-06-07 11:01:45,660][root][INFO] - Step 4080640 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 4080640, mean_episode_return = None, mean_episode_step = 2649.7, total_loss = 53.169, pg_loss = 32.084, baseline_loss = 26.328, entropy_loss = -5.2429, learner_queue_size = 32, train_seconds = 2278.7, _tick = 623, _time = 1.6546e+09)
[2022-06-07 11:01:50,666][root][INFO] - Step 4090880 @ 2045.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 4090880, mean_episode_return = None, mean_episode_step = 2752.9, total_loss = 20.894, pg_loss = 9.7007, baseline_loss = 16.766, entropy_loss = -5.5722, learner_queue_size = 32, train_seconds = 2283.7, _tick = 626, _time = 1.6546e+09)
[2022-06-07 11:01:55,670][root][INFO] - Step 4098560 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4098560, mean_episode_return = 65.483, mean_episode_step = 2350.6, total_loss = 20.264, pg_loss = 3.7548, baseline_loss = 22.224, entropy_loss = -5.7145, learner_queue_size = 32, train_seconds = 2288.7, _tick = 627, _time = 1.6546e+09)
[2022-06-07 11:02:00,674][root][INFO] - Step 4108800 @ 2046.3 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 4108800, mean_episode_return = None, mean_episode_step = 1985.8, total_loss = -66.518, pg_loss = -63.593, baseline_loss = 2.5778, entropy_loss = -5.5036, learner_queue_size = 32, train_seconds = 2293.7, _tick = 629, _time = 1.6546e+09)
[2022-06-07 11:02:05,678][root][INFO] - Step 4116480 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4116480, mean_episode_return = 60.481, mean_episode_step = 2403.8, total_loss = 103.48, pg_loss = 72.319, baseline_loss = 36.607, entropy_loss = -5.4505, learner_queue_size = 32, train_seconds = 2298.7, _tick = 632, _time = 1.6546e+09)
[2022-06-07 11:02:10,682][root][INFO] - Step 4126720 @ 2046.3 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 4126720, mean_episode_return = None, mean_episode_step = 1545.0, total_loss = 125.13, pg_loss = 91.976, baseline_loss = 38.604, entropy_loss = -5.4491, learner_queue_size = 32, train_seconds = 2303.7, _tick = 634, _time = 1.6546e+09)
[2022-06-07 11:02:15,689][root][INFO] - Step 4134400 @ 1534.0 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 4134400, mean_episode_return = None, mean_episode_step = 2590.2, total_loss = 71.089, pg_loss = 38.583, baseline_loss = 37.909, entropy_loss = -5.4029, learner_queue_size = 32, train_seconds = 2308.7, _tick = 635, _time = 1.6546e+09)
[2022-06-07 11:02:20,695][root][INFO] - Step 4144640 @ 2045.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4144640, mean_episode_return = None, mean_episode_step = 2157.3, total_loss = -109.97, pg_loss = -110.1, baseline_loss = 5.7916, entropy_loss = -5.667, learner_queue_size = 32, train_seconds = 2313.7, _tick = 637, _time = 1.6546e+09)
[2022-06-07 11:02:25,701][root][INFO] - Step 4154880 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4154880, mean_episode_return = 4.3496, mean_episode_step = 1803.5, total_loss = 400.27, pg_loss = 259.39, baseline_loss = 146.28, entropy_loss = -5.4043, learner_queue_size = 32, train_seconds = 2318.7, _tick = 640, _time = 1.6546e+09)
[2022-06-07 11:02:30,707][root][INFO] - Step 4162560 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4162560, mean_episode_return = 32.09, mean_episode_step = 2567.6, total_loss = 103.51, pg_loss = 46.774, baseline_loss = 62.134, entropy_loss = -5.3987, learner_queue_size = 32, train_seconds = 2323.7, _tick = 642, _time = 1.6546e+09)
[2022-06-07 11:02:35,710][root][INFO] - Step 4170240 @ 1535.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 4170240, mean_episode_return = None, mean_episode_step = 2085.2, total_loss = -99.766, pg_loss = -98.149, baseline_loss = 3.7452, entropy_loss = -5.362, learner_queue_size = 32, train_seconds = 2328.7, _tick = 643, _time = 1.6546e+09)
[2022-06-07 11:02:40,716][root][INFO] - Step 4180480 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 4180480, mean_episode_return = None, mean_episode_step = 1978.5, total_loss = -129.85, pg_loss = -126.84, baseline_loss = 2.252, entropy_loss = -5.2646, learner_queue_size = 32, train_seconds = 2333.7, _tick = 643, _time = 1.6546e+09)
[2022-06-07 11:02:45,722][root][INFO] - Step 4188160 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4188160, mean_episode_return = None, mean_episode_step = 1886.3, total_loss = 152.28, pg_loss = 108.08, baseline_loss = 49.296, entropy_loss = -5.1005, learner_queue_size = 32, train_seconds = 2338.7, _tick = 644, _time = 1.6546e+09)
[2022-06-07 11:02:50,726][root][INFO] - Step 4198400 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4198400, mean_episode_return = None, mean_episode_step = 3124.5, total_loss = -6.3625, pg_loss = -13.867, baseline_loss = 12.635, entropy_loss = -5.1301, learner_queue_size = 32, train_seconds = 2343.7, _tick = 646, _time = 1.6546e+09)
[2022-06-07 11:02:55,730][root][INFO] - Step 4208640 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 4208640, mean_episode_return = -5.0404, mean_episode_step = 2110.4, total_loss = -104.66, pg_loss = -104.22, baseline_loss = 4.9069, entropy_loss = -5.3415, learner_queue_size = 32, train_seconds = 2348.7, _tick = 649, _time = 1.6546e+09)
[2022-06-07 11:03:00,734][root][INFO] - Step 4216320 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 4216320, mean_episode_return = 123.95, mean_episode_step = 1812.7, total_loss = 340.51, pg_loss = 226.52, baseline_loss = 119.25, entropy_loss = -5.2701, learner_queue_size = 32, train_seconds = 2353.7, _tick = 651, _time = 1.6546e+09)
[2022-06-07 11:03:05,738][root][INFO] - Step 4226560 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 4226560, mean_episode_return = None, mean_episode_step = 3014.8, total_loss = 311.73, pg_loss = 214.34, baseline_loss = 102.69, entropy_loss = -5.2971, learner_queue_size = 32, train_seconds = 2358.7, _tick = 654, _time = 1.6546e+09)
[2022-06-07 11:03:10,742][root][INFO] - Step 4234240 @ 1534.7 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 4234240, mean_episode_return = 59.981, mean_episode_step = 2068.1, total_loss = 149.61, pg_loss = 82.272, baseline_loss = 72.671, entropy_loss = -5.3319, learner_queue_size = 32, train_seconds = 2363.7, _tick = 656, _time = 1.6546e+09)
[2022-06-07 11:03:15,746][root][INFO] - Step 4244480 @ 2046.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 4244480, mean_episode_return = None, mean_episode_step = 2009.5, total_loss = -65.472, pg_loss = -76.612, baseline_loss = 16.53, entropy_loss = -5.3904, learner_queue_size = 32, train_seconds = 2368.7, _tick = 657, _time = 1.6546e+09)
[2022-06-07 11:03:20,750][root][INFO] - Step 4252160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4252160, mean_episode_return = 48.274, mean_episode_step = 2179.3, total_loss = 137.21, pg_loss = 71.059, baseline_loss = 71.38, entropy_loss = -5.2289, learner_queue_size = 32, train_seconds = 2373.7, _tick = 658, _time = 1.6546e+09)
[2022-06-07 11:03:25,754][root][INFO] - Step 4262400 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 4262400, mean_episode_return = None, mean_episode_step = 1878.5, total_loss = -11.452, pg_loss = -21.613, baseline_loss = 14.958, entropy_loss = -4.797, learner_queue_size = 32, train_seconds = 2378.7, _tick = 660, _time = 1.6546e+09)
[2022-06-07 11:03:30,758][root][INFO] - Step 4270080 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4270080, mean_episode_return = 29.57, mean_episode_step = 2114.0, total_loss = -58.109, pg_loss = -58.239, baseline_loss = 4.2758, entropy_loss = -4.1455, learner_queue_size = 32, train_seconds = 2383.8, _tick = 663, _time = 1.6546e+09)
[2022-06-07 11:03:35,759][root][INFO] - Step 4280320 @ 2047.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 4280320, mean_episode_return = -10.77, mean_episode_step = 1589.4, total_loss = -52.503, pg_loss = -76.41, baseline_loss = 28.762, entropy_loss = -4.8546, learner_queue_size = 32, train_seconds = 2388.8, _tick = 665, _time = 1.6546e+09)
[2022-06-07 11:03:40,762][root][INFO] - Step 4288000 @ 1535.1 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 4288000, mean_episode_return = None, mean_episode_step = 2854.8, total_loss = -90.295, pg_loss = -91.779, baseline_loss = 6.2408, entropy_loss = -4.7571, learner_queue_size = 32, train_seconds = 2393.8, _tick = 667, _time = 1.6546e+09)
[2022-06-07 11:03:45,766][root][INFO] - Step 4298240 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4298240, mean_episode_return = None, mean_episode_step = 2220.0, total_loss = -92.867, pg_loss = -92.559, baseline_loss = 4.4995, entropy_loss = -4.8077, learner_queue_size = 32, train_seconds = 2398.8, _tick = 668, _time = 1.6546e+09)
[2022-06-07 11:03:50,772][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 11:03:51,119][root][INFO] - Step 4305920 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 4308480, mean_episode_return = 6.0896, mean_episode_step = 2570.7, total_loss = -8.6048, pg_loss = -19.4, baseline_loss = 15.956, entropy_loss = -5.1603, learner_queue_size = 32, train_seconds = 2403.8, _tick = 669, _time = 1.6546e+09)
[2022-06-07 11:03:56,125][root][INFO] - Step 4316160 @ 1913.0 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 4316160, mean_episode_return = 31.951, mean_episode_step = 2616.2, total_loss = -66.978, pg_loss = -63.655, baseline_loss = 1.7595, entropy_loss = -5.0827, learner_queue_size = 32, train_seconds = 2409.1, _tick = 670, _time = 1.6546e+09)
[2022-06-07 11:04:01,130][root][INFO] - Step 4326400 @ 2045.9 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 4326400, mean_episode_return = 25.99, mean_episode_step = 1587.2, total_loss = -99.454, pg_loss = -103.02, baseline_loss = 8.8737, entropy_loss = -5.3091, learner_queue_size = 32, train_seconds = 2414.1, _tick = 671, _time = 1.6546e+09)
[2022-06-07 11:04:06,134][root][INFO] - Step 4334080 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 4334080, mean_episode_return = None, mean_episode_step = 1836.2, total_loss = 64.041, pg_loss = 36.997, baseline_loss = 32.504, entropy_loss = -5.4598, learner_queue_size = 32, train_seconds = 2419.1, _tick = 672, _time = 1.6546e+09)
[2022-06-07 11:04:11,138][root][INFO] - Step 4344320 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 4344320, mean_episode_return = None, mean_episode_step = 2572.1, total_loss = 113.34, pg_loss = 78.263, baseline_loss = 40.766, entropy_loss = -5.687, learner_queue_size = 32, train_seconds = 2424.1, _tick = 672, _time = 1.6546e+09)
[2022-06-07 11:04:16,142][root][INFO] - Step 4352000 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 4352000, mean_episode_return = 42.767, mean_episode_step = 2282.2, total_loss = -8.1677, pg_loss = -21.468, baseline_loss = 19.025, entropy_loss = -5.7246, learner_queue_size = 32, train_seconds = 2429.1, _tick = 674, _time = 1.6546e+09)
[2022-06-07 11:04:21,146][root][INFO] - Step 4362240 @ 2046.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 4362240, mean_episode_return = 88.155, mean_episode_step = 2105.6, total_loss = -46.287, pg_loss = -65.227, baseline_loss = 24.668, entropy_loss = -5.728, learner_queue_size = 32, train_seconds = 2434.1, _tick = 675, _time = 1.6546e+09)
[2022-06-07 11:04:26,152][root][INFO] - Step 4369920 @ 1534.1 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 4369920, mean_episode_return = 37.444, mean_episode_step = 2528.7, total_loss = -68.603, pg_loss = -66.729, baseline_loss = 3.7679, entropy_loss = -5.6412, learner_queue_size = 32, train_seconds = 2439.1, _tick = 677, _time = 1.6546e+09)
[2022-06-07 11:04:31,158][root][INFO] - Step 4380160 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4380160, mean_episode_return = None, mean_episode_step = 1938.1, total_loss = -20.564, pg_loss = -41.561, baseline_loss = 26.679, entropy_loss = -5.6817, learner_queue_size = 32, train_seconds = 2444.2, _tick = 680, _time = 1.6546e+09)
[2022-06-07 11:04:36,162][root][INFO] - Step 4390400 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4390400, mean_episode_return = 59.121, mean_episode_step = 2727.2, total_loss = 6.4167, pg_loss = -24.169, baseline_loss = 36.07, entropy_loss = -5.484, learner_queue_size = 32, train_seconds = 2449.2, _tick = 681, _time = 1.6546e+09)
[2022-06-07 11:04:41,166][root][INFO] - Step 4398080 @ 1534.8 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 4398080, mean_episode_return = 59.627, mean_episode_step = 1675.3, total_loss = 153.69, pg_loss = 106.48, baseline_loss = 52.743, entropy_loss = -5.5322, learner_queue_size = 32, train_seconds = 2454.2, _tick = 682, _time = 1.6546e+09)
[2022-06-07 11:04:46,170][root][INFO] - Step 4405760 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4405760, mean_episode_return = None, mean_episode_step = 2540.1, total_loss = -64.15, pg_loss = -65.366, baseline_loss = 6.8053, entropy_loss = -5.5887, learner_queue_size = 32, train_seconds = 2459.2, _tick = 683, _time = 1.6546e+09)
[2022-06-07 11:04:51,174][root][INFO] - Step 4416000 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 4416000, mean_episode_return = 71.92, mean_episode_step = 2101.1, total_loss = 45.136, pg_loss = -40.975, baseline_loss = 91.632, entropy_loss = -5.5204, learner_queue_size = 32, train_seconds = 2464.2, _tick = 684, _time = 1.6546e+09)
[2022-06-07 11:04:56,180][root][INFO] - Step 4426240 @ 2045.5 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 4426240, mean_episode_return = 56.487, mean_episode_step = 2111.3, total_loss = -73.725, pg_loss = -83.526, baseline_loss = 15.415, entropy_loss = -5.6136, learner_queue_size = 32, train_seconds = 2469.2, _tick = 685, _time = 1.6546e+09)
[2022-06-07 11:05:01,182][root][INFO] - Step 4433920 @ 1535.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 4433920, mean_episode_return = None, mean_episode_step = 3090.8, total_loss = -59.918, pg_loss = -58.387, baseline_loss = 4.0358, entropy_loss = -5.5665, learner_queue_size = 32, train_seconds = 2474.2, _tick = 686, _time = 1.6546e+09)
[2022-06-07 11:05:06,186][root][INFO] - Step 4444160 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4444160, mean_episode_return = None, mean_episode_step = 2052.8, total_loss = -105.37, pg_loss = -102.92, baseline_loss = 3.1227, entropy_loss = -5.5711, learner_queue_size = 32, train_seconds = 2479.2, _tick = 686, _time = 1.6546e+09)
[2022-06-07 11:05:11,190][root][INFO] - Step 4451840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4451840, mean_episode_return = None, mean_episode_step = 3081.6, total_loss = 1161.8, pg_loss = 386.82, baseline_loss = 780.59, entropy_loss = -5.5735, learner_queue_size = 32, train_seconds = 2484.2, _tick = 687, _time = 1.6546e+09)
[2022-06-07 11:05:16,196][root][INFO] - Step 4462080 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 4462080, mean_episode_return = None, mean_episode_step = 2440.1, total_loss = 80.785, pg_loss = 60.122, baseline_loss = 26.247, entropy_loss = -5.584, learner_queue_size = 32, train_seconds = 2489.2, _tick = 688, _time = 1.6546e+09)
[2022-06-07 11:05:21,202][root][INFO] - Step 4469760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4469760, mean_episode_return = None, mean_episode_step = 1952.7, total_loss = 197.05, pg_loss = 147.44, baseline_loss = 55.011, entropy_loss = -5.4025, learner_queue_size = 32, train_seconds = 2494.2, _tick = 689, _time = 1.6546e+09)
[2022-06-07 11:05:26,206][root][INFO] - Step 4480000 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 4480000, mean_episode_return = None, mean_episode_step = 2116.0, total_loss = 243.55, pg_loss = 138.67, baseline_loss = 110.45, entropy_loss = -5.5732, learner_queue_size = 32, train_seconds = 2499.2, _tick = 689, _time = 1.6546e+09)
[2022-06-07 11:05:31,210][root][INFO] - Step 4487680 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4487680, mean_episode_return = None, mean_episode_step = 3234.5, total_loss = -60.521, pg_loss = -84.422, baseline_loss = 29.262, entropy_loss = -5.3613, learner_queue_size = 32, train_seconds = 2504.2, _tick = 690, _time = 1.6546e+09)
[2022-06-07 11:05:36,216][root][INFO] - Step 4495360 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 4495360, mean_episode_return = None, mean_episode_step = 2695.1, total_loss = -34.053, pg_loss = -31.06, baseline_loss = 1.6433, entropy_loss = -4.636, learner_queue_size = 32, train_seconds = 2509.2, _tick = 690, _time = 1.6546e+09)
[2022-06-07 11:05:41,218][root][INFO] - Step 4505600 @ 2047.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 4505600, mean_episode_return = None, mean_episode_step = 2367.4, total_loss = -51.21, pg_loss = -48.97, baseline_loss = 2.8713, entropy_loss = -5.1115, learner_queue_size = 32, train_seconds = 2514.2, _tick = 693, _time = 1.6546e+09)
[2022-06-07 11:05:46,224][root][INFO] - Step 4513280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4513280, mean_episode_return = -16.401, mean_episode_step = 2653.6, total_loss = -15.179, pg_loss = -29.535, baseline_loss = 19.402, entropy_loss = -5.0462, learner_queue_size = 32, train_seconds = 2519.2, _tick = 695, _time = 1.6546e+09)
[2022-06-07 11:05:51,230][root][INFO] - Step 4523520 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 4523520, mean_episode_return = 52.321, mean_episode_step = 2643.1, total_loss = -117.83, pg_loss = -118.68, baseline_loss = 5.8277, entropy_loss = -4.9758, learner_queue_size = 32, train_seconds = 2524.2, _tick = 698, _time = 1.6546e+09)
[2022-06-07 11:05:56,236][root][INFO] - Step 4531200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4531200, mean_episode_return = 63.62, mean_episode_step = 2340.8, total_loss = 299.0, pg_loss = 213.68, baseline_loss = 90.624, entropy_loss = -5.3059, learner_queue_size = 32, train_seconds = 2529.2, _tick = 700, _time = 1.6546e+09)
[2022-06-07 11:06:01,238][root][INFO] - Step 4541440 @ 2047.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 4541440, mean_episode_return = None, mean_episode_step = 3329.4, total_loss = 285.44, pg_loss = 203.37, baseline_loss = 87.658, entropy_loss = -5.5872, learner_queue_size = 32, train_seconds = 2534.2, _tick = 702, _time = 1.6546e+09)
[2022-06-07 11:06:06,244][root][INFO] - Step 4549120 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4549120, mean_episode_return = None, mean_episode_step = 2376.8, total_loss = 30.203, pg_loss = 16.396, baseline_loss = 19.583, entropy_loss = -5.7762, learner_queue_size = 32, train_seconds = 2539.2, _tick = 702, _time = 1.6546e+09)
[2022-06-07 11:06:11,250][root][INFO] - Step 4559360 @ 2045.6 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 4559360, mean_episode_return = None, mean_episode_step = 2261.8, total_loss = -5.0401, pg_loss = -6.9496, baseline_loss = 7.4729, entropy_loss = -5.5633, learner_queue_size = 32, train_seconds = 2544.2, _tick = 703, _time = 1.6546e+09)
[2022-06-07 11:06:16,254][root][INFO] - Step 4567040 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4567040, mean_episode_return = 11.22, mean_episode_step = 2925.7, total_loss = -80.631, pg_loss = -79.51, baseline_loss = 4.7824, entropy_loss = -5.9035, learner_queue_size = 32, train_seconds = 2549.2, _tick = 704, _time = 1.6546e+09)
[2022-06-07 11:06:21,260][root][INFO] - Step 4577280 @ 2045.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4577280, mean_episode_return = 24.86, mean_episode_step = 2096.3, total_loss = 343.08, pg_loss = 263.35, baseline_loss = 85.597, entropy_loss = -5.8738, learner_queue_size = 32, train_seconds = 2554.3, _tick = 706, _time = 1.6546e+09)
[2022-06-07 11:06:26,262][root][INFO] - Step 4584960 @ 1535.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 4584960, mean_episode_return = 41.195, mean_episode_step = 2754.5, total_loss = -35.229, pg_loss = -42.771, baseline_loss = 13.439, entropy_loss = -5.897, learner_queue_size = 32, train_seconds = 2559.3, _tick = 708, _time = 1.6546e+09)
[2022-06-07 11:06:31,274][root][INFO] - Step 4595200 @ 2043.1 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 4595200, mean_episode_return = None, mean_episode_step = 2702.9, total_loss = 120.87, pg_loss = 82.074, baseline_loss = 44.693, entropy_loss = -5.8951, learner_queue_size = 32, train_seconds = 2564.3, _tick = 709, _time = 1.6546e+09)
[2022-06-07 11:06:36,277][root][INFO] - Step 4602880 @ 1534.9 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4602880, mean_episode_return = None, mean_episode_step = 3054.2, total_loss = -96.665, pg_loss = -92.112, baseline_loss = 1.1061, entropy_loss = -5.6593, learner_queue_size = 32, train_seconds = 2569.3, _tick = 710, _time = 1.6546e+09)
[2022-06-07 11:06:41,282][root][INFO] - Step 4613120 @ 2046.1 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 4613120, mean_episode_return = None, mean_episode_step = 2623.0, total_loss = -58.683, pg_loss = -57.046, baseline_loss = 4.0382, entropy_loss = -5.6746, learner_queue_size = 32, train_seconds = 2574.3, _tick = 713, _time = 1.6546e+09)
[2022-06-07 11:06:46,288][root][INFO] - Step 4620800 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4620800, mean_episode_return = 9.4198, mean_episode_step = 2416.1, total_loss = -103.02, pg_loss = -98.238, baseline_loss = 0.9014, entropy_loss = -5.6831, learner_queue_size = 32, train_seconds = 2579.3, _tick = 715, _time = 1.6546e+09)
[2022-06-07 11:06:51,294][root][INFO] - Step 4631040 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 4631040, mean_episode_return = None, mean_episode_step = 2892.6, total_loss = 17.066, pg_loss = 9.262, baseline_loss = 13.477, entropy_loss = -5.6734, learner_queue_size = 32, train_seconds = 2584.3, _tick = 717, _time = 1.6546e+09)
[2022-06-07 11:06:56,300][root][INFO] - Step 4638720 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 4638720, mean_episode_return = 128.81, mean_episode_step = 2238.6, total_loss = 428.54, pg_loss = 245.93, baseline_loss = 188.33, entropy_loss = -5.7277, learner_queue_size = 32, train_seconds = 2589.3, _tick = 719, _time = 1.6546e+09)
[2022-06-07 11:07:01,306][root][INFO] - Step 4648960 @ 2045.7 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 4648960, mean_episode_return = 20.21, mean_episode_step = 2474.8, total_loss = 132.54, pg_loss = 73.929, baseline_loss = 64.214, entropy_loss = -5.6034, learner_queue_size = 32, train_seconds = 2594.3, _tick = 723, _time = 1.6546e+09)
[2022-06-07 11:07:06,310][root][INFO] - Step 4659200 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 4659200, mean_episode_return = 18.179, mean_episode_step = 1885.0, total_loss = 663.72, pg_loss = 522.43, baseline_loss = 146.95, entropy_loss = -5.67, learner_queue_size = 32, train_seconds = 2599.3, _tick = 726, _time = 1.6546e+09)
[2022-06-07 11:07:11,316][root][INFO] - Step 4666880 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 4666880, mean_episode_return = 141.25, mean_episode_step = 2648.4, total_loss = -139.03, pg_loss = -144.96, baseline_loss = 11.582, entropy_loss = -5.6515, learner_queue_size = 32, train_seconds = 2604.3, _tick = 727, _time = 1.6546e+09)
[2022-06-07 11:07:16,318][root][INFO] - Step 4674560 @ 1535.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4674560, mean_episode_return = 17.58, mean_episode_step = 2252.6, total_loss = 145.39, pg_loss = 99.254, baseline_loss = 51.981, entropy_loss = -5.8407, learner_queue_size = 32, train_seconds = 2609.3, _tick = 730, _time = 1.6546e+09)
[2022-06-07 11:07:21,324][root][INFO] - Step 4684800 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4684800, mean_episode_return = 0.68939, mean_episode_step = 2530.8, total_loss = 202.82, pg_loss = 145.92, baseline_loss = 62.548, entropy_loss = -5.6517, learner_queue_size = 32, train_seconds = 2614.3, _tick = 732, _time = 1.6546e+09)
[2022-06-07 11:07:26,326][root][INFO] - Step 4692480 @ 1535.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 4692480, mean_episode_return = None, mean_episode_step = 2514.8, total_loss = 218.28, pg_loss = 152.07, baseline_loss = 72.042, entropy_loss = -5.8267, learner_queue_size = 32, train_seconds = 2619.3, _tick = 734, _time = 1.6546e+09)
[2022-06-07 11:07:31,330][root][INFO] - Step 4702720 @ 2046.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 4702720, mean_episode_return = 28.717, mean_episode_step = 2806.6, total_loss = 61.175, pg_loss = -13.296, baseline_loss = 80.358, entropy_loss = -5.8874, learner_queue_size = 32, train_seconds = 2624.3, _tick = 736, _time = 1.6546e+09)
[2022-06-07 11:07:36,334][root][INFO] - Step 4710400 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 4710400, mean_episode_return = 159.26, mean_episode_step = 3049.1, total_loss = 135.23, pg_loss = 86.666, baseline_loss = 54.045, entropy_loss = -5.4828, learner_queue_size = 32, train_seconds = 2629.3, _tick = 739, _time = 1.6546e+09)
[2022-06-07 11:07:41,338][root][INFO] - Step 4720640 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4720640, mean_episode_return = 88.777, mean_episode_step = 2970.4, total_loss = 77.909, pg_loss = 39.617, baseline_loss = 43.886, entropy_loss = -5.5944, learner_queue_size = 32, train_seconds = 2634.3, _tick = 743, _time = 1.6546e+09)
[2022-06-07 11:07:46,342][root][INFO] - Step 4728320 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4728320, mean_episode_return = 54.556, mean_episode_step = 2204.0, total_loss = -38.34, pg_loss = -68.586, baseline_loss = 35.874, entropy_loss = -5.6282, learner_queue_size = 32, train_seconds = 2639.3, _tick = 745, _time = 1.6546e+09)
[2022-06-07 11:07:51,348][root][INFO] - Step 4738560 @ 2045.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4738560, mean_episode_return = None, mean_episode_step = 2765.4, total_loss = -6.3792, pg_loss = -5.2799, baseline_loss = 3.9951, entropy_loss = -5.0944, learner_queue_size = 32, train_seconds = 2644.3, _tick = 748, _time = 1.6546e+09)
[2022-06-07 11:07:56,354][root][INFO] - Step 4746240 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4746240, mean_episode_return = 84.865, mean_episode_step = 2339.1, total_loss = 22.706, pg_loss = -15.582, baseline_loss = 43.717, entropy_loss = -5.4287, learner_queue_size = 32, train_seconds = 2649.3, _tick = 750, _time = 1.6546e+09)
[2022-06-07 11:08:01,402][root][INFO] - Step 4756480 @ 2028.7 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 4756480, mean_episode_return = 21.36, mean_episode_step = 1660.2, total_loss = 35.069, pg_loss = 5.0423, baseline_loss = 35.509, entropy_loss = -5.4826, learner_queue_size = 32, train_seconds = 2654.4, _tick = 754, _time = 1.6546e+09)
[2022-06-07 11:08:06,406][root][INFO] - Step 4764160 @ 1534.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 4764160, mean_episode_return = 25.481, mean_episode_step = 2244.7, total_loss = 363.19, pg_loss = 278.77, baseline_loss = 90.054, entropy_loss = -5.6357, learner_queue_size = 32, train_seconds = 2659.4, _tick = 757, _time = 1.6546e+09)
[2022-06-07 11:08:11,410][root][INFO] - Step 4774400 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 4774400, mean_episode_return = 29.862, mean_episode_step = 1856.1, total_loss = -23.092, pg_loss = -49.058, baseline_loss = 31.853, entropy_loss = -5.8869, learner_queue_size = 32, train_seconds = 2664.4, _tick = 760, _time = 1.6546e+09)
[2022-06-07 11:08:16,414][root][INFO] - Step 4782080 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 4782080, mean_episode_return = 81.065, mean_episode_step = 2312.6, total_loss = -60.168, pg_loss = -76.456, baseline_loss = 22.061, entropy_loss = -5.7728, learner_queue_size = 32, train_seconds = 2669.4, _tick = 762, _time = 1.6546e+09)
[2022-06-07 11:08:21,418][root][INFO] - Step 4792320 @ 2046.3 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 4792320, mean_episode_return = None, mean_episode_step = 1690.8, total_loss = 233.52, pg_loss = 168.57, baseline_loss = 70.849, entropy_loss = -5.907, learner_queue_size = 32, train_seconds = 2674.4, _tick = 764, _time = 1.6546e+09)
[2022-06-07 11:08:26,424][root][INFO] - Step 4800000 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4800000, mean_episode_return = None, mean_episode_step = 1998.9, total_loss = -100.33, pg_loss = -97.045, baseline_loss = 2.6319, entropy_loss = -5.9167, learner_queue_size = 32, train_seconds = 2679.4, _tick = 764, _time = 1.6546e+09)
[2022-06-07 11:08:31,430][root][INFO] - Step 4810240 @ 2045.7 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 4810240, mean_episode_return = 24.143, mean_episode_step = 2298.3, total_loss = 102.9, pg_loss = 8.4981, baseline_loss = 100.3, entropy_loss = -5.9016, learner_queue_size = 32, train_seconds = 2684.4, _tick = 766, _time = 1.6546e+09)
[2022-06-07 11:08:36,436][root][INFO] - Step 4817920 @ 1534.1 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 4817920, mean_episode_return = None, mean_episode_step = 1990.0, total_loss = -13.672, pg_loss = -23.509, baseline_loss = 15.391, entropy_loss = -5.5541, learner_queue_size = 32, train_seconds = 2689.4, _tick = 768, _time = 1.6546e+09)
[2022-06-07 11:08:41,442][root][INFO] - Step 4828160 @ 2045.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 4828160, mean_episode_return = 97.348, mean_episode_step = 2293.2, total_loss = -9.4372, pg_loss = -17.361, baseline_loss = 13.516, entropy_loss = -5.5916, learner_queue_size = 32, train_seconds = 2694.4, _tick = 770, _time = 1.6546e+09)
[2022-06-07 11:08:46,446][root][INFO] - Step 4835840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4835840, mean_episode_return = None, mean_episode_step = 1849.6, total_loss = 298.11, pg_loss = 225.75, baseline_loss = 77.923, entropy_loss = -5.5638, learner_queue_size = 32, train_seconds = 2699.4, _tick = 771, _time = 1.6546e+09)
[2022-06-07 11:08:51,451][root][INFO] - Step 4846080 @ 2045.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4846080, mean_episode_return = 29.84, mean_episode_step = 2080.8, total_loss = -150.78, pg_loss = -152.04, baseline_loss = 6.988, entropy_loss = -5.7281, learner_queue_size = 32, train_seconds = 2704.4, _tick = 772, _time = 1.6546e+09)
[2022-06-07 11:08:56,456][root][INFO] - Step 4853760 @ 1534.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4853760, mean_episode_return = 45.67, mean_episode_step = 1734.7, total_loss = -85.858, pg_loss = -87.634, baseline_loss = 7.2869, entropy_loss = -5.5109, learner_queue_size = 32, train_seconds = 2709.5, _tick = 773, _time = 1.6546e+09)
[2022-06-07 11:09:01,462][root][INFO] - Step 4864000 @ 2045.6 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 4864000, mean_episode_return = 9.7091, mean_episode_step = 2585.4, total_loss = 41.317, pg_loss = 2.0204, baseline_loss = 44.913, entropy_loss = -5.6162, learner_queue_size = 32, train_seconds = 2714.5, _tick = 776, _time = 1.6546e+09)
[2022-06-07 11:09:06,466][root][INFO] - Step 4871680 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 4871680, mean_episode_return = 17.38, mean_episode_step = 1982.0, total_loss = 292.94, pg_loss = 175.35, baseline_loss = 123.11, entropy_loss = -5.5175, learner_queue_size = 32, train_seconds = 2719.5, _tick = 778, _time = 1.6546e+09)
[2022-06-07 11:09:11,470][root][INFO] - Step 4881920 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 4881920, mean_episode_return = None, mean_episode_step = 2738.3, total_loss = -91.377, pg_loss = -104.63, baseline_loss = 18.797, entropy_loss = -5.5463, learner_queue_size = 32, train_seconds = 2724.5, _tick = 781, _time = 1.6546e+09)
[2022-06-07 11:09:16,474][root][INFO] - Step 4889600 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 4889600, mean_episode_return = None, mean_episode_step = 1630.3, total_loss = 245.26, pg_loss = 183.16, baseline_loss = 67.534, entropy_loss = -5.4364, learner_queue_size = 32, train_seconds = 2729.5, _tick = 783, _time = 1.6546e+09)
[2022-06-07 11:09:21,478][root][INFO] - Step 4899840 @ 2046.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 4899840, mean_episode_return = 127.13, mean_episode_step = 2084.7, total_loss = 21.344, pg_loss = -21.561, baseline_loss = 48.513, entropy_loss = -5.6083, learner_queue_size = 32, train_seconds = 2734.5, _tick = 786, _time = 1.6546e+09)
[2022-06-07 11:09:26,482][root][INFO] - Step 4907520 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 4907520, mean_episode_return = 103.12, mean_episode_step = 1800.9, total_loss = -65.874, pg_loss = -88.904, baseline_loss = 28.818, entropy_loss = -5.7869, learner_queue_size = 32, train_seconds = 2739.5, _tick = 789, _time = 1.6546e+09)
[2022-06-07 11:09:31,488][root][INFO] - Step 4917760 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 4917760, mean_episode_return = None, mean_episode_step = 2563.8, total_loss = 76.826, pg_loss = 39.969, baseline_loss = 42.456, entropy_loss = -5.599, learner_queue_size = 32, train_seconds = 2744.5, _tick = 790, _time = 1.6546e+09)
[2022-06-07 11:09:36,494][root][INFO] - Step 4925440 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 4925440, mean_episode_return = None, mean_episode_step = 2911.4, total_loss = -70.179, pg_loss = -73.133, baseline_loss = 8.5751, entropy_loss = -5.6207, learner_queue_size = 32, train_seconds = 2749.5, _tick = 791, _time = 1.6546e+09)
[2022-06-07 11:09:41,498][root][INFO] - Step 4935680 @ 2046.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 4935680, mean_episode_return = None, mean_episode_step = 2190.0, total_loss = 243.11, pg_loss = 158.65, baseline_loss = 90.192, entropy_loss = -5.7328, learner_queue_size = 32, train_seconds = 2754.5, _tick = 792, _time = 1.6546e+09)
[2022-06-07 11:09:46,502][root][INFO] - Step 4943360 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 4943360, mean_episode_return = None, mean_episode_step = 2203.5, total_loss = 368.24, pg_loss = 240.5, baseline_loss = 133.37, entropy_loss = -5.6311, learner_queue_size = 32, train_seconds = 2759.5, _tick = 793, _time = 1.6546e+09)
[2022-06-07 11:09:51,508][root][INFO] - Step 4953600 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 4953600, mean_episode_return = 47.863, mean_episode_step = 1841.0, total_loss = -256.77, pg_loss = -259.12, baseline_loss = 7.923, entropy_loss = -5.5708, learner_queue_size = 32, train_seconds = 2764.5, _tick = 796, _time = 1.6546e+09)
[2022-06-07 11:09:56,514][root][INFO] - Step 4961280 @ 1534.1 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 4961280, mean_episode_return = 52.369, mean_episode_step = 2043.4, total_loss = 343.95, pg_loss = 249.25, baseline_loss = 100.15, entropy_loss = -5.4517, learner_queue_size = 32, train_seconds = 2769.5, _tick = 797, _time = 1.6546e+09)
[2022-06-07 11:10:01,520][root][INFO] - Step 4968960 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4968960, mean_episode_return = None, mean_episode_step = 1916.8, total_loss = 194.92, pg_loss = 129.0, baseline_loss = 71.495, entropy_loss = -5.5701, learner_queue_size = 32, train_seconds = 2774.5, _tick = 798, _time = 1.6546e+09)
[2022-06-07 11:10:06,526][root][INFO] - Step 4979200 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 4979200, mean_episode_return = -17.121, mean_episode_step = 3319.3, total_loss = -95.452, pg_loss = -95.258, baseline_loss = 5.0572, entropy_loss = -5.251, learner_queue_size = 32, train_seconds = 2779.5, _tick = 801, _time = 1.6546e+09)
[2022-06-07 11:10:11,530][root][INFO] - Step 4986880 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 4986880, mean_episode_return = 28.49, mean_episode_step = 2166.3, total_loss = -57.204, pg_loss = -78.165, baseline_loss = 26.371, entropy_loss = -5.4096, learner_queue_size = 32, train_seconds = 2784.5, _tick = 802, _time = 1.6546e+09)
[2022-06-07 11:10:16,534][root][INFO] - Step 4997120 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 4997120, mean_episode_return = None, mean_episode_step = 2187.3, total_loss = 111.71, pg_loss = 50.133, baseline_loss = 67.095, entropy_loss = -5.5214, learner_queue_size = 32, train_seconds = 2789.5, _tick = 803, _time = 1.6546e+09)
[2022-06-07 11:10:21,540][root][INFO] - Step 5004800 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5004800, mean_episode_return = 105.29, mean_episode_step = 2226.5, total_loss = 85.587, pg_loss = 50.533, baseline_loss = 40.615, entropy_loss = -5.5613, learner_queue_size = 32, train_seconds = 2794.5, _tick = 805, _time = 1.6546e+09)
[2022-06-07 11:10:26,546][root][INFO] - Step 5015040 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 5015040, mean_episode_return = 113.61, mean_episode_step = 2095.3, total_loss = 253.78, pg_loss = 163.32, baseline_loss = 95.968, entropy_loss = -5.5138, learner_queue_size = 32, train_seconds = 2799.5, _tick = 808, _time = 1.6546e+09)
[2022-06-07 11:10:31,550][root][INFO] - Step 5022720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5022720, mean_episode_return = None, mean_episode_step = 2317.0, total_loss = 440.64, pg_loss = 284.96, baseline_loss = 161.16, entropy_loss = -5.4805, learner_queue_size = 32, train_seconds = 2804.5, _tick = 808, _time = 1.6546e+09)
[2022-06-07 11:10:36,554][root][INFO] - Step 5032960 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 5032960, mean_episode_return = None, mean_episode_step = 2566.2, total_loss = -61.636, pg_loss = -71.256, baseline_loss = 15.11, entropy_loss = -5.4893, learner_queue_size = 32, train_seconds = 2809.5, _tick = 810, _time = 1.6546e+09)
[2022-06-07 11:10:41,560][root][INFO] - Step 5043200 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 5043200, mean_episode_return = None, mean_episode_step = 2025.7, total_loss = 53.831, pg_loss = 29.134, baseline_loss = 30.299, entropy_loss = -5.6024, learner_queue_size = 32, train_seconds = 2814.6, _tick = 810, _time = 1.6546e+09)
[2022-06-07 11:10:46,562][root][INFO] - Step 5050880 @ 1535.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 5050880, mean_episode_return = 19.811, mean_episode_step = 1973.7, total_loss = 31.093, pg_loss = -44.887, baseline_loss = 81.535, entropy_loss = -5.5552, learner_queue_size = 32, train_seconds = 2819.6, _tick = 811, _time = 1.6546e+09)
[2022-06-07 11:10:51,566][root][INFO] - Step 5061120 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 5061120, mean_episode_return = None, mean_episode_step = 2860.7, total_loss = 48.397, pg_loss = 28.289, baseline_loss = 25.812, entropy_loss = -5.7045, learner_queue_size = 32, train_seconds = 2824.6, _tick = 812, _time = 1.6546e+09)
[2022-06-07 11:10:56,570][root][INFO] - Step 5068800 @ 1534.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 5068800, mean_episode_return = None, mean_episode_step = 2054.0, total_loss = -48.93, pg_loss = -60.038, baseline_loss = 16.839, entropy_loss = -5.7314, learner_queue_size = 32, train_seconds = 2829.6, _tick = 812, _time = 1.6546e+09)
[2022-06-07 11:11:01,576][root][INFO] - Step 5079040 @ 2045.7 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 5079040, mean_episode_return = None, mean_episode_step = 2083.2, total_loss = 100.59, pg_loss = 55.158, baseline_loss = 51.196, entropy_loss = -5.762, learner_queue_size = 32, train_seconds = 2834.6, _tick = 812, _time = 1.6546e+09)
[2022-06-07 11:11:06,578][root][INFO] - Step 5086720 @ 1535.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 5086720, mean_episode_return = 122.94, mean_episode_step = 2004.8, total_loss = -146.17, pg_loss = -149.38, baseline_loss = 9.1265, entropy_loss = -5.9123, learner_queue_size = 32, train_seconds = 2839.6, _tick = 813, _time = 1.6546e+09)
[2022-06-07 11:11:11,582][root][INFO] - Step 5096960 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 5096960, mean_episode_return = None, mean_episode_step = 2211.8, total_loss = -59.933, pg_loss = -59.894, baseline_loss = 5.7776, entropy_loss = -5.817, learner_queue_size = 32, train_seconds = 2844.6, _tick = 814, _time = 1.6546e+09)
[2022-06-07 11:11:16,586][root][INFO] - Step 5104640 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 5104640, mean_episode_return = None, mean_episode_step = 2202.4, total_loss = -100.09, pg_loss = -96.202, baseline_loss = 1.7738, entropy_loss = -5.6641, learner_queue_size = 32, train_seconds = 2849.6, _tick = 815, _time = 1.6546e+09)
[2022-06-07 11:11:21,590][root][INFO] - Step 5114880 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 5114880, mean_episode_return = -31.711, mean_episode_step = 2656.3, total_loss = -84.797, pg_loss = -88.175, baseline_loss = 9.2166, entropy_loss = -5.8382, learner_queue_size = 32, train_seconds = 2854.6, _tick = 816, _time = 1.6546e+09)
[2022-06-07 11:11:26,593][root][INFO] - Step 5122560 @ 1535.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 5122560, mean_episode_return = 18.972, mean_episode_step = 2201.0, total_loss = -179.6, pg_loss = -177.19, baseline_loss = 3.4766, entropy_loss = -5.8911, learner_queue_size = 32, train_seconds = 2859.6, _tick = 818, _time = 1.6546e+09)
[2022-06-07 11:11:31,598][root][INFO] - Step 5132800 @ 2046.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5132800, mean_episode_return = None, mean_episode_step = 2493.7, total_loss = -86.672, pg_loss = -82.22, baseline_loss = 1.4893, entropy_loss = -5.9419, learner_queue_size = 32, train_seconds = 2864.6, _tick = 820, _time = 1.6546e+09)
[2022-06-07 11:11:36,602][root][INFO] - Step 5140480 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 5140480, mean_episode_return = 29.264, mean_episode_step = 2012.5, total_loss = -82.35, pg_loss = -80.083, baseline_loss = 3.7209, entropy_loss = -5.9874, learner_queue_size = 32, train_seconds = 2869.6, _tick = 823, _time = 1.6546e+09)
[2022-06-07 11:11:41,606][root][INFO] - Step 5150720 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 5150720, mean_episode_return = None, mean_episode_step = 2661.1, total_loss = 18.008, pg_loss = 6.1072, baseline_loss = 17.947, entropy_loss = -6.0457, learner_queue_size = 32, train_seconds = 2874.6, _tick = 824, _time = 1.6546e+09)
[2022-06-07 11:11:46,610][root][INFO] - Step 5158400 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 5158400, mean_episode_return = -8.2301, mean_episode_step = 2076.3, total_loss = 42.135, pg_loss = 30.369, baseline_loss = 17.65, entropy_loss = -5.8841, learner_queue_size = 32, train_seconds = 2879.6, _tick = 827, _time = 1.6546e+09)
[2022-06-07 11:11:51,614][root][INFO] - Step 5168640 @ 2046.4 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 5168640, mean_episode_return = 38.318, mean_episode_step = 2218.1, total_loss = 215.43, pg_loss = 145.27, baseline_loss = 75.817, entropy_loss = -5.6537, learner_queue_size = 32, train_seconds = 2884.6, _tick = 830, _time = 1.6546e+09)
[2022-06-07 11:11:56,618][root][INFO] - Step 5176320 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 5176320, mean_episode_return = None, mean_episode_step = 1852.2, total_loss = 185.77, pg_loss = 143.36, baseline_loss = 48.379, entropy_loss = -5.9685, learner_queue_size = 32, train_seconds = 2889.6, _tick = 832, _time = 1.6546e+09)
[2022-06-07 11:12:01,622][root][INFO] - Step 5186560 @ 2046.3 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 5186560, mean_episode_return = 0.45943, mean_episode_step = 2333.9, total_loss = 32.575, pg_loss = 17.521, baseline_loss = 21.076, entropy_loss = -6.0217, learner_queue_size = 32, train_seconds = 2894.6, _tick = 835, _time = 1.6546e+09)
[2022-06-07 11:12:06,628][root][INFO] - Step 5196800 @ 2045.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 5196800, mean_episode_return = None, mean_episode_step = 2058.7, total_loss = 59.909, pg_loss = 39.263, baseline_loss = 26.738, entropy_loss = -6.0915, learner_queue_size = 32, train_seconds = 2899.6, _tick = 837, _time = 1.6546e+09)
[2022-06-07 11:12:11,634][root][INFO] - Step 5204480 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 5204480, mean_episode_return = 105.66, mean_episode_step = 2223.0, total_loss = -93.7, pg_loss = -95.176, baseline_loss = 7.7494, entropy_loss = -6.2732, learner_queue_size = 32, train_seconds = 2904.6, _tick = 840, _time = 1.6546e+09)
[2022-06-07 11:12:16,638][root][INFO] - Step 5214720 @ 2046.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 5214720, mean_episode_return = 47.355, mean_episode_step = 1446.2, total_loss = 329.74, pg_loss = 185.77, baseline_loss = 150.18, entropy_loss = -6.2212, learner_queue_size = 32, train_seconds = 2909.6, _tick = 843, _time = 1.6546e+09)
[2022-06-07 11:12:21,642][root][INFO] - Step 5222400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5222400, mean_episode_return = None, mean_episode_step = 2034.2, total_loss = 176.21, pg_loss = 133.39, baseline_loss = 49.038, entropy_loss = -6.2104, learner_queue_size = 32, train_seconds = 2914.6, _tick = 844, _time = 1.6546e+09)
[2022-06-07 11:12:26,648][root][INFO] - Step 5232640 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5232640, mean_episode_return = 146.93, mean_episode_step = 1671.6, total_loss = -186.76, pg_loss = -185.81, baseline_loss = 5.2491, entropy_loss = -6.1963, learner_queue_size = 32, train_seconds = 2919.6, _tick = 848, _time = 1.6546e+09)
[2022-06-07 11:12:31,654][root][INFO] - Step 5240320 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5240320, mean_episode_return = None, mean_episode_step = 2148.0, total_loss = 365.82, pg_loss = 267.28, baseline_loss = 104.52, entropy_loss = -5.9693, learner_queue_size = 32, train_seconds = 2924.6, _tick = 849, _time = 1.6546e+09)
[2022-06-07 11:12:36,660][root][INFO] - Step 5248000 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 5248000, mean_episode_return = 168.72, mean_episode_step = 2506.8, total_loss = -59.506, pg_loss = -64.358, baseline_loss = 10.783, entropy_loss = -5.9308, learner_queue_size = 32, train_seconds = 2929.7, _tick = 852, _time = 1.6546e+09)
[2022-06-07 11:12:41,666][root][INFO] - Step 5258240 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5258240, mean_episode_return = 0.1797, mean_episode_step = 1639.7, total_loss = 37.327, pg_loss = 8.584, baseline_loss = 34.419, entropy_loss = -5.6764, learner_queue_size = 32, train_seconds = 2934.7, _tick = 855, _time = 1.6546e+09)
[2022-06-07 11:12:46,672][root][INFO] - Step 5265920 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5265920, mean_episode_return = None, mean_episode_step = 1905.5, total_loss = 113.69, pg_loss = 72.693, baseline_loss = 46.691, entropy_loss = -5.6984, learner_queue_size = 32, train_seconds = 2939.7, _tick = 857, _time = 1.6546e+09)
[2022-06-07 11:12:51,674][root][INFO] - Step 5276160 @ 2047.0 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 5276160, mean_episode_return = 110.64, mean_episode_step = 1659.3, total_loss = -59.306, pg_loss = -71.785, baseline_loss = 18.342, entropy_loss = -5.8625, learner_queue_size = 32, train_seconds = 2944.7, _tick = 860, _time = 1.6546e+09)
[2022-06-07 11:12:56,678][root][INFO] - Step 5283840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5283840, mean_episode_return = 34.98, mean_episode_step = 2220.2, total_loss = 70.192, pg_loss = 39.486, baseline_loss = 36.645, entropy_loss = -5.9386, learner_queue_size = 32, train_seconds = 2949.7, _tick = 862, _time = 1.6546e+09)
[2022-06-07 11:13:01,682][root][INFO] - Step 5294080 @ 2046.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 5294080, mean_episode_return = None, mean_episode_step = 1789.2, total_loss = 32.798, pg_loss = 13.221, baseline_loss = 25.454, entropy_loss = -5.8769, learner_queue_size = 32, train_seconds = 2954.7, _tick = 862, _time = 1.6546e+09)
[2022-06-07 11:13:06,686][root][INFO] - Step 5301760 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 5301760, mean_episode_return = None, mean_episode_step = 2285.3, total_loss = 20.485, pg_loss = 0.29984, baseline_loss = 26.038, entropy_loss = -5.8524, learner_queue_size = 32, train_seconds = 2959.7, _tick = 864, _time = 1.6546e+09)
[2022-06-07 11:13:11,690][root][INFO] - Step 5312000 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 5312000, mean_episode_return = 45.08, mean_episode_step = 1782.3, total_loss = 23.316, pg_loss = 8.0664, baseline_loss = 20.966, entropy_loss = -5.7165, learner_queue_size = 32, train_seconds = 2964.7, _tick = 867, _time = 1.6546e+09)
[2022-06-07 11:13:16,694][root][INFO] - Step 5319680 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 5319680, mean_episode_return = 40.971, mean_episode_step = 2267.7, total_loss = 119.46, pg_loss = 76.387, baseline_loss = 48.991, entropy_loss = -5.9221, learner_queue_size = 32, train_seconds = 2969.7, _tick = 870, _time = 1.6546e+09)
[2022-06-07 11:13:21,700][root][INFO] - Step 5329920 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 5329920, mean_episode_return = None, mean_episode_step = 1948.5, total_loss = 23.455, pg_loss = 9.197, baseline_loss = 20.407, entropy_loss = -6.1492, learner_queue_size = 32, train_seconds = 2974.7, _tick = 872, _time = 1.6546e+09)
[2022-06-07 11:13:26,706][root][INFO] - Step 5337600 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5337600, mean_episode_return = None, mean_episode_step = 2044.2, total_loss = -55.797, pg_loss = -59.067, baseline_loss = 9.48, entropy_loss = -6.2105, learner_queue_size = 32, train_seconds = 2979.7, _tick = 874, _time = 1.6546e+09)
[2022-06-07 11:13:31,712][root][INFO] - Step 5347840 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5347840, mean_episode_return = 45.02, mean_episode_step = 2118.6, total_loss = 134.43, pg_loss = 95.909, baseline_loss = 44.603, entropy_loss = -6.0873, learner_queue_size = 32, train_seconds = 2984.7, _tick = 877, _time = 1.6546e+09)
[2022-06-07 11:13:36,718][root][INFO] - Step 5355520 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5355520, mean_episode_return = None, mean_episode_step = 2241.6, total_loss = -87.398, pg_loss = -82.994, baseline_loss = 1.4612, entropy_loss = -5.8645, learner_queue_size = 32, train_seconds = 2989.7, _tick = 878, _time = 1.6546e+09)
[2022-06-07 11:13:41,722][root][INFO] - Step 5365760 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 5365760, mean_episode_return = 6.5796, mean_episode_step = 1701.3, total_loss = -151.12, pg_loss = -152.52, baseline_loss = 7.1349, entropy_loss = -5.7343, learner_queue_size = 32, train_seconds = 2994.7, _tick = 881, _time = 1.6546e+09)
[2022-06-07 11:13:46,726][root][INFO] - Step 5373440 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 5373440, mean_episode_return = -31.371, mean_episode_step = 2447.6, total_loss = 23.288, pg_loss = 5.706, baseline_loss = 23.523, entropy_loss = -5.9408, learner_queue_size = 32, train_seconds = 2999.7, _tick = 883, _time = 1.6546e+09)
[2022-06-07 11:13:51,738][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 11:13:51,911][root][INFO] - Step 5383680 @ 2043.1 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 5383680, mean_episode_return = 66.849, mean_episode_step = 2058.5, total_loss = 11.862, pg_loss = -5.288, baseline_loss = 23.17, entropy_loss = -6.02, learner_queue_size = 32, train_seconds = 3004.7, _tick = 886, _time = 1.6546e+09)
[2022-06-07 11:13:56,917][root][INFO] - Step 5391360 @ 1483.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5391360, mean_episode_return = 101.98, mean_episode_step = 1804.0, total_loss = -106.15, pg_loss = -109.63, baseline_loss = 9.3873, entropy_loss = -5.9096, learner_queue_size = 32, train_seconds = 3009.9, _tick = 888, _time = 1.6546e+09)
[2022-06-07 11:14:01,933][root][INFO] - Step 5401600 @ 2041.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5401600, mean_episode_return = -5.0902, mean_episode_step = 1937.8, total_loss = 80.541, pg_loss = 28.461, baseline_loss = 58.169, entropy_loss = -6.0895, learner_queue_size = 32, train_seconds = 3014.9, _tick = 891, _time = 1.6546e+09)
[2022-06-07 11:14:06,938][root][INFO] - Step 5409280 @ 1534.6 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 5409280, mean_episode_return = None, mean_episode_step = 2224.8, total_loss = -30.012, pg_loss = -36.686, baseline_loss = 12.609, entropy_loss = -5.935, learner_queue_size = 32, train_seconds = 3019.9, _tick = 891, _time = 1.6546e+09)
[2022-06-07 11:14:11,942][root][INFO] - Step 5419520 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5419520, mean_episode_return = 44.272, mean_episode_step = 1891.9, total_loss = -93.007, pg_loss = -114.82, baseline_loss = 27.94, entropy_loss = -6.1303, learner_queue_size = 32, train_seconds = 3024.9, _tick = 894, _time = 1.6546e+09)
[2022-06-07 11:14:16,946][root][INFO] - Step 5429760 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 5429760, mean_episode_return = None, mean_episode_step = 1685.5, total_loss = 121.94, pg_loss = 83.824, baseline_loss = 44.239, entropy_loss = -6.1256, learner_queue_size = 32, train_seconds = 3029.9, _tick = 897, _time = 1.6546e+09)
[2022-06-07 11:14:21,950][root][INFO] - Step 5437440 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5437440, mean_episode_return = None, mean_episode_step = 2047.4, total_loss = -51.136, pg_loss = -58.119, baseline_loss = 13.173, entropy_loss = -6.1906, learner_queue_size = 32, train_seconds = 3034.9, _tick = 897, _time = 1.6546e+09)
[2022-06-07 11:14:26,954][root][INFO] - Step 5447680 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5447680, mean_episode_return = None, mean_episode_step = 2143.6, total_loss = -67.557, pg_loss = -62.968, baseline_loss = 1.4726, entropy_loss = -6.0611, learner_queue_size = 32, train_seconds = 3039.9, _tick = 897, _time = 1.6546e+09)
[2022-06-07 11:14:31,958][root][INFO] - Step 5455360 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5455360, mean_episode_return = None, mean_episode_step = 2179.8, total_loss = -87.062, pg_loss = -81.781, baseline_loss = 0.77476, entropy_loss = -6.056, learner_queue_size = 32, train_seconds = 3045.0, _tick = 898, _time = 1.6546e+09)
[2022-06-07 11:14:36,962][root][INFO] - Step 5465600 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 5465600, mean_episode_return = 46.063, mean_episode_step = 2476.6, total_loss = -59.172, pg_loss = -65.02, baseline_loss = 11.949, entropy_loss = -6.1012, learner_queue_size = 32, train_seconds = 3050.0, _tick = 901, _time = 1.6546e+09)
[2022-06-07 11:14:41,966][root][INFO] - Step 5473280 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 5473280, mean_episode_return = None, mean_episode_step = 2946.2, total_loss = 151.12, pg_loss = 125.08, baseline_loss = 32.213, entropy_loss = -6.1668, learner_queue_size = 32, train_seconds = 3055.0, _tick = 902, _time = 1.6546e+09)
[2022-06-07 11:14:46,970][root][INFO] - Step 5483520 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 5483520, mean_episode_return = None, mean_episode_step = 2569.4, total_loss = 188.19, pg_loss = 116.97, baseline_loss = 77.318, entropy_loss = -6.0914, learner_queue_size = 32, train_seconds = 3060.0, _tick = 904, _time = 1.6546e+09)
[2022-06-07 11:14:51,976][root][INFO] - Step 5491200 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 5491200, mean_episode_return = None, mean_episode_step = 2101.8, total_loss = 4.5114, pg_loss = -6.1727, baseline_loss = 16.843, entropy_loss = -6.1589, learner_queue_size = 32, train_seconds = 3065.0, _tick = 906, _time = 1.6546e+09)
[2022-06-07 11:14:56,982][root][INFO] - Step 5501440 @ 2045.5 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 5501440, mean_episode_return = None, mean_episode_step = 1963.5, total_loss = 26.057, pg_loss = 3.9253, baseline_loss = 28.194, entropy_loss = -6.0619, learner_queue_size = 32, train_seconds = 3070.0, _tick = 909, _time = 1.6546e+09)
[2022-06-07 11:15:01,988][root][INFO] - Step 5509120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5509120, mean_episode_return = 3.6795, mean_episode_step = 2440.8, total_loss = 52.627, pg_loss = 26.815, baseline_loss = 31.916, entropy_loss = -6.104, learner_queue_size = 32, train_seconds = 3075.0, _tick = 910, _time = 1.6546e+09)
[2022-06-07 11:15:06,994][root][INFO] - Step 5519360 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5519360, mean_episode_return = None, mean_episode_step = 2666.8, total_loss = -95.006, pg_loss = -93.727, baseline_loss = 4.7169, entropy_loss = -5.9962, learner_queue_size = 32, train_seconds = 3080.0, _tick = 911, _time = 1.6546e+09)
[2022-06-07 11:15:12,000][root][INFO] - Step 5527040 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5527040, mean_episode_return = 18.292, mean_episode_step = 1656.6, total_loss = 464.81, pg_loss = 309.38, baseline_loss = 161.44, entropy_loss = -6.0076, learner_queue_size = 32, train_seconds = 3085.0, _tick = 914, _time = 1.6546e+09)
[2022-06-07 11:15:17,006][root][INFO] - Step 5537280 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 5537280, mean_episode_return = 145.26, mean_episode_step = 2058.2, total_loss = 70.835, pg_loss = 44.496, baseline_loss = 32.276, entropy_loss = -5.9369, learner_queue_size = 32, train_seconds = 3090.0, _tick = 917, _time = 1.6546e+09)
[2022-06-07 11:15:22,010][root][INFO] - Step 5544960 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 5544960, mean_episode_return = None, mean_episode_step = 2099.9, total_loss = 79.957, pg_loss = 51.187, baseline_loss = 34.733, entropy_loss = -5.9623, learner_queue_size = 32, train_seconds = 3095.0, _tick = 919, _time = 1.6546e+09)
[2022-06-07 11:15:27,014][root][INFO] - Step 5552640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5552640, mean_episode_return = -12.18, mean_episode_step = 2131.0, total_loss = 342.26, pg_loss = 241.43, baseline_loss = 106.74, entropy_loss = -5.9121, learner_queue_size = 32, train_seconds = 3100.0, _tick = 920, _time = 1.6546e+09)
[2022-06-07 11:15:32,018][root][INFO] - Step 5562880 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5562880, mean_episode_return = None, mean_episode_step = 2217.2, total_loss = -84.615, pg_loss = -97.933, baseline_loss = 19.209, entropy_loss = -5.8908, learner_queue_size = 32, train_seconds = 3105.0, _tick = 921, _time = 1.6546e+09)
[2022-06-07 11:15:37,022][root][INFO] - Step 5570560 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 5570560, mean_episode_return = None, mean_episode_step = 2209.0, total_loss = 142.55, pg_loss = 96.16, baseline_loss = 52.312, entropy_loss = -5.9239, learner_queue_size = 32, train_seconds = 3110.0, _tick = 921, _time = 1.6546e+09)
[2022-06-07 11:15:42,028][root][INFO] - Step 5580800 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5580800, mean_episode_return = 65.403, mean_episode_step = 1872.4, total_loss = -170.67, pg_loss = -181.94, baseline_loss = 17.278, entropy_loss = -6.0059, learner_queue_size = 32, train_seconds = 3115.0, _tick = 923, _time = 1.6546e+09)
[2022-06-07 11:15:47,030][root][INFO] - Step 5591040 @ 2047.1 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 5591040, mean_episode_return = None, mean_episode_step = 2561.4, total_loss = 55.9, pg_loss = 27.853, baseline_loss = 34.039, entropy_loss = -5.9931, learner_queue_size = 32, train_seconds = 3120.0, _tick = 923, _time = 1.6546e+09)
[2022-06-07 11:15:52,034][root][INFO] - Step 5598720 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5598720, mean_episode_return = None, mean_episode_step = 2696.3, total_loss = 55.924, pg_loss = 27.131, baseline_loss = 34.727, entropy_loss = -5.934, learner_queue_size = 32, train_seconds = 3125.0, _tick = 923, _time = 1.6546e+09)
[2022-06-07 11:15:57,038][root][INFO] - Step 5608960 @ 2046.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 5608960, mean_episode_return = 17.245, mean_episode_step = 2335.4, total_loss = 10.171, pg_loss = -8.0315, baseline_loss = 24.139, entropy_loss = -5.9359, learner_queue_size = 32, train_seconds = 3130.0, _tick = 926, _time = 1.6546e+09)
[2022-06-07 11:16:02,044][root][INFO] - Step 5616640 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 5616640, mean_episode_return = 60.11, mean_episode_step = 2428.3, total_loss = -64.358, pg_loss = -120.78, baseline_loss = 62.54, entropy_loss = -6.1205, learner_queue_size = 32, train_seconds = 3135.0, _tick = 928, _time = 1.6546e+09)
[2022-06-07 11:16:07,046][root][INFO] - Step 5626880 @ 2047.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5626880, mean_episode_return = None, mean_episode_step = 2583.0, total_loss = -97.798, pg_loss = -93.073, baseline_loss = 1.1604, entropy_loss = -5.8849, learner_queue_size = 32, train_seconds = 3140.0, _tick = 930, _time = 1.6546e+09)
[2022-06-07 11:16:12,050][root][INFO] - Step 5634560 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 5634560, mean_episode_return = None, mean_episode_step = 2138.1, total_loss = 594.01, pg_loss = 456.76, baseline_loss = 143.15, entropy_loss = -5.892, learner_queue_size = 32, train_seconds = 3145.0, _tick = 931, _time = 1.6546e+09)
[2022-06-07 11:16:17,054][root][INFO] - Step 5644800 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 5644800, mean_episode_return = None, mean_episode_step = 2376.5, total_loss = 191.64, pg_loss = 135.98, baseline_loss = 61.539, entropy_loss = -5.8829, learner_queue_size = 32, train_seconds = 3150.0, _tick = 934, _time = 1.6546e+09)
[2022-06-07 11:16:22,058][root][INFO] - Step 5652480 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5652480, mean_episode_return = 48.635, mean_episode_step = 2247.6, total_loss = 320.52, pg_loss = 211.76, baseline_loss = 114.58, entropy_loss = -5.8135, learner_queue_size = 32, train_seconds = 3155.1, _tick = 935, _time = 1.6546e+09)
[2022-06-07 11:16:27,061][root][INFO] - Step 5662720 @ 2046.6 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 5662720, mean_episode_return = None, mean_episode_step = 2059.9, total_loss = 141.17, pg_loss = 106.53, baseline_loss = 40.565, entropy_loss = -5.9192, learner_queue_size = 32, train_seconds = 3160.1, _tick = 936, _time = 1.6546e+09)
[2022-06-07 11:16:32,066][root][INFO] - Step 5670400 @ 1534.5 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 5670400, mean_episode_return = 47.785, mean_episode_step = 2317.6, total_loss = 57.732, pg_loss = 1.5751, baseline_loss = 62.086, entropy_loss = -5.9295, learner_queue_size = 32, train_seconds = 3165.1, _tick = 937, _time = 1.6546e+09)
[2022-06-07 11:16:37,072][root][INFO] - Step 5680640 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5680640, mean_episode_return = 54.561, mean_episode_step = 1786.4, total_loss = 12.706, pg_loss = -14.662, baseline_loss = 32.984, entropy_loss = -5.6156, learner_queue_size = 32, train_seconds = 3170.1, _tick = 939, _time = 1.6546e+09)
[2022-06-07 11:16:42,078][root][INFO] - Step 5688320 @ 1534.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 5688320, mean_episode_return = None, mean_episode_step = 2913.1, total_loss = 169.73, pg_loss = 112.09, baseline_loss = 63.168, entropy_loss = -5.5261, learner_queue_size = 32, train_seconds = 3175.1, _tick = 940, _time = 1.6546e+09)
[2022-06-07 11:16:47,082][root][INFO] - Step 5698560 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5698560, mean_episode_return = None, mean_episode_step = 2330.8, total_loss = 112.69, pg_loss = 90.541, baseline_loss = 27.729, entropy_loss = -5.5839, learner_queue_size = 32, train_seconds = 3180.1, _tick = 942, _time = 1.6546e+09)
[2022-06-07 11:16:52,086][root][INFO] - Step 5706240 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 5706240, mean_episode_return = None, mean_episode_step = 2003.2, total_loss = 37.608, pg_loss = 27.24, baseline_loss = 15.966, entropy_loss = -5.5985, learner_queue_size = 32, train_seconds = 3185.1, _tick = 942, _time = 1.6546e+09)
[2022-06-07 11:16:57,092][root][INFO] - Step 5716480 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5716480, mean_episode_return = 53.312, mean_episode_step = 2953.4, total_loss = -64.294, pg_loss = -90.149, baseline_loss = 31.29, entropy_loss = -5.4353, learner_queue_size = 32, train_seconds = 3190.1, _tick = 946, _time = 1.6546e+09)
[2022-06-07 11:17:02,098][root][INFO] - Step 5724160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5724160, mean_episode_return = None, mean_episode_step = 2275.2, total_loss = 6.2256, pg_loss = -2.8288, baseline_loss = 14.397, entropy_loss = -5.343, learner_queue_size = 32, train_seconds = 3195.1, _tick = 947, _time = 1.6546e+09)
[2022-06-07 11:17:07,102][root][INFO] - Step 5734400 @ 2046.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 5734400, mean_episode_return = None, mean_episode_step = 2412.8, total_loss = 212.36, pg_loss = 159.49, baseline_loss = 58.208, entropy_loss = -5.3387, learner_queue_size = 32, train_seconds = 3200.1, _tick = 949, _time = 1.6546e+09)
[2022-06-07 11:17:12,108][root][INFO] - Step 5742080 @ 1534.1 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 5742080, mean_episode_return = 16.64, mean_episode_step = 1760.3, total_loss = 130.64, pg_loss = 73.487, baseline_loss = 62.576, entropy_loss = -5.4256, learner_queue_size = 32, train_seconds = 3205.1, _tick = 952, _time = 1.6546e+09)
[2022-06-07 11:17:17,114][root][INFO] - Step 5752320 @ 2045.6 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 5752320, mean_episode_return = 60.69, mean_episode_step = 2388.2, total_loss = 43.642, pg_loss = -1.3485, baseline_loss = 50.369, entropy_loss = -5.3785, learner_queue_size = 32, train_seconds = 3210.1, _tick = 954, _time = 1.6546e+09)
[2022-06-07 11:17:22,118][root][INFO] - Step 5760000 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 5760000, mean_episode_return = None, mean_episode_step = 2630.4, total_loss = -66.197, pg_loss = -68.823, baseline_loss = 7.9433, entropy_loss = -5.3177, learner_queue_size = 32, train_seconds = 3215.1, _tick = 955, _time = 1.6546e+09)
[2022-06-07 11:17:27,122][root][INFO] - Step 5770240 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 5770240, mean_episode_return = None, mean_episode_step = 2073.3, total_loss = 213.19, pg_loss = 107.39, baseline_loss = 111.09, entropy_loss = -5.2845, learner_queue_size = 32, train_seconds = 3220.1, _tick = 955, _time = 1.6546e+09)
[2022-06-07 11:17:32,128][root][INFO] - Step 5777920 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5777920, mean_episode_return = 26.143, mean_episode_step = 2708.8, total_loss = -117.19, pg_loss = -123.58, baseline_loss = 11.869, entropy_loss = -5.4808, learner_queue_size = 32, train_seconds = 3225.1, _tick = 956, _time = 1.6546e+09)
[2022-06-07 11:17:37,134][root][INFO] - Step 5788160 @ 2045.6 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 5788160, mean_episode_return = None, mean_episode_step = 2030.4, total_loss = 94.14, pg_loss = 34.819, baseline_loss = 64.799, entropy_loss = -5.4777, learner_queue_size = 32, train_seconds = 3230.1, _tick = 958, _time = 1.6546e+09)
[2022-06-07 11:17:42,140][root][INFO] - Step 5798400 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 5798400, mean_episode_return = None, mean_episode_step = 2297.2, total_loss = 289.28, pg_loss = 208.35, baseline_loss = 86.477, entropy_loss = -5.5416, learner_queue_size = 32, train_seconds = 3235.1, _tick = 958, _time = 1.6546e+09)
[2022-06-07 11:17:47,142][root][INFO] - Step 5806080 @ 1535.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 5806080, mean_episode_return = None, mean_episode_step = 2277.8, total_loss = 166.1, pg_loss = 82.113, baseline_loss = 89.516, entropy_loss = -5.5273, learner_queue_size = 32, train_seconds = 3240.1, _tick = 958, _time = 1.6546e+09)
[2022-06-07 11:17:52,148][root][INFO] - Step 5813760 @ 1534.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 5813760, mean_episode_return = -1.11, mean_episode_step = 2100.1, total_loss = -154.21, pg_loss = -159.79, baseline_loss = 10.936, entropy_loss = -5.3544, learner_queue_size = 32, train_seconds = 3245.1, _tick = 959, _time = 1.6546e+09)
[2022-06-07 11:17:57,151][root][INFO] - Step 5824000 @ 2046.7 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 5824000, mean_episode_return = 99.687, mean_episode_step = 2106.3, total_loss = -119.61, pg_loss = -154.54, baseline_loss = 40.289, entropy_loss = -5.3611, learner_queue_size = 32, train_seconds = 3250.1, _tick = 962, _time = 1.6546e+09)
[2022-06-07 11:18:02,157][root][INFO] - Step 5834240 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 5834240, mean_episode_return = None, mean_episode_step = 2644.5, total_loss = 30.524, pg_loss = -1.699, baseline_loss = 37.534, entropy_loss = -5.3114, learner_queue_size = 32, train_seconds = 3255.2, _tick = 964, _time = 1.6546e+09)
[2022-06-07 11:18:07,162][root][INFO] - Step 5841920 @ 1534.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 5841920, mean_episode_return = None, mean_episode_step = 2265.4, total_loss = 121.09, pg_loss = 81.802, baseline_loss = 44.634, entropy_loss = -5.3454, learner_queue_size = 32, train_seconds = 3260.2, _tick = 965, _time = 1.6546e+09)
[2022-06-07 11:18:12,166][root][INFO] - Step 5852160 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 5852160, mean_episode_return = None, mean_episode_step = 2497.0, total_loss = 77.591, pg_loss = 50.17, baseline_loss = 32.902, entropy_loss = -5.4806, learner_queue_size = 32, train_seconds = 3265.2, _tick = 966, _time = 1.6546e+09)
[2022-06-07 11:18:17,172][root][INFO] - Step 5859840 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5859840, mean_episode_return = None, mean_episode_step = 2303.7, total_loss = -77.415, pg_loss = -90.205, baseline_loss = 18.317, entropy_loss = -5.5263, learner_queue_size = 32, train_seconds = 3270.2, _tick = 966, _time = 1.6546e+09)
[2022-06-07 11:18:22,178][root][INFO] - Step 5870080 @ 2045.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 5870080, mean_episode_return = None, mean_episode_step = 2510.5, total_loss = 18.956, pg_loss = -9.538, baseline_loss = 33.932, entropy_loss = -5.4378, learner_queue_size = 32, train_seconds = 3275.2, _tick = 968, _time = 1.6546e+09)
[2022-06-07 11:18:27,182][root][INFO] - Step 5877760 @ 1534.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 5877760, mean_episode_return = -7.1605, mean_episode_step = 1952.2, total_loss = -171.47, pg_loss = -173.39, baseline_loss = 7.565, entropy_loss = -5.6441, learner_queue_size = 32, train_seconds = 3280.2, _tick = 969, _time = 1.6546e+09)
[2022-06-07 11:18:32,186][root][INFO] - Step 5888000 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 5888000, mean_episode_return = 49.74, mean_episode_step = 2431.2, total_loss = -25.066, pg_loss = -47.828, baseline_loss = 28.421, entropy_loss = -5.6596, learner_queue_size = 32, train_seconds = 3285.2, _tick = 972, _time = 1.6546e+09)
[2022-06-07 11:18:37,193][root][INFO] - Step 5895680 @ 1533.9 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 5895680, mean_episode_return = 31.28, mean_episode_step = 1958.3, total_loss = 95.295, pg_loss = 29.061, baseline_loss = 71.932, entropy_loss = -5.698, learner_queue_size = 32, train_seconds = 3290.2, _tick = 974, _time = 1.6546e+09)
[2022-06-07 11:18:42,198][root][INFO] - Step 5903360 @ 1534.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 5903360, mean_episode_return = 13.83, mean_episode_step = 2235.1, total_loss = -33.548, pg_loss = -58.133, baseline_loss = 30.197, entropy_loss = -5.6115, learner_queue_size = 32, train_seconds = 3295.2, _tick = 976, _time = 1.6546e+09)
[2022-06-07 11:18:47,202][root][INFO] - Step 5913600 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 5913600, mean_episode_return = 67.889, mean_episode_step = 2446.8, total_loss = -36.717, pg_loss = -67.255, baseline_loss = 36.203, entropy_loss = -5.6648, learner_queue_size = 32, train_seconds = 3300.2, _tick = 979, _time = 1.6546e+09)
[2022-06-07 11:18:52,206][root][INFO] - Step 5921280 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 5921280, mean_episode_return = 42.712, mean_episode_step = 2622.5, total_loss = -122.79, pg_loss = -137.13, baseline_loss = 20.061, entropy_loss = -5.7223, learner_queue_size = 32, train_seconds = 3305.2, _tick = 982, _time = 1.6546e+09)
[2022-06-07 11:18:57,210][root][INFO] - Step 5931520 @ 2046.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 5931520, mean_episode_return = None, mean_episode_step = 2296.7, total_loss = -0.29326, pg_loss = -11.184, baseline_loss = 16.631, entropy_loss = -5.74, learner_queue_size = 32, train_seconds = 3310.2, _tick = 984, _time = 1.6546e+09)
[2022-06-07 11:19:02,216][root][INFO] - Step 5941760 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 5941760, mean_episode_return = None, mean_episode_step = 2238.2, total_loss = -49.487, pg_loss = -56.809, baseline_loss = 12.898, entropy_loss = -5.5761, learner_queue_size = 32, train_seconds = 3315.2, _tick = 986, _time = 1.6546e+09)
[2022-06-07 11:19:07,222][root][INFO] - Step 5949440 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 5949440, mean_episode_return = 73.003, mean_episode_step = 1861.0, total_loss = 50.678, pg_loss = 17.491, baseline_loss = 38.764, entropy_loss = -5.5771, learner_queue_size = 32, train_seconds = 3320.2, _tick = 987, _time = 1.6546e+09)
[2022-06-07 11:19:12,226][root][INFO] - Step 5959680 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 5959680, mean_episode_return = None, mean_episode_step = 2194.2, total_loss = -154.68, pg_loss = -151.07, baseline_loss = 1.9996, entropy_loss = -5.6096, learner_queue_size = 32, train_seconds = 3325.2, _tick = 988, _time = 1.6546e+09)
[2022-06-07 11:19:17,230][root][INFO] - Step 5967360 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 5967360, mean_episode_return = 56.831, mean_episode_step = 2897.5, total_loss = 79.627, pg_loss = 49.48, baseline_loss = 35.715, entropy_loss = -5.5683, learner_queue_size = 32, train_seconds = 3330.2, _tick = 990, _time = 1.6546e+09)
[2022-06-07 11:19:22,234][root][INFO] - Step 5977600 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 5977600, mean_episode_return = None, mean_episode_step = 1799.1, total_loss = 22.795, pg_loss = 0.67726, baseline_loss = 27.678, entropy_loss = -5.56, learner_queue_size = 32, train_seconds = 3335.2, _tick = 993, _time = 1.6546e+09)
[2022-06-07 11:19:27,238][root][INFO] - Step 5985280 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 5985280, mean_episode_return = None, mean_episode_step = 2797.4, total_loss = -4.8019, pg_loss = -12.696, baseline_loss = 13.449, entropy_loss = -5.5554, learner_queue_size = 32, train_seconds = 3340.2, _tick = 995, _time = 1.6546e+09)
[2022-06-07 11:19:32,242][root][INFO] - Step 5995520 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 5995520, mean_episode_return = None, mean_episode_step = 2289.6, total_loss = 425.36, pg_loss = 329.95, baseline_loss = 100.93, entropy_loss = -5.5325, learner_queue_size = 32, train_seconds = 3345.2, _tick = 995, _time = 1.6546e+09)
[2022-06-07 11:19:37,248][root][INFO] - Step 6003200 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6003200, mean_episode_return = 60.355, mean_episode_step = 2271.9, total_loss = 181.92, pg_loss = 125.38, baseline_loss = 62.083, entropy_loss = -5.5455, learner_queue_size = 32, train_seconds = 3350.2, _tick = 997, _time = 1.6546e+09)
[2022-06-07 11:19:42,254][root][INFO] - Step 6013440 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6013440, mean_episode_return = 33.63, mean_episode_step = 2073.9, total_loss = -50.168, pg_loss = -61.012, baseline_loss = 16.471, entropy_loss = -5.6261, learner_queue_size = 32, train_seconds = 3355.2, _tick = 1000, _time = 1.6546e+09)
[2022-06-07 11:19:47,258][root][INFO] - Step 6021120 @ 1534.7 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 6021120, mean_episode_return = 73.073, mean_episode_step = 2036.0, total_loss = -124.19, pg_loss = -121.77, baseline_loss = 3.2653, entropy_loss = -5.686, learner_queue_size = 32, train_seconds = 3360.3, _tick = 1003, _time = 1.6546e+09)
[2022-06-07 11:19:52,262][root][INFO] - Step 6031360 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 6031360, mean_episode_return = -0.2206, mean_episode_step = 2352.2, total_loss = 51.388, pg_loss = 23.904, baseline_loss = 33.148, entropy_loss = -5.6637, learner_queue_size = 32, train_seconds = 3365.3, _tick = 1006, _time = 1.6546e+09)
[2022-06-07 11:19:57,266][root][INFO] - Step 6039040 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6039040, mean_episode_return = None, mean_episode_step = 2304.7, total_loss = 22.016, pg_loss = -15.541, baseline_loss = 43.112, entropy_loss = -5.5545, learner_queue_size = 32, train_seconds = 3370.3, _tick = 1008, _time = 1.6546e+09)
[2022-06-07 11:20:02,272][root][INFO] - Step 6049280 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6049280, mean_episode_return = None, mean_episode_step = 1849.6, total_loss = 61.956, pg_loss = 37.678, baseline_loss = 29.624, entropy_loss = -5.3465, learner_queue_size = 32, train_seconds = 3375.3, _tick = 1010, _time = 1.6546e+09)
[2022-06-07 11:20:07,278][root][INFO] - Step 6056960 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6056960, mean_episode_return = None, mean_episode_step = 2864.4, total_loss = -52.848, pg_loss = -58.624, baseline_loss = 11.151, entropy_loss = -5.3751, learner_queue_size = 32, train_seconds = 3380.3, _tick = 1011, _time = 1.6546e+09)
[2022-06-07 11:20:12,282][root][INFO] - Step 6067200 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 6067200, mean_episode_return = None, mean_episode_step = 1888.1, total_loss = 80.327, pg_loss = 48.66, baseline_loss = 36.86, entropy_loss = -5.1934, learner_queue_size = 32, train_seconds = 3385.3, _tick = 1013, _time = 1.6546e+09)
[2022-06-07 11:20:17,288][root][INFO] - Step 6074880 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6074880, mean_episode_return = 71.188, mean_episode_step = 2586.6, total_loss = 23.695, pg_loss = -4.5167, baseline_loss = 33.411, entropy_loss = -5.1992, learner_queue_size = 32, train_seconds = 3390.3, _tick = 1015, _time = 1.6546e+09)
[2022-06-07 11:20:22,294][root][INFO] - Step 6085120 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 6085120, mean_episode_return = 0.84453, mean_episode_step = 2021.5, total_loss = 184.6, pg_loss = 63.554, baseline_loss = 126.08, entropy_loss = -5.0392, learner_queue_size = 32, train_seconds = 3395.3, _tick = 1017, _time = 1.6546e+09)
[2022-06-07 11:20:27,300][root][INFO] - Step 6092800 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 6092800, mean_episode_return = 6.8896, mean_episode_step = 2689.0, total_loss = -20.102, pg_loss = -35.937, baseline_loss = 20.918, entropy_loss = -5.0835, learner_queue_size = 32, train_seconds = 3400.3, _tick = 1019, _time = 1.6546e+09)
[2022-06-07 11:20:32,306][root][INFO] - Step 6103040 @ 2045.6 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 6103040, mean_episode_return = 27.77, mean_episode_step = 2047.7, total_loss = 178.99, pg_loss = 111.35, baseline_loss = 72.673, entropy_loss = -5.0282, learner_queue_size = 32, train_seconds = 3405.3, _tick = 1023, _time = 1.6546e+09)
[2022-06-07 11:20:37,310][root][INFO] - Step 6110720 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 6110720, mean_episode_return = 73.907, mean_episode_step = 1843.4, total_loss = 318.93, pg_loss = 211.02, baseline_loss = 113.08, entropy_loss = -5.1707, learner_queue_size = 32, train_seconds = 3410.3, _tick = 1025, _time = 1.6546e+09)
[2022-06-07 11:20:42,314][root][INFO] - Step 6120960 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 6120960, mean_episode_return = None, mean_episode_step = 1585.6, total_loss = 550.05, pg_loss = 316.51, baseline_loss = 238.89, entropy_loss = -5.353, learner_queue_size = 32, train_seconds = 3415.3, _tick = 1027, _time = 1.6546e+09)
[2022-06-07 11:20:47,320][root][INFO] - Step 6128640 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 6128640, mean_episode_return = 19.98, mean_episode_step = 2443.3, total_loss = 17.908, pg_loss = -27.113, baseline_loss = 50.407, entropy_loss = -5.3864, learner_queue_size = 32, train_seconds = 3420.3, _tick = 1029, _time = 1.6546e+09)
[2022-06-07 11:20:52,326][root][INFO] - Step 6138880 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 6138880, mean_episode_return = 15.314, mean_episode_step = 2879.2, total_loss = -154.04, pg_loss = -175.86, baseline_loss = 26.94, entropy_loss = -5.1199, learner_queue_size = 32, train_seconds = 3425.3, _tick = 1033, _time = 1.6546e+09)
[2022-06-07 11:20:57,333][root][INFO] - Step 6146560 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 6146560, mean_episode_return = None, mean_episode_step = 2722.5, total_loss = 2.2961, pg_loss = -13.429, baseline_loss = 20.907, entropy_loss = -5.182, learner_queue_size = 32, train_seconds = 3430.3, _tick = 1035, _time = 1.6546e+09)
[2022-06-07 11:21:02,338][root][INFO] - Step 6156800 @ 2045.7 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 6156800, mean_episode_return = None, mean_episode_step = 1960.4, total_loss = 127.41, pg_loss = 54.134, baseline_loss = 78.216, entropy_loss = -4.9369, learner_queue_size = 32, train_seconds = 3435.3, _tick = 1035, _time = 1.6546e+09)
[2022-06-07 11:21:07,344][root][INFO] - Step 6164480 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 6164480, mean_episode_return = None, mean_episode_step = 2409.5, total_loss = -0.46185, pg_loss = -23.103, baseline_loss = 27.291, entropy_loss = -4.6492, learner_queue_size = 32, train_seconds = 3440.3, _tick = 1036, _time = 1.6546e+09)
[2022-06-07 11:21:12,350][root][INFO] - Step 6174720 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6174720, mean_episode_return = None, mean_episode_step = 2182.1, total_loss = 184.61, pg_loss = 115.37, baseline_loss = 73.901, entropy_loss = -4.6616, learner_queue_size = 32, train_seconds = 3445.3, _tick = 1038, _time = 1.6546e+09)
[2022-06-07 11:21:17,356][root][INFO] - Step 6182400 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6182400, mean_episode_return = None, mean_episode_step = 2023.3, total_loss = 594.98, pg_loss = 418.96, baseline_loss = 180.51, entropy_loss = -4.4857, learner_queue_size = 32, train_seconds = 3450.4, _tick = 1040, _time = 1.6546e+09)
[2022-06-07 11:21:22,362][root][INFO] - Step 6192640 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 6192640, mean_episode_return = 11.479, mean_episode_step = 2290.2, total_loss = 211.85, pg_loss = 139.16, baseline_loss = 77.2, entropy_loss = -4.5159, learner_queue_size = 32, train_seconds = 3455.4, _tick = 1043, _time = 1.6546e+09)
[2022-06-07 11:21:27,366][root][INFO] - Step 6200320 @ 1534.9 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 6200320, mean_episode_return = None, mean_episode_step = 2392.3, total_loss = 124.59, pg_loss = 66.532, baseline_loss = 62.768, entropy_loss = -4.7125, learner_queue_size = 32, train_seconds = 3460.4, _tick = 1044, _time = 1.6546e+09)
[2022-06-07 11:21:32,370][root][INFO] - Step 6210560 @ 2046.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 6210560, mean_episode_return = 38.97, mean_episode_step = 2368.4, total_loss = -113.52, pg_loss = -138.36, baseline_loss = 29.367, entropy_loss = -4.5332, learner_queue_size = 32, train_seconds = 3465.4, _tick = 1045, _time = 1.6546e+09)
[2022-06-07 11:21:37,374][root][INFO] - Step 6218240 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6218240, mean_episode_return = None, mean_episode_step = 2055.6, total_loss = -152.01, pg_loss = -151.42, baseline_loss = 3.9852, entropy_loss = -4.5741, learner_queue_size = 32, train_seconds = 3470.4, _tick = 1046, _time = 1.6546e+09)
[2022-06-07 11:21:42,378][root][INFO] - Step 6228480 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 6228480, mean_episode_return = None, mean_episode_step = 2578.6, total_loss = 211.42, pg_loss = 162.44, baseline_loss = 53.658, entropy_loss = -4.6797, learner_queue_size = 32, train_seconds = 3475.4, _tick = 1046, _time = 1.6546e+09)
[2022-06-07 11:21:47,382][root][INFO] - Step 6238720 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6238720, mean_episode_return = None, mean_episode_step = 2726.1, total_loss = -71.09, pg_loss = -80.714, baseline_loss = 14.334, entropy_loss = -4.7099, learner_queue_size = 32, train_seconds = 3480.4, _tick = 1048, _time = 1.6546e+09)
[2022-06-07 11:21:52,386][root][INFO] - Step 6246400 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 6246400, mean_episode_return = 116.85, mean_episode_step = 2372.8, total_loss = -13.397, pg_loss = -57.768, baseline_loss = 49.082, entropy_loss = -4.7112, learner_queue_size = 32, train_seconds = 3485.4, _tick = 1051, _time = 1.6546e+09)
[2022-06-07 11:21:57,390][root][INFO] - Step 6256640 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6256640, mean_episode_return = 42.351, mean_episode_step = 2630.1, total_loss = -57.202, pg_loss = -89.196, baseline_loss = 36.783, entropy_loss = -4.7885, learner_queue_size = 32, train_seconds = 3490.4, _tick = 1055, _time = 1.6546e+09)
[2022-06-07 11:22:02,396][root][INFO] - Step 6264320 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 6264320, mean_episode_return = 78.456, mean_episode_step = 2078.0, total_loss = 459.7, pg_loss = 288.19, baseline_loss = 176.37, entropy_loss = -4.8563, learner_queue_size = 32, train_seconds = 3495.4, _tick = 1058, _time = 1.6546e+09)
[2022-06-07 11:22:07,402][root][INFO] - Step 6274560 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 6274560, mean_episode_return = 62.523, mean_episode_step = 2425.5, total_loss = -54.8, pg_loss = -77.957, baseline_loss = 27.874, entropy_loss = -4.7165, learner_queue_size = 32, train_seconds = 3500.4, _tick = 1061, _time = 1.6546e+09)
[2022-06-07 11:22:12,408][root][INFO] - Step 6282240 @ 1534.1 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 6282240, mean_episode_return = 73.22, mean_episode_step = 2108.0, total_loss = -41.166, pg_loss = -60.774, baseline_loss = 24.458, entropy_loss = -4.8507, learner_queue_size = 32, train_seconds = 3505.4, _tick = 1063, _time = 1.6546e+09)
[2022-06-07 11:22:17,414][root][INFO] - Step 6292480 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 6292480, mean_episode_return = None, mean_episode_step = 2222.6, total_loss = -13.696, pg_loss = -32.6, baseline_loss = 23.774, entropy_loss = -4.8704, learner_queue_size = 32, train_seconds = 3510.4, _tick = 1066, _time = 1.6546e+09)
[2022-06-07 11:22:22,418][root][INFO] - Step 6300160 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 6300160, mean_episode_return = None, mean_episode_step = 2243.2, total_loss = 30.784, pg_loss = -2.0267, baseline_loss = 37.814, entropy_loss = -5.003, learner_queue_size = 32, train_seconds = 3515.4, _tick = 1066, _time = 1.6546e+09)
[2022-06-07 11:22:27,424][root][INFO] - Step 6310400 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 6310400, mean_episode_return = None, mean_episode_step = 1992.1, total_loss = 205.16, pg_loss = 142.29, baseline_loss = 67.907, entropy_loss = -5.031, learner_queue_size = 32, train_seconds = 3520.4, _tick = 1068, _time = 1.6546e+09)
[2022-06-07 11:22:32,430][root][INFO] - Step 6318080 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6318080, mean_episode_return = None, mean_episode_step = 1616.1, total_loss = 169.25, pg_loss = 107.98, baseline_loss = 66.249, entropy_loss = -4.973, learner_queue_size = 32, train_seconds = 3525.4, _tick = 1069, _time = 1.6546e+09)
[2022-06-07 11:22:37,434][root][INFO] - Step 6328320 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 6328320, mean_episode_return = None, mean_episode_step = 1923.7, total_loss = 248.38, pg_loss = 179.88, baseline_loss = 73.453, entropy_loss = -4.9508, learner_queue_size = 32, train_seconds = 3530.4, _tick = 1070, _time = 1.6546e+09)
[2022-06-07 11:22:42,438][root][INFO] - Step 6336000 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 6336000, mean_episode_return = 26.325, mean_episode_step = 2233.0, total_loss = 168.09, pg_loss = 108.96, baseline_loss = 63.984, entropy_loss = -4.8455, learner_queue_size = 32, train_seconds = 3535.4, _tick = 1072, _time = 1.6546e+09)
[2022-06-07 11:22:47,442][root][INFO] - Step 6346240 @ 2046.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 6346240, mean_episode_return = None, mean_episode_step = 1923.2, total_loss = -54.849, pg_loss = -64.351, baseline_loss = 14.393, entropy_loss = -4.8919, learner_queue_size = 32, train_seconds = 3540.4, _tick = 1074, _time = 1.6546e+09)
[2022-06-07 11:22:52,448][root][INFO] - Step 6353920 @ 1534.1 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 6353920, mean_episode_return = None, mean_episode_step = 2311.8, total_loss = -113.3, pg_loss = -112.38, baseline_loss = 4.2086, entropy_loss = -5.1308, learner_queue_size = 32, train_seconds = 3545.4, _tick = 1075, _time = 1.6546e+09)
[2022-06-07 11:22:57,454][root][INFO] - Step 6364160 @ 2045.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 6364160, mean_episode_return = 60.939, mean_episode_step = 1780.9, total_loss = 176.73, pg_loss = 104.17, baseline_loss = 77.568, entropy_loss = -5.0119, learner_queue_size = 32, train_seconds = 3550.4, _tick = 1078, _time = 1.6546e+09)
[2022-06-07 11:23:02,458][root][INFO] - Step 6374400 @ 2046.6 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 6374400, mean_episode_return = 78.358, mean_episode_step = 2077.8, total_loss = 249.56, pg_loss = 134.41, baseline_loss = 120.0, entropy_loss = -4.8484, learner_queue_size = 32, train_seconds = 3555.5, _tick = 1082, _time = 1.6546e+09)
[2022-06-07 11:23:07,461][root][INFO] - Step 6382080 @ 1534.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6382080, mean_episode_return = None, mean_episode_step = 2104.0, total_loss = 34.683, pg_loss = 9.0292, baseline_loss = 30.551, entropy_loss = -4.8973, learner_queue_size = 32, train_seconds = 3560.5, _tick = 1084, _time = 1.6546e+09)
[2022-06-07 11:23:12,467][root][INFO] - Step 6389760 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 6389760, mean_episode_return = None, mean_episode_step = 2368.0, total_loss = -18.035, pg_loss = -35.547, baseline_loss = 22.356, entropy_loss = -4.8441, learner_queue_size = 32, train_seconds = 3565.5, _tick = 1085, _time = 1.6546e+09)
[2022-06-07 11:23:17,470][root][INFO] - Step 6400000 @ 2046.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6400000, mean_episode_return = -8.2504, mean_episode_step = 1906.1, total_loss = -159.9, pg_loss = -157.7, baseline_loss = 2.5702, entropy_loss = -4.7738, learner_queue_size = 32, train_seconds = 3570.5, _tick = 1087, _time = 1.6546e+09)
[2022-06-07 11:23:22,474][root][INFO] - Step 6410240 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 6410240, mean_episode_return = 6.6898, mean_episode_step = 1788.4, total_loss = 315.31, pg_loss = 171.24, baseline_loss = 148.81, entropy_loss = -4.7356, learner_queue_size = 32, train_seconds = 3575.5, _tick = 1090, _time = 1.6546e+09)
[2022-06-07 11:23:27,478][root][INFO] - Step 6417920 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 6417920, mean_episode_return = None, mean_episode_step = 1701.2, total_loss = 234.81, pg_loss = 137.72, baseline_loss = 101.78, entropy_loss = -4.6857, learner_queue_size = 32, train_seconds = 3580.5, _tick = 1091, _time = 1.6546e+09)
[2022-06-07 11:23:32,482][root][INFO] - Step 6428160 @ 2046.3 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 6428160, mean_episode_return = 62.78, mean_episode_step = 2219.1, total_loss = -106.9, pg_loss = -123.19, baseline_loss = 21.08, entropy_loss = -4.792, learner_queue_size = 32, train_seconds = 3585.5, _tick = 1093, _time = 1.6546e+09)
[2022-06-07 11:23:37,486][root][INFO] - Step 6435840 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 6435840, mean_episode_return = None, mean_episode_step = 2092.5, total_loss = 271.67, pg_loss = 174.38, baseline_loss = 102.15, entropy_loss = -4.8674, learner_queue_size = 32, train_seconds = 3590.5, _tick = 1094, _time = 1.6546e+09)
[2022-06-07 11:23:42,490][root][INFO] - Step 6446080 @ 2046.3 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 6446080, mean_episode_return = 61.922, mean_episode_step = 2496.7, total_loss = -117.89, pg_loss = -142.09, baseline_loss = 29.054, entropy_loss = -4.8575, learner_queue_size = 32, train_seconds = 3595.5, _tick = 1097, _time = 1.6546e+09)
[2022-06-07 11:23:47,502][root][INFO] - Step 6453760 @ 1532.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6453760, mean_episode_return = None, mean_episode_step = 2427.7, total_loss = 2.4128, pg_loss = -9.4214, baseline_loss = 16.764, entropy_loss = -4.9295, learner_queue_size = 32, train_seconds = 3600.5, _tick = 1098, _time = 1.6546e+09)
[2022-06-07 11:23:52,509][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 11:23:52,698][root][INFO] - Step 6464000 @ 2045.2 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 6464000, mean_episode_return = None, mean_episode_step = 1838.8, total_loss = 106.71, pg_loss = 80.193, baseline_loss = 31.265, entropy_loss = -4.7512, learner_queue_size = 32, train_seconds = 3605.5, _tick = 1100, _time = 1.6546e+09)
[2022-06-07 11:23:57,702][root][INFO] - Step 6471680 @ 1478.9 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 6471680, mean_episode_return = -1.8203, mean_episode_step = 2276.3, total_loss = 190.15, pg_loss = 149.29, baseline_loss = 45.667, entropy_loss = -4.8122, learner_queue_size = 32, train_seconds = 3610.7, _tick = 1101, _time = 1.6546e+09)
[2022-06-07 11:24:02,706][root][INFO] - Step 6481920 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 6481920, mean_episode_return = 17.27, mean_episode_step = 2061.4, total_loss = 297.24, pg_loss = 158.52, baseline_loss = 143.47, entropy_loss = -4.7571, learner_queue_size = 32, train_seconds = 3615.7, _tick = 1103, _time = 1.6546e+09)
[2022-06-07 11:24:07,710][root][INFO] - Step 6489600 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 6489600, mean_episode_return = None, mean_episode_step = 2572.5, total_loss = 39.62, pg_loss = 10.749, baseline_loss = 33.809, entropy_loss = -4.9378, learner_queue_size = 32, train_seconds = 3620.7, _tick = 1105, _time = 1.6546e+09)
[2022-06-07 11:24:12,714][root][INFO] - Step 6499840 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 6499840, mean_episode_return = None, mean_episode_step = 2358.3, total_loss = 1.5044, pg_loss = -5.234, baseline_loss = 11.746, entropy_loss = -5.0075, learner_queue_size = 32, train_seconds = 3625.7, _tick = 1106, _time = 1.6546e+09)
[2022-06-07 11:24:17,718][root][INFO] - Step 6507520 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6507520, mean_episode_return = None, mean_episode_step = 2116.3, total_loss = 29.145, pg_loss = 11.856, baseline_loss = 22.403, entropy_loss = -5.1139, learner_queue_size = 32, train_seconds = 3630.7, _tick = 1106, _time = 1.6546e+09)
[2022-06-07 11:24:22,722][root][INFO] - Step 6517760 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6517760, mean_episode_return = 91.033, mean_episode_step = 2767.1, total_loss = -33.154, pg_loss = -101.19, baseline_loss = 73.193, entropy_loss = -5.1619, learner_queue_size = 32, train_seconds = 3635.7, _tick = 1109, _time = 1.6546e+09)
[2022-06-07 11:24:27,726][root][INFO] - Step 6525440 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 6525440, mean_episode_return = 81.938, mean_episode_step = 2144.9, total_loss = -98.336, pg_loss = -99.792, baseline_loss = 6.4195, entropy_loss = -4.9638, learner_queue_size = 32, train_seconds = 3640.7, _tick = 1111, _time = 1.6546e+09)
[2022-06-07 11:24:32,730][root][INFO] - Step 6535680 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 6535680, mean_episode_return = -49.698, mean_episode_step = 2490.6, total_loss = 123.64, pg_loss = 26.787, baseline_loss = 101.73, entropy_loss = -4.8725, learner_queue_size = 32, train_seconds = 3645.7, _tick = 1114, _time = 1.6546e+09)
[2022-06-07 11:24:37,734][root][INFO] - Step 6543360 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 6543360, mean_episode_return = None, mean_episode_step = 1439.2, total_loss = 485.26, pg_loss = 331.87, baseline_loss = 158.53, entropy_loss = -5.1339, learner_queue_size = 32, train_seconds = 3650.7, _tick = 1116, _time = 1.6546e+09)
[2022-06-07 11:24:42,740][root][INFO] - Step 6553600 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 6553600, mean_episode_return = 133.76, mean_episode_step = 2072.4, total_loss = 101.53, pg_loss = 45.196, baseline_loss = 61.695, entropy_loss = -5.3605, learner_queue_size = 32, train_seconds = 3655.7, _tick = 1119, _time = 1.6546e+09)
[2022-06-07 11:24:47,746][root][INFO] - Step 6561280 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 6561280, mean_episode_return = 18.01, mean_episode_step = 1633.3, total_loss = 282.1, pg_loss = 174.25, baseline_loss = 113.08, entropy_loss = -5.2276, learner_queue_size = 32, train_seconds = 3660.7, _tick = 1122, _time = 1.6546e+09)
[2022-06-07 11:24:52,750][root][INFO] - Step 6568960 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 6568960, mean_episode_return = None, mean_episode_step = 2357.1, total_loss = -33.195, pg_loss = -38.631, baseline_loss = 10.456, entropy_loss = -5.02, learner_queue_size = 32, train_seconds = 3665.7, _tick = 1124, _time = 1.6546e+09)
[2022-06-07 11:24:57,756][root][INFO] - Step 6579200 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6579200, mean_episode_return = 63.335, mean_episode_step = 2107.0, total_loss = -83.186, pg_loss = -91.564, baseline_loss = 13.493, entropy_loss = -5.1153, learner_queue_size = 32, train_seconds = 3670.8, _tick = 1128, _time = 1.6546e+09)
[2022-06-07 11:25:02,762][root][INFO] - Step 6589440 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6589440, mean_episode_return = 21.07, mean_episode_step = 1672.2, total_loss = 358.18, pg_loss = 241.39, baseline_loss = 121.69, entropy_loss = -4.9001, learner_queue_size = 32, train_seconds = 3675.8, _tick = 1132, _time = 1.6546e+09)
[2022-06-07 11:25:07,766][root][INFO] - Step 6597120 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6597120, mean_episode_return = 26.523, mean_episode_step = 1730.6, total_loss = 3.068, pg_loss = -40.696, baseline_loss = 48.767, entropy_loss = -5.0033, learner_queue_size = 32, train_seconds = 3680.8, _tick = 1135, _time = 1.6546e+09)
[2022-06-07 11:25:12,772][root][INFO] - Step 6607360 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6607360, mean_episode_return = 56.222, mean_episode_step = 1367.1, total_loss = 235.56, pg_loss = 149.39, baseline_loss = 91.27, entropy_loss = -5.1065, learner_queue_size = 32, train_seconds = 3685.8, _tick = 1138, _time = 1.6546e+09)
[2022-06-07 11:25:17,778][root][INFO] - Step 6615040 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 6615040, mean_episode_return = -33.371, mean_episode_step = 1925.8, total_loss = -23.339, pg_loss = -52.909, baseline_loss = 34.806, entropy_loss = -5.2352, learner_queue_size = 32, train_seconds = 3690.8, _tick = 1141, _time = 1.6546e+09)
[2022-06-07 11:25:22,784][root][INFO] - Step 6622720 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 6622720, mean_episode_return = None, mean_episode_step = 1658.3, total_loss = 328.07, pg_loss = 245.26, baseline_loss = 87.917, entropy_loss = -5.115, learner_queue_size = 32, train_seconds = 3695.8, _tick = 1143, _time = 1.6546e+09)
[2022-06-07 11:25:27,790][root][INFO] - Step 6632960 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 6632960, mean_episode_return = 195.74, mean_episode_step = 2056.7, total_loss = -59.013, pg_loss = -79.553, baseline_loss = 25.672, entropy_loss = -5.1323, learner_queue_size = 32, train_seconds = 3700.8, _tick = 1146, _time = 1.6546e+09)
[2022-06-07 11:25:32,794][root][INFO] - Step 6640640 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 6640640, mean_episode_return = 103.33, mean_episode_step = 1867.2, total_loss = -30.255, pg_loss = -53.46, baseline_loss = 28.248, entropy_loss = -5.0427, learner_queue_size = 32, train_seconds = 3705.8, _tick = 1148, _time = 1.6546e+09)
[2022-06-07 11:25:37,798][root][INFO] - Step 6650880 @ 2046.3 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 6650880, mean_episode_return = 21.78, mean_episode_step = 2483.1, total_loss = -40.606, pg_loss = -61.657, baseline_loss = 26.262, entropy_loss = -5.2109, learner_queue_size = 32, train_seconds = 3710.8, _tick = 1152, _time = 1.6546e+09)
[2022-06-07 11:25:42,800][root][INFO] - Step 6658560 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 6658560, mean_episode_return = None, mean_episode_step = 1435.3, total_loss = 118.46, pg_loss = 81.355, baseline_loss = 42.03, entropy_loss = -4.9268, learner_queue_size = 32, train_seconds = 3715.8, _tick = 1153, _time = 1.6546e+09)
[2022-06-07 11:25:47,806][root][INFO] - Step 6668800 @ 2045.6 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 6668800, mean_episode_return = None, mean_episode_step = 1706.5, total_loss = 17.669, pg_loss = -2.1529, baseline_loss = 25.221, entropy_loss = -5.3995, learner_queue_size = 32, train_seconds = 3720.8, _tick = 1154, _time = 1.6546e+09)
[2022-06-07 11:25:52,810][root][INFO] - Step 6676480 @ 1534.7 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 6676480, mean_episode_return = None, mean_episode_step = 1876.9, total_loss = 25.082, pg_loss = 11.758, baseline_loss = 19.096, entropy_loss = -5.7724, learner_queue_size = 32, train_seconds = 3725.8, _tick = 1156, _time = 1.6546e+09)
[2022-06-07 11:25:57,816][root][INFO] - Step 6686720 @ 2045.5 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 6686720, mean_episode_return = 160.68, mean_episode_step = 1661.1, total_loss = 196.77, pg_loss = 131.7, baseline_loss = 70.76, entropy_loss = -5.6918, learner_queue_size = 32, train_seconds = 3730.8, _tick = 1159, _time = 1.6546e+09)
[2022-06-07 11:26:02,822][root][INFO] - Step 6694400 @ 1534.1 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 6694400, mean_episode_return = None, mean_episode_step = 2507.5, total_loss = 177.94, pg_loss = 137.78, baseline_loss = 45.714, entropy_loss = -5.5571, learner_queue_size = 32, train_seconds = 3735.8, _tick = 1161, _time = 1.6546e+09)
[2022-06-07 11:26:07,828][root][INFO] - Step 6704640 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 6704640, mean_episode_return = 7.3346, mean_episode_step = 1507.7, total_loss = -86.414, pg_loss = -114.33, baseline_loss = 33.503, entropy_loss = -5.5836, learner_queue_size = 32, train_seconds = 3740.8, _tick = 1164, _time = 1.6546e+09)
[2022-06-07 11:26:12,834][root][INFO] - Step 6712320 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6712320, mean_episode_return = None, mean_episode_step = 1951.6, total_loss = -27.45, pg_loss = -43.016, baseline_loss = 21.104, entropy_loss = -5.5389, learner_queue_size = 32, train_seconds = 3745.8, _tick = 1165, _time = 1.6546e+09)
[2022-06-07 11:26:17,840][root][INFO] - Step 6722560 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 6722560, mean_episode_return = None, mean_episode_step = 1508.2, total_loss = 7.2168, pg_loss = -12.511, baseline_loss = 25.157, entropy_loss = -5.429, learner_queue_size = 32, train_seconds = 3750.8, _tick = 1168, _time = 1.6546e+09)
[2022-06-07 11:26:22,846][root][INFO] - Step 6730240 @ 1534.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 6730240, mean_episode_return = None, mean_episode_step = 1700.6, total_loss = -52.678, pg_loss = -55.95, baseline_loss = 8.5961, entropy_loss = -5.3239, learner_queue_size = 32, train_seconds = 3755.8, _tick = 1170, _time = 1.6546e+09)
[2022-06-07 11:26:27,850][root][INFO] - Step 6740480 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6740480, mean_episode_return = None, mean_episode_step = 2206.9, total_loss = 138.02, pg_loss = 85.863, baseline_loss = 57.438, entropy_loss = -5.2809, learner_queue_size = 32, train_seconds = 3760.8, _tick = 1171, _time = 1.6546e+09)
[2022-06-07 11:26:32,856][root][INFO] - Step 6748160 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 6748160, mean_episode_return = 78.646, mean_episode_step = 1600.5, total_loss = -55.978, pg_loss = -95.381, baseline_loss = 44.524, entropy_loss = -5.1206, learner_queue_size = 32, train_seconds = 3765.9, _tick = 1173, _time = 1.6546e+09)
[2022-06-07 11:26:37,862][root][INFO] - Step 6758400 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 6758400, mean_episode_return = 74.647, mean_episode_step = 1705.9, total_loss = 184.19, pg_loss = 109.97, baseline_loss = 79.311, entropy_loss = -5.0928, learner_queue_size = 32, train_seconds = 3770.9, _tick = 1174, _time = 1.6546e+09)
[2022-06-07 11:26:42,866][root][INFO] - Step 6766080 @ 1534.9 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 6766080, mean_episode_return = None, mean_episode_step = 2385.9, total_loss = 182.81, pg_loss = 117.24, baseline_loss = 70.578, entropy_loss = -5.014, learner_queue_size = 32, train_seconds = 3775.9, _tick = 1175, _time = 1.6546e+09)
[2022-06-07 11:26:47,872][root][INFO] - Step 6776320 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 6776320, mean_episode_return = 64.406, mean_episode_step = 1996.1, total_loss = -46.83, pg_loss = -69.056, baseline_loss = 27.293, entropy_loss = -5.0675, learner_queue_size = 32, train_seconds = 3780.9, _tick = 1176, _time = 1.6546e+09)
[2022-06-07 11:26:52,878][root][INFO] - Step 6784000 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 6784000, mean_episode_return = 71.813, mean_episode_step = 2581.1, total_loss = 44.045, pg_loss = 14.018, baseline_loss = 34.845, entropy_loss = -4.8181, learner_queue_size = 32, train_seconds = 3785.9, _tick = 1178, _time = 1.6546e+09)
[2022-06-07 11:26:57,884][root][INFO] - Step 6794240 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 6794240, mean_episode_return = None, mean_episode_step = 1962.8, total_loss = 232.51, pg_loss = 109.86, baseline_loss = 127.07, entropy_loss = -4.4194, learner_queue_size = 32, train_seconds = 3790.9, _tick = 1181, _time = 1.6546e+09)
[2022-06-07 11:27:02,890][root][INFO] - Step 6801920 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6801920, mean_episode_return = None, mean_episode_step = 1932.6, total_loss = 89.155, pg_loss = 47.054, baseline_loss = 46.4, entropy_loss = -4.2984, learner_queue_size = 32, train_seconds = 3795.9, _tick = 1182, _time = 1.6546e+09)
[2022-06-07 11:27:07,894][root][INFO] - Step 6812160 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 6812160, mean_episode_return = 39.993, mean_episode_step = 1908.3, total_loss = 30.73, pg_loss = -24.89, baseline_loss = 59.95, entropy_loss = -4.3296, learner_queue_size = 32, train_seconds = 3800.9, _tick = 1185, _time = 1.6546e+09)
[2022-06-07 11:27:12,900][root][INFO] - Step 6819840 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6819840, mean_episode_return = None, mean_episode_step = 1662.7, total_loss = -83.399, pg_loss = -83.838, baseline_loss = 4.4944, entropy_loss = -4.0549, learner_queue_size = 32, train_seconds = 3805.9, _tick = 1186, _time = 1.6546e+09)
[2022-06-07 11:27:17,906][root][INFO] - Step 6830080 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 6830080, mean_episode_return = None, mean_episode_step = 2582.4, total_loss = 12.88, pg_loss = -18.639, baseline_loss = 35.743, entropy_loss = -4.2246, learner_queue_size = 32, train_seconds = 3810.9, _tick = 1189, _time = 1.6546e+09)
[2022-06-07 11:27:22,910][root][INFO] - Step 6837760 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 6837760, mean_episode_return = 95.51, mean_episode_step = 2176.8, total_loss = -103.1, pg_loss = -103.03, baseline_loss = 4.3296, entropy_loss = -4.403, learner_queue_size = 32, train_seconds = 3815.9, _tick = 1190, _time = 1.6546e+09)
[2022-06-07 11:27:27,914][root][INFO] - Step 6848000 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6848000, mean_episode_return = None, mean_episode_step = 1454.9, total_loss = 18.763, pg_loss = 3.5868, baseline_loss = 19.422, entropy_loss = -4.2459, learner_queue_size = 32, train_seconds = 3820.9, _tick = 1192, _time = 1.6546e+09)
[2022-06-07 11:27:32,918][root][INFO] - Step 6855680 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 6855680, mean_episode_return = None, mean_episode_step = 2331.8, total_loss = 42.714, pg_loss = 1.5962, baseline_loss = 45.534, entropy_loss = -4.4166, learner_queue_size = 32, train_seconds = 3825.9, _tick = 1192, _time = 1.6546e+09)
[2022-06-07 11:27:37,922][root][INFO] - Step 6865920 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 6865920, mean_episode_return = None, mean_episode_step = 2121.1, total_loss = 245.55, pg_loss = 168.46, baseline_loss = 81.397, entropy_loss = -4.3044, learner_queue_size = 32, train_seconds = 3830.9, _tick = 1192, _time = 1.6546e+09)
[2022-06-07 11:27:42,926][root][INFO] - Step 6873600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6873600, mean_episode_return = None, mean_episode_step = 1835.0, total_loss = 61.107, pg_loss = 15.383, baseline_loss = 50.028, entropy_loss = -4.3041, learner_queue_size = 32, train_seconds = 3835.9, _tick = 1193, _time = 1.6546e+09)
[2022-06-07 11:27:47,932][root][INFO] - Step 6883840 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 6883840, mean_episode_return = None, mean_episode_step = 2557.3, total_loss = 495.66, pg_loss = 331.57, baseline_loss = 168.4, entropy_loss = -4.3036, learner_queue_size = 32, train_seconds = 3840.9, _tick = 1194, _time = 1.6546e+09)
[2022-06-07 11:27:52,937][root][INFO] - Step 6891520 @ 1534.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6891520, mean_episode_return = 44.018, mean_episode_step = 2520.8, total_loss = 292.07, pg_loss = 168.75, baseline_loss = 127.48, entropy_loss = -4.1633, learner_queue_size = 32, train_seconds = 3845.9, _tick = 1196, _time = 1.6546e+09)
[2022-06-07 11:27:57,942][root][INFO] - Step 6901760 @ 2046.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 6901760, mean_episode_return = 14.643, mean_episode_step = 1712.9, total_loss = -72.165, pg_loss = -104.97, baseline_loss = 36.811, entropy_loss = -4.0086, learner_queue_size = 32, train_seconds = 3850.9, _tick = 1198, _time = 1.6546e+09)
[2022-06-07 11:28:02,946][root][INFO] - Step 6909440 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 6909440, mean_episode_return = 14.998, mean_episode_step = 1859.1, total_loss = 126.97, pg_loss = 51.467, baseline_loss = 79.34, entropy_loss = -3.8382, learner_queue_size = 32, train_seconds = 3855.9, _tick = 1200, _time = 1.6546e+09)
[2022-06-07 11:28:07,951][root][INFO] - Step 6919680 @ 2046.1 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 6919680, mean_episode_return = None, mean_episode_step = 1734.3, total_loss = 191.83, pg_loss = 130.35, baseline_loss = 65.458, entropy_loss = -3.9834, learner_queue_size = 32, train_seconds = 3860.9, _tick = 1203, _time = 1.6546e+09)
[2022-06-07 11:28:12,956][root][INFO] - Step 6927360 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 6927360, mean_episode_return = None, mean_episode_step = 2038.8, total_loss = 233.94, pg_loss = 125.44, baseline_loss = 112.52, entropy_loss = -4.0199, learner_queue_size = 32, train_seconds = 3866.0, _tick = 1205, _time = 1.6546e+09)
[2022-06-07 11:28:17,958][root][INFO] - Step 6937600 @ 2047.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 6937600, mean_episode_return = 36.342, mean_episode_step = 2114.4, total_loss = 74.17, pg_loss = -35.494, baseline_loss = 113.88, entropy_loss = -4.2185, learner_queue_size = 32, train_seconds = 3871.0, _tick = 1208, _time = 1.6546e+09)
[2022-06-07 11:28:22,964][root][INFO] - Step 6947840 @ 2045.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 6947840, mean_episode_return = None, mean_episode_step = 1624.2, total_loss = 95.139, pg_loss = 42.366, baseline_loss = 56.968, entropy_loss = -4.1963, learner_queue_size = 32, train_seconds = 3876.0, _tick = 1210, _time = 1.6546e+09)
[2022-06-07 11:28:27,966][root][INFO] - Step 6955520 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 6955520, mean_episode_return = 12.8, mean_episode_step = 2092.9, total_loss = -141.79, pg_loss = -144.01, baseline_loss = 6.4564, entropy_loss = -4.2417, learner_queue_size = 32, train_seconds = 3881.0, _tick = 1212, _time = 1.6546e+09)
[2022-06-07 11:28:32,970][root][INFO] - Step 6963200 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 6963200, mean_episode_return = None, mean_episode_step = 1697.3, total_loss = -124.29, pg_loss = -123.58, baseline_loss = 3.501, entropy_loss = -4.2053, learner_queue_size = 32, train_seconds = 3886.0, _tick = 1213, _time = 1.6546e+09)
[2022-06-07 11:28:37,973][root][INFO] - Step 6973440 @ 2046.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 6973440, mean_episode_return = 41.221, mean_episode_step = 2197.0, total_loss = -161.71, pg_loss = -168.42, baseline_loss = 11.086, entropy_loss = -4.3732, learner_queue_size = 32, train_seconds = 3891.0, _tick = 1215, _time = 1.6546e+09)
[2022-06-07 11:28:42,978][root][INFO] - Step 6983680 @ 2046.1 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 6983680, mean_episode_return = None, mean_episode_step = 2003.2, total_loss = -15.459, pg_loss = -20.378, baseline_loss = 9.3011, entropy_loss = -4.3813, learner_queue_size = 32, train_seconds = 3896.0, _tick = 1217, _time = 1.6546e+09)
[2022-06-07 11:28:47,982][root][INFO] - Step 6991360 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 6991360, mean_episode_return = 106.43, mean_episode_step = 1994.1, total_loss = -12.288, pg_loss = -30.102, baseline_loss = 22.094, entropy_loss = -4.2794, learner_queue_size = 32, train_seconds = 3901.0, _tick = 1219, _time = 1.6546e+09)
[2022-06-07 11:28:52,988][root][INFO] - Step 7001600 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7001600, mean_episode_return = 48.784, mean_episode_step = 2684.2, total_loss = -72.461, pg_loss = -84.351, baseline_loss = 16.026, entropy_loss = -4.1356, learner_queue_size = 32, train_seconds = 3906.0, _tick = 1221, _time = 1.6546e+09)
[2022-06-07 11:28:57,990][root][INFO] - Step 7009280 @ 1535.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 7009280, mean_episode_return = None, mean_episode_step = 2092.1, total_loss = 342.33, pg_loss = 239.75, baseline_loss = 107.06, entropy_loss = -4.4901, learner_queue_size = 32, train_seconds = 3911.0, _tick = 1222, _time = 1.6546e+09)
[2022-06-07 11:29:02,994][root][INFO] - Step 7019520 @ 2046.4 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 7019520, mean_episode_return = None, mean_episode_step = 2080.0, total_loss = -11.284, pg_loss = -29.49, baseline_loss = 22.447, entropy_loss = -4.242, learner_queue_size = 32, train_seconds = 3916.0, _tick = 1223, _time = 1.6546e+09)
[2022-06-07 11:29:07,998][root][INFO] - Step 7027200 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7027200, mean_episode_return = 41.195, mean_episode_step = 2307.8, total_loss = 303.35, pg_loss = 108.56, baseline_loss = 199.25, entropy_loss = -4.4519, learner_queue_size = 32, train_seconds = 3921.0, _tick = 1225, _time = 1.6546e+09)
[2022-06-07 11:29:13,002][root][INFO] - Step 7037440 @ 2046.3 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 7037440, mean_episode_return = 43.94, mean_episode_step = 1781.9, total_loss = -60.707, pg_loss = -68.708, baseline_loss = 12.562, entropy_loss = -4.5608, learner_queue_size = 32, train_seconds = 3926.0, _tick = 1226, _time = 1.6546e+09)
[2022-06-07 11:29:18,008][root][INFO] - Step 7045120 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7045120, mean_episode_return = 35.414, mean_episode_step = 1574.1, total_loss = 2.4317, pg_loss = -22.25, baseline_loss = 29.373, entropy_loss = -4.6906, learner_queue_size = 32, train_seconds = 3931.0, _tick = 1229, _time = 1.6546e+09)
[2022-06-07 11:29:23,014][root][INFO] - Step 7055360 @ 2045.5 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 7055360, mean_episode_return = None, mean_episode_step = 2275.8, total_loss = -89.039, pg_loss = -85.437, baseline_loss = 0.79529, entropy_loss = -4.3965, learner_queue_size = 32, train_seconds = 3936.0, _tick = 1230, _time = 1.6546e+09)
[2022-06-07 11:29:28,020][root][INFO] - Step 7063040 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7063040, mean_episode_return = 45.35, mean_episode_step = 2043.1, total_loss = 480.66, pg_loss = 354.86, baseline_loss = 130.27, entropy_loss = -4.4676, learner_queue_size = 32, train_seconds = 3941.0, _tick = 1231, _time = 1.6546e+09)
[2022-06-07 11:29:33,026][root][INFO] - Step 7073280 @ 2045.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7073280, mean_episode_return = None, mean_episode_step = 2120.6, total_loss = 24.64, pg_loss = 0.95194, baseline_loss = 28.207, entropy_loss = -4.5183, learner_queue_size = 32, train_seconds = 3946.0, _tick = 1232, _time = 1.6546e+09)
[2022-06-07 11:29:38,032][root][INFO] - Step 7083520 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7083520, mean_episode_return = 112.24, mean_episode_step = 2458.2, total_loss = -85.837, pg_loss = -105.59, baseline_loss = 24.524, entropy_loss = -4.7741, learner_queue_size = 32, train_seconds = 3951.0, _tick = 1233, _time = 1.6546e+09)
[2022-06-07 11:29:43,038][root][INFO] - Step 7091200 @ 1534.1 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 7091200, mean_episode_return = None, mean_episode_step = 1821.6, total_loss = 238.75, pg_loss = 151.56, baseline_loss = 91.95, entropy_loss = -4.7597, learner_queue_size = 32, train_seconds = 3956.0, _tick = 1234, _time = 1.6546e+09)
[2022-06-07 11:29:48,042][root][INFO] - Step 7098880 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 7098880, mean_episode_return = None, mean_episode_step = 2127.0, total_loss = -31.868, pg_loss = -46.294, baseline_loss = 19.186, entropy_loss = -4.7595, learner_queue_size = 32, train_seconds = 3961.0, _tick = 1234, _time = 1.6546e+09)
[2022-06-07 11:29:53,046][root][INFO] - Step 7109120 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7109120, mean_episode_return = None, mean_episode_step = 2122.3, total_loss = 43.819, pg_loss = -10.295, baseline_loss = 58.86, entropy_loss = -4.7464, learner_queue_size = 32, train_seconds = 3966.0, _tick = 1236, _time = 1.6546e+09)
[2022-06-07 11:29:58,053][root][INFO] - Step 7116800 @ 1533.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7116800, mean_episode_return = 38.25, mean_episode_step = 2815.3, total_loss = 126.79, pg_loss = 74.552, baseline_loss = 56.97, entropy_loss = -4.7334, learner_queue_size = 32, train_seconds = 3971.0, _tick = 1237, _time = 1.6546e+09)
[2022-06-07 11:30:03,059][root][INFO] - Step 7127040 @ 2045.5 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 7127040, mean_episode_return = 39.731, mean_episode_step = 1942.2, total_loss = -107.03, pg_loss = -118.64, baseline_loss = 16.41, entropy_loss = -4.7941, learner_queue_size = 32, train_seconds = 3976.1, _tick = 1238, _time = 1.6546e+09)
[2022-06-07 11:30:08,065][root][INFO] - Step 7137280 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 7137280, mean_episode_return = None, mean_episode_step = 2385.5, total_loss = 55.727, pg_loss = 27.112, baseline_loss = 33.173, entropy_loss = -4.5581, learner_queue_size = 32, train_seconds = 3981.1, _tick = 1239, _time = 1.6546e+09)
[2022-06-07 11:30:13,071][root][INFO] - Step 7144960 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 7144960, mean_episode_return = -0.39036, mean_episode_step = 1841.9, total_loss = 59.035, pg_loss = 29.333, baseline_loss = 34.408, entropy_loss = -4.7057, learner_queue_size = 32, train_seconds = 3986.1, _tick = 1241, _time = 1.6546e+09)
[2022-06-07 11:30:18,074][root][INFO] - Step 7152640 @ 1535.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 7152640, mean_episode_return = 11.71, mean_episode_step = 2204.8, total_loss = 62.783, pg_loss = 26.006, baseline_loss = 41.547, entropy_loss = -4.7696, learner_queue_size = 32, train_seconds = 3991.1, _tick = 1243, _time = 1.6546e+09)
[2022-06-07 11:30:23,081][root][INFO] - Step 7162880 @ 2045.3 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 7162880, mean_episode_return = 12.335, mean_episode_step = 2316.4, total_loss = -136.86, pg_loss = -141.27, baseline_loss = 9.2326, entropy_loss = -4.8222, learner_queue_size = 32, train_seconds = 3996.1, _tick = 1245, _time = 1.6546e+09)
[2022-06-07 11:30:28,087][root][INFO] - Step 7173120 @ 2045.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7173120, mean_episode_return = 72.99, mean_episode_step = 2043.9, total_loss = -139.09, pg_loss = -138.19, baseline_loss = 3.9181, entropy_loss = -4.8161, learner_queue_size = 32, train_seconds = 4001.1, _tick = 1247, _time = 1.6546e+09)
[2022-06-07 11:30:33,090][root][INFO] - Step 7180800 @ 1535.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7180800, mean_episode_return = 103.84, mean_episode_step = 2397.0, total_loss = 44.502, pg_loss = 29.18, baseline_loss = 19.934, entropy_loss = -4.6114, learner_queue_size = 32, train_seconds = 4006.1, _tick = 1248, _time = 1.6546e+09)
[2022-06-07 11:30:38,094][root][INFO] - Step 7191040 @ 2046.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7191040, mean_episode_return = None, mean_episode_step = 2223.2, total_loss = 78.286, pg_loss = 47.884, baseline_loss = 34.806, entropy_loss = -4.4045, learner_queue_size = 32, train_seconds = 4011.1, _tick = 1250, _time = 1.6546e+09)
[2022-06-07 11:30:43,098][root][INFO] - Step 7198720 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7198720, mean_episode_return = None, mean_episode_step = 1839.7, total_loss = -6.4535, pg_loss = -33.607, baseline_loss = 31.991, entropy_loss = -4.8377, learner_queue_size = 32, train_seconds = 4016.1, _tick = 1252, _time = 1.6546e+09)
[2022-06-07 11:30:48,104][root][INFO] - Step 7208960 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7208960, mean_episode_return = 19.139, mean_episode_step = 2526.6, total_loss = -44.119, pg_loss = -54.671, baseline_loss = 15.557, entropy_loss = -5.0054, learner_queue_size = 32, train_seconds = 4021.1, _tick = 1255, _time = 1.6546e+09)
[2022-06-07 11:30:53,110][root][INFO] - Step 7216640 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 7216640, mean_episode_return = None, mean_episode_step = 2093.0, total_loss = -74.018, pg_loss = -74.675, baseline_loss = 5.7874, entropy_loss = -5.1308, learner_queue_size = 32, train_seconds = 4026.1, _tick = 1255, _time = 1.6546e+09)
[2022-06-07 11:30:58,116][root][INFO] - Step 7226880 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7226880, mean_episode_return = 45.361, mean_episode_step = 2643.6, total_loss = 227.17, pg_loss = 161.93, baseline_loss = 70.425, entropy_loss = -5.1876, learner_queue_size = 32, train_seconds = 4031.1, _tick = 1258, _time = 1.6546e+09)
[2022-06-07 11:31:03,122][root][INFO] - Step 7234560 @ 1534.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 7234560, mean_episode_return = 31.313, mean_episode_step = 2414.1, total_loss = -52.951, pg_loss = -66.82, baseline_loss = 19.168, entropy_loss = -5.2987, learner_queue_size = 32, train_seconds = 4036.1, _tick = 1260, _time = 1.6546e+09)
[2022-06-07 11:31:08,128][root][INFO] - Step 7244800 @ 2045.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 7244800, mean_episode_return = None, mean_episode_step = 2356.6, total_loss = -64.678, pg_loss = -66.993, baseline_loss = 7.6833, entropy_loss = -5.3683, learner_queue_size = 32, train_seconds = 4041.1, _tick = 1262, _time = 1.6546e+09)
[2022-06-07 11:31:13,134][root][INFO] - Step 7252480 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7252480, mean_episode_return = 84.817, mean_episode_step = 2202.0, total_loss = -111.72, pg_loss = -113.91, baseline_loss = 7.5415, entropy_loss = -5.3508, learner_queue_size = 32, train_seconds = 4046.1, _tick = 1263, _time = 1.6546e+09)
[2022-06-07 11:31:18,137][root][INFO] - Step 7260160 @ 1535.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7260160, mean_episode_return = None, mean_episode_step = 2055.1, total_loss = -122.23, pg_loss = -119.9, baseline_loss = 3.217, entropy_loss = -5.5409, learner_queue_size = 32, train_seconds = 4051.1, _tick = 1264, _time = 1.6546e+09)
[2022-06-07 11:31:23,142][root][INFO] - Step 7270400 @ 2045.9 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 7270400, mean_episode_return = None, mean_episode_step = 2652.8, total_loss = 154.47, pg_loss = 100.82, baseline_loss = 59.169, entropy_loss = -5.5192, learner_queue_size = 32, train_seconds = 4056.1, _tick = 1264, _time = 1.6546e+09)
[2022-06-07 11:31:28,148][root][INFO] - Step 7278080 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 7278080, mean_episode_return = 35.361, mean_episode_step = 2716.2, total_loss = 344.39, pg_loss = 246.23, baseline_loss = 103.73, entropy_loss = -5.5619, learner_queue_size = 32, train_seconds = 4061.1, _tick = 1266, _time = 1.6546e+09)
[2022-06-07 11:31:33,156][root][INFO] - Step 7288320 @ 2044.9 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 7288320, mean_episode_return = None, mean_episode_step = 2679.8, total_loss = 203.33, pg_loss = 139.56, baseline_loss = 69.116, entropy_loss = -5.3426, learner_queue_size = 32, train_seconds = 4066.2, _tick = 1268, _time = 1.6546e+09)
[2022-06-07 11:31:38,158][root][INFO] - Step 7296000 @ 1535.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7296000, mean_episode_return = -15.251, mean_episode_step = 2716.3, total_loss = -122.3, pg_loss = -120.73, baseline_loss = 3.5592, entropy_loss = -5.1251, learner_queue_size = 32, train_seconds = 4071.2, _tick = 1271, _time = 1.6546e+09)
[2022-06-07 11:31:43,165][root][INFO] - Step 7306240 @ 2045.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7306240, mean_episode_return = None, mean_episode_step = 2103.9, total_loss = -106.51, pg_loss = -104.04, baseline_loss = 2.5323, entropy_loss = -5.0031, learner_queue_size = 32, train_seconds = 4076.2, _tick = 1272, _time = 1.6546e+09)
[2022-06-07 11:31:48,171][root][INFO] - Step 7316480 @ 2045.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7316480, mean_episode_return = 37.301, mean_episode_step = 2532.3, total_loss = 43.422, pg_loss = 6.7551, baseline_loss = 41.644, entropy_loss = -4.9766, learner_queue_size = 32, train_seconds = 4081.2, _tick = 1276, _time = 1.6546e+09)
[2022-06-07 11:31:53,174][root][INFO] - Step 7324160 @ 1535.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7324160, mean_episode_return = None, mean_episode_step = 2524.1, total_loss = 62.686, pg_loss = 32.629, baseline_loss = 35.134, entropy_loss = -5.0775, learner_queue_size = 32, train_seconds = 4086.2, _tick = 1278, _time = 1.6546e+09)
[2022-06-07 11:31:58,178][root][INFO] - Step 7331840 @ 1534.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7331840, mean_episode_return = 18.441, mean_episode_step = 2532.4, total_loss = -18.494, pg_loss = -33.752, baseline_loss = 20.646, entropy_loss = -5.3887, learner_queue_size = 32, train_seconds = 4091.2, _tick = 1280, _time = 1.6546e+09)
[2022-06-07 11:32:03,182][root][INFO] - Step 7342080 @ 2046.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7342080, mean_episode_return = 15.62, mean_episode_step = 2478.9, total_loss = 70.603, pg_loss = 41.486, baseline_loss = 34.534, entropy_loss = -5.417, learner_queue_size = 32, train_seconds = 4096.2, _tick = 1283, _time = 1.6546e+09)
[2022-06-07 11:32:08,186][root][INFO] - Step 7349760 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7349760, mean_episode_return = 132.54, mean_episode_step = 1806.6, total_loss = 231.08, pg_loss = 150.73, baseline_loss = 85.73, entropy_loss = -5.3822, learner_queue_size = 32, train_seconds = 4101.2, _tick = 1285, _time = 1.6546e+09)
[2022-06-07 11:32:13,192][root][INFO] - Step 7360000 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7360000, mean_episode_return = 24.694, mean_episode_step = 2885.8, total_loss = -53.956, pg_loss = -58.74, baseline_loss = 10.173, entropy_loss = -5.3897, learner_queue_size = 32, train_seconds = 4106.2, _tick = 1289, _time = 1.6546e+09)
[2022-06-07 11:32:18,198][root][INFO] - Step 7370240 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7370240, mean_episode_return = None, mean_episode_step = 1646.9, total_loss = 350.08, pg_loss = 253.29, baseline_loss = 102.06, entropy_loss = -5.2668, learner_queue_size = 32, train_seconds = 4111.2, _tick = 1291, _time = 1.6546e+09)
[2022-06-07 11:32:23,204][root][INFO] - Step 7377920 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7377920, mean_episode_return = 36.276, mean_episode_step = 2147.2, total_loss = -39.348, pg_loss = -83.047, baseline_loss = 48.967, entropy_loss = -5.2685, learner_queue_size = 32, train_seconds = 4116.2, _tick = 1294, _time = 1.6546e+09)
[2022-06-07 11:32:28,210][root][INFO] - Step 7388160 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7388160, mean_episode_return = 91.206, mean_episode_step = 1962.2, total_loss = 220.72, pg_loss = 149.67, baseline_loss = 76.366, entropy_loss = -5.312, learner_queue_size = 32, train_seconds = 4121.2, _tick = 1298, _time = 1.6546e+09)
[2022-06-07 11:32:33,214][root][INFO] - Step 7395840 @ 1534.8 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 7395840, mean_episode_return = 106.85, mean_episode_step = 2013.8, total_loss = -10.252, pg_loss = -37.482, baseline_loss = 32.47, entropy_loss = -5.2399, learner_queue_size = 32, train_seconds = 4126.2, _tick = 1299, _time = 1.6546e+09)
[2022-06-07 11:32:38,218][root][INFO] - Step 7406080 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7406080, mean_episode_return = None, mean_episode_step = 2503.6, total_loss = -51.705, pg_loss = -50.848, baseline_loss = 4.3303, entropy_loss = -5.1877, learner_queue_size = 32, train_seconds = 4131.2, _tick = 1301, _time = 1.6546e+09)
[2022-06-07 11:32:43,223][root][INFO] - Step 7413760 @ 1534.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7413760, mean_episode_return = 97.724, mean_episode_step = 2072.2, total_loss = 62.834, pg_loss = 17.829, baseline_loss = 50.476, entropy_loss = -5.4707, learner_queue_size = 32, train_seconds = 4136.2, _tick = 1302, _time = 1.6546e+09)
[2022-06-07 11:32:48,225][root][INFO] - Step 7424000 @ 2047.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7424000, mean_episode_return = None, mean_episode_step = 2044.5, total_loss = 6.6961, pg_loss = 6.7812, baseline_loss = 5.0204, entropy_loss = -5.1055, learner_queue_size = 32, train_seconds = 4141.2, _tick = 1305, _time = 1.6546e+09)
[2022-06-07 11:32:53,230][root][INFO] - Step 7431680 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7431680, mean_episode_return = 187.01, mean_episode_step = 1386.9, total_loss = -11.789, pg_loss = -30.917, baseline_loss = 24.532, entropy_loss = -5.4035, learner_queue_size = 32, train_seconds = 4146.2, _tick = 1308, _time = 1.6546e+09)
[2022-06-07 11:32:58,233][root][INFO] - Step 7441920 @ 2046.9 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 7441920, mean_episode_return = None, mean_episode_step = 1882.0, total_loss = -31.625, pg_loss = -40.79, baseline_loss = 14.598, entropy_loss = -5.4334, learner_queue_size = 32, train_seconds = 4151.2, _tick = 1309, _time = 1.6546e+09)
[2022-06-07 11:33:03,238][root][INFO] - Step 7449600 @ 1534.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7449600, mean_episode_return = 100.64, mean_episode_step = 1619.2, total_loss = 143.15, pg_loss = 98.769, baseline_loss = 49.886, entropy_loss = -5.5067, learner_queue_size = 32, train_seconds = 4156.2, _tick = 1311, _time = 1.6546e+09)
[2022-06-07 11:33:08,242][root][INFO] - Step 7459840 @ 2046.4 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 7459840, mean_episode_return = None, mean_episode_step = 2086.3, total_loss = -110.19, pg_loss = -109.06, baseline_loss = 4.4272, entropy_loss = -5.5534, learner_queue_size = 32, train_seconds = 4161.2, _tick = 1312, _time = 1.6546e+09)
[2022-06-07 11:33:13,248][root][INFO] - Step 7467520 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7467520, mean_episode_return = None, mean_episode_step = 2021.8, total_loss = -37.607, pg_loss = -37.972, baseline_loss = 5.8992, entropy_loss = -5.5343, learner_queue_size = 32, train_seconds = 4166.2, _tick = 1313, _time = 1.6546e+09)
[2022-06-07 11:33:18,254][root][INFO] - Step 7477760 @ 2045.5 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 7477760, mean_episode_return = 22.89, mean_episode_step = 1746.2, total_loss = -81.185, pg_loss = -91.29, baseline_loss = 15.89, entropy_loss = -5.7851, learner_queue_size = 32, train_seconds = 4171.2, _tick = 1316, _time = 1.6546e+09)
[2022-06-07 11:33:23,258][root][INFO] - Step 7485440 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 7485440, mean_episode_return = 107.13, mean_episode_step = 2297.2, total_loss = -50.127, pg_loss = -49.471, baseline_loss = 5.1969, entropy_loss = -5.8532, learner_queue_size = 32, train_seconds = 4176.3, _tick = 1318, _time = 1.6546e+09)
[2022-06-07 11:33:28,262][root][INFO] - Step 7495680 @ 2046.3 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 7495680, mean_episode_return = None, mean_episode_step = 1832.3, total_loss = -52.857, pg_loss = -51.48, baseline_loss = 4.4865, entropy_loss = -5.8634, learner_queue_size = 32, train_seconds = 4181.3, _tick = 1320, _time = 1.6546e+09)
[2022-06-07 11:33:33,266][root][INFO] - Step 7505920 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 7505920, mean_episode_return = 23.28, mean_episode_step = 2267.3, total_loss = 292.82, pg_loss = 176.8, baseline_loss = 121.9, entropy_loss = -5.8742, learner_queue_size = 32, train_seconds = 4186.3, _tick = 1324, _time = 1.6546e+09)
[2022-06-07 11:33:38,270][root][INFO] - Step 7513600 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 7513600, mean_episode_return = 79.967, mean_episode_step = 1979.9, total_loss = 60.744, pg_loss = 24.154, baseline_loss = 42.487, entropy_loss = -5.8959, learner_queue_size = 32, train_seconds = 4191.3, _tick = 1326, _time = 1.6546e+09)
[2022-06-07 11:33:43,276][root][INFO] - Step 7523840 @ 2045.6 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 7523840, mean_episode_return = 9.9492, mean_episode_step = 2373.1, total_loss = 296.32, pg_loss = 209.64, baseline_loss = 92.512, entropy_loss = -5.833, learner_queue_size = 32, train_seconds = 4196.3, _tick = 1330, _time = 1.6546e+09)
[2022-06-07 11:33:48,278][root][INFO] - Step 7531520 @ 1535.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 7531520, mean_episode_return = None, mean_episode_step = 1646.8, total_loss = 99.6, pg_loss = 56.804, baseline_loss = 48.53, entropy_loss = -5.7338, learner_queue_size = 32, train_seconds = 4201.3, _tick = 1330, _time = 1.6546e+09)
[2022-06-07 11:33:53,282][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 11:33:53,497][root][INFO] - Step 7541760 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 7541760, mean_episode_return = 50.384, mean_episode_step = 2169.8, total_loss = 39.266, pg_loss = -3.1614, baseline_loss = 48.244, entropy_loss = -5.8173, learner_queue_size = 32, train_seconds = 4206.3, _tick = 1333, _time = 1.6546e+09)
[2022-06-07 11:33:58,502][root][INFO] - Step 7549440 @ 1471.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7549440, mean_episode_return = 58.697, mean_episode_step = 1440.9, total_loss = 483.5, pg_loss = 328.71, baseline_loss = 160.6, entropy_loss = -5.8119, learner_queue_size = 32, train_seconds = 4211.5, _tick = 1334, _time = 1.6546e+09)
[2022-06-07 11:34:03,506][root][INFO] - Step 7559680 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7559680, mean_episode_return = None, mean_episode_step = 1894.2, total_loss = -192.82, pg_loss = -188.8, baseline_loss = 1.7814, entropy_loss = -5.8095, learner_queue_size = 32, train_seconds = 4216.5, _tick = 1337, _time = 1.6546e+09)
[2022-06-07 11:34:08,512][root][INFO] - Step 7567360 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 7567360, mean_episode_return = None, mean_episode_step = 1643.1, total_loss = 293.03, pg_loss = 203.49, baseline_loss = 95.363, entropy_loss = -5.8227, learner_queue_size = 32, train_seconds = 4221.5, _tick = 1339, _time = 1.6546e+09)
[2022-06-07 11:34:13,518][root][INFO] - Step 7577600 @ 2045.6 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 7577600, mean_episode_return = 63.907, mean_episode_step = 2004.4, total_loss = 71.92, pg_loss = -19.743, baseline_loss = 97.43, entropy_loss = -5.7675, learner_queue_size = 32, train_seconds = 4226.5, _tick = 1343, _time = 1.6546e+09)
[2022-06-07 11:34:18,522][root][INFO] - Step 7587840 @ 2046.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7587840, mean_episode_return = 72.057, mean_episode_step = 1419.8, total_loss = -32.433, pg_loss = -59.675, baseline_loss = 32.838, entropy_loss = -5.5966, learner_queue_size = 32, train_seconds = 4231.5, _tick = 1347, _time = 1.6546e+09)
[2022-06-07 11:34:23,526][root][INFO] - Step 7595520 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 7595520, mean_episode_return = -9.0101, mean_episode_step = 2022.5, total_loss = 99.246, pg_loss = 66.174, baseline_loss = 38.453, entropy_loss = -5.3814, learner_queue_size = 32, train_seconds = 4236.5, _tick = 1350, _time = 1.6546e+09)
[2022-06-07 11:34:28,530][root][INFO] - Step 7605760 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 7605760, mean_episode_return = 120.68, mean_episode_step = 1569.3, total_loss = 12.254, pg_loss = -22.119, baseline_loss = 39.833, entropy_loss = -5.4602, learner_queue_size = 32, train_seconds = 4241.5, _tick = 1352, _time = 1.6546e+09)
[2022-06-07 11:34:33,536][root][INFO] - Step 7613440 @ 1534.1 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 7613440, mean_episode_return = 28.371, mean_episode_step = 1604.7, total_loss = 228.69, pg_loss = 134.87, baseline_loss = 99.162, entropy_loss = -5.3461, learner_queue_size = 32, train_seconds = 4246.5, _tick = 1354, _time = 1.6546e+09)
[2022-06-07 11:34:38,538][root][INFO] - Step 7621120 @ 1535.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 7621120, mean_episode_return = 20.939, mean_episode_step = 1558.5, total_loss = 107.69, pg_loss = 45.873, baseline_loss = 66.99, entropy_loss = -5.1709, learner_queue_size = 32, train_seconds = 4251.5, _tick = 1356, _time = 1.6546e+09)
[2022-06-07 11:34:43,542][root][INFO] - Step 7631360 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 7631360, mean_episode_return = None, mean_episode_step = 1728.1, total_loss = 325.38, pg_loss = 206.34, baseline_loss = 124.21, entropy_loss = -5.1689, learner_queue_size = 32, train_seconds = 4256.5, _tick = 1358, _time = 1.6546e+09)
[2022-06-07 11:34:48,548][root][INFO] - Step 7639040 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 7639040, mean_episode_return = None, mean_episode_step = 1592.6, total_loss = -79.634, pg_loss = -99.466, baseline_loss = 25.09, entropy_loss = -5.2582, learner_queue_size = 32, train_seconds = 4261.5, _tick = 1358, _time = 1.6546e+09)
[2022-06-07 11:34:53,566][root][INFO] - Step 7649280 @ 2040.6 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 7649280, mean_episode_return = None, mean_episode_step = 1470.4, total_loss = -132.14, pg_loss = -134.2, baseline_loss = 7.2366, entropy_loss = -5.1787, learner_queue_size = 32, train_seconds = 4266.6, _tick = 1358, _time = 1.6546e+09)
[2022-06-07 11:34:58,572][root][INFO] - Step 7659520 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7659520, mean_episode_return = 7.1198, mean_episode_step = 1698.2, total_loss = 73.415, pg_loss = 17.522, baseline_loss = 60.961, entropy_loss = -5.068, learner_queue_size = 32, train_seconds = 4271.6, _tick = 1360, _time = 1.6546e+09)
[2022-06-07 11:35:03,578][root][INFO] - Step 7667200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7667200, mean_episode_return = 32.694, mean_episode_step = 1346.0, total_loss = -3.3582, pg_loss = -55.621, baseline_loss = 57.323, entropy_loss = -5.0603, learner_queue_size = 32, train_seconds = 4276.6, _tick = 1363, _time = 1.6546e+09)
[2022-06-07 11:35:08,584][root][INFO] - Step 7674880 @ 1534.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 7674880, mean_episode_return = None, mean_episode_step = 1898.8, total_loss = 37.216, pg_loss = 12.927, baseline_loss = 29.221, entropy_loss = -4.9314, learner_queue_size = 32, train_seconds = 4281.6, _tick = 1364, _time = 1.6546e+09)
[2022-06-07 11:35:13,586][root][INFO] - Step 7685120 @ 2047.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 7685120, mean_episode_return = None, mean_episode_step = 1961.9, total_loss = 126.75, pg_loss = 68.152, baseline_loss = 63.543, entropy_loss = -4.9414, learner_queue_size = 32, train_seconds = 4286.6, _tick = 1365, _time = 1.6546e+09)
[2022-06-07 11:35:18,590][root][INFO] - Step 7695360 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 7695360, mean_episode_return = None, mean_episode_step = 1624.8, total_loss = 115.76, pg_loss = 54.54, baseline_loss = 66.349, entropy_loss = -5.1323, learner_queue_size = 32, train_seconds = 4291.6, _tick = 1366, _time = 1.6546e+09)
[2022-06-07 11:35:23,594][root][INFO] - Step 7703040 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 7703040, mean_episode_return = 44.291, mean_episode_step = 1973.5, total_loss = -57.629, pg_loss = -81.521, baseline_loss = 28.75, entropy_loss = -4.8583, learner_queue_size = 32, train_seconds = 4296.6, _tick = 1367, _time = 1.6546e+09)
[2022-06-07 11:35:28,600][root][INFO] - Step 7713280 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 7713280, mean_episode_return = None, mean_episode_step = 1686.2, total_loss = 224.15, pg_loss = 162.53, baseline_loss = 66.57, entropy_loss = -4.9467, learner_queue_size = 32, train_seconds = 4301.6, _tick = 1367, _time = 1.6546e+09)
[2022-06-07 11:35:33,606][root][INFO] - Step 7720960 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7720960, mean_episode_return = None, mean_episode_step = 2334.9, total_loss = 292.46, pg_loss = 150.47, baseline_loss = 147.07, entropy_loss = -5.0848, learner_queue_size = 32, train_seconds = 4306.6, _tick = 1367, _time = 1.6546e+09)
[2022-06-07 11:35:38,610][root][INFO] - Step 7731200 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 7731200, mean_episode_return = None, mean_episode_step = 1982.7, total_loss = -162.53, pg_loss = -160.58, baseline_loss = 3.1766, entropy_loss = -5.1294, learner_queue_size = 32, train_seconds = 4311.6, _tick = 1368, _time = 1.6546e+09)
[2022-06-07 11:35:43,614][root][INFO] - Step 7738880 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7738880, mean_episode_return = None, mean_episode_step = 2149.8, total_loss = 114.11, pg_loss = 71.811, baseline_loss = 47.297, entropy_loss = -4.9933, learner_queue_size = 32, train_seconds = 4316.6, _tick = 1369, _time = 1.6546e+09)
[2022-06-07 11:35:48,618][root][INFO] - Step 7749120 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 7749120, mean_episode_return = None, mean_episode_step = 2083.5, total_loss = 66.351, pg_loss = 38.627, baseline_loss = 32.883, entropy_loss = -5.1593, learner_queue_size = 32, train_seconds = 4321.6, _tick = 1370, _time = 1.6546e+09)
[2022-06-07 11:35:53,622][root][INFO] - Step 7756800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7756800, mean_episode_return = None, mean_episode_step = 2222.3, total_loss = 185.2, pg_loss = 120.0, baseline_loss = 70.379, entropy_loss = -5.1789, learner_queue_size = 32, train_seconds = 4326.6, _tick = 1371, _time = 1.6546e+09)
[2022-06-07 11:35:58,626][root][INFO] - Step 7767040 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 7767040, mean_episode_return = 83.217, mean_episode_step = 2343.5, total_loss = 168.51, pg_loss = 97.349, baseline_loss = 76.327, entropy_loss = -5.1647, learner_queue_size = 32, train_seconds = 4331.6, _tick = 1373, _time = 1.6546e+09)
[2022-06-07 11:36:03,630][root][INFO] - Step 7774720 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 7774720, mean_episode_return = None, mean_episode_step = 2043.5, total_loss = -50.815, pg_loss = -57.228, baseline_loss = 11.39, entropy_loss = -4.9759, learner_queue_size = 32, train_seconds = 4336.6, _tick = 1374, _time = 1.6546e+09)
[2022-06-07 11:36:08,651][root][INFO] - Step 7784960 @ 2039.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 7784960, mean_episode_return = 1.71, mean_episode_step = 2195.8, total_loss = -226.53, pg_loss = -236.5, baseline_loss = 15.062, entropy_loss = -5.091, learner_queue_size = 32, train_seconds = 4341.6, _tick = 1376, _time = 1.6546e+09)
[2022-06-07 11:36:13,657][root][INFO] - Step 7792640 @ 1534.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 7792640, mean_episode_return = None, mean_episode_step = 1984.8, total_loss = 139.71, pg_loss = 99.915, baseline_loss = 44.899, entropy_loss = -5.1025, learner_queue_size = 32, train_seconds = 4346.7, _tick = 1377, _time = 1.6546e+09)
[2022-06-07 11:36:18,662][root][INFO] - Step 7802880 @ 2045.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 7802880, mean_episode_return = None, mean_episode_step = 2391.2, total_loss = -18.428, pg_loss = -38.89, baseline_loss = 25.62, entropy_loss = -5.1582, learner_queue_size = 32, train_seconds = 4351.7, _tick = 1378, _time = 1.6546e+09)
[2022-06-07 11:36:23,668][root][INFO] - Step 7813120 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 7813120, mean_episode_return = None, mean_episode_step = 2366.1, total_loss = 32.698, pg_loss = 12.114, baseline_loss = 25.544, entropy_loss = -4.96, learner_queue_size = 32, train_seconds = 4356.7, _tick = 1379, _time = 1.6546e+09)
[2022-06-07 11:36:28,674][root][INFO] - Step 7820800 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7820800, mean_episode_return = 61.401, mean_episode_step = 1706.4, total_loss = 23.723, pg_loss = 4.463, baseline_loss = 24.109, entropy_loss = -4.8486, learner_queue_size = 32, train_seconds = 4361.7, _tick = 1381, _time = 1.6546e+09)
[2022-06-07 11:36:33,680][root][INFO] - Step 7831040 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 7831040, mean_episode_return = None, mean_episode_step = 1708.0, total_loss = 78.823, pg_loss = 29.26, baseline_loss = 54.219, entropy_loss = -4.6567, learner_queue_size = 32, train_seconds = 4366.7, _tick = 1382, _time = 1.6546e+09)
[2022-06-07 11:36:38,686][root][INFO] - Step 7838720 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 7838720, mean_episode_return = 57.24, mean_episode_step = 2029.9, total_loss = 29.984, pg_loss = -12.079, baseline_loss = 46.73, entropy_loss = -4.6669, learner_queue_size = 32, train_seconds = 4371.7, _tick = 1384, _time = 1.6546e+09)
[2022-06-07 11:36:43,690][root][INFO] - Step 7848960 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7848960, mean_episode_return = None, mean_episode_step = 2202.2, total_loss = -118.54, pg_loss = -115.77, baseline_loss = 2.0876, entropy_loss = -4.8529, learner_queue_size = 32, train_seconds = 4376.7, _tick = 1387, _time = 1.6546e+09)
[2022-06-07 11:36:48,691][root][INFO] - Step 7856640 @ 1535.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7856640, mean_episode_return = None, mean_episode_step = 2098.7, total_loss = -79.469, pg_loss = -91.94, baseline_loss = 17.412, entropy_loss = -4.9411, learner_queue_size = 32, train_seconds = 4381.7, _tick = 1387, _time = 1.6546e+09)
[2022-06-07 11:36:53,696][root][INFO] - Step 7866880 @ 2046.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 7866880, mean_episode_return = 92.501, mean_episode_step = 2313.1, total_loss = 15.967, pg_loss = -29.34, baseline_loss = 50.274, entropy_loss = -4.967, learner_queue_size = 32, train_seconds = 4386.7, _tick = 1390, _time = 1.6546e+09)
[2022-06-07 11:36:58,702][root][INFO] - Step 7874560 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 7874560, mean_episode_return = None, mean_episode_step = 1861.1, total_loss = 215.76, pg_loss = 124.06, baseline_loss = 96.721, entropy_loss = -5.0221, learner_queue_size = 32, train_seconds = 4391.7, _tick = 1392, _time = 1.6546e+09)
[2022-06-07 11:37:03,706][root][INFO] - Step 7884800 @ 2046.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 7884800, mean_episode_return = None, mean_episode_step = 2473.4, total_loss = 22.035, pg_loss = -5.8739, baseline_loss = 33.06, entropy_loss = -5.151, learner_queue_size = 32, train_seconds = 4396.7, _tick = 1394, _time = 1.6546e+09)
[2022-06-07 11:37:08,712][root][INFO] - Step 7892480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7892480, mean_episode_return = 47.722, mean_episode_step = 2003.7, total_loss = -78.921, pg_loss = -86.474, baseline_loss = 12.616, entropy_loss = -5.0636, learner_queue_size = 32, train_seconds = 4401.7, _tick = 1396, _time = 1.6546e+09)
[2022-06-07 11:37:13,718][root][INFO] - Step 7902720 @ 2045.6 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 7902720, mean_episode_return = -4.6003, mean_episode_step = 2233.0, total_loss = -34.24, pg_loss = -51.113, baseline_loss = 21.866, entropy_loss = -4.9934, learner_queue_size = 32, train_seconds = 4406.7, _tick = 1399, _time = 1.6546e+09)
[2022-06-07 11:37:18,722][root][INFO] - Step 7910400 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 7910400, mean_episode_return = None, mean_episode_step = 2522.4, total_loss = -13.276, pg_loss = -23.536, baseline_loss = 15.268, entropy_loss = -5.0079, learner_queue_size = 32, train_seconds = 4411.7, _tick = 1401, _time = 1.6546e+09)
[2022-06-07 11:37:23,728][root][INFO] - Step 7920640 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 7920640, mean_episode_return = 104.85, mean_episode_step = 2331.8, total_loss = 55.526, pg_loss = 2.1399, baseline_loss = 58.502, entropy_loss = -5.1155, learner_queue_size = 32, train_seconds = 4416.7, _tick = 1404, _time = 1.6546e+09)
[2022-06-07 11:37:28,734][root][INFO] - Step 7928320 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 7928320, mean_episode_return = 24.31, mean_episode_step = 2129.9, total_loss = 293.04, pg_loss = 203.88, baseline_loss = 94.306, entropy_loss = -5.1455, learner_queue_size = 32, train_seconds = 4421.7, _tick = 1407, _time = 1.6546e+09)
[2022-06-07 11:37:33,740][root][INFO] - Step 7938560 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7938560, mean_episode_return = None, mean_episode_step = 2134.9, total_loss = 24.001, pg_loss = -3.5608, baseline_loss = 32.649, entropy_loss = -5.0881, learner_queue_size = 32, train_seconds = 4426.7, _tick = 1408, _time = 1.6546e+09)
[2022-06-07 11:37:38,750][root][INFO] - Step 7946240 @ 1533.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 7946240, mean_episode_return = 33.3, mean_episode_step = 1894.6, total_loss = 60.296, pg_loss = 1.122, baseline_loss = 64.267, entropy_loss = -5.0923, learner_queue_size = 32, train_seconds = 4431.7, _tick = 1411, _time = 1.6546e+09)
[2022-06-07 11:37:43,755][root][INFO] - Step 7956480 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 7956480, mean_episode_return = 124.4, mean_episode_step = 1966.6, total_loss = -111.23, pg_loss = -132.3, baseline_loss = 26.167, entropy_loss = -5.0961, learner_queue_size = 32, train_seconds = 4436.8, _tick = 1415, _time = 1.6546e+09)
[2022-06-07 11:37:48,758][root][INFO] - Step 7966720 @ 2047.0 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 7966720, mean_episode_return = None, mean_episode_step = 2427.1, total_loss = -137.14, pg_loss = -133.54, baseline_loss = 1.5095, entropy_loss = -5.1104, learner_queue_size = 32, train_seconds = 4441.8, _tick = 1415, _time = 1.6546e+09)
[2022-06-07 11:37:53,762][root][INFO] - Step 7974400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7974400, mean_episode_return = 30.769, mean_episode_step = 2038.0, total_loss = 163.53, pg_loss = 93.551, baseline_loss = 75.068, entropy_loss = -5.0913, learner_queue_size = 32, train_seconds = 4446.8, _tick = 1417, _time = 1.6546e+09)
[2022-06-07 11:37:58,766][root][INFO] - Step 7982080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 7982080, mean_episode_return = 78.595, mean_episode_step = 2375.3, total_loss = -108.74, pg_loss = -119.37, baseline_loss = 15.718, entropy_loss = -5.0805, learner_queue_size = 32, train_seconds = 4451.8, _tick = 1418, _time = 1.6546e+09)
[2022-06-07 11:38:03,770][root][INFO] - Step 7992320 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 7992320, mean_episode_return = None, mean_episode_step = 2229.0, total_loss = 9.4421, pg_loss = -13.401, baseline_loss = 28.0, entropy_loss = -5.1571, learner_queue_size = 32, train_seconds = 4456.8, _tick = 1421, _time = 1.6546e+09)
[2022-06-07 11:38:08,774][root][INFO] - Step 8000000 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8000000, mean_episode_return = None, mean_episode_step = 1875.7, total_loss = -47.949, pg_loss = -50.415, baseline_loss = 7.6567, entropy_loss = -5.1905, learner_queue_size = 32, train_seconds = 4461.8, _tick = 1421, _time = 1.6546e+09)
[2022-06-07 11:38:13,780][root][INFO] - Step 8010240 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 8010240, mean_episode_return = 49.213, mean_episode_step = 2128.8, total_loss = 58.798, pg_loss = -2.5599, baseline_loss = 66.527, entropy_loss = -5.1695, learner_queue_size = 32, train_seconds = 4466.8, _tick = 1423, _time = 1.6546e+09)
[2022-06-07 11:38:18,786][root][INFO] - Step 8017920 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 8017920, mean_episode_return = 69.797, mean_episode_step = 2239.6, total_loss = 19.862, pg_loss = -21.278, baseline_loss = 46.253, entropy_loss = -5.1137, learner_queue_size = 32, train_seconds = 4471.8, _tick = 1425, _time = 1.6546e+09)
[2022-06-07 11:38:23,792][root][INFO] - Step 8028160 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8028160, mean_episode_return = 68.85, mean_episode_step = 2402.2, total_loss = 117.48, pg_loss = 57.618, baseline_loss = 64.793, entropy_loss = -4.9352, learner_queue_size = 32, train_seconds = 4476.8, _tick = 1429, _time = 1.6546e+09)
[2022-06-07 11:38:28,798][root][INFO] - Step 8035840 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8035840, mean_episode_return = None, mean_episode_step = 2673.3, total_loss = -76.795, pg_loss = -74.77, baseline_loss = 2.8668, entropy_loss = -4.8923, learner_queue_size = 32, train_seconds = 4481.8, _tick = 1430, _time = 1.6546e+09)
[2022-06-07 11:38:33,802][root][INFO] - Step 8046080 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8046080, mean_episode_return = 60.64, mean_episode_step = 2381.6, total_loss = 26.944, pg_loss = -11.075, baseline_loss = 42.983, entropy_loss = -4.9636, learner_queue_size = 32, train_seconds = 4486.8, _tick = 1432, _time = 1.6546e+09)
[2022-06-07 11:38:38,808][root][INFO] - Step 8053760 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8053760, mean_episode_return = 121.4, mean_episode_step = 2179.1, total_loss = 213.57, pg_loss = 137.65, baseline_loss = 80.991, entropy_loss = -5.0695, learner_queue_size = 32, train_seconds = 4491.8, _tick = 1434, _time = 1.6546e+09)
[2022-06-07 11:38:43,814][root][INFO] - Step 8064000 @ 2045.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8064000, mean_episode_return = 85.902, mean_episode_step = 2504.6, total_loss = -162.28, pg_loss = -175.97, baseline_loss = 18.733, entropy_loss = -5.046, learner_queue_size = 32, train_seconds = 4496.8, _tick = 1436, _time = 1.6546e+09)
[2022-06-07 11:38:48,818][root][INFO] - Step 8071680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8071680, mean_episode_return = 92.473, mean_episode_step = 2341.3, total_loss = 495.11, pg_loss = 308.2, baseline_loss = 191.84, entropy_loss = -4.9274, learner_queue_size = 32, train_seconds = 4501.8, _tick = 1438, _time = 1.6546e+09)
[2022-06-07 11:38:53,824][root][INFO] - Step 8081920 @ 2045.5 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 8081920, mean_episode_return = None, mean_episode_step = 2350.9, total_loss = 74.243, pg_loss = 43.18, baseline_loss = 35.885, entropy_loss = -4.822, learner_queue_size = 32, train_seconds = 4506.8, _tick = 1441, _time = 1.6546e+09)
[2022-06-07 11:38:58,830][root][INFO] - Step 8089600 @ 1534.2 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 8089600, mean_episode_return = 150.48, mean_episode_step = 2181.5, total_loss = 71.338, pg_loss = 16.937, baseline_loss = 59.211, entropy_loss = -4.8112, learner_queue_size = 32, train_seconds = 4511.8, _tick = 1443, _time = 1.6546e+09)
[2022-06-07 11:39:03,834][root][INFO] - Step 8097280 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8097280, mean_episode_return = None, mean_episode_step = 2554.3, total_loss = -112.19, pg_loss = -118.84, baseline_loss = 11.496, entropy_loss = -4.8478, learner_queue_size = 32, train_seconds = 4516.8, _tick = 1444, _time = 1.6546e+09)
[2022-06-07 11:39:08,838][root][INFO] - Step 8107520 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 8107520, mean_episode_return = None, mean_episode_step = 2269.4, total_loss = 41.289, pg_loss = 12.442, baseline_loss = 33.692, entropy_loss = -4.8445, learner_queue_size = 32, train_seconds = 4521.8, _tick = 1446, _time = 1.6546e+09)
[2022-06-07 11:39:13,842][root][INFO] - Step 8117760 @ 2046.3 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 8117760, mean_episode_return = 29.94, mean_episode_step = 2121.2, total_loss = 238.13, pg_loss = 131.13, baseline_loss = 111.84, entropy_loss = -4.8454, learner_queue_size = 32, train_seconds = 4526.8, _tick = 1448, _time = 1.6546e+09)
[2022-06-07 11:39:18,846][root][INFO] - Step 8125440 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 8125440, mean_episode_return = None, mean_episode_step = 2220.2, total_loss = 273.32, pg_loss = 200.48, baseline_loss = 77.794, entropy_loss = -4.95, learner_queue_size = 32, train_seconds = 4531.8, _tick = 1449, _time = 1.6546e+09)
[2022-06-07 11:39:23,852][root][INFO] - Step 8135680 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 8135680, mean_episode_return = None, mean_episode_step = 2270.0, total_loss = 276.78, pg_loss = 206.42, baseline_loss = 75.35, entropy_loss = -4.9908, learner_queue_size = 32, train_seconds = 4536.8, _tick = 1450, _time = 1.6546e+09)
[2022-06-07 11:39:28,858][root][INFO] - Step 8143360 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8143360, mean_episode_return = 72.806, mean_episode_step = 2338.0, total_loss = -54.105, pg_loss = -84.387, baseline_loss = 35.223, entropy_loss = -4.9413, learner_queue_size = 32, train_seconds = 4541.9, _tick = 1453, _time = 1.6546e+09)
[2022-06-07 11:39:33,862][root][INFO] - Step 8153600 @ 2046.3 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 8153600, mean_episode_return = 22.072, mean_episode_step = 2503.1, total_loss = -9.0366, pg_loss = -34.688, baseline_loss = 30.428, entropy_loss = -4.7763, learner_queue_size = 32, train_seconds = 4546.9, _tick = 1455, _time = 1.6546e+09)
[2022-06-07 11:39:38,866][root][INFO] - Step 8161280 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 8161280, mean_episode_return = None, mean_episode_step = 2305.0, total_loss = 177.63, pg_loss = 113.85, baseline_loss = 68.65, entropy_loss = -4.8706, learner_queue_size = 32, train_seconds = 4551.9, _tick = 1455, _time = 1.6546e+09)
[2022-06-07 11:39:43,870][root][INFO] - Step 8171520 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8171520, mean_episode_return = None, mean_episode_step = 1993.2, total_loss = 300.14, pg_loss = 214.6, baseline_loss = 90.391, entropy_loss = -4.8529, learner_queue_size = 32, train_seconds = 4556.9, _tick = 1456, _time = 1.6546e+09)
[2022-06-07 11:39:48,876][root][INFO] - Step 8179200 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8179200, mean_episode_return = None, mean_episode_step = 2658.6, total_loss = -71.143, pg_loss = -69.816, baseline_loss = 3.4459, entropy_loss = -4.7729, learner_queue_size = 32, train_seconds = 4561.9, _tick = 1456, _time = 1.6546e+09)
[2022-06-07 11:39:53,882][root][INFO] - Step 8189440 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 8189440, mean_episode_return = -1.3905, mean_episode_step = 2635.8, total_loss = 123.27, pg_loss = 49.664, baseline_loss = 78.413, entropy_loss = -4.8029, learner_queue_size = 32, train_seconds = 4566.9, _tick = 1460, _time = 1.6546e+09)
[2022-06-07 11:39:58,887][root][INFO] - Step 8197120 @ 1534.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8197120, mean_episode_return = 5.9896, mean_episode_step = 2338.0, total_loss = -80.331, pg_loss = -93.34, baseline_loss = 17.852, entropy_loss = -4.8433, learner_queue_size = 32, train_seconds = 4571.9, _tick = 1461, _time = 1.6546e+09)
[2022-06-07 11:40:03,890][root][INFO] - Step 8207360 @ 2046.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 8207360, mean_episode_return = None, mean_episode_step = 2864.4, total_loss = -60.435, pg_loss = -58.561, baseline_loss = 3.055, entropy_loss = -4.9294, learner_queue_size = 32, train_seconds = 4576.9, _tick = 1461, _time = 1.6546e+09)
[2022-06-07 11:40:08,896][root][INFO] - Step 8215040 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 8215040, mean_episode_return = 21.101, mean_episode_step = 2420.6, total_loss = 154.88, pg_loss = 77.877, baseline_loss = 81.979, entropy_loss = -4.9809, learner_queue_size = 32, train_seconds = 4581.9, _tick = 1463, _time = 1.6546e+09)
[2022-06-07 11:40:13,901][root][INFO] - Step 8225280 @ 2045.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 8225280, mean_episode_return = 70.926, mean_episode_step = 2374.5, total_loss = 142.57, pg_loss = 40.001, baseline_loss = 107.57, entropy_loss = -4.9977, learner_queue_size = 32, train_seconds = 4586.9, _tick = 1466, _time = 1.6546e+09)
[2022-06-07 11:40:18,906][root][INFO] - Step 8235520 @ 2046.1 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 8235520, mean_episode_return = None, mean_episode_step = 2610.7, total_loss = -97.748, pg_loss = -93.976, baseline_loss = 1.2255, entropy_loss = -4.9973, learner_queue_size = 32, train_seconds = 4591.9, _tick = 1468, _time = 1.6546e+09)
[2022-06-07 11:40:23,910][root][INFO] - Step 8243200 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8243200, mean_episode_return = None, mean_episode_step = 2709.0, total_loss = 46.607, pg_loss = 26.508, baseline_loss = 24.994, entropy_loss = -4.8956, learner_queue_size = 32, train_seconds = 4596.9, _tick = 1470, _time = 1.6546e+09)
[2022-06-07 11:40:28,914][root][INFO] - Step 8250880 @ 1534.6 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 8250880, mean_episode_return = 13.39, mean_episode_step = 2098.2, total_loss = 622.44, pg_loss = 340.02, baseline_loss = 287.33, entropy_loss = -4.9028, learner_queue_size = 32, train_seconds = 4601.9, _tick = 1471, _time = 1.6546e+09)
[2022-06-07 11:40:33,920][root][INFO] - Step 8261120 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8261120, mean_episode_return = 162.33, mean_episode_step = 2701.6, total_loss = 105.99, pg_loss = 52.008, baseline_loss = 58.955, entropy_loss = -4.9723, learner_queue_size = 32, train_seconds = 4606.9, _tick = 1473, _time = 1.6546e+09)
[2022-06-07 11:40:38,926][root][INFO] - Step 8268800 @ 1534.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 8268800, mean_episode_return = None, mean_episode_step = 2440.0, total_loss = 200.68, pg_loss = 130.83, baseline_loss = 74.782, entropy_loss = -4.9303, learner_queue_size = 32, train_seconds = 4611.9, _tick = 1475, _time = 1.6546e+09)
[2022-06-07 11:40:43,931][root][INFO] - Step 8279040 @ 2045.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8279040, mean_episode_return = 87.978, mean_episode_step = 2820.5, total_loss = 47.466, pg_loss = 7.6804, baseline_loss = 44.755, entropy_loss = -4.9695, learner_queue_size = 32, train_seconds = 4616.9, _tick = 1477, _time = 1.6546e+09)
[2022-06-07 11:40:48,934][root][INFO] - Step 8289280 @ 2047.0 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 8289280, mean_episode_return = None, mean_episode_step = 2729.2, total_loss = -26.901, pg_loss = -38.359, baseline_loss = 16.374, entropy_loss = -4.9155, learner_queue_size = 32, train_seconds = 4621.9, _tick = 1479, _time = 1.6546e+09)
[2022-06-07 11:40:53,938][root][INFO] - Step 8296960 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 8296960, mean_episode_return = 0.71927, mean_episode_step = 3121.8, total_loss = 42.542, pg_loss = 7.4718, baseline_loss = 39.889, entropy_loss = -4.8185, learner_queue_size = 32, train_seconds = 4626.9, _tick = 1482, _time = 1.6546e+09)
[2022-06-07 11:40:58,942][root][INFO] - Step 8307200 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8307200, mean_episode_return = None, mean_episode_step = 2452.8, total_loss = -132.38, pg_loss = -130.29, baseline_loss = 2.8392, entropy_loss = -4.9243, learner_queue_size = 32, train_seconds = 4631.9, _tick = 1483, _time = 1.6546e+09)
[2022-06-07 11:41:03,948][root][INFO] - Step 8314880 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8314880, mean_episode_return = None, mean_episode_step = 2736.2, total_loss = 28.11, pg_loss = -0.77286, baseline_loss = 33.839, entropy_loss = -4.9564, learner_queue_size = 32, train_seconds = 4636.9, _tick = 1484, _time = 1.6546e+09)
[2022-06-07 11:41:08,954][root][INFO] - Step 8325120 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8325120, mean_episode_return = None, mean_episode_step = 3042.8, total_loss = -7.9067, pg_loss = -22.326, baseline_loss = 19.486, entropy_loss = -5.0663, learner_queue_size = 32, train_seconds = 4641.9, _tick = 1484, _time = 1.6546e+09)
[2022-06-07 11:41:13,960][root][INFO] - Step 8332800 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8332800, mean_episode_return = None, mean_episode_step = 2480.2, total_loss = -15.797, pg_loss = -28.405, baseline_loss = 17.681, entropy_loss = -5.0729, learner_queue_size = 32, train_seconds = 4647.0, _tick = 1484, _time = 1.6546e+09)
[2022-06-07 11:41:18,966][root][INFO] - Step 8343040 @ 2045.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 8343040, mean_episode_return = None, mean_episode_step = 2472.8, total_loss = 90.055, pg_loss = 64.133, baseline_loss = 31.067, entropy_loss = -5.1454, learner_queue_size = 32, train_seconds = 4652.0, _tick = 1484, _time = 1.6546e+09)
[2022-06-07 11:41:23,970][root][INFO] - Step 8350720 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8350720, mean_episode_return = None, mean_episode_step = 3082.2, total_loss = -36.181, pg_loss = -48.636, baseline_loss = 17.597, entropy_loss = -5.143, learner_queue_size = 32, train_seconds = 4657.0, _tick = 1484, _time = 1.6546e+09)
[2022-06-07 11:41:28,974][root][INFO] - Step 8360960 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8360960, mean_episode_return = 288.85, mean_episode_step = 3037.9, total_loss = -7.9742, pg_loss = -34.051, baseline_loss = 31.39, entropy_loss = -5.3129, learner_queue_size = 32, train_seconds = 4662.0, _tick = 1487, _time = 1.6546e+09)
[2022-06-07 11:41:33,980][root][INFO] - Step 8368640 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 8368640, mean_episode_return = 30.603, mean_episode_step = 3059.1, total_loss = -60.734, pg_loss = -64.515, baseline_loss = 8.9705, entropy_loss = -5.1896, learner_queue_size = 32, train_seconds = 4667.0, _tick = 1488, _time = 1.6546e+09)
[2022-06-07 11:41:38,986][root][INFO] - Step 8378880 @ 2045.6 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 8378880, mean_episode_return = None, mean_episode_step = 2113.7, total_loss = 35.494, pg_loss = 22.435, baseline_loss = 18.271, entropy_loss = -5.211, learner_queue_size = 32, train_seconds = 4672.0, _tick = 1489, _time = 1.6546e+09)
[2022-06-07 11:41:43,990][root][INFO] - Step 8386560 @ 1534.7 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 8386560, mean_episode_return = 9.0299, mean_episode_step = 2698.2, total_loss = 172.18, pg_loss = 92.601, baseline_loss = 84.941, entropy_loss = -5.3596, learner_queue_size = 32, train_seconds = 4677.0, _tick = 1490, _time = 1.6546e+09)
[2022-06-07 11:41:49,007][root][INFO] - Step 8396800 @ 2041.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 8396800, mean_episode_return = 58.961, mean_episode_step = 2612.1, total_loss = -49.98, pg_loss = -65.784, baseline_loss = 21.093, entropy_loss = -5.2886, learner_queue_size = 32, train_seconds = 4682.0, _tick = 1492, _time = 1.6546e+09)
[2022-06-07 11:41:54,010][root][INFO] - Step 8404480 @ 1534.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 8404480, mean_episode_return = 79.087, mean_episode_step = 2420.4, total_loss = 46.578, pg_loss = 12.747, baseline_loss = 39.1, entropy_loss = -5.269, learner_queue_size = 32, train_seconds = 4687.0, _tick = 1495, _time = 1.6546e+09)
[2022-06-07 11:41:59,014][root][INFO] - Step 8414720 @ 2046.3 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 8414720, mean_episode_return = None, mean_episode_step = 2760.1, total_loss = -81.552, pg_loss = -82.24, baseline_loss = 5.9094, entropy_loss = -5.2216, learner_queue_size = 32, train_seconds = 4692.0, _tick = 1497, _time = 1.6546e+09)
[2022-06-07 11:42:04,021][root][INFO] - Step 8422400 @ 1534.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8422400, mean_episode_return = None, mean_episode_step = 2732.7, total_loss = 320.32, pg_loss = 236.22, baseline_loss = 89.414, entropy_loss = -5.308, learner_queue_size = 32, train_seconds = 4697.0, _tick = 1499, _time = 1.6546e+09)
[2022-06-07 11:42:09,022][root][INFO] - Step 8432640 @ 2047.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8432640, mean_episode_return = 96.796, mean_episode_step = 2358.0, total_loss = 213.59, pg_loss = 133.72, baseline_loss = 85.244, entropy_loss = -5.3695, learner_queue_size = 32, train_seconds = 4702.0, _tick = 1502, _time = 1.6546e+09)
[2022-06-07 11:42:14,026][root][INFO] - Step 8442880 @ 2046.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 8442880, mean_episode_return = 17.12, mean_episode_step = 2493.4, total_loss = -33.764, pg_loss = -37.478, baseline_loss = 9.1137, entropy_loss = -5.3999, learner_queue_size = 32, train_seconds = 4707.0, _tick = 1504, _time = 1.6546e+09)
[2022-06-07 11:42:19,038][root][INFO] - Step 8450560 @ 1532.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8450560, mean_episode_return = 70.478, mean_episode_step = 2282.0, total_loss = 115.22, pg_loss = 69.146, baseline_loss = 51.399, entropy_loss = -5.3265, learner_queue_size = 32, train_seconds = 4712.0, _tick = 1505, _time = 1.6546e+09)
[2022-06-07 11:42:24,042][root][INFO] - Step 8458240 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 8458240, mean_episode_return = None, mean_episode_step = 3074.2, total_loss = 22.041, pg_loss = 5.708, baseline_loss = 21.588, entropy_loss = -5.2553, learner_queue_size = 32, train_seconds = 4717.0, _tick = 1505, _time = 1.6546e+09)
[2022-06-07 11:42:29,046][root][INFO] - Step 8468480 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 8468480, mean_episode_return = 91.561, mean_episode_step = 2031.2, total_loss = 201.82, pg_loss = 154.99, baseline_loss = 52.121, entropy_loss = -5.2879, learner_queue_size = 32, train_seconds = 4722.0, _tick = 1509, _time = 1.6546e+09)
[2022-06-07 11:42:34,050][root][INFO] - Step 8476160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8476160, mean_episode_return = 87.729, mean_episode_step = 2534.0, total_loss = 61.496, pg_loss = 37.75, baseline_loss = 28.833, entropy_loss = -5.0867, learner_queue_size = 32, train_seconds = 4727.0, _tick = 1512, _time = 1.6546e+09)
[2022-06-07 11:42:39,054][root][INFO] - Step 8486400 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 8486400, mean_episode_return = None, mean_episode_step = 2723.6, total_loss = 177.21, pg_loss = 125.05, baseline_loss = 57.192, entropy_loss = -5.0381, learner_queue_size = 32, train_seconds = 4732.0, _tick = 1513, _time = 1.6546e+09)
[2022-06-07 11:42:44,060][root][INFO] - Step 8494080 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 8494080, mean_episode_return = 24.67, mean_episode_step = 2523.3, total_loss = 44.834, pg_loss = 10.619, baseline_loss = 39.221, entropy_loss = -5.0067, learner_queue_size = 32, train_seconds = 4737.1, _tick = 1516, _time = 1.6546e+09)
[2022-06-07 11:42:49,065][root][INFO] - Step 8504320 @ 2045.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8504320, mean_episode_return = 47.728, mean_episode_step = 2795.4, total_loss = 70.936, pg_loss = 44.403, baseline_loss = 31.479, entropy_loss = -4.9465, learner_queue_size = 32, train_seconds = 4742.1, _tick = 1517, _time = 1.6546e+09)
[2022-06-07 11:42:54,071][root][INFO] - Step 8512000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8512000, mean_episode_return = 18.96, mean_episode_step = 2659.2, total_loss = 67.326, pg_loss = 25.856, baseline_loss = 46.444, entropy_loss = -4.9744, learner_queue_size = 32, train_seconds = 4747.1, _tick = 1518, _time = 1.6546e+09)
[2022-06-07 11:42:59,077][root][INFO] - Step 8522240 @ 2045.6 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 8522240, mean_episode_return = 74.581, mean_episode_step = 2401.4, total_loss = 39.142, pg_loss = 15.037, baseline_loss = 29.259, entropy_loss = -5.154, learner_queue_size = 32, train_seconds = 4752.1, _tick = 1520, _time = 1.6546e+09)
[2022-06-07 11:43:04,084][root][INFO] - Step 8529920 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 8529920, mean_episode_return = None, mean_episode_step = 2596.5, total_loss = 70.589, pg_loss = 10.52, baseline_loss = 65.178, entropy_loss = -5.1091, learner_queue_size = 32, train_seconds = 4757.1, _tick = 1522, _time = 1.6546e+09)
[2022-06-07 11:43:09,090][root][INFO] - Step 8540160 @ 2045.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 8540160, mean_episode_return = None, mean_episode_step = 2941.6, total_loss = -88.263, pg_loss = -86.682, baseline_loss = 3.4908, entropy_loss = -5.0718, learner_queue_size = 32, train_seconds = 4762.1, _tick = 1524, _time = 1.6546e+09)
[2022-06-07 11:43:14,094][root][INFO] - Step 8550400 @ 2046.1 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 8550400, mean_episode_return = None, mean_episode_step = 2527.2, total_loss = -19.027, pg_loss = -28.223, baseline_loss = 14.28, entropy_loss = -5.0844, learner_queue_size = 32, train_seconds = 4767.1, _tick = 1526, _time = 1.6546e+09)
[2022-06-07 11:43:19,100][root][INFO] - Step 8558080 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8558080, mean_episode_return = None, mean_episode_step = 2370.8, total_loss = 130.45, pg_loss = 87.944, baseline_loss = 47.679, entropy_loss = -5.1749, learner_queue_size = 32, train_seconds = 4772.1, _tick = 1526, _time = 1.6546e+09)
[2022-06-07 11:43:24,106][root][INFO] - Step 8568320 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 8568320, mean_episode_return = None, mean_episode_step = 2521.9, total_loss = 111.84, pg_loss = 78.37, baseline_loss = 38.687, entropy_loss = -5.2157, learner_queue_size = 32, train_seconds = 4777.1, _tick = 1526, _time = 1.6546e+09)
[2022-06-07 11:43:29,112][root][INFO] - Step 8576000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8576000, mean_episode_return = None, mean_episode_step = 2954.9, total_loss = 11.542, pg_loss = -0.16727, baseline_loss = 16.89, entropy_loss = -5.181, learner_queue_size = 32, train_seconds = 4782.1, _tick = 1527, _time = 1.6546e+09)
[2022-06-07 11:43:34,118][root][INFO] - Step 8583680 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8583680, mean_episode_return = 30.1, mean_episode_step = 2190.0, total_loss = 404.13, pg_loss = 280.89, baseline_loss = 128.45, entropy_loss = -5.207, learner_queue_size = 32, train_seconds = 4787.1, _tick = 1529, _time = 1.6546e+09)
[2022-06-07 11:43:39,123][root][INFO] - Step 8593920 @ 2046.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8593920, mean_episode_return = None, mean_episode_step = 2822.6, total_loss = -84.652, pg_loss = -81.78, baseline_loss = 2.3083, entropy_loss = -5.1798, learner_queue_size = 32, train_seconds = 4792.1, _tick = 1532, _time = 1.6546e+09)
[2022-06-07 11:43:44,129][root][INFO] - Step 8601600 @ 1534.1 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 8601600, mean_episode_return = 99.618, mean_episode_step = 2370.0, total_loss = 100.62, pg_loss = 39.58, baseline_loss = 66.172, entropy_loss = -5.1293, learner_queue_size = 32, train_seconds = 4797.1, _tick = 1535, _time = 1.6546e+09)
[2022-06-07 11:43:49,137][root][INFO] - Step 8611840 @ 2044.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 8611840, mean_episode_return = None, mean_episode_step = 2356.2, total_loss = -41.885, pg_loss = -40.875, baseline_loss = 4.073, entropy_loss = -5.0832, learner_queue_size = 32, train_seconds = 4802.1, _tick = 1536, _time = 1.6546e+09)
[2022-06-07 11:43:54,144][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 11:43:54,388][root][INFO] - Step 8619520 @ 1534.1 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 8622080, mean_episode_return = 85.413, mean_episode_step = 2422.9, total_loss = 48.99, pg_loss = 11.433, baseline_loss = 42.734, entropy_loss = -5.1778, learner_queue_size = 32, train_seconds = 4807.1, _tick = 1538, _time = 1.6546e+09)
[2022-06-07 11:43:59,394][root][INFO] - Step 8629760 @ 1950.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8629760, mean_episode_return = 78.496, mean_episode_step = 2515.3, total_loss = -9.0847, pg_loss = -25.447, baseline_loss = 21.629, entropy_loss = -5.2669, learner_queue_size = 32, train_seconds = 4812.4, _tick = 1541, _time = 1.6546e+09)
[2022-06-07 11:44:04,398][root][INFO] - Step 8640000 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 8640000, mean_episode_return = None, mean_episode_step = 2870.4, total_loss = -71.767, pg_loss = -72.26, baseline_loss = 5.8917, entropy_loss = -5.3989, learner_queue_size = 32, train_seconds = 4817.4, _tick = 1544, _time = 1.6546e+09)
[2022-06-07 11:44:09,404][root][INFO] - Step 8647680 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8647680, mean_episode_return = 50.634, mean_episode_step = 2305.9, total_loss = 264.41, pg_loss = 182.3, baseline_loss = 87.484, entropy_loss = -5.3716, learner_queue_size = 32, train_seconds = 4822.4, _tick = 1547, _time = 1.6546e+09)
[2022-06-07 11:44:14,410][root][INFO] - Step 8657920 @ 2045.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 8657920, mean_episode_return = 16.3, mean_episode_step = 2294.5, total_loss = 411.15, pg_loss = 279.89, baseline_loss = 136.63, entropy_loss = -5.3807, learner_queue_size = 32, train_seconds = 4827.4, _tick = 1551, _time = 1.6546e+09)
[2022-06-07 11:44:19,414][root][INFO] - Step 8665600 @ 1534.9 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 8665600, mean_episode_return = 42.547, mean_episode_step = 2436.5, total_loss = -84.628, pg_loss = -84.582, baseline_loss = 5.2985, entropy_loss = -5.3439, learner_queue_size = 32, train_seconds = 4832.4, _tick = 1553, _time = 1.6546e+09)
[2022-06-07 11:44:24,418][root][INFO] - Step 8675840 @ 2046.4 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 8675840, mean_episode_return = 94.039, mean_episode_step = 2346.9, total_loss = 271.64, pg_loss = 195.68, baseline_loss = 81.409, entropy_loss = -5.4448, learner_queue_size = 32, train_seconds = 4837.4, _tick = 1556, _time = 1.6546e+09)
[2022-06-07 11:44:29,427][root][INFO] - Step 8683520 @ 1533.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8683520, mean_episode_return = 50.606, mean_episode_step = 2129.2, total_loss = 44.807, pg_loss = 20.221, baseline_loss = 30.068, entropy_loss = -5.4819, learner_queue_size = 32, train_seconds = 4842.4, _tick = 1558, _time = 1.6546e+09)
[2022-06-07 11:44:34,430][root][INFO] - Step 8693760 @ 2046.9 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 8693760, mean_episode_return = 17.065, mean_episode_step = 2785.2, total_loss = 159.4, pg_loss = 103.58, baseline_loss = 61.24, entropy_loss = -5.4115, learner_queue_size = 32, train_seconds = 4847.4, _tick = 1560, _time = 1.6546e+09)
[2022-06-07 11:44:39,436][root][INFO] - Step 8701440 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8701440, mean_episode_return = 99.659, mean_episode_step = 1972.5, total_loss = 171.39, pg_loss = 115.83, baseline_loss = 60.89, entropy_loss = -5.3311, learner_queue_size = 32, train_seconds = 4852.4, _tick = 1561, _time = 1.6546e+09)
[2022-06-07 11:44:44,442][root][INFO] - Step 8711680 @ 2045.5 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 8711680, mean_episode_return = 47.243, mean_episode_step = 2095.4, total_loss = 196.53, pg_loss = 115.8, baseline_loss = 85.971, entropy_loss = -5.2452, learner_queue_size = 32, train_seconds = 4857.4, _tick = 1564, _time = 1.6546e+09)
[2022-06-07 11:44:49,448][root][INFO] - Step 8719360 @ 1534.2 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 8719360, mean_episode_return = 73.585, mean_episode_step = 1515.6, total_loss = 178.98, pg_loss = 102.79, baseline_loss = 81.451, entropy_loss = -5.2639, learner_queue_size = 32, train_seconds = 4862.4, _tick = 1566, _time = 1.6546e+09)
[2022-06-07 11:44:54,454][root][INFO] - Step 8727040 @ 1534.1 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 8727040, mean_episode_return = 39.078, mean_episode_step = 2090.0, total_loss = 194.62, pg_loss = 130.45, baseline_loss = 69.368, entropy_loss = -5.2001, learner_queue_size = 32, train_seconds = 4867.4, _tick = 1569, _time = 1.6546e+09)
[2022-06-07 11:44:59,458][root][INFO] - Step 8737280 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 8737280, mean_episode_return = 20.54, mean_episode_step = 2411.1, total_loss = 9.1803, pg_loss = -17.466, baseline_loss = 31.927, entropy_loss = -5.2804, learner_queue_size = 32, train_seconds = 4872.5, _tick = 1572, _time = 1.6546e+09)
[2022-06-07 11:45:04,461][root][INFO] - Step 8747520 @ 2046.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8747520, mean_episode_return = 70.177, mean_episode_step = 2234.0, total_loss = 129.18, pg_loss = 28.908, baseline_loss = 105.54, entropy_loss = -5.2676, learner_queue_size = 32, train_seconds = 4877.5, _tick = 1574, _time = 1.6546e+09)
[2022-06-07 11:45:09,466][root][INFO] - Step 8755200 @ 1534.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 8755200, mean_episode_return = None, mean_episode_step = 1829.8, total_loss = -40.85, pg_loss = -42.753, baseline_loss = 6.9828, entropy_loss = -5.0795, learner_queue_size = 32, train_seconds = 4882.5, _tick = 1574, _time = 1.6546e+09)
[2022-06-07 11:45:14,470][root][INFO] - Step 8765440 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 8765440, mean_episode_return = 57.132, mean_episode_step = 2464.0, total_loss = 67.501, pg_loss = 26.039, baseline_loss = 46.563, entropy_loss = -5.1004, learner_queue_size = 32, train_seconds = 4887.5, _tick = 1575, _time = 1.6546e+09)
[2022-06-07 11:45:19,476][root][INFO] - Step 8773120 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 8773120, mean_episode_return = None, mean_episode_step = 2028.9, total_loss = -51.954, pg_loss = -76.586, baseline_loss = 29.732, entropy_loss = -5.1003, learner_queue_size = 32, train_seconds = 4892.5, _tick = 1577, _time = 1.6546e+09)
[2022-06-07 11:45:24,478][root][INFO] - Step 8783360 @ 2047.0 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 8783360, mean_episode_return = 40.493, mean_episode_step = 1654.0, total_loss = 213.61, pg_loss = 133.1, baseline_loss = 85.528, entropy_loss = -5.0228, learner_queue_size = 32, train_seconds = 4897.5, _tick = 1581, _time = 1.6546e+09)
[2022-06-07 11:45:29,482][root][INFO] - Step 8791040 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8791040, mean_episode_return = 88.47, mean_episode_step = 2680.3, total_loss = 100.95, pg_loss = 52.747, baseline_loss = 53.201, entropy_loss = -5.0001, learner_queue_size = 32, train_seconds = 4902.5, _tick = 1584, _time = 1.6546e+09)
[2022-06-07 11:45:34,486][root][INFO] - Step 8801280 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 8801280, mean_episode_return = 76.499, mean_episode_step = 1816.6, total_loss = 5.0247, pg_loss = -26.434, baseline_loss = 36.301, entropy_loss = -4.8422, learner_queue_size = 32, train_seconds = 4907.5, _tick = 1587, _time = 1.6546e+09)
[2022-06-07 11:45:39,490][root][INFO] - Step 8808960 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 8808960, mean_episode_return = 51.5, mean_episode_step = 1941.7, total_loss = 46.808, pg_loss = -6.9373, baseline_loss = 58.614, entropy_loss = -4.8679, learner_queue_size = 32, train_seconds = 4912.5, _tick = 1588, _time = 1.6546e+09)
[2022-06-07 11:45:44,494][root][INFO] - Step 8819200 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 8819200, mean_episode_return = 34.66, mean_episode_step = 1546.6, total_loss = 28.909, pg_loss = -23.305, baseline_loss = 57.033, entropy_loss = -4.8196, learner_queue_size = 32, train_seconds = 4917.5, _tick = 1592, _time = 1.6546e+09)
[2022-06-07 11:45:49,498][root][INFO] - Step 8826880 @ 1534.7 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 8826880, mean_episode_return = None, mean_episode_step = 2446.5, total_loss = 29.195, pg_loss = -0.10689, baseline_loss = 34.192, entropy_loss = -4.8897, learner_queue_size = 32, train_seconds = 4922.5, _tick = 1593, _time = 1.6546e+09)
[2022-06-07 11:45:54,502][root][INFO] - Step 8837120 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 8837120, mean_episode_return = None, mean_episode_step = 1562.2, total_loss = 84.133, pg_loss = 29.818, baseline_loss = 59.337, entropy_loss = -5.0212, learner_queue_size = 32, train_seconds = 4927.5, _tick = 1594, _time = 1.6546e+09)
[2022-06-07 11:45:59,506][root][INFO] - Step 8844800 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 8844800, mean_episode_return = 83.927, mean_episode_step = 2077.2, total_loss = -79.828, pg_loss = -103.19, baseline_loss = 28.296, entropy_loss = -4.9309, learner_queue_size = 32, train_seconds = 4932.5, _tick = 1596, _time = 1.6546e+09)
[2022-06-07 11:46:04,512][root][INFO] - Step 8855040 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 8855040, mean_episode_return = 57.361, mean_episode_step = 2760.6, total_loss = -244.36, pg_loss = -255.57, baseline_loss = 16.065, entropy_loss = -4.8526, learner_queue_size = 32, train_seconds = 4937.5, _tick = 1597, _time = 1.6546e+09)
[2022-06-07 11:46:09,518][root][INFO] - Step 8862720 @ 1534.1 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 8862720, mean_episode_return = None, mean_episode_step = 1674.6, total_loss = -37.908, pg_loss = -58.334, baseline_loss = 25.439, entropy_loss = -5.0139, learner_queue_size = 32, train_seconds = 4942.5, _tick = 1598, _time = 1.6546e+09)
[2022-06-07 11:46:14,522][root][INFO] - Step 8872960 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 8872960, mean_episode_return = None, mean_episode_step = 2022.5, total_loss = 118.11, pg_loss = 59.062, baseline_loss = 63.966, entropy_loss = -4.9167, learner_queue_size = 32, train_seconds = 4947.5, _tick = 1599, _time = 1.6546e+09)
[2022-06-07 11:46:19,526][root][INFO] - Step 8880640 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 8880640, mean_episode_return = None, mean_episode_step = 1959.9, total_loss = 133.81, pg_loss = 85.886, baseline_loss = 52.365, entropy_loss = -4.4445, learner_queue_size = 32, train_seconds = 4952.5, _tick = 1600, _time = 1.6546e+09)
[2022-06-07 11:46:24,530][root][INFO] - Step 8890880 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8890880, mean_episode_return = None, mean_episode_step = 2149.2, total_loss = 343.55, pg_loss = 248.21, baseline_loss = 100.06, entropy_loss = -4.7259, learner_queue_size = 32, train_seconds = 4957.5, _tick = 1602, _time = 1.6546e+09)
[2022-06-07 11:46:29,536][root][INFO] - Step 8898560 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 8898560, mean_episode_return = None, mean_episode_step = 2338.6, total_loss = 145.15, pg_loss = 100.43, baseline_loss = 49.517, entropy_loss = -4.7953, learner_queue_size = 32, train_seconds = 4962.5, _tick = 1603, _time = 1.6546e+09)
[2022-06-07 11:46:34,538][root][INFO] - Step 8908800 @ 2047.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 8908800, mean_episode_return = 69.054, mean_episode_step = 1647.6, total_loss = -24.462, pg_loss = -55.763, baseline_loss = 36.234, entropy_loss = -4.9329, learner_queue_size = 32, train_seconds = 4967.5, _tick = 1605, _time = 1.6546e+09)
[2022-06-07 11:46:39,541][root][INFO] - Step 8916480 @ 1535.0 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 8916480, mean_episode_return = None, mean_episode_step = 2946.1, total_loss = -49.689, pg_loss = -64.092, baseline_loss = 19.286, entropy_loss = -4.8827, learner_queue_size = 32, train_seconds = 4972.5, _tick = 1605, _time = 1.6546e+09)
[2022-06-07 11:46:44,546][root][INFO] - Step 8926720 @ 2046.0 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 8926720, mean_episode_return = None, mean_episode_step = 2517.5, total_loss = -75.25, pg_loss = -92.993, baseline_loss = 22.525, entropy_loss = -4.783, learner_queue_size = 32, train_seconds = 4977.5, _tick = 1607, _time = 1.6546e+09)
[2022-06-07 11:46:49,550][root][INFO] - Step 8934400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8934400, mean_episode_return = 100.02, mean_episode_step = 2335.9, total_loss = 110.0, pg_loss = -23.858, baseline_loss = 138.76, entropy_loss = -4.9038, learner_queue_size = 32, train_seconds = 4982.5, _tick = 1608, _time = 1.6546e+09)
[2022-06-07 11:46:54,554][root][INFO] - Step 8944640 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 8944640, mean_episode_return = 2.1068, mean_episode_step = 2260.5, total_loss = 29.591, pg_loss = -24.911, baseline_loss = 59.301, entropy_loss = -4.7983, learner_queue_size = 32, train_seconds = 4987.5, _tick = 1611, _time = 1.6546e+09)
[2022-06-07 11:46:59,558][root][INFO] - Step 8952320 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8952320, mean_episode_return = 122.31, mean_episode_step = 1740.0, total_loss = 187.43, pg_loss = 87.565, baseline_loss = 104.68, entropy_loss = -4.8115, learner_queue_size = 32, train_seconds = 4992.6, _tick = 1614, _time = 1.6546e+09)
[2022-06-07 11:47:04,564][root][INFO] - Step 8962560 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 8962560, mean_episode_return = -4.8208, mean_episode_step = 1822.6, total_loss = 34.587, pg_loss = 0.54227, baseline_loss = 38.795, entropy_loss = -4.7508, learner_queue_size = 32, train_seconds = 4997.6, _tick = 1616, _time = 1.6546e+09)
[2022-06-07 11:47:09,570][root][INFO] - Step 8972800 @ 2045.6 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 8972800, mean_episode_return = None, mean_episode_step = 2046.2, total_loss = 317.89, pg_loss = 196.42, baseline_loss = 126.41, entropy_loss = -4.9343, learner_queue_size = 32, train_seconds = 5002.6, _tick = 1618, _time = 1.6546e+09)
[2022-06-07 11:47:14,574][root][INFO] - Step 8980480 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 8980480, mean_episode_return = None, mean_episode_step = 2481.9, total_loss = -56.726, pg_loss = -64.862, baseline_loss = 13.186, entropy_loss = -5.0499, learner_queue_size = 32, train_seconds = 5007.6, _tick = 1619, _time = 1.6546e+09)
[2022-06-07 11:47:19,578][root][INFO] - Step 8990720 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 8990720, mean_episode_return = 158.54, mean_episode_step = 2020.4, total_loss = -37.439, pg_loss = -56.785, baseline_loss = 24.366, entropy_loss = -5.0198, learner_queue_size = 32, train_seconds = 5012.6, _tick = 1620, _time = 1.6546e+09)
[2022-06-07 11:47:24,582][root][INFO] - Step 8998400 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 8998400, mean_episode_return = 110.37, mean_episode_step = 2328.5, total_loss = 90.424, pg_loss = 32.613, baseline_loss = 62.776, entropy_loss = -4.9651, learner_queue_size = 32, train_seconds = 5017.6, _tick = 1622, _time = 1.6546e+09)
[2022-06-07 11:47:29,588][root][INFO] - Step 9008640 @ 2045.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 9008640, mean_episode_return = None, mean_episode_step = 2357.4, total_loss = -150.75, pg_loss = -146.59, baseline_loss = 0.9119, entropy_loss = -5.0723, learner_queue_size = 32, train_seconds = 5022.6, _tick = 1622, _time = 1.6546e+09)
[2022-06-07 11:47:34,594][root][INFO] - Step 9016320 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 9016320, mean_episode_return = 28.79, mean_episode_step = 2047.8, total_loss = -35.14, pg_loss = -51.775, baseline_loss = 21.782, entropy_loss = -5.1469, learner_queue_size = 32, train_seconds = 5027.6, _tick = 1624, _time = 1.6546e+09)
[2022-06-07 11:47:39,598][root][INFO] - Step 9026560 @ 2046.5 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 9026560, mean_episode_return = None, mean_episode_step = 2383.6, total_loss = 29.852, pg_loss = 10.144, baseline_loss = 24.888, entropy_loss = -5.1798, learner_queue_size = 32, train_seconds = 5032.6, _tick = 1625, _time = 1.6546e+09)
[2022-06-07 11:47:44,604][root][INFO] - Step 9034240 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9034240, mean_episode_return = None, mean_episode_step = 3141.8, total_loss = -138.42, pg_loss = -134.44, baseline_loss = 1.2434, entropy_loss = -5.2242, learner_queue_size = 32, train_seconds = 5037.6, _tick = 1626, _time = 1.6546e+09)
[2022-06-07 11:47:49,610][root][INFO] - Step 9044480 @ 2045.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 9044480, mean_episode_return = 30.41, mean_episode_step = 2074.2, total_loss = -68.438, pg_loss = -93.428, baseline_loss = 30.211, entropy_loss = -5.2203, learner_queue_size = 32, train_seconds = 5042.6, _tick = 1627, _time = 1.6546e+09)
[2022-06-07 11:47:54,614][root][INFO] - Step 9054720 @ 2046.3 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 9054720, mean_episode_return = None, mean_episode_step = 2680.2, total_loss = -17.501, pg_loss = -29.826, baseline_loss = 17.547, entropy_loss = -5.2219, learner_queue_size = 32, train_seconds = 5047.6, _tick = 1628, _time = 1.6546e+09)
[2022-06-07 11:47:59,618][root][INFO] - Step 9062400 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 9062400, mean_episode_return = None, mean_episode_step = 1757.1, total_loss = 175.83, pg_loss = 98.964, baseline_loss = 82.038, entropy_loss = -5.1738, learner_queue_size = 32, train_seconds = 5052.6, _tick = 1629, _time = 1.6546e+09)
[2022-06-07 11:48:04,622][root][INFO] - Step 9070080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9070080, mean_episode_return = None, mean_episode_step = 2483.2, total_loss = -55.927, pg_loss = -70.004, baseline_loss = 19.174, entropy_loss = -5.097, learner_queue_size = 32, train_seconds = 5057.6, _tick = 1629, _time = 1.6546e+09)
[2022-06-07 11:48:09,626][root][INFO] - Step 9080320 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 9080320, mean_episode_return = None, mean_episode_step = 2256.4, total_loss = 10.764, pg_loss = -7.2425, baseline_loss = 23.103, entropy_loss = -5.0966, learner_queue_size = 32, train_seconds = 5062.6, _tick = 1630, _time = 1.6546e+09)
[2022-06-07 11:48:14,630][root][INFO] - Step 9090560 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 9090560, mean_episode_return = None, mean_episode_step = 2069.6, total_loss = 275.12, pg_loss = 175.71, baseline_loss = 104.61, entropy_loss = -5.2004, learner_queue_size = 32, train_seconds = 5067.6, _tick = 1630, _time = 1.6546e+09)
[2022-06-07 11:48:19,632][root][INFO] - Step 9098240 @ 1535.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9098240, mean_episode_return = 56.0, mean_episode_step = 2680.7, total_loss = -110.74, pg_loss = -107.96, baseline_loss = 2.4138, entropy_loss = -5.1956, learner_queue_size = 32, train_seconds = 5072.6, _tick = 1631, _time = 1.6546e+09)
[2022-06-07 11:48:24,638][root][INFO] - Step 9105920 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9105920, mean_episode_return = None, mean_episode_step = 2195.7, total_loss = -101.15, pg_loss = -96.616, baseline_loss = 0.57308, entropy_loss = -5.109, learner_queue_size = 32, train_seconds = 5077.6, _tick = 1631, _time = 1.6546e+09)
[2022-06-07 11:48:29,644][root][INFO] - Step 9116160 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 9116160, mean_episode_return = 1.5295, mean_episode_step = 2888.5, total_loss = -108.72, pg_loss = -105.94, baseline_loss = 2.5558, entropy_loss = -5.3282, learner_queue_size = 32, train_seconds = 5082.6, _tick = 1635, _time = 1.6546e+09)
[2022-06-07 11:48:34,650][root][INFO] - Step 9126400 @ 2045.6 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 9126400, mean_episode_return = None, mean_episode_step = 2398.7, total_loss = -98.919, pg_loss = -94.56, baseline_loss = 1.1017, entropy_loss = -5.46, learner_queue_size = 32, train_seconds = 5087.6, _tick = 1637, _time = 1.6546e+09)
[2022-06-07 11:48:39,656][root][INFO] - Step 9134080 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9134080, mean_episode_return = 55.072, mean_episode_step = 2276.8, total_loss = 194.51, pg_loss = 132.69, baseline_loss = 67.316, entropy_loss = -5.4926, learner_queue_size = 32, train_seconds = 5092.7, _tick = 1639, _time = 1.6546e+09)
[2022-06-07 11:48:44,662][root][INFO] - Step 9144320 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9144320, mean_episode_return = None, mean_episode_step = 2203.9, total_loss = 1189.9, pg_loss = 457.57, baseline_loss = 737.66, entropy_loss = -5.3631, learner_queue_size = 32, train_seconds = 5097.7, _tick = 1640, _time = 1.6546e+09)
[2022-06-07 11:48:49,668][root][INFO] - Step 9152000 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 9152000, mean_episode_return = None, mean_episode_step = 1991.1, total_loss = -25.855, pg_loss = -30.836, baseline_loss = 10.239, entropy_loss = -5.2579, learner_queue_size = 32, train_seconds = 5102.7, _tick = 1640, _time = 1.6546e+09)
[2022-06-07 11:48:54,674][root][INFO] - Step 9162240 @ 2045.7 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 9162240, mean_episode_return = None, mean_episode_step = 2841.6, total_loss = 21.859, pg_loss = 12.211, baseline_loss = 15.119, entropy_loss = -5.4709, learner_queue_size = 32, train_seconds = 5107.7, _tick = 1642, _time = 1.6546e+09)
[2022-06-07 11:48:59,678][root][INFO] - Step 9169920 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 9169920, mean_episode_return = 84.515, mean_episode_step = 3026.6, total_loss = 343.25, pg_loss = 219.57, baseline_loss = 129.01, entropy_loss = -5.3285, learner_queue_size = 32, train_seconds = 5112.7, _tick = 1645, _time = 1.6546e+09)
[2022-06-07 11:49:04,682][root][INFO] - Step 9180160 @ 2046.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 9180160, mean_episode_return = None, mean_episode_step = 2531.5, total_loss = 92.138, pg_loss = 57.538, baseline_loss = 39.792, entropy_loss = -5.1922, learner_queue_size = 32, train_seconds = 5117.7, _tick = 1647, _time = 1.6546e+09)
[2022-06-07 11:49:09,690][root][INFO] - Step 9190400 @ 2044.7 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 9190400, mean_episode_return = None, mean_episode_step = 2375.7, total_loss = 113.47, pg_loss = 80.025, baseline_loss = 38.658, entropy_loss = -5.212, learner_queue_size = 32, train_seconds = 5122.7, _tick = 1649, _time = 1.6546e+09)
[2022-06-07 11:49:14,694][root][INFO] - Step 9198080 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 9198080, mean_episode_return = None, mean_episode_step = 1907.4, total_loss = 12.969, pg_loss = -10.573, baseline_loss = 28.998, entropy_loss = -5.4561, learner_queue_size = 32, train_seconds = 5127.7, _tick = 1650, _time = 1.6546e+09)
[2022-06-07 11:49:19,698][root][INFO] - Step 9208320 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9208320, mean_episode_return = None, mean_episode_step = 2843.3, total_loss = -32.459, pg_loss = -40.922, baseline_loss = 13.804, entropy_loss = -5.3402, learner_queue_size = 32, train_seconds = 5132.7, _tick = 1652, _time = 1.6546e+09)
[2022-06-07 11:49:24,702][root][INFO] - Step 9216000 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9216000, mean_episode_return = None, mean_episode_step = 2660.8, total_loss = 95.577, pg_loss = 53.378, baseline_loss = 47.54, entropy_loss = -5.3412, learner_queue_size = 32, train_seconds = 5137.7, _tick = 1654, _time = 1.6546e+09)
[2022-06-07 11:49:29,706][root][INFO] - Step 9226240 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 9226240, mean_episode_return = None, mean_episode_step = 3116.4, total_loss = 234.03, pg_loss = 148.56, baseline_loss = 90.647, entropy_loss = -5.179, learner_queue_size = 32, train_seconds = 5142.7, _tick = 1655, _time = 1.6546e+09)
[2022-06-07 11:49:34,712][root][INFO] - Step 9233920 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 9233920, mean_episode_return = -4.161, mean_episode_step = 2226.8, total_loss = -50.471, pg_loss = -77.535, baseline_loss = 32.322, entropy_loss = -5.2578, learner_queue_size = 32, train_seconds = 5147.7, _tick = 1657, _time = 1.6546e+09)
[2022-06-07 11:49:39,718][root][INFO] - Step 9244160 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 9244160, mean_episode_return = None, mean_episode_step = 2471.6, total_loss = 384.23, pg_loss = 294.48, baseline_loss = 95.021, entropy_loss = -5.2667, learner_queue_size = 32, train_seconds = 5152.7, _tick = 1659, _time = 1.6546e+09)
[2022-06-07 11:49:44,722][root][INFO] - Step 9251840 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 9251840, mean_episode_return = 3.7595, mean_episode_step = 2277.0, total_loss = -55.466, pg_loss = -66.589, baseline_loss = 16.397, entropy_loss = -5.2738, learner_queue_size = 32, train_seconds = 5157.7, _tick = 1662, _time = 1.6546e+09)
[2022-06-07 11:49:49,728][root][INFO] - Step 9259520 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9259520, mean_episode_return = 75.729, mean_episode_step = 2578.3, total_loss = 60.545, pg_loss = 18.577, baseline_loss = 47.217, entropy_loss = -5.2485, learner_queue_size = 32, train_seconds = 5162.7, _tick = 1663, _time = 1.6546e+09)
[2022-06-07 11:49:54,737][root][INFO] - Step 9269760 @ 2044.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 9269760, mean_episode_return = None, mean_episode_step = 2518.7, total_loss = 68.582, pg_loss = 16.248, baseline_loss = 57.645, entropy_loss = -5.3112, learner_queue_size = 32, train_seconds = 5167.7, _tick = 1665, _time = 1.6546e+09)
[2022-06-07 11:49:59,743][root][INFO] - Step 9277440 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9277440, mean_episode_return = 12.447, mean_episode_step = 2469.4, total_loss = -152.52, pg_loss = -155.2, baseline_loss = 7.9268, entropy_loss = -5.2443, learner_queue_size = 32, train_seconds = 5172.7, _tick = 1668, _time = 1.6546e+09)
[2022-06-07 11:50:04,746][root][INFO] - Step 9287680 @ 2047.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 9287680, mean_episode_return = 105.28, mean_episode_step = 2686.0, total_loss = -52.587, pg_loss = -77.291, baseline_loss = 29.952, entropy_loss = -5.2479, learner_queue_size = 32, train_seconds = 5177.7, _tick = 1670, _time = 1.6546e+09)
[2022-06-07 11:50:09,748][root][INFO] - Step 9295360 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9295360, mean_episode_return = None, mean_episode_step = 2472.8, total_loss = 204.1, pg_loss = 125.31, baseline_loss = 83.89, entropy_loss = -5.1075, learner_queue_size = 32, train_seconds = 5182.7, _tick = 1672, _time = 1.6546e+09)
[2022-06-07 11:50:14,753][root][INFO] - Step 9305600 @ 2045.9 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 9305600, mean_episode_return = 16.04, mean_episode_step = 2525.6, total_loss = 225.05, pg_loss = 120.68, baseline_loss = 109.52, entropy_loss = -5.1473, learner_queue_size = 32, train_seconds = 5187.7, _tick = 1674, _time = 1.6546e+09)
[2022-06-07 11:50:19,758][root][INFO] - Step 9313280 @ 1534.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9313280, mean_episode_return = None, mean_episode_step = 2383.1, total_loss = 360.57, pg_loss = 262.09, baseline_loss = 103.55, entropy_loss = -5.0749, learner_queue_size = 32, train_seconds = 5192.8, _tick = 1676, _time = 1.6546e+09)
[2022-06-07 11:50:24,764][root][INFO] - Step 9323520 @ 2045.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 9323520, mean_episode_return = None, mean_episode_step = 2359.6, total_loss = -79.21, pg_loss = -90.015, baseline_loss = 15.858, entropy_loss = -5.0526, learner_queue_size = 32, train_seconds = 5197.8, _tick = 1677, _time = 1.6546e+09)
[2022-06-07 11:50:29,770][root][INFO] - Step 9331200 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9331200, mean_episode_return = 37.344, mean_episode_step = 2410.8, total_loss = -212.33, pg_loss = -217.28, baseline_loss = 9.87, entropy_loss = -4.9121, learner_queue_size = 32, train_seconds = 5202.8, _tick = 1679, _time = 1.6546e+09)
[2022-06-07 11:50:34,773][root][INFO] - Step 9341440 @ 2047.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9341440, mean_episode_return = 17.02, mean_episode_step = 2470.8, total_loss = -129.1, pg_loss = -130.88, baseline_loss = 6.5616, entropy_loss = -4.7772, learner_queue_size = 32, train_seconds = 5207.8, _tick = 1682, _time = 1.6546e+09)
[2022-06-07 11:50:39,779][root][INFO] - Step 9349120 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 9349120, mean_episode_return = -15.691, mean_episode_step = 1839.6, total_loss = -58.052, pg_loss = -77.57, baseline_loss = 24.384, entropy_loss = -4.8669, learner_queue_size = 32, train_seconds = 5212.8, _tick = 1684, _time = 1.6546e+09)
[2022-06-07 11:50:44,785][root][INFO] - Step 9359360 @ 2045.6 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 9359360, mean_episode_return = 58.824, mean_episode_step = 2215.7, total_loss = 106.12, pg_loss = 44.462, baseline_loss = 66.442, entropy_loss = -4.782, learner_queue_size = 32, train_seconds = 5217.8, _tick = 1686, _time = 1.6546e+09)
[2022-06-07 11:50:49,790][root][INFO] - Step 9367040 @ 1534.3 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 9367040, mean_episode_return = 73.019, mean_episode_step = 2344.8, total_loss = 433.94, pg_loss = 289.4, baseline_loss = 149.29, entropy_loss = -4.7481, learner_queue_size = 32, train_seconds = 5222.8, _tick = 1688, _time = 1.6546e+09)
[2022-06-07 11:50:54,794][root][INFO] - Step 9377280 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 9377280, mean_episode_return = None, mean_episode_step = 2239.4, total_loss = 133.64, pg_loss = 78.003, baseline_loss = 60.385, entropy_loss = -4.7476, learner_queue_size = 32, train_seconds = 5227.8, _tick = 1690, _time = 1.6546e+09)
[2022-06-07 11:50:59,798][root][INFO] - Step 9384960 @ 1534.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 9384960, mean_episode_return = None, mean_episode_step = 2344.2, total_loss = -106.9, pg_loss = -104.21, baseline_loss = 2.0506, entropy_loss = -4.7447, learner_queue_size = 32, train_seconds = 5232.8, _tick = 1691, _time = 1.6546e+09)
[2022-06-07 11:51:04,801][root][INFO] - Step 9395200 @ 2047.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9395200, mean_episode_return = 15.171, mean_episode_step = 2619.7, total_loss = 503.74, pg_loss = 269.9, baseline_loss = 238.7, entropy_loss = -4.8568, learner_queue_size = 32, train_seconds = 5237.8, _tick = 1692, _time = 1.6546e+09)
[2022-06-07 11:51:09,806][root][INFO] - Step 9405440 @ 2045.8 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 9405440, mean_episode_return = 33.563, mean_episode_step = 1971.5, total_loss = 159.89, pg_loss = 81.803, baseline_loss = 83.074, entropy_loss = -4.9834, learner_queue_size = 32, train_seconds = 5242.8, _tick = 1694, _time = 1.6546e+09)
[2022-06-07 11:51:14,810][root][INFO] - Step 9413120 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9413120, mean_episode_return = None, mean_episode_step = 1985.2, total_loss = -28.52, pg_loss = -47.93, baseline_loss = 24.422, entropy_loss = -5.0124, learner_queue_size = 32, train_seconds = 5247.8, _tick = 1694, _time = 1.6546e+09)
[2022-06-07 11:51:19,814][root][INFO] - Step 9420800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9420800, mean_episode_return = 88.841, mean_episode_step = 2353.2, total_loss = 353.41, pg_loss = 220.97, baseline_loss = 137.27, entropy_loss = -4.8358, learner_queue_size = 32, train_seconds = 5252.8, _tick = 1696, _time = 1.6546e+09)
[2022-06-07 11:51:24,820][root][INFO] - Step 9431040 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9431040, mean_episode_return = 2.6496, mean_episode_step = 2143.5, total_loss = -50.275, pg_loss = -66.893, baseline_loss = 21.268, entropy_loss = -4.6511, learner_queue_size = 32, train_seconds = 5257.8, _tick = 1699, _time = 1.6546e+09)
[2022-06-07 11:51:29,826][root][INFO] - Step 9441280 @ 2045.7 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 9441280, mean_episode_return = 74.14, mean_episode_step = 2667.0, total_loss = 39.401, pg_loss = 17.176, baseline_loss = 26.885, entropy_loss = -4.6601, learner_queue_size = 32, train_seconds = 5262.8, _tick = 1702, _time = 1.6546e+09)
[2022-06-07 11:51:34,830][root][INFO] - Step 9448960 @ 1534.6 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 9448960, mean_episode_return = 41.056, mean_episode_step = 2292.0, total_loss = -43.084, pg_loss = -67.309, baseline_loss = 28.928, entropy_loss = -4.7028, learner_queue_size = 32, train_seconds = 5267.8, _tick = 1705, _time = 1.6546e+09)
[2022-06-07 11:51:39,834][root][INFO] - Step 9459200 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 9459200, mean_episode_return = 7.2296, mean_episode_step = 1856.6, total_loss = 77.249, pg_loss = 7.3673, baseline_loss = 74.644, entropy_loss = -4.7621, learner_queue_size = 32, train_seconds = 5272.8, _tick = 1709, _time = 1.6546e+09)
[2022-06-07 11:51:44,840][root][INFO] - Step 9466880 @ 1534.1 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 9466880, mean_episode_return = -8.3401, mean_episode_step = 1988.7, total_loss = -22.202, pg_loss = -37.719, baseline_loss = 20.405, entropy_loss = -4.8883, learner_queue_size = 32, train_seconds = 5277.8, _tick = 1712, _time = 1.6546e+09)
[2022-06-07 11:51:49,846][root][INFO] - Step 9477120 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9477120, mean_episode_return = 91.657, mean_episode_step = 2425.6, total_loss = 67.125, pg_loss = -10.081, baseline_loss = 82.118, entropy_loss = -4.9129, learner_queue_size = 32, train_seconds = 5282.8, _tick = 1715, _time = 1.6546e+09)
[2022-06-07 11:51:54,850][root][INFO] - Step 9484800 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9484800, mean_episode_return = None, mean_episode_step = 2423.8, total_loss = 28.029, pg_loss = -4.6851, baseline_loss = 37.516, entropy_loss = -4.8016, learner_queue_size = 32, train_seconds = 5287.8, _tick = 1717, _time = 1.6546e+09)
[2022-06-07 11:51:59,854][root][INFO] - Step 9495040 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9495040, mean_episode_return = None, mean_episode_step = 2084.6, total_loss = 152.62, pg_loss = 115.2, baseline_loss = 42.113, entropy_loss = -4.6952, learner_queue_size = 32, train_seconds = 5292.8, _tick = 1719, _time = 1.6546e+09)
[2022-06-07 11:52:04,858][root][INFO] - Step 9502720 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 9502720, mean_episode_return = None, mean_episode_step = 2291.9, total_loss = -9.2019, pg_loss = -22.387, baseline_loss = 17.755, entropy_loss = -4.5702, learner_queue_size = 32, train_seconds = 5297.9, _tick = 1721, _time = 1.6546e+09)
[2022-06-07 11:52:09,864][root][INFO] - Step 9512960 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9512960, mean_episode_return = None, mean_episode_step = 1729.8, total_loss = -139.62, pg_loss = -137.08, baseline_loss = 2.3686, entropy_loss = -4.9136, learner_queue_size = 32, train_seconds = 5302.9, _tick = 1723, _time = 1.6546e+09)
[2022-06-07 11:52:14,866][root][INFO] - Step 9520640 @ 1535.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 9520640, mean_episode_return = 65.061, mean_episode_step = 2643.1, total_loss = 144.4, pg_loss = 66.113, baseline_loss = 83.165, entropy_loss = -4.8785, learner_queue_size = 32, train_seconds = 5307.9, _tick = 1725, _time = 1.6546e+09)
[2022-06-07 11:52:19,870][root][INFO] - Step 9530880 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 9530880, mean_episode_return = 8.73, mean_episode_step = 2297.1, total_loss = -216.96, pg_loss = -233.8, baseline_loss = 21.767, entropy_loss = -4.9276, learner_queue_size = 32, train_seconds = 5312.9, _tick = 1728, _time = 1.6546e+09)
[2022-06-07 11:52:24,874][root][INFO] - Step 9538560 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 9538560, mean_episode_return = 25.686, mean_episode_step = 2342.4, total_loss = 82.515, pg_loss = 38.026, baseline_loss = 49.563, entropy_loss = -5.0748, learner_queue_size = 32, train_seconds = 5317.9, _tick = 1730, _time = 1.6546e+09)
[2022-06-07 11:52:29,878][root][INFO] - Step 9548800 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9548800, mean_episode_return = 9.9098, mean_episode_step = 2745.0, total_loss = 41.521, pg_loss = 14.704, baseline_loss = 31.892, entropy_loss = -5.0745, learner_queue_size = 32, train_seconds = 5322.9, _tick = 1732, _time = 1.6546e+09)
[2022-06-07 11:52:34,882][root][INFO] - Step 9556480 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 9556480, mean_episode_return = 77.106, mean_episode_step = 1798.1, total_loss = 211.29, pg_loss = 115.7, baseline_loss = 100.6, entropy_loss = -5.0044, learner_queue_size = 32, train_seconds = 5327.9, _tick = 1734, _time = 1.6546e+09)
[2022-06-07 11:52:39,886][root][INFO] - Step 9566720 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 9566720, mean_episode_return = None, mean_episode_step = 2456.1, total_loss = 303.14, pg_loss = 204.38, baseline_loss = 103.83, entropy_loss = -5.0601, learner_queue_size = 32, train_seconds = 5332.9, _tick = 1736, _time = 1.6546e+09)
[2022-06-07 11:52:44,890][root][INFO] - Step 9574400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9574400, mean_episode_return = None, mean_episode_step = 1923.8, total_loss = 364.82, pg_loss = 263.81, baseline_loss = 106.02, entropy_loss = -5.0142, learner_queue_size = 32, train_seconds = 5337.9, _tick = 1736, _time = 1.6546e+09)
[2022-06-07 11:52:49,892][root][INFO] - Step 9582080 @ 1535.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 9582080, mean_episode_return = 11.959, mean_episode_step = 2029.4, total_loss = -145.92, pg_loss = -151.6, baseline_loss = 10.631, entropy_loss = -4.9508, learner_queue_size = 32, train_seconds = 5342.9, _tick = 1739, _time = 1.6546e+09)
[2022-06-07 11:52:54,898][root][INFO] - Step 9592320 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9592320, mean_episode_return = 63.37, mean_episode_step = 2437.8, total_loss = -192.05, pg_loss = -196.86, baseline_loss = 9.8295, entropy_loss = -5.0227, learner_queue_size = 32, train_seconds = 5347.9, _tick = 1742, _time = 1.6546e+09)
[2022-06-07 11:52:59,904][root][INFO] - Step 9602560 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9602560, mean_episode_return = None, mean_episode_step = 2123.0, total_loss = 8.772, pg_loss = -23.73, baseline_loss = 37.394, entropy_loss = -4.8927, learner_queue_size = 32, train_seconds = 5352.9, _tick = 1744, _time = 1.6546e+09)
[2022-06-07 11:53:04,910][root][INFO] - Step 9610240 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9610240, mean_episode_return = None, mean_episode_step = 2809.8, total_loss = 50.77, pg_loss = 18.835, baseline_loss = 36.823, entropy_loss = -4.8874, learner_queue_size = 32, train_seconds = 5357.9, _tick = 1745, _time = 1.6546e+09)
[2022-06-07 11:53:09,914][root][INFO] - Step 9620480 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9620480, mean_episode_return = None, mean_episode_step = 2315.2, total_loss = -2.9775, pg_loss = -30.74, baseline_loss = 32.64, entropy_loss = -4.878, learner_queue_size = 32, train_seconds = 5362.9, _tick = 1747, _time = 1.6546e+09)
[2022-06-07 11:53:14,918][root][INFO] - Step 9628160 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 9628160, mean_episode_return = 17.73, mean_episode_step = 1893.7, total_loss = 694.1, pg_loss = 441.14, baseline_loss = 257.72, entropy_loss = -4.7577, learner_queue_size = 32, train_seconds = 5367.9, _tick = 1750, _time = 1.6546e+09)
[2022-06-07 11:53:19,924][root][INFO] - Step 9638400 @ 2045.5 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 9638400, mean_episode_return = 59.691, mean_episode_step = 1523.4, total_loss = -38.672, pg_loss = -81.674, baseline_loss = 47.88, entropy_loss = -4.8773, learner_queue_size = 32, train_seconds = 5372.9, _tick = 1753, _time = 1.6546e+09)
[2022-06-07 11:53:24,930][root][INFO] - Step 9646080 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9646080, mean_episode_return = None, mean_episode_step = 2226.0, total_loss = -137.35, pg_loss = -136.87, baseline_loss = 4.3714, entropy_loss = -4.8522, learner_queue_size = 32, train_seconds = 5377.9, _tick = 1754, _time = 1.6546e+09)
[2022-06-07 11:53:29,934][root][INFO] - Step 9656320 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9656320, mean_episode_return = None, mean_episode_step = 2099.9, total_loss = -41.069, pg_loss = -42.201, baseline_loss = 6.0493, entropy_loss = -4.9164, learner_queue_size = 32, train_seconds = 5382.9, _tick = 1754, _time = 1.6546e+09)
[2022-06-07 11:53:34,938][root][INFO] - Step 9664000 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 9664000, mean_episode_return = None, mean_episode_step = 1850.7, total_loss = 359.82, pg_loss = 258.28, baseline_loss = 106.53, entropy_loss = -4.9911, learner_queue_size = 32, train_seconds = 5387.9, _tick = 1754, _time = 1.6546e+09)
[2022-06-07 11:53:39,942][root][INFO] - Step 9674240 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 9674240, mean_episode_return = None, mean_episode_step = 2502.5, total_loss = 112.31, pg_loss = 41.132, baseline_loss = 76.159, entropy_loss = -4.9782, learner_queue_size = 32, train_seconds = 5392.9, _tick = 1756, _time = 1.6546e+09)
[2022-06-07 11:53:44,946][root][INFO] - Step 9681920 @ 1534.7 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 9681920, mean_episode_return = 38.09, mean_episode_step = 2127.6, total_loss = 99.47, pg_loss = 31.417, baseline_loss = 72.979, entropy_loss = -4.9256, learner_queue_size = 32, train_seconds = 5397.9, _tick = 1759, _time = 1.6546e+09)
[2022-06-07 11:53:49,952][root][INFO] - Step 9692160 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9692160, mean_episode_return = 68.297, mean_episode_step = 1846.6, total_loss = 11.782, pg_loss = -50.271, baseline_loss = 67.223, entropy_loss = -5.1703, learner_queue_size = 32, train_seconds = 5402.9, _tick = 1762, _time = 1.6546e+09)
[2022-06-07 11:53:54,958][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 11:53:55,203][root][INFO] - Step 9699840 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 9699840, mean_episode_return = 26.44, mean_episode_step = 2256.4, total_loss = -68.007, pg_loss = -100.74, baseline_loss = 37.954, entropy_loss = -5.2184, learner_queue_size = 32, train_seconds = 5408.0, _tick = 1765, _time = 1.6546e+09)
[2022-06-07 11:54:00,206][root][INFO] - Step 9710080 @ 1951.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 9710080, mean_episode_return = 1.2996, mean_episode_step = 1637.9, total_loss = 9.0809, pg_loss = -17.191, baseline_loss = 31.391, entropy_loss = -5.1195, learner_queue_size = 32, train_seconds = 5413.2, _tick = 1767, _time = 1.6546e+09)
[2022-06-07 11:54:05,210][root][INFO] - Step 9717760 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 9717760, mean_episode_return = None, mean_episode_step = 1905.8, total_loss = 460.46, pg_loss = 341.56, baseline_loss = 124.1, entropy_loss = -5.2023, learner_queue_size = 32, train_seconds = 5418.2, _tick = 1769, _time = 1.6546e+09)
[2022-06-07 11:54:10,216][root][INFO] - Step 9728000 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 9728000, mean_episode_return = None, mean_episode_step = 2324.8, total_loss = 287.08, pg_loss = 202.1, baseline_loss = 90.19, entropy_loss = -5.2134, learner_queue_size = 32, train_seconds = 5423.2, _tick = 1769, _time = 1.6546e+09)
[2022-06-07 11:54:15,222][root][INFO] - Step 9735680 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 9735680, mean_episode_return = None, mean_episode_step = 1496.0, total_loss = 427.74, pg_loss = 300.27, baseline_loss = 132.52, entropy_loss = -5.0517, learner_queue_size = 32, train_seconds = 5428.2, _tick = 1770, _time = 1.6546e+09)
[2022-06-07 11:54:20,228][root][INFO] - Step 9745920 @ 2045.5 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 9745920, mean_episode_return = 57.217, mean_episode_step = 2071.0, total_loss = -25.313, pg_loss = -51.887, baseline_loss = 31.331, entropy_loss = -4.7575, learner_queue_size = 32, train_seconds = 5433.2, _tick = 1774, _time = 1.6546e+09)
[2022-06-07 11:54:25,234][root][INFO] - Step 9753600 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 9753600, mean_episode_return = 19.955, mean_episode_step = 2127.6, total_loss = -20.044, pg_loss = -41.424, baseline_loss = 25.907, entropy_loss = -4.5272, learner_queue_size = 32, train_seconds = 5438.2, _tick = 1777, _time = 1.6546e+09)
[2022-06-07 11:54:30,246][root][INFO] - Step 9763840 @ 2043.1 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 9763840, mean_episode_return = 42.125, mean_episode_step = 1880.6, total_loss = -127.35, pg_loss = -217.61, baseline_loss = 94.816, entropy_loss = -4.5606, learner_queue_size = 32, train_seconds = 5443.2, _tick = 1779, _time = 1.6546e+09)
[2022-06-07 11:54:35,252][root][INFO] - Step 9771520 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9771520, mean_episode_return = 55.419, mean_episode_step = 1711.0, total_loss = 372.3, pg_loss = 178.88, baseline_loss = 197.97, entropy_loss = -4.5483, learner_queue_size = 32, train_seconds = 5448.2, _tick = 1781, _time = 1.6546e+09)
[2022-06-07 11:54:40,258][root][INFO] - Step 9781760 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9781760, mean_episode_return = None, mean_episode_step = 2220.3, total_loss = -47.469, pg_loss = -47.374, baseline_loss = 4.3515, entropy_loss = -4.4471, learner_queue_size = 32, train_seconds = 5453.3, _tick = 1784, _time = 1.6546e+09)
[2022-06-07 11:54:45,262][root][INFO] - Step 9789440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9789440, mean_episode_return = None, mean_episode_step = 1818.7, total_loss = 55.51, pg_loss = 42.702, baseline_loss = 17.172, entropy_loss = -4.3637, learner_queue_size = 32, train_seconds = 5458.3, _tick = 1786, _time = 1.6546e+09)
[2022-06-07 11:54:50,266][root][INFO] - Step 9799680 @ 2046.3 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 9799680, mean_episode_return = 17.24, mean_episode_step = 1771.5, total_loss = -227.23, pg_loss = -245.21, baseline_loss = 22.661, entropy_loss = -4.6738, learner_queue_size = 32, train_seconds = 5463.3, _tick = 1788, _time = 1.6546e+09)
[2022-06-07 11:54:55,270][root][INFO] - Step 9807360 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9807360, mean_episode_return = -16.151, mean_episode_step = 1642.0, total_loss = -26.666, pg_loss = -55.211, baseline_loss = 33.279, entropy_loss = -4.7346, learner_queue_size = 32, train_seconds = 5468.3, _tick = 1790, _time = 1.6546e+09)
[2022-06-07 11:55:00,276][root][INFO] - Step 9817600 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9817600, mean_episode_return = 100.69, mean_episode_step = 1710.4, total_loss = -80.61, pg_loss = -107.77, baseline_loss = 31.883, entropy_loss = -4.7271, learner_queue_size = 32, train_seconds = 5473.3, _tick = 1793, _time = 1.6546e+09)
[2022-06-07 11:55:05,282][root][INFO] - Step 9825280 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 9825280, mean_episode_return = 23.53, mean_episode_step = 1849.6, total_loss = -36.088, pg_loss = -76.233, baseline_loss = 44.818, entropy_loss = -4.6727, learner_queue_size = 32, train_seconds = 5478.3, _tick = 1794, _time = 1.6546e+09)
[2022-06-07 11:55:10,288][root][INFO] - Step 9835520 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 9835520, mean_episode_return = None, mean_episode_step = 1884.2, total_loss = -33.401, pg_loss = -45.723, baseline_loss = 17.168, entropy_loss = -4.8468, learner_queue_size = 32, train_seconds = 5483.3, _tick = 1795, _time = 1.6546e+09)
[2022-06-07 11:55:15,294][root][INFO] - Step 9843200 @ 1534.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 9843200, mean_episode_return = 22.001, mean_episode_step = 2214.7, total_loss = -32.759, pg_loss = -41.866, baseline_loss = 13.91, entropy_loss = -4.8037, learner_queue_size = 32, train_seconds = 5488.3, _tick = 1798, _time = 1.6546e+09)
[2022-06-07 11:55:20,298][root][INFO] - Step 9853440 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 9853440, mean_episode_return = None, mean_episode_step = 1572.0, total_loss = 253.17, pg_loss = 164.58, baseline_loss = 93.629, entropy_loss = -5.0304, learner_queue_size = 32, train_seconds = 5493.3, _tick = 1800, _time = 1.6546e+09)
[2022-06-07 11:55:25,304][root][INFO] - Step 9861120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9861120, mean_episode_return = 58.011, mean_episode_step = 2095.9, total_loss = -0.66107, pg_loss = -35.083, baseline_loss = 39.51, entropy_loss = -5.0876, learner_queue_size = 32, train_seconds = 5498.3, _tick = 1801, _time = 1.6546e+09)
[2022-06-07 11:55:30,310][root][INFO] - Step 9871360 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 9871360, mean_episode_return = -7.8307, mean_episode_step = 1915.7, total_loss = -85.059, pg_loss = -112.49, baseline_loss = 32.408, entropy_loss = -4.9782, learner_queue_size = 32, train_seconds = 5503.3, _tick = 1805, _time = 1.6546e+09)
[2022-06-07 11:55:35,314][root][INFO] - Step 9879040 @ 1534.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 9879040, mean_episode_return = None, mean_episode_step = 2435.4, total_loss = 82.905, pg_loss = 39.95, baseline_loss = 47.964, entropy_loss = -5.0087, learner_queue_size = 32, train_seconds = 5508.3, _tick = 1805, _time = 1.6546e+09)
[2022-06-07 11:55:40,318][root][INFO] - Step 9889280 @ 2046.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 9889280, mean_episode_return = None, mean_episode_step = 2547.5, total_loss = 83.933, pg_loss = 36.75, baseline_loss = 52.391, entropy_loss = -5.2078, learner_queue_size = 32, train_seconds = 5513.3, _tick = 1805, _time = 1.6546e+09)
[2022-06-07 11:55:45,322][root][INFO] - Step 9896960 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 9896960, mean_episode_return = 89.695, mean_episode_step = 2076.2, total_loss = 111.84, pg_loss = 11.907, baseline_loss = 104.93, entropy_loss = -4.9971, learner_queue_size = 32, train_seconds = 5518.3, _tick = 1808, _time = 1.6546e+09)
[2022-06-07 11:55:50,328][root][INFO] - Step 9907200 @ 2045.5 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 9907200, mean_episode_return = None, mean_episode_step = 1686.7, total_loss = 259.81, pg_loss = 193.57, baseline_loss = 71.143, entropy_loss = -4.9023, learner_queue_size = 32, train_seconds = 5523.3, _tick = 1810, _time = 1.6546e+09)
[2022-06-07 11:55:55,334][root][INFO] - Step 9914880 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 9914880, mean_episode_return = 73.331, mean_episode_step = 2272.0, total_loss = -84.643, pg_loss = -119.34, baseline_loss = 39.577, entropy_loss = -4.8787, learner_queue_size = 32, train_seconds = 5528.3, _tick = 1813, _time = 1.6546e+09)
[2022-06-07 11:56:00,338][root][INFO] - Step 9925120 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 9925120, mean_episode_return = None, mean_episode_step = 1670.1, total_loss = -98.974, pg_loss = -99.418, baseline_loss = 5.1613, entropy_loss = -4.7171, learner_queue_size = 32, train_seconds = 5533.3, _tick = 1815, _time = 1.6546e+09)
[2022-06-07 11:56:05,342][root][INFO] - Step 9932800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9932800, mean_episode_return = 2.6997, mean_episode_step = 1636.6, total_loss = 73.728, pg_loss = 14.447, baseline_loss = 63.735, entropy_loss = -4.455, learner_queue_size = 32, train_seconds = 5538.3, _tick = 1818, _time = 1.6546e+09)
[2022-06-07 11:56:10,346][root][INFO] - Step 9943040 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9943040, mean_episode_return = 53.582, mean_episode_step = 1522.3, total_loss = 118.67, pg_loss = 77.994, baseline_loss = 45.552, entropy_loss = -4.8772, learner_queue_size = 32, train_seconds = 5543.3, _tick = 1821, _time = 1.6546e+09)
[2022-06-07 11:56:15,350][root][INFO] - Step 9950720 @ 1534.8 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 9950720, mean_episode_return = 137.04, mean_episode_step = 1968.4, total_loss = -2.5465, pg_loss = -32.734, baseline_loss = 35.071, entropy_loss = -4.8837, learner_queue_size = 32, train_seconds = 5548.3, _tick = 1823, _time = 1.6546e+09)
[2022-06-07 11:56:20,356][root][INFO] - Step 9960960 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 9960960, mean_episode_return = None, mean_episode_step = 2705.1, total_loss = 155.46, pg_loss = 109.96, baseline_loss = 50.253, entropy_loss = -4.746, learner_queue_size = 32, train_seconds = 5553.4, _tick = 1825, _time = 1.6546e+09)
[2022-06-07 11:56:25,361][root][INFO] - Step 9968640 @ 1534.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9968640, mean_episode_return = None, mean_episode_step = 1913.8, total_loss = -145.35, pg_loss = -142.97, baseline_loss = 2.4147, entropy_loss = -4.7966, learner_queue_size = 32, train_seconds = 5558.4, _tick = 1826, _time = 1.6546e+09)
[2022-06-07 11:56:30,367][root][INFO] - Step 9978880 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 9978880, mean_episode_return = 31.909, mean_episode_step = 2097.2, total_loss = -197.29, pg_loss = -199.07, baseline_loss = 6.6123, entropy_loss = -4.8363, learner_queue_size = 32, train_seconds = 5563.4, _tick = 1827, _time = 1.6546e+09)
[2022-06-07 11:56:35,370][root][INFO] - Step 9986560 @ 1535.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 9986560, mean_episode_return = None, mean_episode_step = 1919.5, total_loss = 19.201, pg_loss = -13.357, baseline_loss = 37.492, entropy_loss = -4.9339, learner_queue_size = 32, train_seconds = 5568.4, _tick = 1827, _time = 1.6546e+09)
[2022-06-07 11:56:40,374][root][INFO] - Step 9996800 @ 2046.3 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 9996800, mean_episode_return = None, mean_episode_step = 2490.6, total_loss = 166.38, pg_loss = 105.64, baseline_loss = 65.563, entropy_loss = -4.8239, learner_queue_size = 32, train_seconds = 5573.4, _tick = 1828, _time = 1.6546e+09)
[2022-06-07 11:56:45,378][root][INFO] - Step 10004480 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 10004480, mean_episode_return = 102.81, mean_episode_step = 2519.7, total_loss = -90.574, pg_loss = -113.61, baseline_loss = 27.894, entropy_loss = -4.8555, learner_queue_size = 32, train_seconds = 5578.4, _tick = 1830, _time = 1.6546e+09)
[2022-06-07 11:56:50,382][root][INFO] - Step 10014720 @ 2046.3 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 10014720, mean_episode_return = 59.193, mean_episode_step = 1880.5, total_loss = 41.969, pg_loss = -42.051, baseline_loss = 88.855, entropy_loss = -4.8354, learner_queue_size = 32, train_seconds = 5583.4, _tick = 1834, _time = 1.6546e+09)
[2022-06-07 11:56:55,388][root][INFO] - Step 10022400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10022400, mean_episode_return = None, mean_episode_step = 2247.6, total_loss = -144.33, pg_loss = -142.28, baseline_loss = 2.8531, entropy_loss = -4.9003, learner_queue_size = 32, train_seconds = 5588.4, _tick = 1835, _time = 1.6546e+09)
[2022-06-07 11:57:00,394][root][INFO] - Step 10032640 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10032640, mean_episode_return = 61.905, mean_episode_step = 2009.4, total_loss = 10.928, pg_loss = -35.726, baseline_loss = 51.346, entropy_loss = -4.6928, learner_queue_size = 32, train_seconds = 5593.4, _tick = 1838, _time = 1.6546e+09)
[2022-06-07 11:57:05,401][root][INFO] - Step 10040320 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 10040320, mean_episode_return = 28.463, mean_episode_step = 2460.3, total_loss = -24.621, pg_loss = -73.345, baseline_loss = 53.33, entropy_loss = -4.6059, learner_queue_size = 32, train_seconds = 5598.4, _tick = 1840, _time = 1.6546e+09)
[2022-06-07 11:57:10,406][root][INFO] - Step 10050560 @ 2045.8 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 10050560, mean_episode_return = None, mean_episode_step = 2470.2, total_loss = -47.284, pg_loss = -48.505, baseline_loss = 5.8995, entropy_loss = -4.6787, learner_queue_size = 32, train_seconds = 5603.4, _tick = 1841, _time = 1.6546e+09)
[2022-06-07 11:57:15,412][root][INFO] - Step 10058240 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10058240, mean_episode_return = None, mean_episode_step = 1796.5, total_loss = 86.865, pg_loss = 47.233, baseline_loss = 44.446, entropy_loss = -4.8142, learner_queue_size = 32, train_seconds = 5608.4, _tick = 1842, _time = 1.6546e+09)
[2022-06-07 11:57:20,418][root][INFO] - Step 10068480 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10068480, mean_episode_return = 42.054, mean_episode_step = 2399.0, total_loss = -40.206, pg_loss = -48.021, baseline_loss = 12.704, entropy_loss = -4.8889, learner_queue_size = 32, train_seconds = 5613.4, _tick = 1843, _time = 1.6546e+09)
[2022-06-07 11:57:25,424][root][INFO] - Step 10076160 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10076160, mean_episode_return = 82.513, mean_episode_step = 2363.6, total_loss = 40.595, pg_loss = 14.202, baseline_loss = 31.407, entropy_loss = -5.0138, learner_queue_size = 32, train_seconds = 5618.4, _tick = 1846, _time = 1.6546e+09)
[2022-06-07 11:57:30,430][root][INFO] - Step 10086400 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10086400, mean_episode_return = 30.241, mean_episode_step = 2718.1, total_loss = -142.89, pg_loss = -146.99, baseline_loss = 9.0803, entropy_loss = -4.9814, learner_queue_size = 32, train_seconds = 5623.4, _tick = 1847, _time = 1.6546e+09)
[2022-06-07 11:57:35,434][root][INFO] - Step 10094080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10094080, mean_episode_return = 6.8296, mean_episode_step = 1879.8, total_loss = -97.35, pg_loss = -96.376, baseline_loss = 4.0961, entropy_loss = -5.0703, learner_queue_size = 32, train_seconds = 5628.4, _tick = 1849, _time = 1.6546e+09)
[2022-06-07 11:57:40,440][root][INFO] - Step 10104320 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 10104320, mean_episode_return = None, mean_episode_step = 2104.9, total_loss = 50.825, pg_loss = 22.963, baseline_loss = 33.008, entropy_loss = -5.1461, learner_queue_size = 32, train_seconds = 5633.4, _tick = 1852, _time = 1.6546e+09)
[2022-06-07 11:57:45,446][root][INFO] - Step 10112000 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10112000, mean_episode_return = None, mean_episode_step = 2633.0, total_loss = 41.778, pg_loss = 15.118, baseline_loss = 31.729, entropy_loss = -5.0691, learner_queue_size = 32, train_seconds = 5638.4, _tick = 1852, _time = 1.6546e+09)
[2022-06-07 11:57:50,452][root][INFO] - Step 10122240 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 10122240, mean_episode_return = -30.291, mean_episode_step = 2289.7, total_loss = -86.496, pg_loss = -84.724, baseline_loss = 3.0472, entropy_loss = -4.8188, learner_queue_size = 32, train_seconds = 5643.4, _tick = 1854, _time = 1.6546e+09)
[2022-06-07 11:57:55,454][root][INFO] - Step 10129920 @ 1535.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10129920, mean_episode_return = -0.51026, mean_episode_step = 1991.6, total_loss = -87.198, pg_loss = -92.728, baseline_loss = 10.355, entropy_loss = -4.8255, learner_queue_size = 32, train_seconds = 5648.4, _tick = 1857, _time = 1.6546e+09)
[2022-06-07 11:58:00,458][root][INFO] - Step 10140160 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 10140160, mean_episode_return = 39.302, mean_episode_step = 1804.2, total_loss = 463.21, pg_loss = 320.74, baseline_loss = 147.39, entropy_loss = -4.9219, learner_queue_size = 32, train_seconds = 5653.5, _tick = 1860, _time = 1.6546e+09)
[2022-06-07 11:58:05,462][root][INFO] - Step 10147840 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10147840, mean_episode_return = None, mean_episode_step = 1699.4, total_loss = -4.8645, pg_loss = -12.254, baseline_loss = 12.436, entropy_loss = -5.0456, learner_queue_size = 32, train_seconds = 5658.5, _tick = 1860, _time = 1.6546e+09)
[2022-06-07 11:58:10,474][root][INFO] - Step 10158080 @ 2043.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 10158080, mean_episode_return = None, mean_episode_step = 1901.0, total_loss = -7.8765, pg_loss = -37.242, baseline_loss = 34.403, entropy_loss = -5.0366, learner_queue_size = 32, train_seconds = 5663.5, _tick = 1861, _time = 1.6546e+09)
[2022-06-07 11:58:15,478][root][INFO] - Step 10165760 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 10165760, mean_episode_return = 16.69, mean_episode_step = 2468.2, total_loss = -33.884, pg_loss = -63.836, baseline_loss = 35.068, entropy_loss = -5.1156, learner_queue_size = 32, train_seconds = 5668.5, _tick = 1863, _time = 1.6546e+09)
[2022-06-07 11:58:20,482][root][INFO] - Step 10176000 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 10176000, mean_episode_return = 117.74, mean_episode_step = 2118.2, total_loss = 61.98, pg_loss = 39.12, baseline_loss = 27.783, entropy_loss = -4.924, learner_queue_size = 32, train_seconds = 5673.5, _tick = 1867, _time = 1.6546e+09)
[2022-06-07 11:58:25,488][root][INFO] - Step 10183680 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10183680, mean_episode_return = None, mean_episode_step = 2087.6, total_loss = -36.526, pg_loss = -35.44, baseline_loss = 3.8044, entropy_loss = -4.8897, learner_queue_size = 32, train_seconds = 5678.5, _tick = 1868, _time = 1.6546e+09)
[2022-06-07 11:58:30,489][root][INFO] - Step 10191360 @ 1535.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 10191360, mean_episode_return = None, mean_episode_step = 2490.8, total_loss = -15.478, pg_loss = -30.289, baseline_loss = 19.569, entropy_loss = -4.7582, learner_queue_size = 32, train_seconds = 5683.5, _tick = 1869, _time = 1.6546e+09)
[2022-06-07 11:58:35,494][root][INFO] - Step 10201600 @ 2045.9 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 10201600, mean_episode_return = None, mean_episode_step = 2627.2, total_loss = 77.361, pg_loss = 54.337, baseline_loss = 27.817, entropy_loss = -4.793, learner_queue_size = 32, train_seconds = 5688.5, _tick = 1871, _time = 1.6546e+09)
[2022-06-07 11:58:40,500][root][INFO] - Step 10209280 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10209280, mean_episode_return = None, mean_episode_step = 2019.7, total_loss = 208.69, pg_loss = 113.46, baseline_loss = 100.08, entropy_loss = -4.8462, learner_queue_size = 32, train_seconds = 5693.5, _tick = 1872, _time = 1.6546e+09)
[2022-06-07 11:58:45,506][root][INFO] - Step 10219520 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 10219520, mean_episode_return = 36.872, mean_episode_step = 1912.9, total_loss = 183.63, pg_loss = 135.77, baseline_loss = 52.863, entropy_loss = -5.0012, learner_queue_size = 32, train_seconds = 5698.5, _tick = 1875, _time = 1.6546e+09)
[2022-06-07 11:58:50,512][root][INFO] - Step 10229760 @ 2045.5 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 10229760, mean_episode_return = 46.543, mean_episode_step = 2014.1, total_loss = -59.196, pg_loss = -76.746, baseline_loss = 22.662, entropy_loss = -5.1119, learner_queue_size = 32, train_seconds = 5703.5, _tick = 1877, _time = 1.6546e+09)
[2022-06-07 11:58:55,518][root][INFO] - Step 10237440 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10237440, mean_episode_return = None, mean_episode_step = 2079.9, total_loss = 80.183, pg_loss = 36.185, baseline_loss = 49.047, entropy_loss = -5.049, learner_queue_size = 32, train_seconds = 5708.5, _tick = 1877, _time = 1.6546e+09)
[2022-06-07 11:59:00,522][root][INFO] - Step 10247680 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 10247680, mean_episode_return = 87.294, mean_episode_step = 2893.6, total_loss = -110.77, pg_loss = -111.4, baseline_loss = 5.6284, entropy_loss = -4.9918, learner_queue_size = 32, train_seconds = 5713.5, _tick = 1879, _time = 1.6546e+09)
[2022-06-07 11:59:05,526][root][INFO] - Step 10255360 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 10255360, mean_episode_return = 84.109, mean_episode_step = 2009.6, total_loss = -100.13, pg_loss = -100.54, baseline_loss = 5.463, entropy_loss = -5.0537, learner_queue_size = 32, train_seconds = 5718.5, _tick = 1880, _time = 1.6546e+09)
[2022-06-07 11:59:10,530][root][INFO] - Step 10265600 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10265600, mean_episode_return = 59.535, mean_episode_step = 2144.5, total_loss = 398.46, pg_loss = 275.36, baseline_loss = 127.99, entropy_loss = -4.8849, learner_queue_size = 32, train_seconds = 5723.5, _tick = 1882, _time = 1.6546e+09)
[2022-06-07 11:59:15,534][root][INFO] - Step 10273280 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 10273280, mean_episode_return = 73.676, mean_episode_step = 2373.9, total_loss = 402.51, pg_loss = 279.24, baseline_loss = 128.2, entropy_loss = -4.9396, learner_queue_size = 32, train_seconds = 5728.5, _tick = 1884, _time = 1.6546e+09)
[2022-06-07 11:59:20,538][root][INFO] - Step 10283520 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 10283520, mean_episode_return = None, mean_episode_step = 2485.3, total_loss = 50.743, pg_loss = 23.546, baseline_loss = 32.125, entropy_loss = -4.9279, learner_queue_size = 32, train_seconds = 5733.5, _tick = 1887, _time = 1.6546e+09)
[2022-06-07 11:59:25,542][root][INFO] - Step 10291200 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10291200, mean_episode_return = None, mean_episode_step = 1981.7, total_loss = 302.81, pg_loss = 220.56, baseline_loss = 87.189, entropy_loss = -4.9447, learner_queue_size = 32, train_seconds = 5738.5, _tick = 1888, _time = 1.6546e+09)
[2022-06-07 11:59:30,548][root][INFO] - Step 10301440 @ 2045.5 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 10301440, mean_episode_return = 101.04, mean_episode_step = 1902.3, total_loss = 16.794, pg_loss = -4.3401, baseline_loss = 26.017, entropy_loss = -4.8826, learner_queue_size = 32, train_seconds = 5743.5, _tick = 1891, _time = 1.6546e+09)
[2022-06-07 11:59:35,554][root][INFO] - Step 10309120 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10309120, mean_episode_return = None, mean_episode_step = 1874.9, total_loss = 120.01, pg_loss = 80.938, baseline_loss = 44.032, entropy_loss = -4.9636, learner_queue_size = 32, train_seconds = 5748.5, _tick = 1892, _time = 1.6546e+09)
[2022-06-07 11:59:40,558][root][INFO] - Step 10319360 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 10319360, mean_episode_return = 77.281, mean_episode_step = 2362.2, total_loss = 16.878, pg_loss = -11.699, baseline_loss = 33.593, entropy_loss = -5.0166, learner_queue_size = 32, train_seconds = 5753.6, _tick = 1896, _time = 1.6546e+09)
[2022-06-07 11:59:45,562][root][INFO] - Step 10327040 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 10327040, mean_episode_return = 102.81, mean_episode_step = 1998.8, total_loss = 66.916, pg_loss = -1.133, baseline_loss = 72.953, entropy_loss = -4.9044, learner_queue_size = 32, train_seconds = 5758.6, _tick = 1898, _time = 1.6546e+09)
[2022-06-07 11:59:50,568][root][INFO] - Step 10337280 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 10337280, mean_episode_return = 150.48, mean_episode_step = 2265.8, total_loss = 68.639, pg_loss = 9.397, baseline_loss = 64.055, entropy_loss = -4.8129, learner_queue_size = 32, train_seconds = 5763.6, _tick = 1901, _time = 1.6546e+09)
[2022-06-07 11:59:55,574][root][INFO] - Step 10344960 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 10344960, mean_episode_return = 116.14, mean_episode_step = 2626.2, total_loss = -156.46, pg_loss = -173.32, baseline_loss = 21.606, entropy_loss = -4.7401, learner_queue_size = 32, train_seconds = 5768.6, _tick = 1903, _time = 1.6546e+09)
[2022-06-07 12:00:00,578][root][INFO] - Step 10355200 @ 2046.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 10355200, mean_episode_return = 4.0796, mean_episode_step = 2704.0, total_loss = 2.315, pg_loss = -44.928, baseline_loss = 52.027, entropy_loss = -4.7838, learner_queue_size = 32, train_seconds = 5773.6, _tick = 1906, _time = 1.6546e+09)
[2022-06-07 12:00:05,582][root][INFO] - Step 10362880 @ 1534.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 10362880, mean_episode_return = 67.192, mean_episode_step = 1736.8, total_loss = 114.72, pg_loss = 59.03, baseline_loss = 60.403, entropy_loss = -4.7173, learner_queue_size = 32, train_seconds = 5778.6, _tick = 1908, _time = 1.6546e+09)
[2022-06-07 12:00:10,586][root][INFO] - Step 10373120 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 10373120, mean_episode_return = None, mean_episode_step = 2012.7, total_loss = 96.661, pg_loss = 52.758, baseline_loss = 48.642, entropy_loss = -4.7399, learner_queue_size = 32, train_seconds = 5783.6, _tick = 1910, _time = 1.6546e+09)
[2022-06-07 12:00:15,590][root][INFO] - Step 10380800 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 10380800, mean_episode_return = None, mean_episode_step = 1721.4, total_loss = 382.87, pg_loss = 253.83, baseline_loss = 133.79, entropy_loss = -4.7423, learner_queue_size = 32, train_seconds = 5788.6, _tick = 1910, _time = 1.6546e+09)
[2022-06-07 12:00:20,596][root][INFO] - Step 10391040 @ 2045.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10391040, mean_episode_return = None, mean_episode_step = 2378.8, total_loss = -76.28, pg_loss = -100.2, baseline_loss = 28.702, entropy_loss = -4.7862, learner_queue_size = 32, train_seconds = 5793.6, _tick = 1913, _time = 1.6546e+09)
[2022-06-07 12:00:25,602][root][INFO] - Step 10398720 @ 1534.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10398720, mean_episode_return = 36.992, mean_episode_step = 1727.8, total_loss = 338.68, pg_loss = 230.79, baseline_loss = 112.6, entropy_loss = -4.709, learner_queue_size = 32, train_seconds = 5798.6, _tick = 1915, _time = 1.6546e+09)
[2022-06-07 12:00:30,606][root][INFO] - Step 10408960 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 10408960, mean_episode_return = None, mean_episode_step = 2338.1, total_loss = 46.836, pg_loss = 13.439, baseline_loss = 38.223, entropy_loss = -4.8251, learner_queue_size = 32, train_seconds = 5803.6, _tick = 1916, _time = 1.6546e+09)
[2022-06-07 12:00:35,610][root][INFO] - Step 10416640 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 10416640, mean_episode_return = 24.87, mean_episode_step = 2711.5, total_loss = -104.62, pg_loss = -114.92, baseline_loss = 15.14, entropy_loss = -4.8397, learner_queue_size = 32, train_seconds = 5808.6, _tick = 1918, _time = 1.6546e+09)
[2022-06-07 12:00:40,616][root][INFO] - Step 10426880 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10426880, mean_episode_return = None, mean_episode_step = 2157.7, total_loss = 332.19, pg_loss = 232.96, baseline_loss = 104.15, entropy_loss = -4.919, learner_queue_size = 32, train_seconds = 5813.6, _tick = 1919, _time = 1.6546e+09)
[2022-06-07 12:00:45,618][root][INFO] - Step 10434560 @ 1535.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10434560, mean_episode_return = 32.812, mean_episode_step = 2235.7, total_loss = 71.462, pg_loss = 29.057, baseline_loss = 47.326, entropy_loss = -4.9202, learner_queue_size = 32, train_seconds = 5818.6, _tick = 1921, _time = 1.6546e+09)
[2022-06-07 12:00:50,631][root][INFO] - Step 10444800 @ 2042.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 10444800, mean_episode_return = 33.32, mean_episode_step = 1948.1, total_loss = -13.513, pg_loss = -39.058, baseline_loss = 30.68, entropy_loss = -5.1354, learner_queue_size = 32, train_seconds = 5823.6, _tick = 1923, _time = 1.6546e+09)
[2022-06-07 12:00:55,634][root][INFO] - Step 10452480 @ 1535.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10452480, mean_episode_return = 37.036, mean_episode_step = 2364.1, total_loss = 140.06, pg_loss = 75.622, baseline_loss = 69.643, entropy_loss = -5.2086, learner_queue_size = 32, train_seconds = 5828.6, _tick = 1924, _time = 1.6546e+09)
[2022-06-07 12:01:00,638][root][INFO] - Step 10462720 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 10462720, mean_episode_return = 17.3, mean_episode_step = 1850.4, total_loss = 422.3, pg_loss = 281.5, baseline_loss = 146.06, entropy_loss = -5.2554, learner_queue_size = 32, train_seconds = 5833.6, _tick = 1927, _time = 1.6546e+09)
[2022-06-07 12:01:05,642][root][INFO] - Step 10470400 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 10470400, mean_episode_return = 47.615, mean_episode_step = 2164.0, total_loss = 233.9, pg_loss = 144.89, baseline_loss = 94.283, entropy_loss = -5.2688, learner_queue_size = 32, train_seconds = 5838.6, _tick = 1930, _time = 1.6546e+09)
[2022-06-07 12:01:10,646][root][INFO] - Step 10480640 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 10480640, mean_episode_return = None, mean_episode_step = 2176.4, total_loss = -14.424, pg_loss = -30.876, baseline_loss = 21.845, entropy_loss = -5.3928, learner_queue_size = 32, train_seconds = 5843.6, _tick = 1931, _time = 1.6546e+09)
[2022-06-07 12:01:15,650][root][INFO] - Step 10488320 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10488320, mean_episode_return = None, mean_episode_step = 2097.6, total_loss = -76.751, pg_loss = -86.569, baseline_loss = 15.373, entropy_loss = -5.5551, learner_queue_size = 32, train_seconds = 5848.6, _tick = 1933, _time = 1.6546e+09)
[2022-06-07 12:01:20,656][root][INFO] - Step 10498560 @ 2045.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10498560, mean_episode_return = None, mean_episode_step = 2340.6, total_loss = 51.282, pg_loss = 26.258, baseline_loss = 30.607, entropy_loss = -5.5833, learner_queue_size = 32, train_seconds = 5853.7, _tick = 1935, _time = 1.6546e+09)
[2022-06-07 12:01:25,662][root][INFO] - Step 10506240 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10506240, mean_episode_return = None, mean_episode_step = 1864.2, total_loss = 38.455, pg_loss = 27.401, baseline_loss = 16.644, entropy_loss = -5.5903, learner_queue_size = 32, train_seconds = 5858.7, _tick = 1936, _time = 1.6546e+09)
[2022-06-07 12:01:30,666][root][INFO] - Step 10516480 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10516480, mean_episode_return = 24.16, mean_episode_step = 2490.6, total_loss = -66.252, pg_loss = -90.318, baseline_loss = 29.753, entropy_loss = -5.6864, learner_queue_size = 32, train_seconds = 5863.7, _tick = 1937, _time = 1.6546e+09)
[2022-06-07 12:01:35,670][root][INFO] - Step 10524160 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10524160, mean_episode_return = 4.9748, mean_episode_step = 2825.5, total_loss = -53.419, pg_loss = -85.13, baseline_loss = 37.381, entropy_loss = -5.6692, learner_queue_size = 32, train_seconds = 5868.7, _tick = 1940, _time = 1.6546e+09)
[2022-06-07 12:01:40,676][root][INFO] - Step 10534400 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10534400, mean_episode_return = None, mean_episode_step = 2367.5, total_loss = -95.841, pg_loss = -97.027, baseline_loss = 6.8446, entropy_loss = -5.6592, learner_queue_size = 32, train_seconds = 5873.7, _tick = 1943, _time = 1.6546e+09)
[2022-06-07 12:01:45,679][root][INFO] - Step 10544640 @ 2046.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10544640, mean_episode_return = -110.78, mean_episode_step = 2768.1, total_loss = 475.49, pg_loss = 335.77, baseline_loss = 145.39, entropy_loss = -5.6712, learner_queue_size = 32, train_seconds = 5878.7, _tick = 1944, _time = 1.6546e+09)
[2022-06-07 12:01:50,685][root][INFO] - Step 10552320 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 10552320, mean_episode_return = None, mean_episode_step = 1762.0, total_loss = 156.57, pg_loss = 103.41, baseline_loss = 58.735, entropy_loss = -5.5757, learner_queue_size = 32, train_seconds = 5883.7, _tick = 1946, _time = 1.6546e+09)
[2022-06-07 12:01:55,690][root][INFO] - Step 10562560 @ 2045.9 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10562560, mean_episode_return = 42.402, mean_episode_step = 1998.3, total_loss = 74.421, pg_loss = 47.704, baseline_loss = 32.291, entropy_loss = -5.5737, learner_queue_size = 32, train_seconds = 5888.7, _tick = 1950, _time = 1.6546e+09)
[2022-06-07 12:02:00,694][root][INFO] - Step 10570240 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 10570240, mean_episode_return = 125.7, mean_episode_step = 2228.8, total_loss = 31.456, pg_loss = -5.589, baseline_loss = 42.634, entropy_loss = -5.5891, learner_queue_size = 32, train_seconds = 5893.7, _tick = 1952, _time = 1.6546e+09)
[2022-06-07 12:02:05,698][root][INFO] - Step 10580480 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 10580480, mean_episode_return = 3.4796, mean_episode_step = 1867.4, total_loss = 53.002, pg_loss = 16.018, baseline_loss = 42.451, entropy_loss = -5.4675, learner_queue_size = 32, train_seconds = 5898.7, _tick = 1954, _time = 1.6546e+09)
[2022-06-07 12:02:10,704][root][INFO] - Step 10588160 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10588160, mean_episode_return = None, mean_episode_step = 2325.7, total_loss = -17.891, pg_loss = -26.131, baseline_loss = 13.743, entropy_loss = -5.5028, learner_queue_size = 32, train_seconds = 5903.7, _tick = 1956, _time = 1.6546e+09)
[2022-06-07 12:02:15,710][root][INFO] - Step 10598400 @ 2045.7 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 10598400, mean_episode_return = 55.171, mean_episode_step = 2507.5, total_loss = 99.55, pg_loss = 62.309, baseline_loss = 42.717, entropy_loss = -5.4757, learner_queue_size = 32, train_seconds = 5908.7, _tick = 1957, _time = 1.6546e+09)
[2022-06-07 12:02:20,714][root][INFO] - Step 10606080 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 10606080, mean_episode_return = None, mean_episode_step = 1968.8, total_loss = 2.718, pg_loss = -12.296, baseline_loss = 20.402, entropy_loss = -5.3886, learner_queue_size = 32, train_seconds = 5913.7, _tick = 1958, _time = 1.6546e+09)
[2022-06-07 12:02:25,718][root][INFO] - Step 10616320 @ 2046.3 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 10616320, mean_episode_return = 56.581, mean_episode_step = 2622.4, total_loss = -13.664, pg_loss = -46.198, baseline_loss = 37.962, entropy_loss = -5.4282, learner_queue_size = 32, train_seconds = 5918.7, _tick = 1962, _time = 1.6546e+09)
[2022-06-07 12:02:30,724][root][INFO] - Step 10624000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10624000, mean_episode_return = 64.173, mean_episode_step = 2061.6, total_loss = 419.85, pg_loss = 287.05, baseline_loss = 138.24, entropy_loss = -5.4294, learner_queue_size = 32, train_seconds = 5923.7, _tick = 1963, _time = 1.6546e+09)
[2022-06-07 12:02:35,730][root][INFO] - Step 10634240 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 10634240, mean_episode_return = None, mean_episode_step = 1621.2, total_loss = 188.29, pg_loss = 132.5, baseline_loss = 61.216, entropy_loss = -5.429, learner_queue_size = 32, train_seconds = 5928.7, _tick = 1965, _time = 1.6546e+09)
[2022-06-07 12:02:40,734][root][INFO] - Step 10641920 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10641920, mean_episode_return = None, mean_episode_step = 2144.2, total_loss = -135.61, pg_loss = -136.95, baseline_loss = 6.6722, entropy_loss = -5.339, learner_queue_size = 32, train_seconds = 5933.7, _tick = 1965, _time = 1.6546e+09)
[2022-06-07 12:02:45,738][root][INFO] - Step 10652160 @ 2046.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 10652160, mean_episode_return = 55.263, mean_episode_step = 2603.5, total_loss = -110.14, pg_loss = -111.28, baseline_loss = 6.6271, entropy_loss = -5.4867, learner_queue_size = 32, train_seconds = 5938.7, _tick = 1967, _time = 1.6546e+09)
[2022-06-07 12:02:50,744][root][INFO] - Step 10659840 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10659840, mean_episode_return = -7.3811, mean_episode_step = 2188.0, total_loss = 71.324, pg_loss = 17.452, baseline_loss = 59.371, entropy_loss = -5.4984, learner_queue_size = 32, train_seconds = 5943.7, _tick = 1969, _time = 1.6546e+09)
[2022-06-07 12:02:55,750][root][INFO] - Step 10670080 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 10670080, mean_episode_return = None, mean_episode_step = 2000.3, total_loss = 32.113, pg_loss = 3.9565, baseline_loss = 33.687, entropy_loss = -5.5304, learner_queue_size = 32, train_seconds = 5948.7, _tick = 1969, _time = 1.6546e+09)
[2022-06-07 12:03:00,754][root][INFO] - Step 10677760 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 10677760, mean_episode_return = -5.0306, mean_episode_step = 2763.7, total_loss = 60.618, pg_loss = 9.7119, baseline_loss = 56.399, entropy_loss = -5.4928, learner_queue_size = 32, train_seconds = 5953.7, _tick = 1971, _time = 1.6546e+09)
[2022-06-07 12:03:05,760][root][INFO] - Step 10688000 @ 2045.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 10688000, mean_episode_return = None, mean_episode_step = 2141.1, total_loss = 173.2, pg_loss = 121.45, baseline_loss = 57.154, entropy_loss = -5.4, learner_queue_size = 32, train_seconds = 5958.8, _tick = 1972, _time = 1.6546e+09)
[2022-06-07 12:03:10,766][root][INFO] - Step 10695680 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10695680, mean_episode_return = None, mean_episode_step = 1932.4, total_loss = 373.63, pg_loss = 267.56, baseline_loss = 111.49, entropy_loss = -5.4161, learner_queue_size = 32, train_seconds = 5963.8, _tick = 1973, _time = 1.6546e+09)
[2022-06-07 12:03:15,770][root][INFO] - Step 10705920 @ 2046.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 10705920, mean_episode_return = None, mean_episode_step = 2076.5, total_loss = -61.21, pg_loss = -65.828, baseline_loss = 10.077, entropy_loss = -5.4596, learner_queue_size = 32, train_seconds = 5968.8, _tick = 1975, _time = 1.6546e+09)
[2022-06-07 12:03:20,774][root][INFO] - Step 10713600 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10713600, mean_episode_return = -14.425, mean_episode_step = 2318.1, total_loss = 82.748, pg_loss = 13.711, baseline_loss = 74.406, entropy_loss = -5.3685, learner_queue_size = 32, train_seconds = 5973.8, _tick = 1978, _time = 1.6546e+09)
[2022-06-07 12:03:25,780][root][INFO] - Step 10723840 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 10723840, mean_episode_return = None, mean_episode_step = 2141.6, total_loss = -51.103, pg_loss = -54.051, baseline_loss = 8.2412, entropy_loss = -5.2933, learner_queue_size = 32, train_seconds = 5978.8, _tick = 1980, _time = 1.6546e+09)
[2022-06-07 12:03:30,786][root][INFO] - Step 10731520 @ 1534.1 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 10731520, mean_episode_return = None, mean_episode_step = 1996.4, total_loss = -34.58, pg_loss = -56.959, baseline_loss = 27.748, entropy_loss = -5.3688, learner_queue_size = 32, train_seconds = 5983.8, _tick = 1982, _time = 1.6546e+09)
[2022-06-07 12:03:35,792][root][INFO] - Step 10741760 @ 2045.6 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 10741760, mean_episode_return = 49.496, mean_episode_step = 2655.8, total_loss = 112.68, pg_loss = 58.274, baseline_loss = 59.826, entropy_loss = -5.4185, learner_queue_size = 32, train_seconds = 5988.8, _tick = 1984, _time = 1.6546e+09)
[2022-06-07 12:03:40,798][root][INFO] - Step 10749440 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 10749440, mean_episode_return = None, mean_episode_step = 2163.9, total_loss = 28.432, pg_loss = 5.0954, baseline_loss = 28.754, entropy_loss = -5.4169, learner_queue_size = 32, train_seconds = 5993.8, _tick = 1986, _time = 1.6546e+09)
[2022-06-07 12:03:45,802][root][INFO] - Step 10759680 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10759680, mean_episode_return = 68.239, mean_episode_step = 2358.5, total_loss = 29.342, pg_loss = -10.219, baseline_loss = 45.012, entropy_loss = -5.4506, learner_queue_size = 32, train_seconds = 5998.8, _tick = 1989, _time = 1.6546e+09)
[2022-06-07 12:03:50,806][root][INFO] - Step 10767360 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 10767360, mean_episode_return = None, mean_episode_step = 2413.6, total_loss = 5.8795, pg_loss = -25.129, baseline_loss = 36.489, entropy_loss = -5.4809, learner_queue_size = 32, train_seconds = 6003.8, _tick = 1989, _time = 1.6546e+09)
[2022-06-07 12:03:55,812][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 12:03:56,023][root][INFO] - Step 10777600 @ 2045.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 10777600, mean_episode_return = None, mean_episode_step = 2669.3, total_loss = -32.779, pg_loss = -49.58, baseline_loss = 22.196, entropy_loss = -5.3956, learner_queue_size = 32, train_seconds = 6008.8, _tick = 1990, _time = 1.6546e+09)
[2022-06-07 12:04:01,029][root][INFO] - Step 10787840 @ 1962.8 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 10787840, mean_episode_return = None, mean_episode_step = 2472.6, total_loss = -54.789, pg_loss = -63.325, baseline_loss = 13.913, entropy_loss = -5.3775, learner_queue_size = 32, train_seconds = 6014.0, _tick = 1992, _time = 1.6546e+09)
[2022-06-07 12:04:06,034][root][INFO] - Step 10795520 @ 1534.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10795520, mean_episode_return = 174.81, mean_episode_step = 2241.6, total_loss = -32.844, pg_loss = -59.234, baseline_loss = 31.882, entropy_loss = -5.493, learner_queue_size = 32, train_seconds = 6019.0, _tick = 1994, _time = 1.6546e+09)
[2022-06-07 12:04:11,040][root][INFO] - Step 10803200 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 10803200, mean_episode_return = 37.112, mean_episode_step = 2165.3, total_loss = 251.53, pg_loss = 182.83, baseline_loss = 74.214, entropy_loss = -5.5187, learner_queue_size = 32, train_seconds = 6024.0, _tick = 1996, _time = 1.6546e+09)
[2022-06-07 12:04:16,046][root][INFO] - Step 10813440 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 10813440, mean_episode_return = 158.56, mean_episode_step = 2001.0, total_loss = 61.773, pg_loss = 18.207, baseline_loss = 49.04, entropy_loss = -5.474, learner_queue_size = 32, train_seconds = 6029.0, _tick = 2000, _time = 1.6546e+09)
[2022-06-07 12:04:21,052][root][INFO] - Step 10821120 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 10821120, mean_episode_return = None, mean_episode_step = 2305.5, total_loss = 13.846, pg_loss = -3.4256, baseline_loss = 22.925, entropy_loss = -5.6533, learner_queue_size = 32, train_seconds = 6034.0, _tick = 2000, _time = 1.6546e+09)
[2022-06-07 12:04:26,058][root][INFO] - Step 10831360 @ 2045.5 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 10831360, mean_episode_return = 56.064, mean_episode_step = 2021.0, total_loss = -5.2327, pg_loss = -34.999, baseline_loss = 35.418, entropy_loss = -5.6516, learner_queue_size = 32, train_seconds = 6039.1, _tick = 2003, _time = 1.6546e+09)
[2022-06-07 12:04:31,060][root][INFO] - Step 10839040 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10839040, mean_episode_return = 5.6893, mean_episode_step = 2909.6, total_loss = 427.42, pg_loss = 257.11, baseline_loss = 175.91, entropy_loss = -5.6052, learner_queue_size = 32, train_seconds = 6044.1, _tick = 2006, _time = 1.6546e+09)
[2022-06-07 12:04:36,066][root][INFO] - Step 10849280 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 10849280, mean_episode_return = None, mean_episode_step = 2211.0, total_loss = -22.199, pg_loss = -42.671, baseline_loss = 26.076, entropy_loss = -5.6032, learner_queue_size = 32, train_seconds = 6049.1, _tick = 2007, _time = 1.6546e+09)
[2022-06-07 12:04:41,072][root][INFO] - Step 10856960 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10856960, mean_episode_return = None, mean_episode_step = 2503.1, total_loss = -2.8848, pg_loss = -23.227, baseline_loss = 25.941, entropy_loss = -5.5984, learner_queue_size = 32, train_seconds = 6054.1, _tick = 2009, _time = 1.6546e+09)
[2022-06-07 12:04:46,078][root][INFO] - Step 10867200 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10867200, mean_episode_return = 36.891, mean_episode_step = 1868.2, total_loss = -15.597, pg_loss = -48.351, baseline_loss = 38.255, entropy_loss = -5.5003, learner_queue_size = 32, train_seconds = 6059.1, _tick = 2012, _time = 1.6546e+09)
[2022-06-07 12:04:51,082][root][INFO] - Step 10874880 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 10874880, mean_episode_return = 37.555, mean_episode_step = 1832.0, total_loss = 322.05, pg_loss = 200.4, baseline_loss = 127.17, entropy_loss = -5.516, learner_queue_size = 32, train_seconds = 6064.1, _tick = 2013, _time = 1.6546e+09)
[2022-06-07 12:04:56,086][root][INFO] - Step 10885120 @ 2046.3 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 10885120, mean_episode_return = 66.204, mean_episode_step = 2268.9, total_loss = -51.026, pg_loss = -90.875, baseline_loss = 45.33, entropy_loss = -5.4811, learner_queue_size = 32, train_seconds = 6069.1, _tick = 2014, _time = 1.6546e+09)
[2022-06-07 12:05:01,090][root][INFO] - Step 10892800 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 10892800, mean_episode_return = None, mean_episode_step = 1947.9, total_loss = 98.267, pg_loss = 52.738, baseline_loss = 50.876, entropy_loss = -5.3465, learner_queue_size = 32, train_seconds = 6074.1, _tick = 2014, _time = 1.6546e+09)
[2022-06-07 12:05:06,094][root][INFO] - Step 10903040 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 10903040, mean_episode_return = None, mean_episode_step = 1829.6, total_loss = -95.563, pg_loss = -91.718, baseline_loss = 1.3333, entropy_loss = -5.1788, learner_queue_size = 32, train_seconds = 6079.1, _tick = 2015, _time = 1.6546e+09)
[2022-06-07 12:05:11,098][root][INFO] - Step 10910720 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 10910720, mean_episode_return = 39.27, mean_episode_step = 2086.8, total_loss = -23.572, pg_loss = -39.248, baseline_loss = 20.816, entropy_loss = -5.141, learner_queue_size = 32, train_seconds = 6084.1, _tick = 2017, _time = 1.6546e+09)
[2022-06-07 12:05:16,102][root][INFO] - Step 10920960 @ 2046.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 10920960, mean_episode_return = None, mean_episode_step = 1481.1, total_loss = 216.75, pg_loss = 145.51, baseline_loss = 76.344, entropy_loss = -5.1122, learner_queue_size = 32, train_seconds = 6089.1, _tick = 2020, _time = 1.6546e+09)
[2022-06-07 12:05:21,108][root][INFO] - Step 10928640 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 10928640, mean_episode_return = 76.485, mean_episode_step = 1856.2, total_loss = -57.987, pg_loss = -76.613, baseline_loss = 23.59, entropy_loss = -4.965, learner_queue_size = 32, train_seconds = 6094.1, _tick = 2022, _time = 1.6546e+09)
[2022-06-07 12:05:26,121][root][INFO] - Step 10938880 @ 2042.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 10938880, mean_episode_return = None, mean_episode_step = 2112.8, total_loss = -20.776, pg_loss = -49.096, baseline_loss = 33.623, entropy_loss = -5.3026, learner_queue_size = 32, train_seconds = 6099.1, _tick = 2023, _time = 1.6546e+09)
[2022-06-07 12:05:31,127][root][INFO] - Step 10946560 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 10946560, mean_episode_return = 184.2, mean_episode_step = 2067.6, total_loss = 30.199, pg_loss = 6.8308, baseline_loss = 28.646, entropy_loss = -5.277, learner_queue_size = 32, train_seconds = 6104.1, _tick = 2025, _time = 1.6546e+09)
[2022-06-07 12:05:36,130][root][INFO] - Step 10956800 @ 2046.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10956800, mean_episode_return = None, mean_episode_step = 2108.4, total_loss = 168.34, pg_loss = 102.18, baseline_loss = 71.424, entropy_loss = -5.2639, learner_queue_size = 32, train_seconds = 6109.1, _tick = 2027, _time = 1.6546e+09)
[2022-06-07 12:05:41,136][root][INFO] - Step 10967040 @ 2045.5 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 10967040, mean_episode_return = None, mean_episode_step = 2573.4, total_loss = 354.2, pg_loss = 257.16, baseline_loss = 102.34, entropy_loss = -5.3027, learner_queue_size = 32, train_seconds = 6114.1, _tick = 2028, _time = 1.6546e+09)
[2022-06-07 12:05:46,142][root][INFO] - Step 10974720 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 10974720, mean_episode_return = None, mean_episode_step = 2044.9, total_loss = 3.2742, pg_loss = -3.5063, baseline_loss = 12.041, entropy_loss = -5.2604, learner_queue_size = 32, train_seconds = 6119.1, _tick = 2030, _time = 1.6546e+09)
[2022-06-07 12:05:51,146][root][INFO] - Step 10984960 @ 2046.4 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 10984960, mean_episode_return = None, mean_episode_step = 1959.3, total_loss = -70.843, pg_loss = -88.761, baseline_loss = 23.256, entropy_loss = -5.3377, learner_queue_size = 32, train_seconds = 6124.1, _tick = 2032, _time = 1.6546e+09)
[2022-06-07 12:05:56,150][root][INFO] - Step 10992640 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 10992640, mean_episode_return = None, mean_episode_step = 2018.8, total_loss = 168.56, pg_loss = 117.73, baseline_loss = 56.121, entropy_loss = -5.2924, learner_queue_size = 32, train_seconds = 6129.1, _tick = 2033, _time = 1.6546e+09)
[2022-06-07 12:06:01,156][root][INFO] - Step 11002880 @ 2045.6 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 11002880, mean_episode_return = None, mean_episode_step = 2332.6, total_loss = -107.65, pg_loss = -104.1, baseline_loss = 1.5738, entropy_loss = -5.1279, learner_queue_size = 32, train_seconds = 6134.2, _tick = 2034, _time = 1.6546e+09)
[2022-06-07 12:06:06,158][root][INFO] - Step 11010560 @ 1535.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 11010560, mean_episode_return = None, mean_episode_step = 1879.6, total_loss = -82.364, pg_loss = -82.339, baseline_loss = 5.1462, entropy_loss = -5.1706, learner_queue_size = 32, train_seconds = 6139.2, _tick = 2034, _time = 1.6546e+09)
[2022-06-07 12:06:11,162][root][INFO] - Step 11020800 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 11020800, mean_episode_return = None, mean_episode_step = 2515.0, total_loss = -34.021, pg_loss = -47.325, baseline_loss = 18.604, entropy_loss = -5.2997, learner_queue_size = 32, train_seconds = 6144.2, _tick = 2036, _time = 1.6546e+09)
[2022-06-07 12:06:16,166][root][INFO] - Step 11028480 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11028480, mean_episode_return = None, mean_episode_step = 2139.6, total_loss = 48.802, pg_loss = 13.357, baseline_loss = 40.684, entropy_loss = -5.2387, learner_queue_size = 32, train_seconds = 6149.2, _tick = 2037, _time = 1.6546e+09)
[2022-06-07 12:06:21,170][root][INFO] - Step 11038720 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 11038720, mean_episode_return = None, mean_episode_step = 2221.3, total_loss = 61.72, pg_loss = 35.545, baseline_loss = 31.377, entropy_loss = -5.2018, learner_queue_size = 32, train_seconds = 6154.2, _tick = 2039, _time = 1.6546e+09)
[2022-06-07 12:06:26,174][root][INFO] - Step 11046400 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 11046400, mean_episode_return = 112.21, mean_episode_step = 2841.7, total_loss = -30.113, pg_loss = -49.221, baseline_loss = 24.229, entropy_loss = -5.1203, learner_queue_size = 32, train_seconds = 6159.2, _tick = 2042, _time = 1.6546e+09)
[2022-06-07 12:06:31,178][root][INFO] - Step 11054080 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 11054080, mean_episode_return = None, mean_episode_step = 1748.1, total_loss = 130.05, pg_loss = 78.358, baseline_loss = 56.702, entropy_loss = -5.0101, learner_queue_size = 32, train_seconds = 6164.2, _tick = 2042, _time = 1.6546e+09)
[2022-06-07 12:06:36,182][root][INFO] - Step 11064320 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11064320, mean_episode_return = None, mean_episode_step = 2542.2, total_loss = 47.121, pg_loss = 13.696, baseline_loss = 38.496, entropy_loss = -5.0715, learner_queue_size = 32, train_seconds = 6169.2, _tick = 2043, _time = 1.6546e+09)
[2022-06-07 12:06:41,188][root][INFO] - Step 11074560 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11074560, mean_episode_return = 78.719, mean_episode_step = 2812.6, total_loss = -47.373, pg_loss = -56.801, baseline_loss = 14.59, entropy_loss = -5.1615, learner_queue_size = 32, train_seconds = 6174.2, _tick = 2045, _time = 1.6546e+09)
[2022-06-07 12:06:46,195][root][INFO] - Step 11082240 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11082240, mean_episode_return = 67.845, mean_episode_step = 1828.7, total_loss = -66.728, pg_loss = -94.613, baseline_loss = 32.924, entropy_loss = -5.0385, learner_queue_size = 32, train_seconds = 6179.2, _tick = 2046, _time = 1.6546e+09)
[2022-06-07 12:06:51,198][root][INFO] - Step 11092480 @ 2046.6 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 11092480, mean_episode_return = 61.161, mean_episode_step = 2202.5, total_loss = 1.9365, pg_loss = -30.19, baseline_loss = 37.176, entropy_loss = -5.0503, learner_queue_size = 32, train_seconds = 6184.2, _tick = 2050, _time = 1.6546e+09)
[2022-06-07 12:06:56,202][root][INFO] - Step 11100160 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11100160, mean_episode_return = None, mean_episode_step = 1911.0, total_loss = -125.87, pg_loss = -121.33, baseline_loss = 0.56694, entropy_loss = -5.1076, learner_queue_size = 32, train_seconds = 6189.2, _tick = 2050, _time = 1.6546e+09)
[2022-06-07 12:07:01,208][root][INFO] - Step 11110400 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 11110400, mean_episode_return = None, mean_episode_step = 1746.8, total_loss = 108.56, pg_loss = 64.173, baseline_loss = 49.589, entropy_loss = -5.2067, learner_queue_size = 32, train_seconds = 6194.2, _tick = 2051, _time = 1.6546e+09)
[2022-06-07 12:07:06,214][root][INFO] - Step 11118080 @ 1534.1 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 11118080, mean_episode_return = 44.21, mean_episode_step = 2748.9, total_loss = -93.666, pg_loss = -94.275, baseline_loss = 5.7394, entropy_loss = -5.1298, learner_queue_size = 32, train_seconds = 6199.2, _tick = 2052, _time = 1.6546e+09)
[2022-06-07 12:07:11,218][root][INFO] - Step 11128320 @ 2046.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 11128320, mean_episode_return = None, mean_episode_step = 2288.3, total_loss = -40.345, pg_loss = -61.706, baseline_loss = 26.527, entropy_loss = -5.1656, learner_queue_size = 32, train_seconds = 6204.2, _tick = 2052, _time = 1.6546e+09)
[2022-06-07 12:07:16,223][root][INFO] - Step 11136000 @ 1534.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 11136000, mean_episode_return = None, mean_episode_step = 2212.3, total_loss = -61.108, pg_loss = -62.118, baseline_loss = 6.3482, entropy_loss = -5.3384, learner_queue_size = 32, train_seconds = 6209.2, _tick = 2053, _time = 1.6546e+09)
[2022-06-07 12:07:21,226][root][INFO] - Step 11146240 @ 2046.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11146240, mean_episode_return = None, mean_episode_step = 2132.7, total_loss = 69.475, pg_loss = 39.251, baseline_loss = 35.598, entropy_loss = -5.3738, learner_queue_size = 32, train_seconds = 6214.2, _tick = 2054, _time = 1.6546e+09)
[2022-06-07 12:07:26,232][root][INFO] - Step 11153920 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11153920, mean_episode_return = None, mean_episode_step = 2158.5, total_loss = -72.093, pg_loss = -71.244, baseline_loss = 4.3812, entropy_loss = -5.2302, learner_queue_size = 32, train_seconds = 6219.2, _tick = 2055, _time = 1.6546e+09)
[2022-06-07 12:07:31,238][root][INFO] - Step 11164160 @ 2045.6 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 11164160, mean_episode_return = -9.2254, mean_episode_step = 2928.8, total_loss = -39.859, pg_loss = -62.817, baseline_loss = 28.227, entropy_loss = -5.2687, learner_queue_size = 32, train_seconds = 6224.2, _tick = 2056, _time = 1.6546e+09)
[2022-06-07 12:07:36,242][root][INFO] - Step 11174400 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11174400, mean_episode_return = None, mean_episode_step = 2108.8, total_loss = 475.52, pg_loss = 277.18, baseline_loss = 203.75, entropy_loss = -5.4067, learner_queue_size = 32, train_seconds = 6229.2, _tick = 2056, _time = 1.6546e+09)
[2022-06-07 12:07:41,246][root][INFO] - Step 11182080 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 11182080, mean_episode_return = None, mean_episode_step = 2520.5, total_loss = -130.08, pg_loss = -125.41, baseline_loss = 0.61156, entropy_loss = -5.2848, learner_queue_size = 32, train_seconds = 6234.2, _tick = 2057, _time = 1.6546e+09)
[2022-06-07 12:07:46,248][root][INFO] - Step 11192320 @ 2047.2 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 11192320, mean_episode_return = 53.432, mean_episode_step = 2502.2, total_loss = 102.57, pg_loss = 53.183, baseline_loss = 54.662, entropy_loss = -5.2754, learner_queue_size = 32, train_seconds = 6239.2, _tick = 2059, _time = 1.6546e+09)
[2022-06-07 12:07:51,250][root][INFO] - Step 11200000 @ 1535.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11200000, mean_episode_return = 29.64, mean_episode_step = 2304.8, total_loss = 119.15, pg_loss = 75.839, baseline_loss = 48.642, entropy_loss = -5.3342, learner_queue_size = 32, train_seconds = 6244.2, _tick = 2062, _time = 1.6546e+09)
[2022-06-07 12:07:56,256][root][INFO] - Step 11210240 @ 2045.6 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 11210240, mean_episode_return = None, mean_episode_step = 2303.1, total_loss = -39.617, pg_loss = -43.063, baseline_loss = 8.7954, entropy_loss = -5.3499, learner_queue_size = 32, train_seconds = 6249.3, _tick = 2065, _time = 1.6546e+09)
[2022-06-07 12:08:01,262][root][INFO] - Step 11220480 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 11220480, mean_episode_return = 49.807, mean_episode_step = 2137.8, total_loss = 60.406, pg_loss = 1.102, baseline_loss = 64.511, entropy_loss = -5.2064, learner_queue_size = 32, train_seconds = 6254.3, _tick = 2068, _time = 1.6546e+09)
[2022-06-07 12:08:06,266][root][INFO] - Step 11228160 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 11228160, mean_episode_return = 6.2994, mean_episode_step = 2350.4, total_loss = 239.69, pg_loss = 163.3, baseline_loss = 81.608, entropy_loss = -5.2156, learner_queue_size = 32, train_seconds = 6259.3, _tick = 2070, _time = 1.6546e+09)
[2022-06-07 12:08:11,270][root][INFO] - Step 11235840 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 11235840, mean_episode_return = -0.94036, mean_episode_step = 2298.7, total_loss = -65.853, pg_loss = -80.126, baseline_loss = 19.613, entropy_loss = -5.3403, learner_queue_size = 32, train_seconds = 6264.3, _tick = 2071, _time = 1.6546e+09)
[2022-06-07 12:08:16,274][root][INFO] - Step 11246080 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 11246080, mean_episode_return = 35.812, mean_episode_step = 2406.1, total_loss = -19.076, pg_loss = -21.384, baseline_loss = 7.6684, entropy_loss = -5.36, learner_queue_size = 32, train_seconds = 6269.3, _tick = 2072, _time = 1.6546e+09)
[2022-06-07 12:08:21,280][root][INFO] - Step 11253760 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 11253760, mean_episode_return = 89.538, mean_episode_step = 2207.5, total_loss = 81.289, pg_loss = 46.262, baseline_loss = 40.4, entropy_loss = -5.373, learner_queue_size = 32, train_seconds = 6274.3, _tick = 2073, _time = 1.6546e+09)
[2022-06-07 12:08:26,286][root][INFO] - Step 11264000 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11264000, mean_episode_return = 15.964, mean_episode_step = 2486.8, total_loss = -34.041, pg_loss = -82.669, baseline_loss = 53.855, entropy_loss = -5.2269, learner_queue_size = 32, train_seconds = 6279.3, _tick = 2077, _time = 1.6546e+09)
[2022-06-07 12:08:31,292][root][INFO] - Step 11271680 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11271680, mean_episode_return = 17.96, mean_episode_step = 2054.0, total_loss = 556.43, pg_loss = 401.9, baseline_loss = 159.72, entropy_loss = -5.1814, learner_queue_size = 32, train_seconds = 6284.3, _tick = 2080, _time = 1.6546e+09)
[2022-06-07 12:08:36,298][root][INFO] - Step 11281920 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 11281920, mean_episode_return = None, mean_episode_step = 2273.3, total_loss = 161.12, pg_loss = 95.155, baseline_loss = 71.118, entropy_loss = -5.1506, learner_queue_size = 32, train_seconds = 6289.3, _tick = 2082, _time = 1.6546e+09)
[2022-06-07 12:08:41,302][root][INFO] - Step 11289600 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11289600, mean_episode_return = 3.2898, mean_episode_step = 2574.3, total_loss = 131.14, pg_loss = 96.77, baseline_loss = 39.334, entropy_loss = -4.9683, learner_queue_size = 32, train_seconds = 6294.3, _tick = 2084, _time = 1.6546e+09)
[2022-06-07 12:08:46,306][root][INFO] - Step 11299840 @ 2046.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 11299840, mean_episode_return = 1.5002, mean_episode_step = 2958.2, total_loss = 19.334, pg_loss = -24.935, baseline_loss = 49.181, entropy_loss = -4.9121, learner_queue_size = 32, train_seconds = 6299.3, _tick = 2087, _time = 1.6546e+09)
[2022-06-07 12:08:51,310][root][INFO] - Step 11310080 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 11310080, mean_episode_return = None, mean_episode_step = 2248.1, total_loss = 719.52, pg_loss = 439.44, baseline_loss = 284.88, entropy_loss = -4.8034, learner_queue_size = 32, train_seconds = 6304.3, _tick = 2089, _time = 1.6546e+09)
[2022-06-07 12:08:56,322][root][INFO] - Step 11317760 @ 1532.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 11317760, mean_episode_return = None, mean_episode_step = 2166.4, total_loss = 57.631, pg_loss = 23.986, baseline_loss = 38.461, entropy_loss = -4.8157, learner_queue_size = 32, train_seconds = 6309.3, _tick = 2090, _time = 1.6546e+09)
[2022-06-07 12:09:01,326][root][INFO] - Step 11328000 @ 2046.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 11328000, mean_episode_return = None, mean_episode_step = 2195.8, total_loss = -37.079, pg_loss = -54.417, baseline_loss = 22.271, entropy_loss = -4.9323, learner_queue_size = 32, train_seconds = 6314.3, _tick = 2091, _time = 1.6546e+09)
[2022-06-07 12:09:06,330][root][INFO] - Step 11335680 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11335680, mean_episode_return = None, mean_episode_step = 1922.0, total_loss = 23.05, pg_loss = 8.5485, baseline_loss = 19.389, entropy_loss = -4.8876, learner_queue_size = 32, train_seconds = 6319.3, _tick = 2091, _time = 1.6546e+09)
[2022-06-07 12:09:11,336][root][INFO] - Step 11345920 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 11345920, mean_episode_return = 94.149, mean_episode_step = 2361.0, total_loss = 188.13, pg_loss = 102.88, baseline_loss = 90.155, entropy_loss = -4.9003, learner_queue_size = 32, train_seconds = 6324.3, _tick = 2094, _time = 1.6546e+09)
[2022-06-07 12:09:16,342][root][INFO] - Step 11353600 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11353600, mean_episode_return = None, mean_episode_step = 2236.3, total_loss = 186.34, pg_loss = 138.08, baseline_loss = 53.016, entropy_loss = -4.7619, learner_queue_size = 32, train_seconds = 6329.3, _tick = 2096, _time = 1.6546e+09)
[2022-06-07 12:09:21,346][root][INFO] - Step 11363840 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 11363840, mean_episode_return = None, mean_episode_step = 2866.2, total_loss = 69.332, pg_loss = 23.808, baseline_loss = 50.363, entropy_loss = -4.8384, learner_queue_size = 32, train_seconds = 6334.3, _tick = 2098, _time = 1.6546e+09)
[2022-06-07 12:09:26,350][root][INFO] - Step 11371520 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11371520, mean_episode_return = 35.575, mean_episode_step = 2592.0, total_loss = -44.409, pg_loss = -80.814, baseline_loss = 41.345, entropy_loss = -4.9394, learner_queue_size = 32, train_seconds = 6339.3, _tick = 2100, _time = 1.6546e+09)
[2022-06-07 12:09:31,356][root][INFO] - Step 11381760 @ 2045.6 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 11381760, mean_episode_return = None, mean_episode_step = 2322.6, total_loss = 200.3, pg_loss = 139.28, baseline_loss = 66.06, entropy_loss = -5.043, learner_queue_size = 32, train_seconds = 6344.4, _tick = 2101, _time = 1.6546e+09)
[2022-06-07 12:09:36,362][root][INFO] - Step 11389440 @ 1534.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 11389440, mean_episode_return = None, mean_episode_step = 2434.4, total_loss = 320.2, pg_loss = 223.23, baseline_loss = 102.0, entropy_loss = -5.033, learner_queue_size = 32, train_seconds = 6349.4, _tick = 2102, _time = 1.6546e+09)
[2022-06-07 12:09:41,366][root][INFO] - Step 11399680 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 11399680, mean_episode_return = None, mean_episode_step = 2413.6, total_loss = 139.01, pg_loss = 94.089, baseline_loss = 49.913, entropy_loss = -4.992, learner_queue_size = 32, train_seconds = 6354.4, _tick = 2103, _time = 1.6546e+09)
[2022-06-07 12:09:46,372][root][INFO] - Step 11407360 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11407360, mean_episode_return = 14.037, mean_episode_step = 2026.1, total_loss = 261.21, pg_loss = 117.67, baseline_loss = 148.45, entropy_loss = -4.9161, learner_queue_size = 32, train_seconds = 6359.4, _tick = 2106, _time = 1.6546e+09)
[2022-06-07 12:09:51,378][root][INFO] - Step 11417600 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11417600, mean_episode_return = 22.13, mean_episode_step = 1800.5, total_loss = -2.2752, pg_loss = -37.449, baseline_loss = 40.107, entropy_loss = -4.9332, learner_queue_size = 32, train_seconds = 6364.4, _tick = 2109, _time = 1.6546e+09)
[2022-06-07 12:09:56,384][root][INFO] - Step 11425280 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 11425280, mean_episode_return = -2.8709, mean_episode_step = 1890.9, total_loss = 115.87, pg_loss = 84.415, baseline_loss = 36.35, entropy_loss = -4.8931, learner_queue_size = 32, train_seconds = 6369.4, _tick = 2111, _time = 1.6546e+09)
[2022-06-07 12:10:01,390][root][INFO] - Step 11435520 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 11435520, mean_episode_return = None, mean_episode_step = 2137.6, total_loss = 234.88, pg_loss = 150.37, baseline_loss = 89.434, entropy_loss = -4.9251, learner_queue_size = 32, train_seconds = 6374.4, _tick = 2111, _time = 1.6546e+09)
[2022-06-07 12:10:06,396][root][INFO] - Step 11445760 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11445760, mean_episode_return = -11.3, mean_episode_step = 2511.3, total_loss = -65.032, pg_loss = -80.773, baseline_loss = 20.538, entropy_loss = -4.7965, learner_queue_size = 32, train_seconds = 6379.4, _tick = 2113, _time = 1.6546e+09)
[2022-06-07 12:10:11,402][root][INFO] - Step 11453440 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 11453440, mean_episode_return = -43.429, mean_episode_step = 2350.3, total_loss = 15.266, pg_loss = -3.5184, baseline_loss = 23.469, entropy_loss = -4.6843, learner_queue_size = 32, train_seconds = 6384.4, _tick = 2115, _time = 1.6546e+09)
[2022-06-07 12:10:16,408][root][INFO] - Step 11463680 @ 2045.5 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 11463680, mean_episode_return = 36.576, mean_episode_step = 2355.3, total_loss = 53.651, pg_loss = -14.612, baseline_loss = 73.064, entropy_loss = -4.8022, learner_queue_size = 32, train_seconds = 6389.4, _tick = 2119, _time = 1.6546e+09)
[2022-06-07 12:10:21,414][root][INFO] - Step 11471360 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11471360, mean_episode_return = None, mean_episode_step = 2020.2, total_loss = 273.08, pg_loss = 197.43, baseline_loss = 80.54, entropy_loss = -4.8851, learner_queue_size = 32, train_seconds = 6394.4, _tick = 2120, _time = 1.6546e+09)
[2022-06-07 12:10:26,418][root][INFO] - Step 11481600 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 11481600, mean_episode_return = None, mean_episode_step = 1993.7, total_loss = 335.45, pg_loss = 257.2, baseline_loss = 83.215, entropy_loss = -4.9638, learner_queue_size = 32, train_seconds = 6399.4, _tick = 2120, _time = 1.6546e+09)
[2022-06-07 12:10:31,422][root][INFO] - Step 11489280 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11489280, mean_episode_return = None, mean_episode_step = 2238.7, total_loss = -64.285, pg_loss = -72.512, baseline_loss = 13.169, entropy_loss = -4.9416, learner_queue_size = 32, train_seconds = 6404.4, _tick = 2122, _time = 1.6546e+09)
[2022-06-07 12:10:36,426][root][INFO] - Step 11499520 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 11499520, mean_episode_return = 67.541, mean_episode_step = 2220.8, total_loss = -126.92, pg_loss = -131.53, baseline_loss = 9.7591, entropy_loss = -5.1438, learner_queue_size = 32, train_seconds = 6409.4, _tick = 2126, _time = 1.6546e+09)
[2022-06-07 12:10:41,432][root][INFO] - Step 11507200 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11507200, mean_episode_return = 5.0094, mean_episode_step = 2552.0, total_loss = 31.782, pg_loss = -11.289, baseline_loss = 48.277, entropy_loss = -5.2058, learner_queue_size = 32, train_seconds = 6414.4, _tick = 2127, _time = 1.6546e+09)
[2022-06-07 12:10:46,438][root][INFO] - Step 11517440 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11517440, mean_episode_return = 51.261, mean_episode_step = 1822.6, total_loss = -2.6541, pg_loss = -27.532, baseline_loss = 30.179, entropy_loss = -5.3002, learner_queue_size = 32, train_seconds = 6419.4, _tick = 2130, _time = 1.6546e+09)
[2022-06-07 12:10:51,444][root][INFO] - Step 11525120 @ 1534.1 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 11525120, mean_episode_return = None, mean_episode_step = 2202.3, total_loss = 138.95, pg_loss = 82.084, baseline_loss = 62.141, entropy_loss = -5.2732, learner_queue_size = 32, train_seconds = 6424.4, _tick = 2131, _time = 1.6546e+09)
[2022-06-07 12:10:56,450][root][INFO] - Step 11532800 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11532800, mean_episode_return = 76.707, mean_episode_step = 1871.9, total_loss = 57.489, pg_loss = 25.558, baseline_loss = 37.151, entropy_loss = -5.2207, learner_queue_size = 32, train_seconds = 6429.4, _tick = 2133, _time = 1.6546e+09)
[2022-06-07 12:11:01,456][root][INFO] - Step 11543040 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11543040, mean_episode_return = -4.51, mean_episode_step = 2728.0, total_loss = -130.07, pg_loss = -146.44, baseline_loss = 21.635, entropy_loss = -5.2661, learner_queue_size = 32, train_seconds = 6434.5, _tick = 2134, _time = 1.6546e+09)
[2022-06-07 12:11:06,462][root][INFO] - Step 11550720 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11550720, mean_episode_return = None, mean_episode_step = 1933.6, total_loss = 32.406, pg_loss = -14.48, baseline_loss = 52.164, entropy_loss = -5.2776, learner_queue_size = 32, train_seconds = 6439.5, _tick = 2136, _time = 1.6546e+09)
[2022-06-07 12:11:11,466][root][INFO] - Step 11560960 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 11560960, mean_episode_return = None, mean_episode_step = 2727.6, total_loss = 27.478, pg_loss = 6.4034, baseline_loss = 26.186, entropy_loss = -5.1114, learner_queue_size = 32, train_seconds = 6444.5, _tick = 2137, _time = 1.6546e+09)
[2022-06-07 12:11:16,472][root][INFO] - Step 11571200 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 11571200, mean_episode_return = 85.615, mean_episode_step = 2681.3, total_loss = 322.28, pg_loss = 220.82, baseline_loss = 106.73, entropy_loss = -5.2635, learner_queue_size = 32, train_seconds = 6449.5, _tick = 2140, _time = 1.6546e+09)
[2022-06-07 12:11:21,474][root][INFO] - Step 11578880 @ 1535.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 11578880, mean_episode_return = 21.66, mean_episode_step = 2065.9, total_loss = -45.564, pg_loss = -63.577, baseline_loss = 23.197, entropy_loss = -5.1837, learner_queue_size = 32, train_seconds = 6454.5, _tick = 2143, _time = 1.6546e+09)
[2022-06-07 12:11:26,478][root][INFO] - Step 11589120 @ 2046.3 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 11589120, mean_episode_return = None, mean_episode_step = 2281.0, total_loss = 111.85, pg_loss = 21.952, baseline_loss = 95.114, entropy_loss = -5.2118, learner_queue_size = 32, train_seconds = 6459.5, _tick = 2144, _time = 1.6546e+09)
[2022-06-07 12:11:31,482][root][INFO] - Step 11596800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11596800, mean_episode_return = 33.4, mean_episode_step = 2248.4, total_loss = -74.485, pg_loss = -82.104, baseline_loss = 12.755, entropy_loss = -5.1365, learner_queue_size = 32, train_seconds = 6464.5, _tick = 2147, _time = 1.6546e+09)
[2022-06-07 12:11:36,488][root][INFO] - Step 11607040 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 11607040, mean_episode_return = None, mean_episode_step = 2175.4, total_loss = 260.9, pg_loss = 167.87, baseline_loss = 98.169, entropy_loss = -5.1356, learner_queue_size = 32, train_seconds = 6469.5, _tick = 2149, _time = 1.6546e+09)
[2022-06-07 12:11:41,494][root][INFO] - Step 11614720 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11614720, mean_episode_return = 23.048, mean_episode_step = 2281.0, total_loss = 76.471, pg_loss = 12.506, baseline_loss = 69.097, entropy_loss = -5.1315, learner_queue_size = 32, train_seconds = 6474.5, _tick = 2152, _time = 1.6546e+09)
[2022-06-07 12:11:46,500][root][INFO] - Step 11624960 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 11624960, mean_episode_return = 71.244, mean_episode_step = 2581.5, total_loss = 179.95, pg_loss = 58.839, baseline_loss = 126.27, entropy_loss = -5.1568, learner_queue_size = 32, train_seconds = 6479.5, _tick = 2154, _time = 1.6546e+09)
[2022-06-07 12:11:51,506][root][INFO] - Step 11632640 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 11632640, mean_episode_return = None, mean_episode_step = 1991.6, total_loss = 6.611, pg_loss = -18.91, baseline_loss = 30.628, entropy_loss = -5.1078, learner_queue_size = 32, train_seconds = 6484.5, _tick = 2155, _time = 1.6546e+09)
[2022-06-07 12:11:56,510][root][INFO] - Step 11642880 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 11642880, mean_episode_return = 61.2, mean_episode_step = 2458.6, total_loss = -9.8949, pg_loss = -34.617, baseline_loss = 30.045, entropy_loss = -5.3234, learner_queue_size = 32, train_seconds = 6489.5, _tick = 2158, _time = 1.6546e+09)
[2022-06-07 12:12:01,514][root][INFO] - Step 11650560 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 11650560, mean_episode_return = 29.322, mean_episode_step = 2484.2, total_loss = 53.086, pg_loss = 21.186, baseline_loss = 37.224, entropy_loss = -5.3244, learner_queue_size = 32, train_seconds = 6494.5, _tick = 2160, _time = 1.6546e+09)
[2022-06-07 12:12:06,518][root][INFO] - Step 11660800 @ 2046.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 11660800, mean_episode_return = None, mean_episode_step = 2090.3, total_loss = 40.2, pg_loss = 19.602, baseline_loss = 25.91, entropy_loss = -5.3114, learner_queue_size = 32, train_seconds = 6499.5, _tick = 2163, _time = 1.6546e+09)
[2022-06-07 12:12:11,524][root][INFO] - Step 11668480 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 11668480, mean_episode_return = 48.958, mean_episode_step = 2460.5, total_loss = 269.63, pg_loss = 185.96, baseline_loss = 88.956, entropy_loss = -5.2808, learner_queue_size = 32, train_seconds = 6504.5, _tick = 2165, _time = 1.6546e+09)
[2022-06-07 12:12:16,530][root][INFO] - Step 11678720 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 11678720, mean_episode_return = 61.959, mean_episode_step = 1714.3, total_loss = -48.78, pg_loss = -65.851, baseline_loss = 22.217, entropy_loss = -5.1466, learner_queue_size = 32, train_seconds = 6509.5, _tick = 2169, _time = 1.6546e+09)
[2022-06-07 12:12:21,536][root][INFO] - Step 11686400 @ 1534.1 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 11686400, mean_episode_return = 48.611, mean_episode_step = 2311.1, total_loss = 323.37, pg_loss = 202.24, baseline_loss = 126.34, entropy_loss = -5.2063, learner_queue_size = 32, train_seconds = 6514.5, _tick = 2171, _time = 1.6546e+09)
[2022-06-07 12:12:26,542][root][INFO] - Step 11696640 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11696640, mean_episode_return = None, mean_episode_step = 2074.5, total_loss = 36.905, pg_loss = 7.3469, baseline_loss = 34.768, entropy_loss = -5.2097, learner_queue_size = 32, train_seconds = 6519.5, _tick = 2172, _time = 1.6546e+09)
[2022-06-07 12:12:31,548][root][INFO] - Step 11704320 @ 1534.1 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 11704320, mean_episode_return = None, mean_episode_step = 2039.9, total_loss = -99.098, pg_loss = -97.765, baseline_loss = 3.8269, entropy_loss = -5.1595, learner_queue_size = 32, train_seconds = 6524.5, _tick = 2173, _time = 1.6546e+09)
[2022-06-07 12:12:36,554][root][INFO] - Step 11714560 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11714560, mean_episode_return = None, mean_episode_step = 1734.7, total_loss = 121.92, pg_loss = 78.947, baseline_loss = 48.196, entropy_loss = -5.2201, learner_queue_size = 32, train_seconds = 6529.5, _tick = 2175, _time = 1.6546e+09)
[2022-06-07 12:12:41,558][root][INFO] - Step 11724800 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 11724800, mean_episode_return = 48.615, mean_episode_step = 2191.8, total_loss = 183.11, pg_loss = 99.916, baseline_loss = 88.445, entropy_loss = -5.2483, learner_queue_size = 32, train_seconds = 6534.6, _tick = 2177, _time = 1.6546e+09)
[2022-06-07 12:12:46,564][root][INFO] - Step 11732480 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11732480, mean_episode_return = 56.668, mean_episode_step = 2349.8, total_loss = 19.38, pg_loss = -39.069, baseline_loss = 63.628, entropy_loss = -5.1783, learner_queue_size = 32, train_seconds = 6539.6, _tick = 2179, _time = 1.6546e+09)
[2022-06-07 12:12:51,570][root][INFO] - Step 11742720 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 11742720, mean_episode_return = None, mean_episode_step = 2450.8, total_loss = -5.5644, pg_loss = -18.969, baseline_loss = 18.712, entropy_loss = -5.3073, learner_queue_size = 32, train_seconds = 6544.6, _tick = 2180, _time = 1.6546e+09)
[2022-06-07 12:12:56,574][root][INFO] - Step 11750400 @ 1534.8 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 11750400, mean_episode_return = None, mean_episode_step = 1918.4, total_loss = 476.54, pg_loss = 349.62, baseline_loss = 132.31, entropy_loss = -5.3946, learner_queue_size = 32, train_seconds = 6549.6, _tick = 2181, _time = 1.6546e+09)
[2022-06-07 12:13:01,580][root][INFO] - Step 11758080 @ 1534.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 11758080, mean_episode_return = None, mean_episode_step = 2053.2, total_loss = -113.04, pg_loss = -112.33, baseline_loss = 4.6433, entropy_loss = -5.3517, learner_queue_size = 32, train_seconds = 6554.6, _tick = 2181, _time = 1.6546e+09)
[2022-06-07 12:13:06,586][root][INFO] - Step 11768320 @ 2045.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11768320, mean_episode_return = 122.94, mean_episode_step = 1797.4, total_loss = 198.93, pg_loss = 120.63, baseline_loss = 83.719, entropy_loss = -5.4172, learner_queue_size = 32, train_seconds = 6559.6, _tick = 2184, _time = 1.6546e+09)
[2022-06-07 12:13:11,590][root][INFO] - Step 11776000 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 11776000, mean_episode_return = 163.49, mean_episode_step = 2033.3, total_loss = -54.953, pg_loss = -85.529, baseline_loss = 35.938, entropy_loss = -5.3613, learner_queue_size = 32, train_seconds = 6564.6, _tick = 2187, _time = 1.6546e+09)
[2022-06-07 12:13:16,594][root][INFO] - Step 11786240 @ 2046.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 11786240, mean_episode_return = 145.58, mean_episode_step = 2070.2, total_loss = 556.44, pg_loss = 386.84, baseline_loss = 174.96, entropy_loss = -5.3666, learner_queue_size = 32, train_seconds = 6569.6, _tick = 2189, _time = 1.6546e+09)
[2022-06-07 12:13:21,600][root][INFO] - Step 11793920 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 11793920, mean_episode_return = None, mean_episode_step = 1759.0, total_loss = -98.442, pg_loss = -99.78, baseline_loss = 6.6272, entropy_loss = -5.2892, learner_queue_size = 32, train_seconds = 6574.6, _tick = 2189, _time = 1.6546e+09)
[2022-06-07 12:13:26,606][root][INFO] - Step 11804160 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 11804160, mean_episode_return = 54.482, mean_episode_step = 1896.0, total_loss = 442.43, pg_loss = 291.07, baseline_loss = 156.71, entropy_loss = -5.3573, learner_queue_size = 32, train_seconds = 6579.6, _tick = 2192, _time = 1.6546e+09)
[2022-06-07 12:13:31,612][root][INFO] - Step 11811840 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 11811840, mean_episode_return = 85.334, mean_episode_step = 2514.2, total_loss = 45.369, pg_loss = -34.507, baseline_loss = 85.175, entropy_loss = -5.2993, learner_queue_size = 32, train_seconds = 6584.6, _tick = 2195, _time = 1.6546e+09)
[2022-06-07 12:13:36,614][root][INFO] - Step 11822080 @ 2047.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 11822080, mean_episode_return = 33.97, mean_episode_step = 2115.2, total_loss = 276.42, pg_loss = 167.7, baseline_loss = 114.14, entropy_loss = -5.4198, learner_queue_size = 32, train_seconds = 6589.6, _tick = 2197, _time = 1.6546e+09)
[2022-06-07 12:13:41,619][root][INFO] - Step 11832320 @ 2045.9 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 11832320, mean_episode_return = 89.996, mean_episode_step = 1800.4, total_loss = 3208.7, pg_loss = 1138.6, baseline_loss = 2075.5, entropy_loss = -5.3869, learner_queue_size = 32, train_seconds = 6594.6, _tick = 2200, _time = 1.6546e+09)
[2022-06-07 12:13:46,625][root][INFO] - Step 11840000 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 11840000, mean_episode_return = 60.217, mean_episode_step = 2094.2, total_loss = -115.04, pg_loss = -138.27, baseline_loss = 28.604, entropy_loss = -5.3772, learner_queue_size = 32, train_seconds = 6599.6, _tick = 2203, _time = 1.6546e+09)
[2022-06-07 12:13:51,630][root][INFO] - Step 11850240 @ 2045.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 11850240, mean_episode_return = 66.672, mean_episode_step = 1636.9, total_loss = 128.42, pg_loss = 40.477, baseline_loss = 93.36, entropy_loss = -5.4163, learner_queue_size = 32, train_seconds = 6604.6, _tick = 2205, _time = 1.6546e+09)
[2022-06-07 12:13:56,634][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 12:13:56,817][root][INFO] - Step 11857920 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 11857920, mean_episode_return = None, mean_episode_step = 2415.2, total_loss = -72.885, pg_loss = -85.828, baseline_loss = 18.285, entropy_loss = -5.3418, learner_queue_size = 32, train_seconds = 6609.6, _tick = 2206, _time = 1.6546e+09)
[2022-06-07 12:14:01,823][root][INFO] - Step 11868160 @ 1973.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 11868160, mean_episode_return = 53.084, mean_episode_step = 2402.9, total_loss = -139.68, pg_loss = -142.9, baseline_loss = 8.486, entropy_loss = -5.2704, learner_queue_size = 32, train_seconds = 6614.8, _tick = 2209, _time = 1.6546e+09)
[2022-06-07 12:14:06,830][root][INFO] - Step 11875840 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 11875840, mean_episode_return = None, mean_episode_step = 1961.1, total_loss = 284.36, pg_loss = 201.43, baseline_loss = 88.245, entropy_loss = -5.3175, learner_queue_size = 32, train_seconds = 6619.8, _tick = 2210, _time = 1.6546e+09)
[2022-06-07 12:14:11,834][root][INFO] - Step 11886080 @ 2046.2 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 11886080, mean_episode_return = -1.4387, mean_episode_step = 2013.7, total_loss = 457.84, pg_loss = 298.0, baseline_loss = 165.12, entropy_loss = -5.2821, learner_queue_size = 32, train_seconds = 6624.8, _tick = 2212, _time = 1.6546e+09)
[2022-06-07 12:14:16,838][root][INFO] - Step 11893760 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 11893760, mean_episode_return = 18.71, mean_episode_step = 2032.0, total_loss = -38.843, pg_loss = -60.679, baseline_loss = 27.056, entropy_loss = -5.2193, learner_queue_size = 32, train_seconds = 6629.8, _tick = 2213, _time = 1.6546e+09)
[2022-06-07 12:14:21,841][root][INFO] - Step 11904000 @ 2046.6 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 11904000, mean_episode_return = -1.1803, mean_episode_step = 2120.2, total_loss = 206.75, pg_loss = 137.05, baseline_loss = 74.842, entropy_loss = -5.1477, learner_queue_size = 32, train_seconds = 6634.8, _tick = 2217, _time = 1.6546e+09)
[2022-06-07 12:14:26,846][root][INFO] - Step 11911680 @ 1534.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 11911680, mean_episode_return = None, mean_episode_step = 2251.5, total_loss = -14.678, pg_loss = -22.717, baseline_loss = 13.22, entropy_loss = -5.1811, learner_queue_size = 32, train_seconds = 6639.8, _tick = 2218, _time = 1.6546e+09)
[2022-06-07 12:14:31,852][root][INFO] - Step 11921920 @ 2045.5 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 11921920, mean_episode_return = 44.538, mean_episode_step = 2136.3, total_loss = -56.049, pg_loss = -77.257, baseline_loss = 26.609, entropy_loss = -5.4005, learner_queue_size = 32, train_seconds = 6644.8, _tick = 2220, _time = 1.6546e+09)
[2022-06-07 12:14:36,858][root][INFO] - Step 11929600 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 11929600, mean_episode_return = 24.759, mean_episode_step = 1776.5, total_loss = 329.93, pg_loss = 243.02, baseline_loss = 92.309, entropy_loss = -5.4078, learner_queue_size = 32, train_seconds = 6649.9, _tick = 2223, _time = 1.6546e+09)
[2022-06-07 12:14:41,862][root][INFO] - Step 11939840 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 11939840, mean_episode_return = 87.765, mean_episode_step = 2042.2, total_loss = 28.024, pg_loss = -53.486, baseline_loss = 86.861, entropy_loss = -5.351, learner_queue_size = 32, train_seconds = 6654.9, _tick = 2227, _time = 1.6546e+09)
[2022-06-07 12:14:46,866][root][INFO] - Step 11947520 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 11947520, mean_episode_return = 18.73, mean_episode_step = 1782.1, total_loss = 163.05, pg_loss = 107.7, baseline_loss = 60.76, entropy_loss = -5.406, learner_queue_size = 32, train_seconds = 6659.9, _tick = 2229, _time = 1.6546e+09)
[2022-06-07 12:14:51,870][root][INFO] - Step 11957760 @ 2046.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 11957760, mean_episode_return = 127.36, mean_episode_step = 2205.2, total_loss = -87.337, pg_loss = -101.65, baseline_loss = 19.692, entropy_loss = -5.3744, learner_queue_size = 32, train_seconds = 6664.9, _tick = 2231, _time = 1.6546e+09)
[2022-06-07 12:14:56,874][root][INFO] - Step 11965440 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 11965440, mean_episode_return = 33.21, mean_episode_step = 2008.4, total_loss = 164.29, pg_loss = 101.74, baseline_loss = 67.899, entropy_loss = -5.3538, learner_queue_size = 32, train_seconds = 6669.9, _tick = 2233, _time = 1.6546e+09)
[2022-06-07 12:15:01,878][root][INFO] - Step 11975680 @ 2046.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 11975680, mean_episode_return = 86.096, mean_episode_step = 1621.3, total_loss = 70.056, pg_loss = 37.557, baseline_loss = 37.921, entropy_loss = -5.4211, learner_queue_size = 32, train_seconds = 6674.9, _tick = 2237, _time = 1.6546e+09)
[2022-06-07 12:15:06,884][root][INFO] - Step 11983360 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 11983360, mean_episode_return = None, mean_episode_step = 2360.9, total_loss = 5.4336, pg_loss = -6.4129, baseline_loss = 17.389, entropy_loss = -5.5428, learner_queue_size = 32, train_seconds = 6679.9, _tick = 2238, _time = 1.6546e+09)
[2022-06-07 12:15:11,890][root][INFO] - Step 11993600 @ 2045.6 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 11993600, mean_episode_return = None, mean_episode_step = 1288.1, total_loss = 419.4, pg_loss = 289.73, baseline_loss = 135.19, entropy_loss = -5.5186, learner_queue_size = 32, train_seconds = 6684.9, _tick = 2239, _time = 1.6546e+09)
[2022-06-07 12:15:16,894][root][INFO] - Step 12001280 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12001280, mean_episode_return = 32.504, mean_episode_step = 2232.3, total_loss = -40.124, pg_loss = -86.094, baseline_loss = 51.399, entropy_loss = -5.4286, learner_queue_size = 32, train_seconds = 6689.9, _tick = 2241, _time = 1.6546e+09)
[2022-06-07 12:15:21,898][root][INFO] - Step 12011520 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 12011520, mean_episode_return = 78.943, mean_episode_step = 1822.1, total_loss = -135.23, pg_loss = -153.4, baseline_loss = 23.642, entropy_loss = -5.4703, learner_queue_size = 32, train_seconds = 6694.9, _tick = 2242, _time = 1.6546e+09)
[2022-06-07 12:15:26,902][root][INFO] - Step 12019200 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12019200, mean_episode_return = 32.848, mean_episode_step = 2037.3, total_loss = 523.16, pg_loss = 326.63, baseline_loss = 202.0, entropy_loss = -5.4731, learner_queue_size = 32, train_seconds = 6699.9, _tick = 2244, _time = 1.6546e+09)
[2022-06-07 12:15:31,906][root][INFO] - Step 12029440 @ 2046.3 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 12029440, mean_episode_return = 3.2495, mean_episode_step = 1962.5, total_loss = -3.7152, pg_loss = -38.363, baseline_loss = 40.146, entropy_loss = -5.4989, learner_queue_size = 32, train_seconds = 6704.9, _tick = 2247, _time = 1.6546e+09)
[2022-06-07 12:15:36,910][root][INFO] - Step 12037120 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12037120, mean_episode_return = 42.706, mean_episode_step = 1995.9, total_loss = -107.02, pg_loss = -110.95, baseline_loss = 9.4355, entropy_loss = -5.5049, learner_queue_size = 32, train_seconds = 6709.9, _tick = 2248, _time = 1.6546e+09)
[2022-06-07 12:15:41,914][root][INFO] - Step 12047360 @ 2046.4 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 12047360, mean_episode_return = None, mean_episode_step = 1838.9, total_loss = 76.389, pg_loss = 53.49, baseline_loss = 28.51, entropy_loss = -5.6099, learner_queue_size = 32, train_seconds = 6714.9, _tick = 2250, _time = 1.6546e+09)
[2022-06-07 12:15:46,918][root][INFO] - Step 12055040 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 12055040, mean_episode_return = None, mean_episode_step = 1434.4, total_loss = 124.38, pg_loss = 82.836, baseline_loss = 47.213, entropy_loss = -5.6699, learner_queue_size = 32, train_seconds = 6719.9, _tick = 2251, _time = 1.6546e+09)
[2022-06-07 12:15:51,922][root][INFO] - Step 12062720 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12062720, mean_episode_return = None, mean_episode_step = 1521.8, total_loss = 422.48, pg_loss = 332.43, baseline_loss = 95.694, entropy_loss = -5.6497, learner_queue_size = 32, train_seconds = 6724.9, _tick = 2251, _time = 1.6546e+09)
[2022-06-07 12:15:56,928][root][INFO] - Step 12072960 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12072960, mean_episode_return = None, mean_episode_step = 2121.5, total_loss = -118.73, pg_loss = -115.53, baseline_loss = 2.3266, entropy_loss = -5.5289, learner_queue_size = 32, train_seconds = 6729.9, _tick = 2253, _time = 1.6546e+09)
[2022-06-07 12:16:01,933][root][INFO] - Step 12083200 @ 2045.9 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12083200, mean_episode_return = 41.12, mean_episode_step = 1932.6, total_loss = 22.07, pg_loss = -2.4454, baseline_loss = 30.042, entropy_loss = -5.5272, learner_queue_size = 32, train_seconds = 6734.9, _tick = 2255, _time = 1.6546e+09)
[2022-06-07 12:16:06,938][root][INFO] - Step 12090880 @ 1534.6 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 12090880, mean_episode_return = 29.802, mean_episode_step = 1664.2, total_loss = 135.2, pg_loss = 76.371, baseline_loss = 64.237, entropy_loss = -5.4067, learner_queue_size = 32, train_seconds = 6739.9, _tick = 2257, _time = 1.6546e+09)
[2022-06-07 12:16:11,942][root][INFO] - Step 12101120 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12101120, mean_episode_return = None, mean_episode_step = 1670.8, total_loss = 242.48, pg_loss = 164.65, baseline_loss = 83.142, entropy_loss = -5.3078, learner_queue_size = 32, train_seconds = 6744.9, _tick = 2257, _time = 1.6546e+09)
[2022-06-07 12:16:16,946][root][INFO] - Step 12108800 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 12108800, mean_episode_return = None, mean_episode_step = 1851.7, total_loss = 27.369, pg_loss = 11.558, baseline_loss = 20.968, entropy_loss = -5.1575, learner_queue_size = 32, train_seconds = 6749.9, _tick = 2259, _time = 1.6546e+09)
[2022-06-07 12:16:21,950][root][INFO] - Step 12119040 @ 2046.3 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 12119040, mean_episode_return = 0.62, mean_episode_step = 1627.0, total_loss = 6.5764, pg_loss = -34.363, baseline_loss = 46.01, entropy_loss = -5.0705, learner_queue_size = 32, train_seconds = 6754.9, _tick = 2262, _time = 1.6546e+09)
[2022-06-07 12:16:26,954][root][INFO] - Step 12126720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12126720, mean_episode_return = None, mean_episode_step = 2189.1, total_loss = 173.98, pg_loss = 56.959, baseline_loss = 122.05, entropy_loss = -5.0329, learner_queue_size = 32, train_seconds = 6759.9, _tick = 2264, _time = 1.6546e+09)
[2022-06-07 12:16:31,958][root][INFO] - Step 12136960 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 12136960, mean_episode_return = None, mean_episode_step = 1914.8, total_loss = -134.94, pg_loss = -133.01, baseline_loss = 3.2194, entropy_loss = -5.1507, learner_queue_size = 32, train_seconds = 6765.0, _tick = 2265, _time = 1.6546e+09)
[2022-06-07 12:16:36,962][root][INFO] - Step 12144640 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 12144640, mean_episode_return = 32.0, mean_episode_step = 1782.4, total_loss = -56.207, pg_loss = -92.83, baseline_loss = 41.719, entropy_loss = -5.096, learner_queue_size = 32, train_seconds = 6770.0, _tick = 2268, _time = 1.6546e+09)
[2022-06-07 12:16:41,966][root][INFO] - Step 12154880 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12154880, mean_episode_return = None, mean_episode_step = 1980.1, total_loss = -11.161, pg_loss = -34.474, baseline_loss = 28.547, entropy_loss = -5.2338, learner_queue_size = 32, train_seconds = 6775.0, _tick = 2268, _time = 1.6546e+09)
[2022-06-07 12:16:46,970][root][INFO] - Step 12162560 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12162560, mean_episode_return = None, mean_episode_step = 1914.9, total_loss = -1.7023, pg_loss = -35.556, baseline_loss = 39.01, entropy_loss = -5.1567, learner_queue_size = 32, train_seconds = 6780.0, _tick = 2269, _time = 1.6546e+09)
[2022-06-07 12:16:51,974][root][INFO] - Step 12172800 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 12172800, mean_episode_return = 46.169, mean_episode_step = 2008.8, total_loss = -181.66, pg_loss = -186.86, baseline_loss = 10.282, entropy_loss = -5.0736, learner_queue_size = 32, train_seconds = 6785.0, _tick = 2272, _time = 1.6546e+09)
[2022-06-07 12:16:56,978][root][INFO] - Step 12180480 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12180480, mean_episode_return = None, mean_episode_step = 2371.7, total_loss = -126.46, pg_loss = -125.57, baseline_loss = 4.1124, entropy_loss = -5.0052, learner_queue_size = 32, train_seconds = 6790.0, _tick = 2272, _time = 1.6546e+09)
[2022-06-07 12:17:01,982][root][INFO] - Step 12190720 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 12190720, mean_episode_return = 92.365, mean_episode_step = 1757.1, total_loss = 59.432, pg_loss = 12.958, baseline_loss = 51.662, entropy_loss = -5.1875, learner_queue_size = 32, train_seconds = 6795.0, _tick = 2274, _time = 1.6546e+09)
[2022-06-07 12:17:06,986][root][INFO] - Step 12198400 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 12198400, mean_episode_return = 40.523, mean_episode_step = 2342.8, total_loss = -83.118, pg_loss = -103.35, baseline_loss = 25.419, entropy_loss = -5.1921, learner_queue_size = 32, train_seconds = 6800.0, _tick = 2276, _time = 1.6546e+09)
[2022-06-07 12:17:11,990][root][INFO] - Step 12208640 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12208640, mean_episode_return = 2.9797, mean_episode_step = 2334.2, total_loss = 100.08, pg_loss = 50.085, baseline_loss = 55.206, entropy_loss = -5.2106, learner_queue_size = 32, train_seconds = 6805.0, _tick = 2278, _time = 1.6546e+09)
[2022-06-07 12:17:16,994][root][INFO] - Step 12216320 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12216320, mean_episode_return = None, mean_episode_step = 2089.3, total_loss = 165.4, pg_loss = 110.29, baseline_loss = 60.297, entropy_loss = -5.1861, learner_queue_size = 32, train_seconds = 6810.0, _tick = 2278, _time = 1.6546e+09)
[2022-06-07 12:17:21,998][root][INFO] - Step 12226560 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 12226560, mean_episode_return = 18.976, mean_episode_step = 2532.6, total_loss = -86.085, pg_loss = -115.24, baseline_loss = 34.252, entropy_loss = -5.0993, learner_queue_size = 32, train_seconds = 6815.0, _tick = 2281, _time = 1.6546e+09)
[2022-06-07 12:17:27,002][root][INFO] - Step 12234240 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 12234240, mean_episode_return = 78.649, mean_episode_step = 2459.5, total_loss = -147.13, pg_loss = -162.56, baseline_loss = 20.623, entropy_loss = -5.19, learner_queue_size = 32, train_seconds = 6820.0, _tick = 2283, _time = 1.6546e+09)
[2022-06-07 12:17:32,006][root][INFO] - Step 12244480 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 12244480, mean_episode_return = None, mean_episode_step = 2005.1, total_loss = 300.08, pg_loss = 210.62, baseline_loss = 94.723, entropy_loss = -5.2659, learner_queue_size = 32, train_seconds = 6825.0, _tick = 2285, _time = 1.6546e+09)
[2022-06-07 12:17:37,010][root][INFO] - Step 12252160 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12252160, mean_episode_return = 49.171, mean_episode_step = 1990.0, total_loss = 557.07, pg_loss = 159.4, baseline_loss = 402.94, entropy_loss = -5.2736, learner_queue_size = 32, train_seconds = 6830.0, _tick = 2287, _time = 1.6546e+09)
[2022-06-07 12:17:42,014][root][INFO] - Step 12262400 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 12262400, mean_episode_return = None, mean_episode_step = 2297.1, total_loss = -32.919, pg_loss = -50.082, baseline_loss = 22.479, entropy_loss = -5.3158, learner_queue_size = 32, train_seconds = 6835.0, _tick = 2290, _time = 1.6546e+09)
[2022-06-07 12:17:47,020][root][INFO] - Step 12270080 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12270080, mean_episode_return = None, mean_episode_step = 1887.3, total_loss = 131.54, pg_loss = 79.198, baseline_loss = 57.598, entropy_loss = -5.2555, learner_queue_size = 32, train_seconds = 6840.0, _tick = 2292, _time = 1.6546e+09)
[2022-06-07 12:17:52,026][root][INFO] - Step 12280320 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12280320, mean_episode_return = None, mean_episode_step = 2299.1, total_loss = -124.3, pg_loss = -121.14, baseline_loss = 2.3697, entropy_loss = -5.53, learner_queue_size = 32, train_seconds = 6845.0, _tick = 2293, _time = 1.6546e+09)
[2022-06-07 12:17:57,032][root][INFO] - Step 12288000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12288000, mean_episode_return = 3.2797, mean_episode_step = 2114.2, total_loss = 167.38, pg_loss = 90.249, baseline_loss = 82.645, entropy_loss = -5.5143, learner_queue_size = 32, train_seconds = 6850.0, _tick = 2296, _time = 1.6546e+09)
[2022-06-07 12:18:02,038][root][INFO] - Step 12298240 @ 2045.6 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 12298240, mean_episode_return = 36.224, mean_episode_step = 1886.4, total_loss = -42.758, pg_loss = -93.156, baseline_loss = 55.857, entropy_loss = -5.4588, learner_queue_size = 32, train_seconds = 6855.0, _tick = 2298, _time = 1.6546e+09)
[2022-06-07 12:18:07,042][root][INFO] - Step 12305920 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 12305920, mean_episode_return = 21.782, mean_episode_step = 1854.2, total_loss = 176.24, pg_loss = 90.605, baseline_loss = 91.061, entropy_loss = -5.4215, learner_queue_size = 32, train_seconds = 6860.0, _tick = 2301, _time = 1.6546e+09)
[2022-06-07 12:18:12,046][root][INFO] - Step 12316160 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12316160, mean_episode_return = None, mean_episode_step = 2022.0, total_loss = -25.127, pg_loss = -47.862, baseline_loss = 28.16, entropy_loss = -5.4256, learner_queue_size = 32, train_seconds = 6865.0, _tick = 2302, _time = 1.6546e+09)
[2022-06-07 12:18:17,052][root][INFO] - Step 12326400 @ 2045.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 12326400, mean_episode_return = None, mean_episode_step = 2318.4, total_loss = 13.009, pg_loss = -4.8677, baseline_loss = 23.258, entropy_loss = -5.3818, learner_queue_size = 32, train_seconds = 6870.0, _tick = 2304, _time = 1.6546e+09)
[2022-06-07 12:18:22,054][root][INFO] - Step 12334080 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12334080, mean_episode_return = None, mean_episode_step = 2257.9, total_loss = 232.0, pg_loss = 169.28, baseline_loss = 68.061, entropy_loss = -5.3431, learner_queue_size = 32, train_seconds = 6875.0, _tick = 2305, _time = 1.6546e+09)
[2022-06-07 12:18:27,060][root][INFO] - Step 12344320 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 12344320, mean_episode_return = 72.645, mean_episode_step = 1777.2, total_loss = -133.65, pg_loss = -168.21, baseline_loss = 39.813, entropy_loss = -5.2491, learner_queue_size = 32, train_seconds = 6880.1, _tick = 2308, _time = 1.6546e+09)
[2022-06-07 12:18:32,066][root][INFO] - Step 12352000 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 12352000, mean_episode_return = -3.97, mean_episode_step = 2257.2, total_loss = -15.234, pg_loss = -24.871, baseline_loss = 14.905, entropy_loss = -5.2685, learner_queue_size = 32, train_seconds = 6885.1, _tick = 2311, _time = 1.6546e+09)
[2022-06-07 12:18:37,070][root][INFO] - Step 12362240 @ 2046.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 12362240, mean_episode_return = None, mean_episode_step = 1690.4, total_loss = 15.864, pg_loss = -7.2225, baseline_loss = 28.441, entropy_loss = -5.3545, learner_queue_size = 32, train_seconds = 6890.1, _tick = 2313, _time = 1.6546e+09)
[2022-06-07 12:18:42,074][root][INFO] - Step 12369920 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 12369920, mean_episode_return = None, mean_episode_step = 1874.2, total_loss = -4.5891, pg_loss = -31.388, baseline_loss = 32.23, entropy_loss = -5.4313, learner_queue_size = 32, train_seconds = 6895.1, _tick = 2314, _time = 1.6546e+09)
[2022-06-07 12:18:47,078][root][INFO] - Step 12380160 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 12380160, mean_episode_return = -0.96043, mean_episode_step = 1992.6, total_loss = 130.56, pg_loss = 64.489, baseline_loss = 71.498, entropy_loss = -5.4242, learner_queue_size = 32, train_seconds = 6900.1, _tick = 2316, _time = 1.6546e+09)
[2022-06-07 12:18:52,084][root][INFO] - Step 12387840 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 12387840, mean_episode_return = 73.334, mean_episode_step = 2430.2, total_loss = 43.324, pg_loss = -15.056, baseline_loss = 63.864, entropy_loss = -5.4839, learner_queue_size = 32, train_seconds = 6905.1, _tick = 2318, _time = 1.6546e+09)
[2022-06-07 12:18:57,090][root][INFO] - Step 12398080 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12398080, mean_episode_return = -4.4404, mean_episode_step = 1752.9, total_loss = 189.6, pg_loss = 122.13, baseline_loss = 72.785, entropy_loss = -5.3219, learner_queue_size = 32, train_seconds = 6910.1, _tick = 2322, _time = 1.6546e+09)
[2022-06-07 12:19:02,094][root][INFO] - Step 12405760 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12405760, mean_episode_return = 112.11, mean_episode_step = 2111.1, total_loss = 227.38, pg_loss = 135.12, baseline_loss = 97.585, entropy_loss = -5.331, learner_queue_size = 32, train_seconds = 6915.1, _tick = 2324, _time = 1.6546e+09)
[2022-06-07 12:19:07,098][root][INFO] - Step 12416000 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 12416000, mean_episode_return = 64.12, mean_episode_step = 2385.5, total_loss = -24.168, pg_loss = -91.066, baseline_loss = 72.365, entropy_loss = -5.4672, learner_queue_size = 32, train_seconds = 6920.1, _tick = 2327, _time = 1.6546e+09)
[2022-06-07 12:19:12,113][root][INFO] - Step 12423680 @ 1531.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12423680, mean_episode_return = 101.48, mean_episode_step = 1717.2, total_loss = 273.47, pg_loss = 143.52, baseline_loss = 135.4, entropy_loss = -5.4557, learner_queue_size = 32, train_seconds = 6925.1, _tick = 2330, _time = 1.6546e+09)
[2022-06-07 12:19:17,118][root][INFO] - Step 12431360 @ 1534.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12431360, mean_episode_return = None, mean_episode_step = 2023.5, total_loss = -26.694, pg_loss = -41.744, baseline_loss = 20.579, entropy_loss = -5.5289, learner_queue_size = 32, train_seconds = 6930.1, _tick = 2331, _time = 1.6546e+09)
[2022-06-07 12:19:22,122][root][INFO] - Step 12441600 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 12441600, mean_episode_return = 132.29, mean_episode_step = 1631.7, total_loss = 41.233, pg_loss = -17.01, baseline_loss = 63.551, entropy_loss = -5.3079, learner_queue_size = 32, train_seconds = 6935.1, _tick = 2335, _time = 1.6546e+09)
[2022-06-07 12:19:27,126][root][INFO] - Step 12451840 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12451840, mean_episode_return = 28.275, mean_episode_step = 1869.4, total_loss = 144.41, pg_loss = 59.534, baseline_loss = 90.248, entropy_loss = -5.3704, learner_queue_size = 32, train_seconds = 6940.1, _tick = 2338, _time = 1.6546e+09)
[2022-06-07 12:19:32,130][root][INFO] - Step 12459520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12459520, mean_episode_return = 33.562, mean_episode_step = 1699.4, total_loss = 94.941, pg_loss = -39.053, baseline_loss = 139.27, entropy_loss = -5.2777, learner_queue_size = 32, train_seconds = 6945.1, _tick = 2340, _time = 1.6546e+09)
[2022-06-07 12:19:37,134][root][INFO] - Step 12469760 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 12469760, mean_episode_return = None, mean_episode_step = 1630.3, total_loss = 220.39, pg_loss = 136.22, baseline_loss = 89.24, entropy_loss = -5.0648, learner_queue_size = 32, train_seconds = 6950.1, _tick = 2342, _time = 1.6546e+09)
[2022-06-07 12:19:42,138][root][INFO] - Step 12477440 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 12477440, mean_episode_return = None, mean_episode_step = 1872.1, total_loss = 201.19, pg_loss = 133.61, baseline_loss = 72.488, entropy_loss = -4.9147, learner_queue_size = 32, train_seconds = 6955.1, _tick = 2343, _time = 1.6546e+09)
[2022-06-07 12:19:47,144][root][INFO] - Step 12487680 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12487680, mean_episode_return = 37.638, mean_episode_step = 1722.3, total_loss = -90.861, pg_loss = -121.09, baseline_loss = 35.414, entropy_loss = -5.1819, learner_queue_size = 32, train_seconds = 6960.1, _tick = 2347, _time = 1.6546e+09)
[2022-06-07 12:19:52,150][root][INFO] - Step 12495360 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 12495360, mean_episode_return = -19.48, mean_episode_step = 1666.3, total_loss = 10.832, pg_loss = -7.4698, baseline_loss = 23.564, entropy_loss = -5.2624, learner_queue_size = 32, train_seconds = 6965.1, _tick = 2349, _time = 1.6546e+09)
[2022-06-07 12:19:57,154][root][INFO] - Step 12505600 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 12505600, mean_episode_return = 32.62, mean_episode_step = 1728.0, total_loss = -20.561, pg_loss = -54.353, baseline_loss = 39.014, entropy_loss = -5.2228, learner_queue_size = 32, train_seconds = 6970.1, _tick = 2351, _time = 1.6546e+09)
[2022-06-07 12:20:02,160][root][INFO] - Step 12513280 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12513280, mean_episode_return = None, mean_episode_step = 1844.4, total_loss = 331.44, pg_loss = 240.84, baseline_loss = 95.796, entropy_loss = -5.197, learner_queue_size = 32, train_seconds = 6975.2, _tick = 2351, _time = 1.6546e+09)
[2022-06-07 12:20:07,166][root][INFO] - Step 12523520 @ 2045.6 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 12523520, mean_episode_return = None, mean_episode_step = 1720.9, total_loss = 359.22, pg_loss = 231.6, baseline_loss = 132.81, entropy_loss = -5.1964, learner_queue_size = 32, train_seconds = 6980.2, _tick = 2353, _time = 1.6546e+09)
[2022-06-07 12:20:12,170][root][INFO] - Step 12531200 @ 1534.7 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 12531200, mean_episode_return = 67.993, mean_episode_step = 1660.7, total_loss = 60.42, pg_loss = 15.534, baseline_loss = 49.951, entropy_loss = -5.0657, learner_queue_size = 32, train_seconds = 6985.2, _tick = 2355, _time = 1.6546e+09)
[2022-06-07 12:20:17,176][root][INFO] - Step 12541440 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12541440, mean_episode_return = None, mean_episode_step = 2031.6, total_loss = 181.93, pg_loss = 92.576, baseline_loss = 94.282, entropy_loss = -4.9274, learner_queue_size = 32, train_seconds = 6990.2, _tick = 2357, _time = 1.6546e+09)
[2022-06-07 12:20:22,182][root][INFO] - Step 12549120 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12549120, mean_episode_return = None, mean_episode_step = 2148.2, total_loss = -119.11, pg_loss = -116.59, baseline_loss = 2.2598, entropy_loss = -4.7824, learner_queue_size = 32, train_seconds = 6995.2, _tick = 2358, _time = 1.6546e+09)
[2022-06-07 12:20:27,186][root][INFO] - Step 12556800 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12556800, mean_episode_return = None, mean_episode_step = 1796.7, total_loss = -99.057, pg_loss = -95.842, baseline_loss = 1.4405, entropy_loss = -4.6561, learner_queue_size = 32, train_seconds = 7000.2, _tick = 2358, _time = 1.6546e+09)
[2022-06-07 12:20:32,190][root][INFO] - Step 12567040 @ 2046.3 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 12567040, mean_episode_return = None, mean_episode_step = 1492.2, total_loss = 7.25, pg_loss = -14.815, baseline_loss = 26.885, entropy_loss = -4.8199, learner_queue_size = 32, train_seconds = 7005.2, _tick = 2360, _time = 1.6546e+09)
[2022-06-07 12:20:37,196][root][INFO] - Step 12577280 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12577280, mean_episode_return = 51.811, mean_episode_step = 1882.2, total_loss = -115.44, pg_loss = -118.36, baseline_loss = 8.0185, entropy_loss = -5.1008, learner_queue_size = 32, train_seconds = 7010.2, _tick = 2361, _time = 1.6546e+09)
[2022-06-07 12:20:42,206][root][INFO] - Step 12584960 @ 1532.9 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 12584960, mean_episode_return = None, mean_episode_step = 2076.3, total_loss = -83.671, pg_loss = -89.13, baseline_loss = 10.591, entropy_loss = -5.1317, learner_queue_size = 32, train_seconds = 7015.2, _tick = 2361, _time = 1.6546e+09)
[2022-06-07 12:20:47,212][root][INFO] - Step 12592640 @ 1534.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 12592640, mean_episode_return = None, mean_episode_step = 1867.9, total_loss = 147.3, pg_loss = 84.005, baseline_loss = 68.448, entropy_loss = -5.1536, learner_queue_size = 32, train_seconds = 7020.2, _tick = 2362, _time = 1.6546e+09)
[2022-06-07 12:20:52,218][root][INFO] - Step 12602880 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12602880, mean_episode_return = None, mean_episode_step = 2345.7, total_loss = 142.5, pg_loss = 89.906, baseline_loss = 57.628, entropy_loss = -5.032, learner_queue_size = 32, train_seconds = 7025.2, _tick = 2362, _time = 1.6546e+09)
[2022-06-07 12:20:57,224][root][INFO] - Step 12610560 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12610560, mean_episode_return = None, mean_episode_step = 1728.6, total_loss = 125.33, pg_loss = 88.794, baseline_loss = 41.598, entropy_loss = -5.0595, learner_queue_size = 32, train_seconds = 7030.2, _tick = 2362, _time = 1.6546e+09)
[2022-06-07 12:21:02,230][root][INFO] - Step 12620800 @ 2045.6 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 12620800, mean_episode_return = 97.286, mean_episode_step = 2063.5, total_loss = -160.12, pg_loss = -165.79, baseline_loss = 10.642, entropy_loss = -4.9725, learner_queue_size = 32, train_seconds = 7035.2, _tick = 2364, _time = 1.6546e+09)
[2022-06-07 12:21:07,234][root][INFO] - Step 12631040 @ 2046.3 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 12631040, mean_episode_return = 48.31, mean_episode_step = 1502.9, total_loss = -55.613, pg_loss = -69.515, baseline_loss = 18.957, entropy_loss = -5.0552, learner_queue_size = 32, train_seconds = 7040.2, _tick = 2366, _time = 1.6546e+09)
[2022-06-07 12:21:12,240][root][INFO] - Step 12638720 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 12638720, mean_episode_return = None, mean_episode_step = 1984.8, total_loss = -115.89, pg_loss = -114.36, baseline_loss = 3.5436, entropy_loss = -5.0721, learner_queue_size = 32, train_seconds = 7045.2, _tick = 2367, _time = 1.6546e+09)
[2022-06-07 12:21:17,246][root][INFO] - Step 12648960 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 12648960, mean_episode_return = -6.2904, mean_episode_step = 2321.0, total_loss = 624.06, pg_loss = 435.04, baseline_loss = 194.11, entropy_loss = -5.0941, learner_queue_size = 32, train_seconds = 7050.2, _tick = 2370, _time = 1.6546e+09)
[2022-06-07 12:21:22,248][root][INFO] - Step 12656640 @ 1535.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12656640, mean_episode_return = 20.06, mean_episode_step = 2116.7, total_loss = -49.474, pg_loss = -78.297, baseline_loss = 33.856, entropy_loss = -5.0329, learner_queue_size = 32, train_seconds = 7055.2, _tick = 2373, _time = 1.6546e+09)
[2022-06-07 12:21:27,250][root][INFO] - Step 12666880 @ 2047.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 12666880, mean_episode_return = None, mean_episode_step = 1573.0, total_loss = 210.14, pg_loss = 142.25, baseline_loss = 72.895, entropy_loss = -5.0086, learner_queue_size = 32, train_seconds = 7060.2, _tick = 2376, _time = 1.6546e+09)
[2022-06-07 12:21:32,256][root][INFO] - Step 12674560 @ 1534.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 12674560, mean_episode_return = None, mean_episode_step = 2230.3, total_loss = -104.58, pg_loss = -103.96, baseline_loss = 4.2588, entropy_loss = -4.8843, learner_queue_size = 32, train_seconds = 7065.3, _tick = 2378, _time = 1.6546e+09)
[2022-06-07 12:21:37,262][root][INFO] - Step 12684800 @ 2045.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 12684800, mean_episode_return = None, mean_episode_step = 1981.0, total_loss = 58.458, pg_loss = 27.15, baseline_loss = 36.148, entropy_loss = -4.8399, learner_queue_size = 32, train_seconds = 7070.3, _tick = 2380, _time = 1.6546e+09)
[2022-06-07 12:21:42,266][root][INFO] - Step 12692480 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12692480, mean_episode_return = None, mean_episode_step = 1865.7, total_loss = 277.26, pg_loss = 182.75, baseline_loss = 99.353, entropy_loss = -4.844, learner_queue_size = 32, train_seconds = 7075.3, _tick = 2381, _time = 1.6546e+09)
[2022-06-07 12:21:47,272][root][INFO] - Step 12702720 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12702720, mean_episode_return = None, mean_episode_step = 2125.7, total_loss = 118.14, pg_loss = 82.657, baseline_loss = 40.297, entropy_loss = -4.8178, learner_queue_size = 32, train_seconds = 7080.3, _tick = 2382, _time = 1.6546e+09)
[2022-06-07 12:21:52,278][root][INFO] - Step 12710400 @ 1534.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 12710400, mean_episode_return = 7.6793, mean_episode_step = 2045.3, total_loss = -139.38, pg_loss = -141.36, baseline_loss = 6.8037, entropy_loss = -4.815, learner_queue_size = 32, train_seconds = 7085.3, _tick = 2383, _time = 1.6546e+09)
[2022-06-07 12:21:57,284][root][INFO] - Step 12720640 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 12720640, mean_episode_return = None, mean_episode_step = 2139.2, total_loss = 224.39, pg_loss = 128.75, baseline_loss = 100.31, entropy_loss = -4.6726, learner_queue_size = 32, train_seconds = 7090.3, _tick = 2385, _time = 1.6546e+09)
[2022-06-07 12:22:02,290][root][INFO] - Step 12730880 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12730880, mean_episode_return = None, mean_episode_step = 1475.0, total_loss = 69.375, pg_loss = 31.193, baseline_loss = 42.722, entropy_loss = -4.5398, learner_queue_size = 32, train_seconds = 7095.3, _tick = 2385, _time = 1.6546e+09)
[2022-06-07 12:22:07,294][root][INFO] - Step 12738560 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 12738560, mean_episode_return = 46.873, mean_episode_step = 2729.4, total_loss = 1.8789, pg_loss = -41.894, baseline_loss = 48.461, entropy_loss = -4.6879, learner_queue_size = 32, train_seconds = 7100.3, _tick = 2388, _time = 1.6546e+09)
[2022-06-07 12:22:12,300][root][INFO] - Step 12748800 @ 2045.5 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 12748800, mean_episode_return = None, mean_episode_step = 2363.1, total_loss = 32.742, pg_loss = 4.4498, baseline_loss = 32.992, entropy_loss = -4.6998, learner_queue_size = 32, train_seconds = 7105.3, _tick = 2389, _time = 1.6546e+09)
[2022-06-07 12:22:17,306][root][INFO] - Step 12756480 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 12756480, mean_episode_return = 3.9997, mean_episode_step = 2057.7, total_loss = -196.24, pg_loss = -203.93, baseline_loss = 12.447, entropy_loss = -4.7544, learner_queue_size = 32, train_seconds = 7110.3, _tick = 2390, _time = 1.6546e+09)
[2022-06-07 12:22:22,310][root][INFO] - Step 12764160 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12764160, mean_episode_return = None, mean_episode_step = 1985.3, total_loss = 418.72, pg_loss = 310.54, baseline_loss = 113.01, entropy_loss = -4.8238, learner_queue_size = 32, train_seconds = 7115.3, _tick = 2390, _time = 1.6546e+09)
[2022-06-07 12:22:27,314][root][INFO] - Step 12774400 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 12774400, mean_episode_return = None, mean_episode_step = 1912.3, total_loss = -127.94, pg_loss = -126.02, baseline_loss = 2.9799, entropy_loss = -4.9013, learner_queue_size = 32, train_seconds = 7120.3, _tick = 2391, _time = 1.6546e+09)
[2022-06-07 12:22:32,318][root][INFO] - Step 12782080 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 12782080, mean_episode_return = None, mean_episode_step = 2291.5, total_loss = 206.89, pg_loss = 162.93, baseline_loss = 48.933, entropy_loss = -4.9698, learner_queue_size = 32, train_seconds = 7125.3, _tick = 2393, _time = 1.6546e+09)
[2022-06-07 12:22:37,322][root][INFO] - Step 12792320 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12792320, mean_episode_return = None, mean_episode_step = 2772.8, total_loss = 825.19, pg_loss = 468.6, baseline_loss = 361.57, entropy_loss = -4.9778, learner_queue_size = 32, train_seconds = 7130.3, _tick = 2396, _time = 1.6546e+09)
[2022-06-07 12:22:42,326][root][INFO] - Step 12800000 @ 1534.7 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 12800000, mean_episode_return = None, mean_episode_step = 1924.3, total_loss = -23.343, pg_loss = -48.476, baseline_loss = 30.16, entropy_loss = -5.0269, learner_queue_size = 32, train_seconds = 7135.3, _tick = 2397, _time = 1.6546e+09)
[2022-06-07 12:22:47,332][root][INFO] - Step 12807680 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12807680, mean_episode_return = None, mean_episode_step = 2384.8, total_loss = 12.628, pg_loss = -6.9275, baseline_loss = 24.473, entropy_loss = -4.9171, learner_queue_size = 32, train_seconds = 7140.3, _tick = 2398, _time = 1.6546e+09)
[2022-06-07 12:22:52,334][root][INFO] - Step 12817920 @ 2047.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12817920, mean_episode_return = None, mean_episode_step = 1645.1, total_loss = 189.72, pg_loss = 134.05, baseline_loss = 60.709, entropy_loss = -5.0353, learner_queue_size = 32, train_seconds = 7145.3, _tick = 2400, _time = 1.6546e+09)
[2022-06-07 12:22:57,338][root][INFO] - Step 12828160 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 12828160, mean_episode_return = None, mean_episode_step = 2237.8, total_loss = 114.69, pg_loss = 51.291, baseline_loss = 68.432, entropy_loss = -5.0359, learner_queue_size = 32, train_seconds = 7150.3, _tick = 2402, _time = 1.6546e+09)
[2022-06-07 12:23:02,357][root][INFO] - Step 12835840 @ 1530.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12835840, mean_episode_return = None, mean_episode_step = 1945.5, total_loss = -32.436, pg_loss = -37.145, baseline_loss = 9.6505, entropy_loss = -4.9419, learner_queue_size = 32, train_seconds = 7155.4, _tick = 2404, _time = 1.6546e+09)
[2022-06-07 12:23:07,363][root][INFO] - Step 12843520 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12843520, mean_episode_return = 29.271, mean_episode_step = 1832.5, total_loss = 152.89, pg_loss = 118.55, baseline_loss = 39.363, entropy_loss = -5.0246, learner_queue_size = 32, train_seconds = 7160.4, _tick = 2407, _time = 1.6546e+09)
[2022-06-07 12:23:12,369][root][INFO] - Step 12853760 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 12853760, mean_episode_return = -11.037, mean_episode_step = 1498.4, total_loss = -110.36, pg_loss = -115.33, baseline_loss = 10.051, entropy_loss = -5.0792, learner_queue_size = 32, train_seconds = 7165.4, _tick = 2411, _time = 1.6546e+09)
[2022-06-07 12:23:17,374][root][INFO] - Step 12861440 @ 1534.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 12861440, mean_episode_return = 51.692, mean_episode_step = 2560.8, total_loss = 97.701, pg_loss = 63.698, baseline_loss = 39.043, entropy_loss = -5.0401, learner_queue_size = 32, train_seconds = 7170.4, _tick = 2412, _time = 1.6546e+09)
[2022-06-07 12:23:22,378][root][INFO] - Step 12871680 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 12871680, mean_episode_return = None, mean_episode_step = 1832.3, total_loss = 379.2, pg_loss = 266.09, baseline_loss = 118.29, entropy_loss = -5.1875, learner_queue_size = 32, train_seconds = 7175.4, _tick = 2414, _time = 1.6546e+09)
[2022-06-07 12:23:27,382][root][INFO] - Step 12879360 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 12879360, mean_episode_return = 13.96, mean_episode_step = 2268.6, total_loss = 214.65, pg_loss = 144.73, baseline_loss = 75.178, entropy_loss = -5.267, learner_queue_size = 32, train_seconds = 7180.4, _tick = 2416, _time = 1.6546e+09)
[2022-06-07 12:23:32,386][root][INFO] - Step 12889600 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12889600, mean_episode_return = None, mean_episode_step = 1943.3, total_loss = 1088.3, pg_loss = 812.67, baseline_loss = 280.86, entropy_loss = -5.1834, learner_queue_size = 32, train_seconds = 7185.4, _tick = 2416, _time = 1.6546e+09)
[2022-06-07 12:23:37,390][root][INFO] - Step 12897280 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 12897280, mean_episode_return = 61.041, mean_episode_step = 2168.9, total_loss = -80.513, pg_loss = -95.234, baseline_loss = 20.132, entropy_loss = -5.4117, learner_queue_size = 32, train_seconds = 7190.4, _tick = 2417, _time = 1.6546e+09)
[2022-06-07 12:23:42,396][root][INFO] - Step 12907520 @ 2045.5 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 12907520, mean_episode_return = 50.471, mean_episode_step = 2243.2, total_loss = -1.2805, pg_loss = -46.439, baseline_loss = 50.498, entropy_loss = -5.3397, learner_queue_size = 32, train_seconds = 7195.4, _tick = 2418, _time = 1.6546e+09)
[2022-06-07 12:23:47,402][root][INFO] - Step 12915200 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 12915200, mean_episode_return = 19.67, mean_episode_step = 2371.6, total_loss = -37.175, pg_loss = -64.542, baseline_loss = 32.732, entropy_loss = -5.3654, learner_queue_size = 32, train_seconds = 7200.4, _tick = 2421, _time = 1.6546e+09)
[2022-06-07 12:23:52,414][root][INFO] - Step 12925440 @ 2043.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 12925440, mean_episode_return = 67.606, mean_episode_step = 2191.4, total_loss = 410.47, pg_loss = 320.44, baseline_loss = 95.349, entropy_loss = -5.3254, learner_queue_size = 32, train_seconds = 7205.4, _tick = 2423, _time = 1.6546e+09)
[2022-06-07 12:23:57,418][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 12:23:57,603][root][INFO] - Step 12933120 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 12933120, mean_episode_return = 75.849, mean_episode_step = 2383.4, total_loss = -10.028, pg_loss = -51.522, baseline_loss = 46.804, entropy_loss = -5.3109, learner_queue_size = 32, train_seconds = 7210.4, _tick = 2425, _time = 1.6546e+09)
[2022-06-07 12:24:02,606][root][INFO] - Step 12943360 @ 1973.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 12943360, mean_episode_return = None, mean_episode_step = 2635.2, total_loss = 46.835, pg_loss = 3.2977, baseline_loss = 48.915, entropy_loss = -5.3781, learner_queue_size = 32, train_seconds = 7215.6, _tick = 2425, _time = 1.6546e+09)
[2022-06-07 12:24:07,612][root][INFO] - Step 12951040 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 12951040, mean_episode_return = None, mean_episode_step = 2137.5, total_loss = 11.57, pg_loss = -12.521, baseline_loss = 29.378, entropy_loss = -5.2863, learner_queue_size = 32, train_seconds = 7220.6, _tick = 2427, _time = 1.6546e+09)
[2022-06-07 12:24:12,614][root][INFO] - Step 12961280 @ 2047.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 12961280, mean_episode_return = None, mean_episode_step = 2153.8, total_loss = 156.47, pg_loss = 86.13, baseline_loss = 75.714, entropy_loss = -5.3701, learner_queue_size = 32, train_seconds = 7225.6, _tick = 2427, _time = 1.6546e+09)
[2022-06-07 12:24:17,618][root][INFO] - Step 12968960 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 12968960, mean_episode_return = None, mean_episode_step = 2504.7, total_loss = -172.5, pg_loss = -168.31, baseline_loss = 1.1844, entropy_loss = -5.3799, learner_queue_size = 32, train_seconds = 7230.6, _tick = 2428, _time = 1.6546e+09)
[2022-06-07 12:24:22,624][root][INFO] - Step 12979200 @ 2045.5 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 12979200, mean_episode_return = None, mean_episode_step = 2118.8, total_loss = 227.69, pg_loss = 172.2, baseline_loss = 60.662, entropy_loss = -5.1731, learner_queue_size = 32, train_seconds = 7235.6, _tick = 2429, _time = 1.6546e+09)
[2022-06-07 12:24:27,630][root][INFO] - Step 12986880 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 12986880, mean_episode_return = 8.1902, mean_episode_step = 1806.6, total_loss = 255.06, pg_loss = 175.96, baseline_loss = 84.286, entropy_loss = -5.1914, learner_queue_size = 32, train_seconds = 7240.6, _tick = 2432, _time = 1.6546e+09)
[2022-06-07 12:24:32,634][root][INFO] - Step 12997120 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 12997120, mean_episode_return = None, mean_episode_step = 2513.1, total_loss = 196.75, pg_loss = 124.38, baseline_loss = 77.723, entropy_loss = -5.3564, learner_queue_size = 32, train_seconds = 7245.6, _tick = 2433, _time = 1.6546e+09)
[2022-06-07 12:24:37,638][root][INFO] - Step 13004800 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13004800, mean_episode_return = None, mean_episode_step = 3172.5, total_loss = -83.777, pg_loss = -87.144, baseline_loss = 8.6886, entropy_loss = -5.3218, learner_queue_size = 32, train_seconds = 7250.6, _tick = 2434, _time = 1.6546e+09)
[2022-06-07 12:24:42,641][root][INFO] - Step 13015040 @ 2046.7 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 13015040, mean_episode_return = None, mean_episode_step = 1875.5, total_loss = -51.634, pg_loss = -66.056, baseline_loss = 19.712, entropy_loss = -5.2909, learner_queue_size = 32, train_seconds = 7255.6, _tick = 2435, _time = 1.6546e+09)
[2022-06-07 12:24:47,646][root][INFO] - Step 13022720 @ 1534.5 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 13022720, mean_episode_return = None, mean_episode_step = 2222.6, total_loss = 30.895, pg_loss = 2.888, baseline_loss = 33.299, entropy_loss = -5.2919, learner_queue_size = 32, train_seconds = 7260.6, _tick = 2436, _time = 1.6546e+09)
[2022-06-07 12:24:52,650][root][INFO] - Step 13030400 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13030400, mean_episode_return = 63.565, mean_episode_step = 2008.6, total_loss = -169.32, pg_loss = -173.51, baseline_loss = 9.4371, entropy_loss = -5.2521, learner_queue_size = 32, train_seconds = 7265.6, _tick = 2439, _time = 1.6546e+09)
[2022-06-07 12:24:57,654][root][INFO] - Step 13040640 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 13040640, mean_episode_return = 31.676, mean_episode_step = 2701.7, total_loss = -58.205, pg_loss = -94.258, baseline_loss = 41.259, entropy_loss = -5.2055, learner_queue_size = 32, train_seconds = 7270.6, _tick = 2441, _time = 1.6546e+09)
[2022-06-07 12:25:02,660][root][INFO] - Step 13048320 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13048320, mean_episode_return = 57.591, mean_episode_step = 2251.1, total_loss = -97.479, pg_loss = -125.45, baseline_loss = 33.228, entropy_loss = -5.2558, learner_queue_size = 32, train_seconds = 7275.7, _tick = 2443, _time = 1.6546e+09)
[2022-06-07 12:25:07,666][root][INFO] - Step 13058560 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13058560, mean_episode_return = 42.852, mean_episode_step = 2020.7, total_loss = -25.088, pg_loss = -58.168, baseline_loss = 38.328, entropy_loss = -5.2483, learner_queue_size = 32, train_seconds = 7280.7, _tick = 2445, _time = 1.6546e+09)
[2022-06-07 12:25:12,672][root][INFO] - Step 13066240 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 13066240, mean_episode_return = 25.27, mean_episode_step = 2673.3, total_loss = 145.84, pg_loss = 45.869, baseline_loss = 105.24, entropy_loss = -5.2668, learner_queue_size = 32, train_seconds = 7285.7, _tick = 2447, _time = 1.6546e+09)
[2022-06-07 12:25:17,678][root][INFO] - Step 13076480 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 13076480, mean_episode_return = 38.546, mean_episode_step = 2745.5, total_loss = 113.87, pg_loss = 38.992, baseline_loss = 80.175, entropy_loss = -5.3003, learner_queue_size = 32, train_seconds = 7290.7, _tick = 2450, _time = 1.6546e+09)
[2022-06-07 12:25:22,682][root][INFO] - Step 13084160 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 13084160, mean_episode_return = 72.93, mean_episode_step = 1831.5, total_loss = -19.672, pg_loss = -57.329, baseline_loss = 42.93, entropy_loss = -5.2736, learner_queue_size = 32, train_seconds = 7295.7, _tick = 2453, _time = 1.6546e+09)
[2022-06-07 12:25:27,688][root][INFO] - Step 13094400 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13094400, mean_episode_return = -4.9105, mean_episode_step = 2855.7, total_loss = 357.15, pg_loss = 229.29, baseline_loss = 133.17, entropy_loss = -5.317, learner_queue_size = 32, train_seconds = 7300.7, _tick = 2455, _time = 1.6546e+09)
[2022-06-07 12:25:32,694][root][INFO] - Step 13102080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13102080, mean_episode_return = 84.819, mean_episode_step = 1874.3, total_loss = 143.31, pg_loss = 80.812, baseline_loss = 67.681, entropy_loss = -5.1862, learner_queue_size = 32, train_seconds = 7305.7, _tick = 2458, _time = 1.6546e+09)
[2022-06-07 12:25:37,700][root][INFO] - Step 13112320 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13112320, mean_episode_return = None, mean_episode_step = 2100.6, total_loss = 69.454, pg_loss = 35.614, baseline_loss = 39.012, entropy_loss = -5.1717, learner_queue_size = 32, train_seconds = 7310.7, _tick = 2461, _time = 1.6546e+09)
[2022-06-07 12:25:42,706][root][INFO] - Step 13120000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13120000, mean_episode_return = 96.909, mean_episode_step = 1662.1, total_loss = 192.63, pg_loss = 110.17, baseline_loss = 87.65, entropy_loss = -5.1826, learner_queue_size = 32, train_seconds = 7315.7, _tick = 2463, _time = 1.6546e+09)
[2022-06-07 12:25:47,712][root][INFO] - Step 13130240 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 13130240, mean_episode_return = 44.61, mean_episode_step = 2036.7, total_loss = 22.505, pg_loss = -23.699, baseline_loss = 51.385, entropy_loss = -5.1812, learner_queue_size = 32, train_seconds = 7320.7, _tick = 2465, _time = 1.6546e+09)
[2022-06-07 12:25:52,718][root][INFO] - Step 13137920 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13137920, mean_episode_return = 128.93, mean_episode_step = 2129.2, total_loss = 57.838, pg_loss = 17.837, baseline_loss = 45.192, entropy_loss = -5.1905, learner_queue_size = 32, train_seconds = 7325.7, _tick = 2467, _time = 1.6546e+09)
[2022-06-07 12:25:57,724][root][INFO] - Step 13148160 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13148160, mean_episode_return = None, mean_episode_step = 2225.3, total_loss = -35.876, pg_loss = -43.872, baseline_loss = 13.191, entropy_loss = -5.1948, learner_queue_size = 32, train_seconds = 7330.7, _tick = 2468, _time = 1.6546e+09)
[2022-06-07 12:26:02,730][root][INFO] - Step 13158400 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 13158400, mean_episode_return = 74.27, mean_episode_step = 1721.9, total_loss = 365.13, pg_loss = 258.98, baseline_loss = 111.36, entropy_loss = -5.2181, learner_queue_size = 32, train_seconds = 7335.7, _tick = 2470, _time = 1.6546e+09)
[2022-06-07 12:26:07,734][root][INFO] - Step 13166080 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13166080, mean_episode_return = -2.62, mean_episode_step = 2209.4, total_loss = 413.79, pg_loss = 292.31, baseline_loss = 126.78, entropy_loss = -5.2993, learner_queue_size = 32, train_seconds = 7340.7, _tick = 2473, _time = 1.6546e+09)
[2022-06-07 12:26:12,740][root][INFO] - Step 13176320 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13176320, mean_episode_return = None, mean_episode_step = 2024.7, total_loss = -39.408, pg_loss = -55.561, baseline_loss = 21.522, entropy_loss = -5.3691, learner_queue_size = 32, train_seconds = 7345.7, _tick = 2476, _time = 1.6546e+09)
[2022-06-07 12:26:17,746][root][INFO] - Step 13184000 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 13184000, mean_episode_return = 60.362, mean_episode_step = 1543.0, total_loss = 446.87, pg_loss = 315.94, baseline_loss = 136.35, entropy_loss = -5.4131, learner_queue_size = 32, train_seconds = 7350.7, _tick = 2479, _time = 1.6546e+09)
[2022-06-07 12:26:22,750][root][INFO] - Step 13194240 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 13194240, mean_episode_return = None, mean_episode_step = 2583.2, total_loss = 37.365, pg_loss = 8.3475, baseline_loss = 34.429, entropy_loss = -5.4115, learner_queue_size = 32, train_seconds = 7355.7, _tick = 2482, _time = 1.6546e+09)
[2022-06-07 12:26:27,756][root][INFO] - Step 13201920 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13201920, mean_episode_return = 23.204, mean_episode_step = 2257.0, total_loss = 99.434, pg_loss = 45.479, baseline_loss = 59.462, entropy_loss = -5.5073, learner_queue_size = 32, train_seconds = 7360.8, _tick = 2485, _time = 1.6546e+09)
[2022-06-07 12:26:32,762][root][INFO] - Step 13212160 @ 2045.6 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 13212160, mean_episode_return = 92.948, mean_episode_step = 2047.3, total_loss = 43.136, pg_loss = 9.8409, baseline_loss = 38.779, entropy_loss = -5.4837, learner_queue_size = 32, train_seconds = 7365.8, _tick = 2487, _time = 1.6546e+09)
[2022-06-07 12:26:37,766][root][INFO] - Step 13219840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13219840, mean_episode_return = None, mean_episode_step = 1766.0, total_loss = -76.862, pg_loss = -77.561, baseline_loss = 6.2371, entropy_loss = -5.539, learner_queue_size = 32, train_seconds = 7370.8, _tick = 2488, _time = 1.6546e+09)
[2022-06-07 12:26:42,772][root][INFO] - Step 13230080 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13230080, mean_episode_return = None, mean_episode_step = 2324.3, total_loss = -31.299, pg_loss = -57.359, baseline_loss = 31.583, entropy_loss = -5.5232, learner_queue_size = 32, train_seconds = 7375.8, _tick = 2489, _time = 1.6546e+09)
[2022-06-07 12:26:47,778][root][INFO] - Step 13237760 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 13237760, mean_episode_return = 118.93, mean_episode_step = 2305.9, total_loss = 142.58, pg_loss = 82.584, baseline_loss = 65.501, entropy_loss = -5.5089, learner_queue_size = 32, train_seconds = 7380.8, _tick = 2490, _time = 1.6546e+09)
[2022-06-07 12:26:52,784][root][INFO] - Step 13248000 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 13248000, mean_episode_return = 34.31, mean_episode_step = 2280.4, total_loss = 54.325, pg_loss = -14.55, baseline_loss = 74.396, entropy_loss = -5.5204, learner_queue_size = 32, train_seconds = 7385.8, _tick = 2491, _time = 1.6546e+09)
[2022-06-07 12:26:57,789][root][INFO] - Step 13255680 @ 1534.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13255680, mean_episode_return = None, mean_episode_step = 1783.7, total_loss = 395.8, pg_loss = 305.65, baseline_loss = 95.737, entropy_loss = -5.596, learner_queue_size = 32, train_seconds = 7390.8, _tick = 2493, _time = 1.6546e+09)
[2022-06-07 12:27:02,794][root][INFO] - Step 13265920 @ 2046.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 13265920, mean_episode_return = 17.67, mean_episode_step = 2836.1, total_loss = -107.69, pg_loss = -116.73, baseline_loss = 14.597, entropy_loss = -5.5569, learner_queue_size = 32, train_seconds = 7395.8, _tick = 2497, _time = 1.6546e+09)
[2022-06-07 12:27:07,798][root][INFO] - Step 13273600 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 13273600, mean_episode_return = 106.02, mean_episode_step = 2417.1, total_loss = 192.55, pg_loss = 132.38, baseline_loss = 65.646, entropy_loss = -5.4718, learner_queue_size = 32, train_seconds = 7400.8, _tick = 2500, _time = 1.6546e+09)
[2022-06-07 12:27:12,802][root][INFO] - Step 13283840 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13283840, mean_episode_return = None, mean_episode_step = 2391.0, total_loss = 188.46, pg_loss = 141.08, baseline_loss = 52.949, entropy_loss = -5.573, learner_queue_size = 32, train_seconds = 7405.8, _tick = 2502, _time = 1.6546e+09)
[2022-06-07 12:27:17,806][root][INFO] - Step 13291520 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13291520, mean_episode_return = 25.202, mean_episode_step = 2223.8, total_loss = 18.448, pg_loss = -21.661, baseline_loss = 45.673, entropy_loss = -5.5647, learner_queue_size = 32, train_seconds = 7410.8, _tick = 2503, _time = 1.6546e+09)
[2022-06-07 12:27:22,810][root][INFO] - Step 13301760 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 13301760, mean_episode_return = 110.39, mean_episode_step = 2176.7, total_loss = -34.381, pg_loss = -66.722, baseline_loss = 37.917, entropy_loss = -5.576, learner_queue_size = 32, train_seconds = 7415.8, _tick = 2506, _time = 1.6546e+09)
[2022-06-07 12:27:27,814][root][INFO] - Step 13309440 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 13309440, mean_episode_return = None, mean_episode_step = 2352.6, total_loss = 5.8374, pg_loss = -7.0489, baseline_loss = 18.455, entropy_loss = -5.569, learner_queue_size = 32, train_seconds = 7420.8, _tick = 2507, _time = 1.6546e+09)
[2022-06-07 12:27:32,818][root][INFO] - Step 13319680 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 13319680, mean_episode_return = 78.831, mean_episode_step = 1755.8, total_loss = -3.109, pg_loss = -26.993, baseline_loss = 29.426, entropy_loss = -5.5413, learner_queue_size = 32, train_seconds = 7425.8, _tick = 2508, _time = 1.6546e+09)
[2022-06-07 12:27:37,822][root][INFO] - Step 13329920 @ 2046.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 13329920, mean_episode_return = None, mean_episode_step = 2182.3, total_loss = -123.75, pg_loss = -119.91, baseline_loss = 1.6738, entropy_loss = -5.5204, learner_queue_size = 32, train_seconds = 7430.8, _tick = 2510, _time = 1.6546e+09)
[2022-06-07 12:27:42,826][root][INFO] - Step 13337600 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 13337600, mean_episode_return = 37.952, mean_episode_step = 2622.4, total_loss = 11.3, pg_loss = -20.844, baseline_loss = 37.703, entropy_loss = -5.5598, learner_queue_size = 32, train_seconds = 7435.8, _tick = 2512, _time = 1.6546e+09)
[2022-06-07 12:27:47,830][root][INFO] - Step 13347840 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 13347840, mean_episode_return = None, mean_episode_step = 1782.6, total_loss = 348.14, pg_loss = 267.91, baseline_loss = 85.849, entropy_loss = -5.6148, learner_queue_size = 32, train_seconds = 7440.8, _tick = 2513, _time = 1.6546e+09)
[2022-06-07 12:27:52,836][root][INFO] - Step 13355520 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13355520, mean_episode_return = None, mean_episode_step = 2085.9, total_loss = 342.06, pg_loss = 224.1, baseline_loss = 123.54, entropy_loss = -5.5762, learner_queue_size = 32, train_seconds = 7445.8, _tick = 2514, _time = 1.6546e+09)
[2022-06-07 12:27:57,842][root][INFO] - Step 13363200 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13363200, mean_episode_return = None, mean_episode_step = 2163.8, total_loss = 111.42, pg_loss = 80.573, baseline_loss = 36.502, entropy_loss = -5.6584, learner_queue_size = 32, train_seconds = 7450.8, _tick = 2516, _time = 1.6546e+09)
[2022-06-07 12:28:02,846][root][INFO] - Step 13373440 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13373440, mean_episode_return = 52.375, mean_episode_step = 1970.2, total_loss = 14.334, pg_loss = -4.9971, baseline_loss = 25.097, entropy_loss = -5.7661, learner_queue_size = 32, train_seconds = 7455.8, _tick = 2518, _time = 1.6546e+09)
[2022-06-07 12:28:07,850][root][INFO] - Step 13383680 @ 2046.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 13383680, mean_episode_return = None, mean_episode_step = 2695.4, total_loss = -100.02, pg_loss = -96.81, baseline_loss = 2.4353, entropy_loss = -5.6506, learner_queue_size = 32, train_seconds = 7460.8, _tick = 2520, _time = 1.6546e+09)
[2022-06-07 12:28:12,854][root][INFO] - Step 13391360 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13391360, mean_episode_return = None, mean_episode_step = 2729.4, total_loss = 77.275, pg_loss = 41.205, baseline_loss = 41.826, entropy_loss = -5.7556, learner_queue_size = 32, train_seconds = 7465.8, _tick = 2521, _time = 1.6546e+09)
[2022-06-07 12:28:17,860][root][INFO] - Step 13399040 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13399040, mean_episode_return = None, mean_episode_step = 1898.7, total_loss = 371.3, pg_loss = 266.24, baseline_loss = 110.8, entropy_loss = -5.7394, learner_queue_size = 32, train_seconds = 7470.9, _tick = 2522, _time = 1.6546e+09)
[2022-06-07 12:28:22,866][root][INFO] - Step 13409280 @ 2045.5 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 13409280, mean_episode_return = 12.103, mean_episode_step = 2194.5, total_loss = -134.5, pg_loss = -145.65, baseline_loss = 16.885, entropy_loss = -5.7319, learner_queue_size = 32, train_seconds = 7475.9, _tick = 2524, _time = 1.6546e+09)
[2022-06-07 12:28:27,870][root][INFO] - Step 13416960 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 13416960, mean_episode_return = -5.0083, mean_episode_step = 2322.3, total_loss = 43.188, pg_loss = 11.25, baseline_loss = 37.696, entropy_loss = -5.7587, learner_queue_size = 32, train_seconds = 7480.9, _tick = 2527, _time = 1.6546e+09)
[2022-06-07 12:28:32,876][root][INFO] - Step 13427200 @ 2045.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 13427200, mean_episode_return = 28.106, mean_episode_step = 1690.4, total_loss = -52.268, pg_loss = -88.193, baseline_loss = 41.726, entropy_loss = -5.8004, learner_queue_size = 32, train_seconds = 7485.9, _tick = 2530, _time = 1.6546e+09)
[2022-06-07 12:28:37,882][root][INFO] - Step 13434880 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13434880, mean_episode_return = 116.23, mean_episode_step = 3297.7, total_loss = -43.723, pg_loss = -64.915, baseline_loss = 27.034, entropy_loss = -5.842, learner_queue_size = 32, train_seconds = 7490.9, _tick = 2533, _time = 1.6546e+09)
[2022-06-07 12:28:42,886][root][INFO] - Step 13445120 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 13445120, mean_episode_return = None, mean_episode_step = 2196.9, total_loss = 261.09, pg_loss = 185.49, baseline_loss = 81.586, entropy_loss = -5.994, learner_queue_size = 32, train_seconds = 7495.9, _tick = 2534, _time = 1.6546e+09)
[2022-06-07 12:28:47,892][root][INFO] - Step 13455360 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13455360, mean_episode_return = -28.621, mean_episode_step = 2093.1, total_loss = 121.79, pg_loss = 88.796, baseline_loss = 38.988, entropy_loss = -5.9945, learner_queue_size = 32, train_seconds = 7500.9, _tick = 2536, _time = 1.6546e+09)
[2022-06-07 12:28:52,898][root][INFO] - Step 13463040 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 13463040, mean_episode_return = 19.011, mean_episode_step = 2575.5, total_loss = -93.982, pg_loss = -107.23, baseline_loss = 19.156, entropy_loss = -5.9084, learner_queue_size = 32, train_seconds = 7505.9, _tick = 2539, _time = 1.6546e+09)
[2022-06-07 12:28:57,902][root][INFO] - Step 13473280 @ 2046.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 13473280, mean_episode_return = 97.807, mean_episode_step = 1988.7, total_loss = 268.42, pg_loss = 160.61, baseline_loss = 113.58, entropy_loss = -5.7637, learner_queue_size = 32, train_seconds = 7510.9, _tick = 2543, _time = 1.6546e+09)
[2022-06-07 12:29:02,906][root][INFO] - Step 13480960 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 13480960, mean_episode_return = 25.5, mean_episode_step = 1885.8, total_loss = 266.38, pg_loss = 208.85, baseline_loss = 63.268, entropy_loss = -5.7361, learner_queue_size = 32, train_seconds = 7515.9, _tick = 2546, _time = 1.6546e+09)
[2022-06-07 12:29:07,912][root][INFO] - Step 13491200 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 13491200, mean_episode_return = 92.456, mean_episode_step = 2336.5, total_loss = 42.937, pg_loss = 7.7417, baseline_loss = 40.989, entropy_loss = -5.7939, learner_queue_size = 32, train_seconds = 7520.9, _tick = 2548, _time = 1.6546e+09)
[2022-06-07 12:29:12,918][root][INFO] - Step 13498880 @ 1534.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 13498880, mean_episode_return = 81.045, mean_episode_step = 1688.0, total_loss = 150.77, pg_loss = 92.179, baseline_loss = 64.393, entropy_loss = -5.8023, learner_queue_size = 32, train_seconds = 7525.9, _tick = 2550, _time = 1.6546e+09)
[2022-06-07 12:29:17,922][root][INFO] - Step 13509120 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 13509120, mean_episode_return = None, mean_episode_step = 1764.9, total_loss = 216.13, pg_loss = 158.63, baseline_loss = 63.231, entropy_loss = -5.7338, learner_queue_size = 32, train_seconds = 7530.9, _tick = 2553, _time = 1.6546e+09)
[2022-06-07 12:29:22,926][root][INFO] - Step 13519360 @ 2046.4 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 13519360, mean_episode_return = None, mean_episode_step = 2027.8, total_loss = -48.578, pg_loss = -56.394, baseline_loss = 13.504, entropy_loss = -5.6882, learner_queue_size = 32, train_seconds = 7535.9, _tick = 2555, _time = 1.6546e+09)
[2022-06-07 12:29:27,930][root][INFO] - Step 13527040 @ 1534.7 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 13527040, mean_episode_return = None, mean_episode_step = 2517.1, total_loss = 531.01, pg_loss = 405.25, baseline_loss = 131.49, entropy_loss = -5.7233, learner_queue_size = 32, train_seconds = 7540.9, _tick = 2555, _time = 1.6546e+09)
[2022-06-07 12:29:32,934][root][INFO] - Step 13537280 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 13537280, mean_episode_return = None, mean_episode_step = 2391.3, total_loss = 51.126, pg_loss = 23.486, baseline_loss = 33.336, entropy_loss = -5.6957, learner_queue_size = 32, train_seconds = 7545.9, _tick = 2556, _time = 1.6546e+09)
[2022-06-07 12:29:37,935][root][INFO] - Step 13544960 @ 1535.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13544960, mean_episode_return = None, mean_episode_step = 2311.3, total_loss = 146.32, pg_loss = 95.421, baseline_loss = 56.614, entropy_loss = -5.7121, learner_queue_size = 32, train_seconds = 7550.9, _tick = 2558, _time = 1.6546e+09)
[2022-06-07 12:29:42,941][root][INFO] - Step 13555200 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13555200, mean_episode_return = None, mean_episode_step = 2625.4, total_loss = 114.53, pg_loss = 72.708, baseline_loss = 47.563, entropy_loss = -5.7418, learner_queue_size = 32, train_seconds = 7555.9, _tick = 2559, _time = 1.6546e+09)
[2022-06-07 12:29:47,946][root][INFO] - Step 13562880 @ 1534.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13562880, mean_episode_return = 95.883, mean_episode_step = 2070.4, total_loss = 193.18, pg_loss = 89.593, baseline_loss = 109.25, entropy_loss = -5.6609, learner_queue_size = 32, train_seconds = 7560.9, _tick = 2562, _time = 1.6546e+09)
[2022-06-07 12:29:52,950][root][INFO] - Step 13570560 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13570560, mean_episode_return = 38.822, mean_episode_step = 2506.6, total_loss = -4.1381, pg_loss = -11.555, baseline_loss = 13.101, entropy_loss = -5.6844, learner_queue_size = 32, train_seconds = 7565.9, _tick = 2563, _time = 1.6546e+09)
[2022-06-07 12:29:57,956][root][INFO] - Step 13580800 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13580800, mean_episode_return = None, mean_episode_step = 2752.4, total_loss = 161.65, pg_loss = 120.1, baseline_loss = 47.27, entropy_loss = -5.72, learner_queue_size = 32, train_seconds = 7571.0, _tick = 2566, _time = 1.6546e+09)
[2022-06-07 12:30:02,958][root][INFO] - Step 13588480 @ 1535.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13588480, mean_episode_return = None, mean_episode_step = 1885.4, total_loss = -71.583, pg_loss = -77.966, baseline_loss = 12.037, entropy_loss = -5.6536, learner_queue_size = 32, train_seconds = 7576.0, _tick = 2568, _time = 1.6546e+09)
[2022-06-07 12:30:07,962][root][INFO] - Step 13598720 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 13598720, mean_episode_return = None, mean_episode_step = 1952.1, total_loss = 385.68, pg_loss = 270.33, baseline_loss = 121.03, entropy_loss = -5.685, learner_queue_size = 32, train_seconds = 7581.0, _tick = 2569, _time = 1.6546e+09)
[2022-06-07 12:30:12,966][root][INFO] - Step 13606400 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13606400, mean_episode_return = 46.073, mean_episode_step = 1898.0, total_loss = 412.18, pg_loss = 270.76, baseline_loss = 146.99, entropy_loss = -5.5748, learner_queue_size = 32, train_seconds = 7586.0, _tick = 2572, _time = 1.6546e+09)
[2022-06-07 12:30:17,970][root][INFO] - Step 13616640 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13616640, mean_episode_return = 36.411, mean_episode_step = 2238.8, total_loss = 41.871, pg_loss = -2.7093, baseline_loss = 50.133, entropy_loss = -5.5533, learner_queue_size = 32, train_seconds = 7591.0, _tick = 2573, _time = 1.6546e+09)
[2022-06-07 12:30:22,976][root][INFO] - Step 13624320 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13624320, mean_episode_return = None, mean_episode_step = 1609.6, total_loss = 20.763, pg_loss = -7.9425, baseline_loss = 34.269, entropy_loss = -5.5637, learner_queue_size = 32, train_seconds = 7596.0, _tick = 2574, _time = 1.6546e+09)
[2022-06-07 12:30:27,978][root][INFO] - Step 13634560 @ 2047.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 13634560, mean_episode_return = None, mean_episode_step = 2475.3, total_loss = 252.13, pg_loss = 172.99, baseline_loss = 84.665, entropy_loss = -5.5246, learner_queue_size = 32, train_seconds = 7601.0, _tick = 2575, _time = 1.6546e+09)
[2022-06-07 12:30:32,982][root][INFO] - Step 13642240 @ 1534.7 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 13642240, mean_episode_return = 27.42, mean_episode_step = 1826.8, total_loss = -56.435, pg_loss = -75.113, baseline_loss = 24.275, entropy_loss = -5.5968, learner_queue_size = 32, train_seconds = 7606.0, _tick = 2577, _time = 1.6546e+09)
[2022-06-07 12:30:37,988][root][INFO] - Step 13652480 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 13652480, mean_episode_return = 91.008, mean_episode_step = 2208.1, total_loss = -102.99, pg_loss = -117.48, baseline_loss = 19.964, entropy_loss = -5.4809, learner_queue_size = 32, train_seconds = 7611.0, _tick = 2580, _time = 1.6546e+09)
[2022-06-07 12:30:42,994][root][INFO] - Step 13660160 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 13660160, mean_episode_return = 36.832, mean_episode_step = 2016.5, total_loss = -112.53, pg_loss = -127.42, baseline_loss = 20.423, entropy_loss = -5.5294, learner_queue_size = 32, train_seconds = 7616.0, _tick = 2581, _time = 1.6546e+09)
[2022-06-07 12:30:47,998][root][INFO] - Step 13670400 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13670400, mean_episode_return = 30.021, mean_episode_step = 1709.3, total_loss = 173.85, pg_loss = 91.109, baseline_loss = 88.268, entropy_loss = -5.5302, learner_queue_size = 32, train_seconds = 7621.0, _tick = 2585, _time = 1.6546e+09)
[2022-06-07 12:30:53,002][root][INFO] - Step 13678080 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 13678080, mean_episode_return = 42.19, mean_episode_step = 1819.2, total_loss = 68.856, pg_loss = 10.52, baseline_loss = 63.78, entropy_loss = -5.4438, learner_queue_size = 32, train_seconds = 7626.0, _tick = 2587, _time = 1.6546e+09)
[2022-06-07 12:30:58,008][root][INFO] - Step 13688320 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13688320, mean_episode_return = None, mean_episode_step = 1910.5, total_loss = 2.5849, pg_loss = -25.795, baseline_loss = 33.817, entropy_loss = -5.4363, learner_queue_size = 32, train_seconds = 7631.0, _tick = 2589, _time = 1.6546e+09)
[2022-06-07 12:31:03,014][root][INFO] - Step 13696000 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13696000, mean_episode_return = None, mean_episode_step = 2322.0, total_loss = -33.766, pg_loss = -49.152, baseline_loss = 20.711, entropy_loss = -5.3252, learner_queue_size = 32, train_seconds = 7636.0, _tick = 2590, _time = 1.6546e+09)
[2022-06-07 12:31:08,018][root][INFO] - Step 13706240 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 13706240, mean_episode_return = None, mean_episode_step = 2602.6, total_loss = 25.944, pg_loss = 7.1456, baseline_loss = 23.978, entropy_loss = -5.1799, learner_queue_size = 32, train_seconds = 7641.0, _tick = 2592, _time = 1.6546e+09)
[2022-06-07 12:31:13,022][root][INFO] - Step 13713920 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13713920, mean_episode_return = None, mean_episode_step = 2001.1, total_loss = 260.81, pg_loss = 169.33, baseline_loss = 96.622, entropy_loss = -5.1381, learner_queue_size = 32, train_seconds = 7646.0, _tick = 2594, _time = 1.6546e+09)
[2022-06-07 12:31:18,026][root][INFO] - Step 13724160 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 13724160, mean_episode_return = None, mean_episode_step = 1617.4, total_loss = 314.47, pg_loss = 218.72, baseline_loss = 100.91, entropy_loss = -5.1629, learner_queue_size = 32, train_seconds = 7651.0, _tick = 2596, _time = 1.6546e+09)
[2022-06-07 12:31:23,030][root][INFO] - Step 13731840 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13731840, mean_episode_return = None, mean_episode_step = 2274.0, total_loss = 35.564, pg_loss = -11.588, baseline_loss = 52.473, entropy_loss = -5.3205, learner_queue_size = 32, train_seconds = 7656.0, _tick = 2597, _time = 1.6546e+09)
[2022-06-07 12:31:28,034][root][INFO] - Step 13742080 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13742080, mean_episode_return = None, mean_episode_step = 2556.9, total_loss = 180.79, pg_loss = 74.266, baseline_loss = 111.84, entropy_loss = -5.3244, learner_queue_size = 32, train_seconds = 7661.0, _tick = 2599, _time = 1.6546e+09)
[2022-06-07 12:31:33,040][root][INFO] - Step 13752320 @ 2045.6 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 13752320, mean_episode_return = 47.811, mean_episode_step = 2324.7, total_loss = 319.15, pg_loss = 225.85, baseline_loss = 98.639, entropy_loss = -5.3386, learner_queue_size = 32, train_seconds = 7666.0, _tick = 2603, _time = 1.6546e+09)
[2022-06-07 12:31:38,042][root][INFO] - Step 13760000 @ 1535.3 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 13760000, mean_episode_return = 23.08, mean_episode_step = 1924.2, total_loss = -177.56, pg_loss = -177.12, baseline_loss = 4.9297, entropy_loss = -5.3736, learner_queue_size = 32, train_seconds = 7671.0, _tick = 2606, _time = 1.6546e+09)
[2022-06-07 12:31:43,046][root][INFO] - Step 13770240 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 13770240, mean_episode_return = 33.662, mean_episode_step = 1852.0, total_loss = 1.1521, pg_loss = -55.619, baseline_loss = 62.154, entropy_loss = -5.3831, learner_queue_size = 32, train_seconds = 7676.0, _tick = 2608, _time = 1.6546e+09)
[2022-06-07 12:31:48,050][root][INFO] - Step 13777920 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 13777920, mean_episode_return = 18.76, mean_episode_step = 1912.1, total_loss = 24.801, pg_loss = -10.526, baseline_loss = 40.631, entropy_loss = -5.3036, learner_queue_size = 32, train_seconds = 7681.0, _tick = 2610, _time = 1.6546e+09)
[2022-06-07 12:31:53,056][root][INFO] - Step 13788160 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13788160, mean_episode_return = 81.37, mean_episode_step = 2524.4, total_loss = 140.59, pg_loss = 87.494, baseline_loss = 58.389, entropy_loss = -5.2944, learner_queue_size = 32, train_seconds = 7686.1, _tick = 2613, _time = 1.6546e+09)
[2022-06-07 12:31:58,062][root][INFO] - Step 13795840 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13795840, mean_episode_return = 69.059, mean_episode_step = 1696.0, total_loss = -157.95, pg_loss = -155.96, baseline_loss = 3.3102, entropy_loss = -5.305, learner_queue_size = 32, train_seconds = 7691.1, _tick = 2615, _time = 1.6546e+09)
[2022-06-07 12:32:03,068][root][INFO] - Step 13806080 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 13806080, mean_episode_return = 81.054, mean_episode_step = 1362.6, total_loss = 17.542, pg_loss = -53.308, baseline_loss = 76.171, entropy_loss = -5.3212, learner_queue_size = 32, train_seconds = 7696.1, _tick = 2617, _time = 1.6546e+09)
[2022-06-07 12:32:08,074][root][INFO] - Step 13813760 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13813760, mean_episode_return = 14.159, mean_episode_step = 2234.7, total_loss = 69.6, pg_loss = 28.89, baseline_loss = 46.072, entropy_loss = -5.3632, learner_queue_size = 32, train_seconds = 7701.1, _tick = 2619, _time = 1.6546e+09)
[2022-06-07 12:32:13,078][root][INFO] - Step 13824000 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 13824000, mean_episode_return = 24.2, mean_episode_step = 2132.8, total_loss = 116.14, pg_loss = 61.037, baseline_loss = 60.41, entropy_loss = -5.3074, learner_queue_size = 32, train_seconds = 7706.1, _tick = 2621, _time = 1.6546e+09)
[2022-06-07 12:32:18,082][root][INFO] - Step 13831680 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13831680, mean_episode_return = -4.0206, mean_episode_step = 2297.2, total_loss = 203.15, pg_loss = 90.083, baseline_loss = 118.41, entropy_loss = -5.3406, learner_queue_size = 32, train_seconds = 7711.1, _tick = 2624, _time = 1.6546e+09)
[2022-06-07 12:32:23,086][root][INFO] - Step 13841920 @ 2046.4 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 13841920, mean_episode_return = -78.979, mean_episode_step = 1742.1, total_loss = 270.22, pg_loss = 193.01, baseline_loss = 82.462, entropy_loss = -5.2534, learner_queue_size = 32, train_seconds = 7716.1, _tick = 2627, _time = 1.6546e+09)
[2022-06-07 12:32:28,090][root][INFO] - Step 13849600 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 13849600, mean_episode_return = None, mean_episode_step = 1903.4, total_loss = 66.134, pg_loss = 22.861, baseline_loss = 48.503, entropy_loss = -5.2295, learner_queue_size = 32, train_seconds = 7721.1, _tick = 2627, _time = 1.6546e+09)
[2022-06-07 12:32:33,096][root][INFO] - Step 13859840 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13859840, mean_episode_return = 64.071, mean_episode_step = 1939.4, total_loss = 58.447, pg_loss = 0.35981, baseline_loss = 63.297, entropy_loss = -5.2102, learner_queue_size = 32, train_seconds = 7726.1, _tick = 2630, _time = 1.6546e+09)
[2022-06-07 12:32:38,102][root][INFO] - Step 13867520 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 13867520, mean_episode_return = 76.883, mean_episode_step = 1526.5, total_loss = 22.217, pg_loss = -34.42, baseline_loss = 61.916, entropy_loss = -5.279, learner_queue_size = 32, train_seconds = 7731.1, _tick = 2632, _time = 1.6546e+09)
[2022-06-07 12:32:43,109][root][INFO] - Step 13877760 @ 2045.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13877760, mean_episode_return = 37.942, mean_episode_step = 1546.3, total_loss = 158.75, pg_loss = 66.527, baseline_loss = 97.521, entropy_loss = -5.3, learner_queue_size = 32, train_seconds = 7736.1, _tick = 2635, _time = 1.6546e+09)
[2022-06-07 12:32:48,116][root][INFO] - Step 13888000 @ 2045.2 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 13888000, mean_episode_return = 54.082, mean_episode_step = 2006.7, total_loss = 80.483, pg_loss = 26.038, baseline_loss = 59.713, entropy_loss = -5.2678, learner_queue_size = 32, train_seconds = 7741.1, _tick = 2639, _time = 1.6546e+09)
[2022-06-07 12:32:53,122][root][INFO] - Step 13895680 @ 1534.0 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 13895680, mean_episode_return = None, mean_episode_step = 1481.1, total_loss = 11.449, pg_loss = -15.403, baseline_loss = 32.183, entropy_loss = -5.3317, learner_queue_size = 32, train_seconds = 7746.1, _tick = 2640, _time = 1.6546e+09)
[2022-06-07 12:32:58,126][root][INFO] - Step 13905920 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 13905920, mean_episode_return = 70.462, mean_episode_step = 1792.6, total_loss = 61.887, pg_loss = 21.389, baseline_loss = 45.774, entropy_loss = -5.2757, learner_queue_size = 32, train_seconds = 7751.1, _tick = 2642, _time = 1.6546e+09)
[2022-06-07 12:33:03,130][root][INFO] - Step 13913600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 13913600, mean_episode_return = -8.5001, mean_episode_step = 1879.6, total_loss = 167.42, pg_loss = 73.75, baseline_loss = 98.925, entropy_loss = -5.2514, learner_queue_size = 32, train_seconds = 7756.1, _tick = 2644, _time = 1.6546e+09)
[2022-06-07 12:33:08,136][root][INFO] - Step 13923840 @ 2045.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 13923840, mean_episode_return = None, mean_episode_step = 1303.8, total_loss = -20.191, pg_loss = -42.276, baseline_loss = 27.323, entropy_loss = -5.2389, learner_queue_size = 32, train_seconds = 7761.1, _tick = 2646, _time = 1.6546e+09)
[2022-06-07 12:33:13,142][root][INFO] - Step 13931520 @ 1534.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 13931520, mean_episode_return = 127.79, mean_episode_step = 1459.4, total_loss = 271.65, pg_loss = 165.32, baseline_loss = 111.56, entropy_loss = -5.2333, learner_queue_size = 32, train_seconds = 7766.1, _tick = 2648, _time = 1.6546e+09)
[2022-06-07 12:33:18,148][root][INFO] - Step 13941760 @ 2045.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 13941760, mean_episode_return = 39.454, mean_episode_step = 1601.1, total_loss = 13.639, pg_loss = -31.655, baseline_loss = 50.41, entropy_loss = -5.1168, learner_queue_size = 32, train_seconds = 7771.1, _tick = 2652, _time = 1.6546e+09)
[2022-06-07 12:33:23,150][root][INFO] - Step 13949440 @ 1535.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 13949440, mean_episode_return = 58.04, mean_episode_step = 1816.4, total_loss = -51.657, pg_loss = -67.915, baseline_loss = 21.362, entropy_loss = -5.1039, learner_queue_size = 32, train_seconds = 7776.1, _tick = 2654, _time = 1.6546e+09)
[2022-06-07 12:33:28,163][root][INFO] - Step 13957120 @ 1531.9 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13957120, mean_episode_return = 66.197, mean_episode_step = 1647.8, total_loss = 447.33, pg_loss = 313.92, baseline_loss = 138.43, entropy_loss = -5.0248, learner_queue_size = 32, train_seconds = 7781.2, _tick = 2656, _time = 1.6546e+09)
[2022-06-07 12:33:33,166][root][INFO] - Step 13967360 @ 2046.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 13967360, mean_episode_return = 75.841, mean_episode_step = 2035.6, total_loss = -33.177, pg_loss = -157.0, baseline_loss = 128.97, entropy_loss = -5.1473, learner_queue_size = 32, train_seconds = 7786.2, _tick = 2657, _time = 1.6546e+09)
[2022-06-07 12:33:38,172][root][INFO] - Step 13975040 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 13975040, mean_episode_return = 0.98999, mean_episode_step = 1944.0, total_loss = -34.092, pg_loss = -72.559, baseline_loss = 43.522, entropy_loss = -5.0551, learner_queue_size = 32, train_seconds = 7791.2, _tick = 2659, _time = 1.6546e+09)
[2022-06-07 12:33:43,178][root][INFO] - Step 13985280 @ 2045.6 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 13985280, mean_episode_return = None, mean_episode_step = 1808.4, total_loss = 52.764, pg_loss = 17.74, baseline_loss = 40.019, entropy_loss = -4.9954, learner_queue_size = 32, train_seconds = 7796.2, _tick = 2662, _time = 1.6546e+09)
[2022-06-07 12:33:48,182][root][INFO] - Step 13995520 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 13995520, mean_episode_return = -9.7902, mean_episode_step = 1636.5, total_loss = -56.574, pg_loss = -66.106, baseline_loss = 14.414, entropy_loss = -4.8817, learner_queue_size = 32, train_seconds = 7801.2, _tick = 2665, _time = 1.6546e+09)
[2022-06-07 12:33:53,186][root][INFO] - Step 14003200 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 14003200, mean_episode_return = None, mean_episode_step = 1466.5, total_loss = 10.076, pg_loss = -9.9411, baseline_loss = 25.002, entropy_loss = -4.9858, learner_queue_size = 32, train_seconds = 7806.2, _tick = 2666, _time = 1.6546e+09)
[2022-06-07 12:33:58,192][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 12:33:58,427][root][INFO] - Step 14010880 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14010880, mean_episode_return = 26.106, mean_episode_step = 1776.8, total_loss = 246.24, pg_loss = 155.6, baseline_loss = 95.593, entropy_loss = -4.9596, learner_queue_size = 32, train_seconds = 7811.2, _tick = 2669, _time = 1.6546e+09)
[2022-06-07 12:34:03,430][root][INFO] - Step 14021120 @ 1955.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14021120, mean_episode_return = 30.921, mean_episode_step = 1992.7, total_loss = -155.0, pg_loss = -155.96, baseline_loss = 5.9223, entropy_loss = -4.9652, learner_queue_size = 32, train_seconds = 7816.4, _tick = 2671, _time = 1.6546e+09)
[2022-06-07 12:34:08,434][root][INFO] - Step 14028800 @ 1534.7 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 14028800, mean_episode_return = None, mean_episode_step = 1719.5, total_loss = 1.8641, pg_loss = -40.22, baseline_loss = 47.06, entropy_loss = -4.9762, learner_queue_size = 32, train_seconds = 7821.4, _tick = 2671, _time = 1.6546e+09)
[2022-06-07 12:34:13,438][root][INFO] - Step 14039040 @ 2046.4 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 14039040, mean_episode_return = 4.9197, mean_episode_step = 1754.8, total_loss = 76.729, pg_loss = -23.644, baseline_loss = 105.4, entropy_loss = -5.027, learner_queue_size = 32, train_seconds = 7826.4, _tick = 2672, _time = 1.6546e+09)
[2022-06-07 12:34:18,442][root][INFO] - Step 14046720 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 14046720, mean_episode_return = None, mean_episode_step = 2274.0, total_loss = -112.66, pg_loss = -109.98, baseline_loss = 2.1436, entropy_loss = -4.8234, learner_queue_size = 32, train_seconds = 7831.4, _tick = 2674, _time = 1.6546e+09)
[2022-06-07 12:34:23,448][root][INFO] - Step 14056960 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 14056960, mean_episode_return = 35.156, mean_episode_step = 1992.3, total_loss = -61.249, pg_loss = -84.073, baseline_loss = 27.611, entropy_loss = -4.7877, learner_queue_size = 32, train_seconds = 7836.4, _tick = 2677, _time = 1.6546e+09)
[2022-06-07 12:34:28,454][root][INFO] - Step 14067200 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 14067200, mean_episode_return = 13.145, mean_episode_step = 1983.1, total_loss = -66.903, pg_loss = -109.25, baseline_loss = 47.198, entropy_loss = -4.8534, learner_queue_size = 32, train_seconds = 7841.4, _tick = 2680, _time = 1.6546e+09)
[2022-06-07 12:34:33,460][root][INFO] - Step 14074880 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14074880, mean_episode_return = 25.09, mean_episode_step = 1507.2, total_loss = 33.608, pg_loss = -11.627, baseline_loss = 50.143, entropy_loss = -4.9088, learner_queue_size = 32, train_seconds = 7846.5, _tick = 2683, _time = 1.6546e+09)
[2022-06-07 12:34:38,466][root][INFO] - Step 14082560 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14082560, mean_episode_return = 42.87, mean_episode_step = 1774.5, total_loss = 187.04, pg_loss = 101.78, baseline_loss = 90.18, entropy_loss = -4.917, learner_queue_size = 32, train_seconds = 7851.5, _tick = 2686, _time = 1.6546e+09)
[2022-06-07 12:34:43,470][root][INFO] - Step 14092800 @ 2046.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 14092800, mean_episode_return = 85.467, mean_episode_step = 1324.2, total_loss = 188.19, pg_loss = 111.24, baseline_loss = 81.841, entropy_loss = -4.8921, learner_queue_size = 32, train_seconds = 7856.5, _tick = 2687, _time = 1.6546e+09)
[2022-06-07 12:34:48,474][root][INFO] - Step 14100480 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 14100480, mean_episode_return = None, mean_episode_step = 1905.0, total_loss = 62.382, pg_loss = 13.194, baseline_loss = 54.152, entropy_loss = -4.9642, learner_queue_size = 32, train_seconds = 7861.5, _tick = 2689, _time = 1.6546e+09)
[2022-06-07 12:34:53,480][root][INFO] - Step 14110720 @ 2045.5 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 14110720, mean_episode_return = None, mean_episode_step = 1660.2, total_loss = 28.718, pg_loss = 1.7736, baseline_loss = 31.953, entropy_loss = -5.0087, learner_queue_size = 32, train_seconds = 7866.5, _tick = 2689, _time = 1.6546e+09)
[2022-06-07 12:34:58,486][root][INFO] - Step 14120960 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 14120960, mean_episode_return = None, mean_episode_step = 2377.2, total_loss = 145.01, pg_loss = 87.912, baseline_loss = 62.031, entropy_loss = -4.9381, learner_queue_size = 32, train_seconds = 7871.5, _tick = 2689, _time = 1.6546e+09)
[2022-06-07 12:35:03,492][root][INFO] - Step 14128640 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14128640, mean_episode_return = None, mean_episode_step = 2003.1, total_loss = 95.405, pg_loss = 52.32, baseline_loss = 48.073, entropy_loss = -4.9876, learner_queue_size = 32, train_seconds = 7876.5, _tick = 2689, _time = 1.6546e+09)
[2022-06-07 12:35:08,498][root][INFO] - Step 14138880 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 14138880, mean_episode_return = 35.955, mean_episode_step = 1964.7, total_loss = 231.19, pg_loss = 149.26, baseline_loss = 87.048, entropy_loss = -5.1193, learner_queue_size = 32, train_seconds = 7881.5, _tick = 2692, _time = 1.6546e+09)
[2022-06-07 12:35:13,502][root][INFO] - Step 14146560 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 14146560, mean_episode_return = None, mean_episode_step = 2055.2, total_loss = 303.87, pg_loss = 204.05, baseline_loss = 105.04, entropy_loss = -5.2151, learner_queue_size = 32, train_seconds = 7886.5, _tick = 2693, _time = 1.6546e+09)
[2022-06-07 12:35:18,532][root][INFO] - Step 14156800 @ 2035.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 14156800, mean_episode_return = 13.92, mean_episode_step = 1725.8, total_loss = -23.359, pg_loss = -56.405, baseline_loss = 38.256, entropy_loss = -5.2097, learner_queue_size = 32, train_seconds = 7891.5, _tick = 2696, _time = 1.6546e+09)
[2022-06-07 12:35:23,538][root][INFO] - Step 14164480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14164480, mean_episode_return = None, mean_episode_step = 1988.7, total_loss = -8.097, pg_loss = -24.293, baseline_loss = 21.358, entropy_loss = -5.1627, learner_queue_size = 32, train_seconds = 7896.5, _tick = 2696, _time = 1.6546e+09)
[2022-06-07 12:35:28,542][root][INFO] - Step 14174720 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 14174720, mean_episode_return = None, mean_episode_step = 1919.7, total_loss = 41.5, pg_loss = 19.03, baseline_loss = 27.619, entropy_loss = -5.1492, learner_queue_size = 32, train_seconds = 7901.5, _tick = 2696, _time = 1.6546e+09)
[2022-06-07 12:35:33,548][root][INFO] - Step 14182400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14182400, mean_episode_return = 30.61, mean_episode_step = 1848.3, total_loss = 20.0, pg_loss = -12.998, baseline_loss = 38.186, entropy_loss = -5.1874, learner_queue_size = 32, train_seconds = 7906.5, _tick = 2698, _time = 1.6546e+09)
[2022-06-07 12:35:38,554][root][INFO] - Step 14192640 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14192640, mean_episode_return = None, mean_episode_step = 1974.2, total_loss = 250.12, pg_loss = 169.73, baseline_loss = 85.54, entropy_loss = -5.1539, learner_queue_size = 32, train_seconds = 7911.5, _tick = 2700, _time = 1.6546e+09)
[2022-06-07 12:35:43,558][root][INFO] - Step 14200320 @ 1534.7 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 14200320, mean_episode_return = None, mean_episode_step = 1940.1, total_loss = -85.842, pg_loss = -94.775, baseline_loss = 14.135, entropy_loss = -5.2013, learner_queue_size = 32, train_seconds = 7916.6, _tick = 2702, _time = 1.6546e+09)
[2022-06-07 12:35:48,564][root][INFO] - Step 14210560 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14210560, mean_episode_return = 12.28, mean_episode_step = 1853.5, total_loss = 428.67, pg_loss = 275.03, baseline_loss = 158.84, entropy_loss = -5.1967, learner_queue_size = 32, train_seconds = 7921.6, _tick = 2706, _time = 1.6546e+09)
[2022-06-07 12:35:53,570][root][INFO] - Step 14218240 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14218240, mean_episode_return = None, mean_episode_step = 2287.5, total_loss = -168.51, pg_loss = -164.24, baseline_loss = 0.93788, entropy_loss = -5.2091, learner_queue_size = 32, train_seconds = 7926.6, _tick = 2706, _time = 1.6546e+09)
[2022-06-07 12:35:58,576][root][INFO] - Step 14228480 @ 2045.5 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 14228480, mean_episode_return = None, mean_episode_step = 2496.8, total_loss = 36.232, pg_loss = 12.524, baseline_loss = 28.914, entropy_loss = -5.2061, learner_queue_size = 32, train_seconds = 7931.6, _tick = 2708, _time = 1.6546e+09)
[2022-06-07 12:36:03,582][root][INFO] - Step 14236160 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 14236160, mean_episode_return = 68.473, mean_episode_step = 2130.2, total_loss = 81.128, pg_loss = 21.722, baseline_loss = 64.616, entropy_loss = -5.2106, learner_queue_size = 32, train_seconds = 7936.6, _tick = 2710, _time = 1.6546e+09)
[2022-06-07 12:36:08,586][root][INFO] - Step 14246400 @ 2046.3 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 14246400, mean_episode_return = None, mean_episode_step = 2502.1, total_loss = 430.89, pg_loss = 239.46, baseline_loss = 196.66, entropy_loss = -5.2289, learner_queue_size = 32, train_seconds = 7941.6, _tick = 2710, _time = 1.6546e+09)
[2022-06-07 12:36:13,590][root][INFO] - Step 14254080 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14254080, mean_episode_return = 51.355, mean_episode_step = 2188.3, total_loss = -144.91, pg_loss = -148.13, baseline_loss = 8.4718, entropy_loss = -5.2465, learner_queue_size = 32, train_seconds = 7946.6, _tick = 2711, _time = 1.6546e+09)
[2022-06-07 12:36:18,594][root][INFO] - Step 14264320 @ 2046.4 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 14264320, mean_episode_return = None, mean_episode_step = 2587.2, total_loss = -58.231, pg_loss = -73.988, baseline_loss = 21.038, entropy_loss = -5.2808, learner_queue_size = 32, train_seconds = 7951.6, _tick = 2711, _time = 1.6546e+09)
[2022-06-07 12:36:23,606][root][INFO] - Step 14272000 @ 1532.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14272000, mean_episode_return = 149.45, mean_episode_step = 2503.3, total_loss = 290.32, pg_loss = 169.38, baseline_loss = 126.31, entropy_loss = -5.3738, learner_queue_size = 32, train_seconds = 7956.6, _tick = 2713, _time = 1.6546e+09)
[2022-06-07 12:36:28,610][root][INFO] - Step 14282240 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 14282240, mean_episode_return = 69.229, mean_episode_step = 2019.7, total_loss = -32.201, pg_loss = -102.74, baseline_loss = 75.882, entropy_loss = -5.339, learner_queue_size = 32, train_seconds = 7961.6, _tick = 2714, _time = 1.6546e+09)
[2022-06-07 12:36:33,614][root][INFO] - Step 14289920 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14289920, mean_episode_return = 3.6397, mean_episode_step = 2381.4, total_loss = 244.72, pg_loss = 50.288, baseline_loss = 199.8, entropy_loss = -5.3698, learner_queue_size = 32, train_seconds = 7966.6, _tick = 2715, _time = 1.6546e+09)
[2022-06-07 12:36:38,618][root][INFO] - Step 14300160 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14300160, mean_episode_return = None, mean_episode_step = 2686.3, total_loss = 98.631, pg_loss = 43.096, baseline_loss = 60.936, entropy_loss = -5.4008, learner_queue_size = 32, train_seconds = 7971.6, _tick = 2716, _time = 1.6546e+09)
[2022-06-07 12:36:43,622][root][INFO] - Step 14307840 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 14307840, mean_episode_return = None, mean_episode_step = 1977.2, total_loss = -83.157, pg_loss = -94.318, baseline_loss = 16.455, entropy_loss = -5.2943, learner_queue_size = 32, train_seconds = 7976.6, _tick = 2716, _time = 1.6546e+09)
[2022-06-07 12:36:48,628][root][INFO] - Step 14318080 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14318080, mean_episode_return = 15.209, mean_episode_step = 2429.7, total_loss = -71.977, pg_loss = -94.725, baseline_loss = 27.998, entropy_loss = -5.2498, learner_queue_size = 32, train_seconds = 7981.6, _tick = 2718, _time = 1.6546e+09)
[2022-06-07 12:36:53,634][root][INFO] - Step 14325760 @ 1534.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 14325760, mean_episode_return = 31.343, mean_episode_step = 1952.5, total_loss = 213.46, pg_loss = 98.134, baseline_loss = 120.6, entropy_loss = -5.2739, learner_queue_size = 32, train_seconds = 7986.6, _tick = 2721, _time = 1.6546e+09)
[2022-06-07 12:36:58,638][root][INFO] - Step 14336000 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 14336000, mean_episode_return = 22.93, mean_episode_step = 2114.7, total_loss = 1304.0, pg_loss = 525.04, baseline_loss = 784.2, entropy_loss = -5.2275, learner_queue_size = 32, train_seconds = 7991.6, _tick = 2723, _time = 1.6546e+09)
[2022-06-07 12:37:03,642][root][INFO] - Step 14343680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14343680, mean_episode_return = 63.231, mean_episode_step = 2451.2, total_loss = -188.14, pg_loss = -227.84, baseline_loss = 44.883, entropy_loss = -5.1792, learner_queue_size = 32, train_seconds = 7996.6, _tick = 2726, _time = 1.6546e+09)
[2022-06-07 12:37:08,646][root][INFO] - Step 14353920 @ 2046.3 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 14353920, mean_episode_return = 41.533, mean_episode_step = 2341.6, total_loss = 102.23, pg_loss = 23.932, baseline_loss = 83.532, entropy_loss = -5.2365, learner_queue_size = 32, train_seconds = 8001.6, _tick = 2730, _time = 1.6546e+09)
[2022-06-07 12:37:13,652][root][INFO] - Step 14361600 @ 1534.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 14361600, mean_episode_return = 52.691, mean_episode_step = 1875.1, total_loss = 642.18, pg_loss = 396.47, baseline_loss = 251.03, entropy_loss = -5.3285, learner_queue_size = 32, train_seconds = 8006.6, _tick = 2732, _time = 1.6546e+09)
[2022-06-07 12:37:18,658][root][INFO] - Step 14371840 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14371840, mean_episode_return = 50.915, mean_episode_step = 1973.3, total_loss = -93.95, pg_loss = -139.03, baseline_loss = 50.438, entropy_loss = -5.3614, learner_queue_size = 32, train_seconds = 8011.7, _tick = 2735, _time = 1.6546e+09)
[2022-06-07 12:37:23,664][root][INFO] - Step 14379520 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14379520, mean_episode_return = None, mean_episode_step = 2404.6, total_loss = 283.45, pg_loss = 208.5, baseline_loss = 80.314, entropy_loss = -5.3569, learner_queue_size = 32, train_seconds = 8016.7, _tick = 2735, _time = 1.6546e+09)
[2022-06-07 12:37:28,670][root][INFO] - Step 14389760 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14389760, mean_episode_return = None, mean_episode_step = 1735.7, total_loss = 48.691, pg_loss = 17.235, baseline_loss = 36.787, entropy_loss = -5.3305, learner_queue_size = 32, train_seconds = 8021.7, _tick = 2736, _time = 1.6546e+09)
[2022-06-07 12:37:33,676][root][INFO] - Step 14397440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14397440, mean_episode_return = 26.95, mean_episode_step = 1832.8, total_loss = 166.59, pg_loss = 120.78, baseline_loss = 51.035, entropy_loss = -5.2291, learner_queue_size = 32, train_seconds = 8026.7, _tick = 2738, _time = 1.6546e+09)
[2022-06-07 12:37:38,682][root][INFO] - Step 14407680 @ 2045.6 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 14407680, mean_episode_return = None, mean_episode_step = 2425.1, total_loss = 226.17, pg_loss = 93.181, baseline_loss = 138.25, entropy_loss = -5.2648, learner_queue_size = 32, train_seconds = 8031.7, _tick = 2741, _time = 1.6546e+09)
[2022-06-07 12:37:43,688][root][INFO] - Step 14415360 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14415360, mean_episode_return = None, mean_episode_step = 2088.0, total_loss = -69.413, pg_loss = -75.297, baseline_loss = 11.096, entropy_loss = -5.2117, learner_queue_size = 32, train_seconds = 8036.7, _tick = 2742, _time = 1.6546e+09)
[2022-06-07 12:37:48,690][root][INFO] - Step 14425600 @ 2047.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14425600, mean_episode_return = 41.544, mean_episode_step = 2080.4, total_loss = 339.93, pg_loss = 230.9, baseline_loss = 114.25, entropy_loss = -5.226, learner_queue_size = 32, train_seconds = 8041.7, _tick = 2746, _time = 1.6546e+09)
[2022-06-07 12:37:53,694][root][INFO] - Step 14435840 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 14435840, mean_episode_return = 14.35, mean_episode_step = 2317.6, total_loss = 126.36, pg_loss = 72.77, baseline_loss = 58.718, entropy_loss = -5.1259, learner_queue_size = 32, train_seconds = 8046.7, _tick = 2750, _time = 1.6546e+09)
[2022-06-07 12:37:58,698][root][INFO] - Step 14443520 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 14443520, mean_episode_return = 19.135, mean_episode_step = 1818.0, total_loss = 228.48, pg_loss = 127.1, baseline_loss = 106.49, entropy_loss = -5.1154, learner_queue_size = 32, train_seconds = 8051.7, _tick = 2753, _time = 1.6546e+09)
[2022-06-07 12:38:03,702][root][INFO] - Step 14451200 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 14451200, mean_episode_return = -0.32, mean_episode_step = 1710.1, total_loss = -134.6, pg_loss = -152.53, baseline_loss = 23.047, entropy_loss = -5.1194, learner_queue_size = 32, train_seconds = 8056.7, _tick = 2755, _time = 1.6546e+09)
[2022-06-07 12:38:08,725][root][INFO] - Step 14461440 @ 2038.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 14461440, mean_episode_return = 96.831, mean_episode_step = 2002.4, total_loss = 326.87, pg_loss = 163.03, baseline_loss = 168.98, entropy_loss = -5.1463, learner_queue_size = 32, train_seconds = 8061.7, _tick = 2758, _time = 1.6546e+09)
[2022-06-07 12:38:13,731][root][INFO] - Step 14469120 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14469120, mean_episode_return = 79.681, mean_episode_step = 1583.7, total_loss = -122.91, pg_loss = -144.62, baseline_loss = 26.832, entropy_loss = -5.1216, learner_queue_size = 32, train_seconds = 8066.7, _tick = 2759, _time = 1.6546e+09)
[2022-06-07 12:38:18,734][root][INFO] - Step 14479360 @ 2047.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 14479360, mean_episode_return = -0.85492, mean_episode_step = 1680.9, total_loss = 366.18, pg_loss = 199.04, baseline_loss = 172.19, entropy_loss = -5.0519, learner_queue_size = 32, train_seconds = 8071.7, _tick = 2763, _time = 1.6546e+09)
[2022-06-07 12:38:23,738][root][INFO] - Step 14489600 @ 2046.3 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 14489600, mean_episode_return = 45.24, mean_episode_step = 2105.6, total_loss = -108.37, pg_loss = -137.01, baseline_loss = 33.647, entropy_loss = -5.0074, learner_queue_size = 32, train_seconds = 8076.7, _tick = 2767, _time = 1.6546e+09)
[2022-06-07 12:38:28,742][root][INFO] - Step 14497280 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 14497280, mean_episode_return = 53.226, mean_episode_step = 1558.9, total_loss = -42.381, pg_loss = -62.233, baseline_loss = 24.836, entropy_loss = -4.9851, learner_queue_size = 32, train_seconds = 8081.7, _tick = 2768, _time = 1.6546e+09)
[2022-06-07 12:38:33,748][root][INFO] - Step 14507520 @ 2045.6 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 14507520, mean_episode_return = -6.8606, mean_episode_step = 2074.7, total_loss = 104.03, pg_loss = 53.946, baseline_loss = 54.983, entropy_loss = -4.9048, learner_queue_size = 32, train_seconds = 8086.7, _tick = 2771, _time = 1.6546e+09)
[2022-06-07 12:38:38,754][root][INFO] - Step 14515200 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 14515200, mean_episode_return = 55.718, mean_episode_step = 1878.7, total_loss = -184.16, pg_loss = -239.09, baseline_loss = 59.696, entropy_loss = -4.7611, learner_queue_size = 32, train_seconds = 8091.7, _tick = 2774, _time = 1.6546e+09)
[2022-06-07 12:38:43,760][root][INFO] - Step 14525440 @ 2045.5 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 14525440, mean_episode_return = None, mean_episode_step = 1872.2, total_loss = -168.03, pg_loss = -169.25, baseline_loss = 5.9647, entropy_loss = -4.744, learner_queue_size = 32, train_seconds = 8096.8, _tick = 2775, _time = 1.6546e+09)
[2022-06-07 12:38:48,766][root][INFO] - Step 14533120 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 14533120, mean_episode_return = None, mean_episode_step = 1806.1, total_loss = 76.345, pg_loss = -8.972, baseline_loss = 90.075, entropy_loss = -4.7583, learner_queue_size = 32, train_seconds = 8101.8, _tick = 2776, _time = 1.6546e+09)
[2022-06-07 12:38:53,772][root][INFO] - Step 14540800 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14540800, mean_episode_return = None, mean_episode_step = 1653.4, total_loss = -36.77, pg_loss = -40.388, baseline_loss = 8.0412, entropy_loss = -4.4237, learner_queue_size = 32, train_seconds = 8106.8, _tick = 2778, _time = 1.6546e+09)
[2022-06-07 12:38:58,775][root][INFO] - Step 14551040 @ 2046.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 14551040, mean_episode_return = None, mean_episode_step = 2216.9, total_loss = 161.29, pg_loss = 93.192, baseline_loss = 72.679, entropy_loss = -4.5816, learner_queue_size = 32, train_seconds = 8111.8, _tick = 2781, _time = 1.6546e+09)
[2022-06-07 12:39:03,780][root][INFO] - Step 14558720 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14558720, mean_episode_return = None, mean_episode_step = 1534.0, total_loss = -59.148, pg_loss = -74.574, baseline_loss = 20.154, entropy_loss = -4.728, learner_queue_size = 32, train_seconds = 8116.8, _tick = 2783, _time = 1.6546e+09)
[2022-06-07 12:39:08,782][root][INFO] - Step 14568960 @ 2047.0 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 14568960, mean_episode_return = 23.189, mean_episode_step = 2044.2, total_loss = 40.444, pg_loss = 4.3778, baseline_loss = 40.915, entropy_loss = -4.8486, learner_queue_size = 32, train_seconds = 8121.8, _tick = 2784, _time = 1.6546e+09)
[2022-06-07 12:39:13,788][root][INFO] - Step 14579200 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 14579200, mean_episode_return = None, mean_episode_step = 1187.7, total_loss = 194.15, pg_loss = 141.73, baseline_loss = 57.3, entropy_loss = -4.8871, learner_queue_size = 32, train_seconds = 8126.8, _tick = 2786, _time = 1.6546e+09)
[2022-06-07 12:39:18,794][root][INFO] - Step 14586880 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 14586880, mean_episode_return = None, mean_episode_step = 1625.3, total_loss = -70.902, pg_loss = -71.547, baseline_loss = 5.4853, entropy_loss = -4.8401, learner_queue_size = 32, train_seconds = 8131.8, _tick = 2787, _time = 1.6546e+09)
[2022-06-07 12:39:23,798][root][INFO] - Step 14597120 @ 2046.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 14597120, mean_episode_return = None, mean_episode_step = 1846.8, total_loss = -15.893, pg_loss = -19.039, baseline_loss = 7.6877, entropy_loss = -4.5414, learner_queue_size = 32, train_seconds = 8136.8, _tick = 2788, _time = 1.6546e+09)
[2022-06-07 12:39:28,802][root][INFO] - Step 14604800 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 14604800, mean_episode_return = None, mean_episode_step = 2141.5, total_loss = -77.834, pg_loss = -90.686, baseline_loss = 17.651, entropy_loss = -4.8001, learner_queue_size = 32, train_seconds = 8141.8, _tick = 2789, _time = 1.6546e+09)
[2022-06-07 12:39:33,814][root][INFO] - Step 14615040 @ 2043.1 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 14615040, mean_episode_return = -8.62, mean_episode_step = 2252.9, total_loss = -106.61, pg_loss = -121.45, baseline_loss = 19.382, entropy_loss = -4.5486, learner_queue_size = 32, train_seconds = 8146.8, _tick = 2791, _time = 1.6546e+09)
[2022-06-07 12:39:38,818][root][INFO] - Step 14622720 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 14622720, mean_episode_return = None, mean_episode_step = 2032.5, total_loss = -116.23, pg_loss = -115.27, baseline_loss = 3.552, entropy_loss = -4.5099, learner_queue_size = 32, train_seconds = 8151.8, _tick = 2791, _time = 1.6546e+09)
[2022-06-07 12:39:43,822][root][INFO] - Step 14632960 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14632960, mean_episode_return = None, mean_episode_step = 2098.1, total_loss = 74.534, pg_loss = 39.282, baseline_loss = 39.558, entropy_loss = -4.3065, learner_queue_size = 32, train_seconds = 8156.8, _tick = 2792, _time = 1.6546e+09)
[2022-06-07 12:39:48,826][root][INFO] - Step 14640640 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14640640, mean_episode_return = None, mean_episode_step = 1896.7, total_loss = 213.5, pg_loss = 153.66, baseline_loss = 64.414, entropy_loss = -4.5759, learner_queue_size = 32, train_seconds = 8161.8, _tick = 2792, _time = 1.6546e+09)
[2022-06-07 12:39:53,828][root][INFO] - Step 14650880 @ 2047.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 14650880, mean_episode_return = None, mean_episode_step = 2363.6, total_loss = 101.27, pg_loss = 60.378, baseline_loss = 45.469, entropy_loss = -4.5815, learner_queue_size = 32, train_seconds = 8166.8, _tick = 2793, _time = 1.6546e+09)
[2022-06-07 12:39:58,830][root][INFO] - Step 14658560 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14658560, mean_episode_return = 71.007, mean_episode_step = 1667.7, total_loss = 93.518, pg_loss = 26.361, baseline_loss = 71.874, entropy_loss = -4.7162, learner_queue_size = 32, train_seconds = 8171.8, _tick = 2795, _time = 1.6546e+09)
[2022-06-07 12:40:03,834][root][INFO] - Step 14668800 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 14668800, mean_episode_return = 19.14, mean_episode_step = 1910.6, total_loss = -50.259, pg_loss = -67.574, baseline_loss = 22.114, entropy_loss = -4.7994, learner_queue_size = 32, train_seconds = 8176.8, _tick = 2796, _time = 1.6546e+09)
[2022-06-07 12:40:08,838][root][INFO] - Step 14676480 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 14676480, mean_episode_return = None, mean_episode_step = 1894.5, total_loss = -12.868, pg_loss = -36.868, baseline_loss = 28.79, entropy_loss = -4.7904, learner_queue_size = 32, train_seconds = 8181.8, _tick = 2797, _time = 1.6546e+09)
[2022-06-07 12:40:13,844][root][INFO] - Step 14686720 @ 2045.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 14686720, mean_episode_return = 45.451, mean_episode_step = 2526.4, total_loss = 91.196, pg_loss = 45.906, baseline_loss = 49.996, entropy_loss = -4.7058, learner_queue_size = 32, train_seconds = 8186.8, _tick = 2799, _time = 1.6546e+09)
[2022-06-07 12:40:18,850][root][INFO] - Step 14694400 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14694400, mean_episode_return = 38.32, mean_episode_step = 2106.4, total_loss = 274.06, pg_loss = 177.34, baseline_loss = 101.39, entropy_loss = -4.6697, learner_queue_size = 32, train_seconds = 8191.8, _tick = 2800, _time = 1.6546e+09)
[2022-06-07 12:40:23,854][root][INFO] - Step 14704640 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 14704640, mean_episode_return = 15.765, mean_episode_step = 2295.1, total_loss = 1.4419, pg_loss = -35.727, baseline_loss = 41.837, entropy_loss = -4.6687, learner_queue_size = 32, train_seconds = 8196.8, _tick = 2801, _time = 1.6546e+09)
[2022-06-07 12:40:28,860][root][INFO] - Step 14712320 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14712320, mean_episode_return = None, mean_episode_step = 2416.1, total_loss = -7.0801, pg_loss = -22.564, baseline_loss = 19.997, entropy_loss = -4.5136, learner_queue_size = 32, train_seconds = 8201.9, _tick = 2801, _time = 1.6546e+09)
[2022-06-07 12:40:33,862][root][INFO] - Step 14722560 @ 2047.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14722560, mean_episode_return = None, mean_episode_step = 2357.5, total_loss = 78.105, pg_loss = 19.89, baseline_loss = 62.772, entropy_loss = -4.5567, learner_queue_size = 32, train_seconds = 8206.9, _tick = 2802, _time = 1.6546e+09)
[2022-06-07 12:40:38,868][root][INFO] - Step 14730240 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14730240, mean_episode_return = None, mean_episode_step = 2406.8, total_loss = 237.82, pg_loss = 145.5, baseline_loss = 96.971, entropy_loss = -4.6548, learner_queue_size = 32, train_seconds = 8211.9, _tick = 2804, _time = 1.6546e+09)
[2022-06-07 12:40:43,874][root][INFO] - Step 14740480 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 14740480, mean_episode_return = 123.6, mean_episode_step = 1830.8, total_loss = -77.92, pg_loss = -82.389, baseline_loss = 9.0754, entropy_loss = -4.6065, learner_queue_size = 32, train_seconds = 8216.9, _tick = 2806, _time = 1.6546e+09)
[2022-06-07 12:40:48,880][root][INFO] - Step 14748160 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14748160, mean_episode_return = None, mean_episode_step = 2333.7, total_loss = -121.17, pg_loss = -117.88, baseline_loss = 1.3476, entropy_loss = -4.6367, learner_queue_size = 32, train_seconds = 8221.9, _tick = 2806, _time = 1.6546e+09)
[2022-06-07 12:40:53,886][root][INFO] - Step 14758400 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14758400, mean_episode_return = 44.46, mean_episode_step = 2368.3, total_loss = -37.988, pg_loss = -71.677, baseline_loss = 38.133, entropy_loss = -4.4433, learner_queue_size = 32, train_seconds = 8226.9, _tick = 2809, _time = 1.6546e+09)
[2022-06-07 12:40:58,890][root][INFO] - Step 14766080 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 14766080, mean_episode_return = None, mean_episode_step = 2310.8, total_loss = -87.406, pg_loss = -95.223, baseline_loss = 12.223, entropy_loss = -4.4053, learner_queue_size = 32, train_seconds = 8231.9, _tick = 2810, _time = 1.6546e+09)
[2022-06-07 12:41:03,894][root][INFO] - Step 14776320 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 14776320, mean_episode_return = 52.761, mean_episode_step = 2138.5, total_loss = 2.6181, pg_loss = -31.751, baseline_loss = 38.708, entropy_loss = -4.3386, learner_queue_size = 32, train_seconds = 8236.9, _tick = 2813, _time = 1.6546e+09)
[2022-06-07 12:41:08,898][root][INFO] - Step 14784000 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14784000, mean_episode_return = None, mean_episode_step = 2621.9, total_loss = 60.322, pg_loss = 21.646, baseline_loss = 43.076, entropy_loss = -4.4004, learner_queue_size = 32, train_seconds = 8241.9, _tick = 2814, _time = 1.6546e+09)
[2022-06-07 12:41:13,901][root][INFO] - Step 14794240 @ 2046.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 14794240, mean_episode_return = 86.875, mean_episode_step = 2848.9, total_loss = -130.47, pg_loss = -132.92, baseline_loss = 7.0276, entropy_loss = -4.5831, learner_queue_size = 32, train_seconds = 8246.9, _tick = 2816, _time = 1.6546e+09)
[2022-06-07 12:41:18,906][root][INFO] - Step 14801920 @ 1534.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 14801920, mean_episode_return = 47.253, mean_episode_step = 2319.2, total_loss = -105.68, pg_loss = -112.61, baseline_loss = 11.584, entropy_loss = -4.6536, learner_queue_size = 32, train_seconds = 8251.9, _tick = 2817, _time = 1.6546e+09)
[2022-06-07 12:41:23,910][root][INFO] - Step 14812160 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 14812160, mean_episode_return = None, mean_episode_step = 2538.5, total_loss = 70.157, pg_loss = 5.5691, baseline_loss = 69.317, entropy_loss = -4.7291, learner_queue_size = 32, train_seconds = 8256.9, _tick = 2817, _time = 1.6546e+09)
[2022-06-07 12:41:28,914][root][INFO] - Step 14819840 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 14819840, mean_episode_return = None, mean_episode_step = 2340.9, total_loss = 590.49, pg_loss = 319.92, baseline_loss = 275.21, entropy_loss = -4.6373, learner_queue_size = 32, train_seconds = 8261.9, _tick = 2818, _time = 1.6546e+09)
[2022-06-07 12:41:33,918][root][INFO] - Step 14830080 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 14830080, mean_episode_return = 29.38, mean_episode_step = 2089.9, total_loss = -96.846, pg_loss = -125.38, baseline_loss = 33.112, entropy_loss = -4.58, learner_queue_size = 32, train_seconds = 8266.9, _tick = 2820, _time = 1.6546e+09)
[2022-06-07 12:41:38,922][root][INFO] - Step 14837760 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 14837760, mean_episode_return = None, mean_episode_step = 2492.1, total_loss = 141.89, pg_loss = 97.971, baseline_loss = 48.463, entropy_loss = -4.5439, learner_queue_size = 32, train_seconds = 8271.9, _tick = 2821, _time = 1.6546e+09)
[2022-06-07 12:41:43,927][root][INFO] - Step 14848000 @ 2045.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 14848000, mean_episode_return = 21.255, mean_episode_step = 2132.4, total_loss = 32.191, pg_loss = -11.552, baseline_loss = 48.46, entropy_loss = -4.717, learner_queue_size = 32, train_seconds = 8276.9, _tick = 2824, _time = 1.6546e+09)
[2022-06-07 12:41:48,930][root][INFO] - Step 14855680 @ 1535.1 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 14855680, mean_episode_return = None, mean_episode_step = 2420.5, total_loss = 61.813, pg_loss = 31.386, baseline_loss = 35.204, entropy_loss = -4.7776, learner_queue_size = 32, train_seconds = 8281.9, _tick = 2826, _time = 1.6546e+09)
[2022-06-07 12:41:53,934][root][INFO] - Step 14865920 @ 2046.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 14865920, mean_episode_return = 32.953, mean_episode_step = 2338.6, total_loss = 126.9, pg_loss = 78.917, baseline_loss = 52.839, entropy_loss = -4.8558, learner_queue_size = 32, train_seconds = 8286.9, _tick = 2829, _time = 1.6546e+09)
[2022-06-07 12:41:58,940][root][INFO] - Step 14873600 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 14873600, mean_episode_return = None, mean_episode_step = 2210.3, total_loss = 87.596, pg_loss = 48.734, baseline_loss = 43.714, entropy_loss = -4.8516, learner_queue_size = 32, train_seconds = 8291.9, _tick = 2829, _time = 1.6546e+09)
[2022-06-07 12:42:03,942][root][INFO] - Step 14883840 @ 2047.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 14883840, mean_episode_return = 122.89, mean_episode_step = 2030.8, total_loss = 36.296, pg_loss = 20.504, baseline_loss = 20.722, entropy_loss = -4.9297, learner_queue_size = 32, train_seconds = 8296.9, _tick = 2831, _time = 1.6546e+09)
[2022-06-07 12:42:08,946][root][INFO] - Step 14891520 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14891520, mean_episode_return = None, mean_episode_step = 2700.8, total_loss = 77.458, pg_loss = 49.391, baseline_loss = 32.978, entropy_loss = -4.9108, learner_queue_size = 32, train_seconds = 8301.9, _tick = 2833, _time = 1.6546e+09)
[2022-06-07 12:42:13,950][root][INFO] - Step 14901760 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 14901760, mean_episode_return = None, mean_episode_step = 2212.3, total_loss = 34.065, pg_loss = 11.709, baseline_loss = 27.421, entropy_loss = -5.0652, learner_queue_size = 32, train_seconds = 8306.9, _tick = 2836, _time = 1.6546e+09)
[2022-06-07 12:42:18,958][root][INFO] - Step 14912000 @ 2044.7 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 14912000, mean_episode_return = 75.613, mean_episode_step = 2573.8, total_loss = 59.527, pg_loss = 27.37, baseline_loss = 37.115, entropy_loss = -4.9575, learner_queue_size = 32, train_seconds = 8312.0, _tick = 2840, _time = 1.6546e+09)
[2022-06-07 12:42:23,962][root][INFO] - Step 14919680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 14919680, mean_episode_return = 40.38, mean_episode_step = 2173.4, total_loss = -212.35, pg_loss = -223.02, baseline_loss = 15.63, entropy_loss = -4.9514, learner_queue_size = 32, train_seconds = 8317.0, _tick = 2843, _time = 1.6546e+09)
[2022-06-07 12:42:28,966][root][INFO] - Step 14929920 @ 2046.3 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 14929920, mean_episode_return = 95.206, mean_episode_step = 2164.5, total_loss = -75.651, pg_loss = -98.232, baseline_loss = 27.508, entropy_loss = -4.9273, learner_queue_size = 32, train_seconds = 8322.0, _tick = 2847, _time = 1.6546e+09)
[2022-06-07 12:42:33,970][root][INFO] - Step 14937600 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 14937600, mean_episode_return = 33.229, mean_episode_step = 2257.6, total_loss = 30.327, pg_loss = -29.843, baseline_loss = 65.209, entropy_loss = -5.039, learner_queue_size = 32, train_seconds = 8327.0, _tick = 2849, _time = 1.6546e+09)
[2022-06-07 12:42:38,976][root][INFO] - Step 14947840 @ 2045.6 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 14947840, mean_episode_return = 25.574, mean_episode_step = 2774.5, total_loss = 217.71, pg_loss = 147.09, baseline_loss = 75.731, entropy_loss = -5.121, learner_queue_size = 32, train_seconds = 8332.0, _tick = 2852, _time = 1.6546e+09)
[2022-06-07 12:42:43,978][root][INFO] - Step 14955520 @ 1535.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 14955520, mean_episode_return = 55.459, mean_episode_step = 2351.0, total_loss = 164.57, pg_loss = 41.551, baseline_loss = 128.03, entropy_loss = -5.019, learner_queue_size = 32, train_seconds = 8337.0, _tick = 2854, _time = 1.6546e+09)
[2022-06-07 12:42:48,984][root][INFO] - Step 14965760 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 14965760, mean_episode_return = 61.42, mean_episode_step = 1681.8, total_loss = 6.5471, pg_loss = -48.327, baseline_loss = 59.83, entropy_loss = -4.9565, learner_queue_size = 32, train_seconds = 8342.0, _tick = 2856, _time = 1.6546e+09)
[2022-06-07 12:42:53,990][root][INFO] - Step 14973440 @ 1534.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 14973440, mean_episode_return = None, mean_episode_step = 2241.1, total_loss = 97.715, pg_loss = 55.117, baseline_loss = 47.42, entropy_loss = -4.8218, learner_queue_size = 32, train_seconds = 8347.0, _tick = 2858, _time = 1.6546e+09)
[2022-06-07 12:42:58,994][root][INFO] - Step 14983680 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 14983680, mean_episode_return = 73.61, mean_episode_step = 2119.5, total_loss = 228.2, pg_loss = 144.85, baseline_loss = 88.211, entropy_loss = -4.8567, learner_queue_size = 32, train_seconds = 8352.0, _tick = 2861, _time = 1.6546e+09)
[2022-06-07 12:43:03,998][root][INFO] - Step 14991360 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 14991360, mean_episode_return = 81.991, mean_episode_step = 2266.4, total_loss = 250.52, pg_loss = 158.08, baseline_loss = 97.31, entropy_loss = -4.8772, learner_queue_size = 32, train_seconds = 8357.0, _tick = 2862, _time = 1.6546e+09)
[2022-06-07 12:43:09,002][root][INFO] - Step 15001600 @ 2046.3 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 15001600, mean_episode_return = 97.816, mean_episode_step = 2296.9, total_loss = 157.39, pg_loss = 69.013, baseline_loss = 93.34, entropy_loss = -4.9658, learner_queue_size = 32, train_seconds = 8362.0, _tick = 2866, _time = 1.6546e+09)
[2022-06-07 12:43:14,005][root][INFO] - Step 15009280 @ 1535.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 15009280, mean_episode_return = 18.51, mean_episode_step = 2242.5, total_loss = -90.405, pg_loss = -124.09, baseline_loss = 38.659, entropy_loss = -4.9773, learner_queue_size = 32, train_seconds = 8367.0, _tick = 2868, _time = 1.6546e+09)
[2022-06-07 12:43:19,023][root][INFO] - Step 15019520 @ 2040.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 15019520, mean_episode_return = 17.57, mean_episode_step = 2477.7, total_loss = 122.16, pg_loss = 33.512, baseline_loss = 93.658, entropy_loss = -5.0077, learner_queue_size = 32, train_seconds = 8372.0, _tick = 2871, _time = 1.6546e+09)
[2022-06-07 12:43:24,026][root][INFO] - Step 15027200 @ 1535.0 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 15027200, mean_episode_return = 69.492, mean_episode_step = 1833.8, total_loss = 68.257, pg_loss = -9.8552, baseline_loss = 83.142, entropy_loss = -5.0301, learner_queue_size = 32, train_seconds = 8377.0, _tick = 2872, _time = 1.6546e+09)
[2022-06-07 12:43:29,032][root][INFO] - Step 15037440 @ 2045.7 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 15037440, mean_episode_return = 22.301, mean_episode_step = 2409.7, total_loss = 210.14, pg_loss = 114.95, baseline_loss = 100.19, entropy_loss = -5.0007, learner_queue_size = 32, train_seconds = 8382.0, _tick = 2876, _time = 1.6546e+09)
[2022-06-07 12:43:34,037][root][INFO] - Step 15045120 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15045120, mean_episode_return = 87.351, mean_episode_step = 1955.3, total_loss = -93.794, pg_loss = -107.15, baseline_loss = 18.444, entropy_loss = -5.0904, learner_queue_size = 32, train_seconds = 8387.0, _tick = 2878, _time = 1.6546e+09)
[2022-06-07 12:43:39,043][root][INFO] - Step 15055360 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 15055360, mean_episode_return = -18.101, mean_episode_step = 2606.3, total_loss = -51.125, pg_loss = -77.43, baseline_loss = 31.353, entropy_loss = -5.0478, learner_queue_size = 32, train_seconds = 8392.0, _tick = 2882, _time = 1.6546e+09)
[2022-06-07 12:43:44,046][root][INFO] - Step 15065600 @ 2046.9 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 15065600, mean_episode_return = None, mean_episode_step = 2006.1, total_loss = 98.416, pg_loss = 39.278, baseline_loss = 64.185, entropy_loss = -5.0472, learner_queue_size = 32, train_seconds = 8397.0, _tick = 2885, _time = 1.6546e+09)
[2022-06-07 12:43:49,050][root][INFO] - Step 15073280 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 15073280, mean_episode_return = None, mean_episode_step = 2382.3, total_loss = 42.626, pg_loss = 17.637, baseline_loss = 29.965, entropy_loss = -4.9763, learner_queue_size = 32, train_seconds = 8402.0, _tick = 2886, _time = 1.6546e+09)
[2022-06-07 12:43:54,056][root][INFO] - Step 15083520 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 15083520, mean_episode_return = 31.621, mean_episode_step = 2185.8, total_loss = 34.22, pg_loss = -14.615, baseline_loss = 53.9, entropy_loss = -5.0644, learner_queue_size = 32, train_seconds = 8407.1, _tick = 2889, _time = 1.6546e+09)
[2022-06-07 12:43:59,058][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 12:43:59,239][root][INFO] - Step 15091200 @ 1535.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 15091200, mean_episode_return = 4.2197, mean_episode_step = 2060.8, total_loss = 56.827, pg_loss = 10.16, baseline_loss = 51.643, entropy_loss = -4.9759, learner_queue_size = 32, train_seconds = 8412.1, _tick = 2890, _time = 1.6546e+09)
[2022-06-07 12:44:04,242][root][INFO] - Step 15101440 @ 1975.3 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 15101440, mean_episode_return = 23.23, mean_episode_step = 1970.5, total_loss = 310.19, pg_loss = 187.44, baseline_loss = 127.69, entropy_loss = -4.9361, learner_queue_size = 32, train_seconds = 8417.2, _tick = 2894, _time = 1.6546e+09)
[2022-06-07 12:44:09,248][root][INFO] - Step 15109120 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15109120, mean_episode_return = 7.4597, mean_episode_step = 1676.7, total_loss = 306.78, pg_loss = 169.52, baseline_loss = 142.22, entropy_loss = -4.9549, learner_queue_size = 32, train_seconds = 8422.2, _tick = 2896, _time = 1.6546e+09)
[2022-06-07 12:44:14,254][root][INFO] - Step 15119360 @ 2045.6 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 15119360, mean_episode_return = 73.028, mean_episode_step = 1866.6, total_loss = -37.043, pg_loss = -73.919, baseline_loss = 41.891, entropy_loss = -5.0151, learner_queue_size = 32, train_seconds = 8427.2, _tick = 2899, _time = 1.6546e+09)
[2022-06-07 12:44:19,258][root][INFO] - Step 15129600 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 15129600, mean_episode_return = 135.87, mean_episode_step = 2043.3, total_loss = -13.067, pg_loss = -46.07, baseline_loss = 37.921, entropy_loss = -4.9176, learner_queue_size = 32, train_seconds = 8432.3, _tick = 2901, _time = 1.6546e+09)
[2022-06-07 12:44:24,262][root][INFO] - Step 15137280 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 15137280, mean_episode_return = None, mean_episode_step = 1751.3, total_loss = -125.92, pg_loss = -128.13, baseline_loss = 7.0238, entropy_loss = -4.8127, learner_queue_size = 32, train_seconds = 8437.3, _tick = 2902, _time = 1.6546e+09)
[2022-06-07 12:44:29,266][root][INFO] - Step 15147520 @ 2046.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 15147520, mean_episode_return = 30.631, mean_episode_step = 2333.8, total_loss = -197.23, pg_loss = -200.7, baseline_loss = 8.325, entropy_loss = -4.8526, learner_queue_size = 32, train_seconds = 8442.3, _tick = 2906, _time = 1.6546e+09)
[2022-06-07 12:44:34,270][root][INFO] - Step 15155200 @ 1534.8 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 15155200, mean_episode_return = None, mean_episode_step = 1556.5, total_loss = -133.88, pg_loss = -132.28, baseline_loss = 3.2317, entropy_loss = -4.8359, learner_queue_size = 32, train_seconds = 8447.3, _tick = 2907, _time = 1.6546e+09)
[2022-06-07 12:44:39,274][root][INFO] - Step 15165440 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 15165440, mean_episode_return = None, mean_episode_step = 2022.9, total_loss = 188.25, pg_loss = 134.29, baseline_loss = 58.788, entropy_loss = -4.8259, learner_queue_size = 32, train_seconds = 8452.3, _tick = 2908, _time = 1.6546e+09)
[2022-06-07 12:44:44,278][root][INFO] - Step 15173120 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 15173120, mean_episode_return = 70.432, mean_episode_step = 1961.9, total_loss = 67.954, pg_loss = 12.479, baseline_loss = 60.216, entropy_loss = -4.7404, learner_queue_size = 32, train_seconds = 8457.3, _tick = 2910, _time = 1.6546e+09)
[2022-06-07 12:44:49,284][root][INFO] - Step 15183360 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 15183360, mean_episode_return = None, mean_episode_step = 1473.4, total_loss = -36.299, pg_loss = -49.661, baseline_loss = 18.163, entropy_loss = -4.8014, learner_queue_size = 32, train_seconds = 8462.3, _tick = 2911, _time = 1.6546e+09)
[2022-06-07 12:44:54,290][root][INFO] - Step 15191040 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15191040, mean_episode_return = 94.059, mean_episode_step = 2259.3, total_loss = -105.2, pg_loss = -123.65, baseline_loss = 23.201, entropy_loss = -4.7529, learner_queue_size = 32, train_seconds = 8467.3, _tick = 2913, _time = 1.6546e+09)
[2022-06-07 12:44:59,294][root][INFO] - Step 15201280 @ 2046.5 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 15201280, mean_episode_return = None, mean_episode_step = 2272.7, total_loss = 117.91, pg_loss = 62.129, baseline_loss = 60.532, entropy_loss = -4.7463, learner_queue_size = 32, train_seconds = 8472.3, _tick = 2915, _time = 1.6546e+09)
[2022-06-07 12:45:04,298][root][INFO] - Step 15208960 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 15208960, mean_episode_return = 2.8, mean_episode_step = 1630.1, total_loss = -67.609, pg_loss = -80.197, baseline_loss = 17.363, entropy_loss = -4.7748, learner_queue_size = 32, train_seconds = 8477.3, _tick = 2917, _time = 1.6546e+09)
[2022-06-07 12:45:09,302][root][INFO] - Step 15219200 @ 2046.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 15219200, mean_episode_return = None, mean_episode_step = 1852.5, total_loss = -138.18, pg_loss = -137.05, baseline_loss = 3.6782, entropy_loss = -4.8075, learner_queue_size = 32, train_seconds = 8482.3, _tick = 2918, _time = 1.6546e+09)
[2022-06-07 12:45:14,308][root][INFO] - Step 15229440 @ 2045.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 15229440, mean_episode_return = 27.7, mean_episode_step = 1684.6, total_loss = 23.771, pg_loss = -24.575, baseline_loss = 53.151, entropy_loss = -4.8059, learner_queue_size = 32, train_seconds = 8487.3, _tick = 2920, _time = 1.6546e+09)
[2022-06-07 12:45:19,310][root][INFO] - Step 15237120 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15237120, mean_episode_return = None, mean_episode_step = 2709.3, total_loss = 93.102, pg_loss = 39.392, baseline_loss = 58.539, entropy_loss = -4.8286, learner_queue_size = 32, train_seconds = 8492.3, _tick = 2920, _time = 1.6546e+09)
[2022-06-07 12:45:24,316][root][INFO] - Step 15244800 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 15244800, mean_episode_return = None, mean_episode_step = 1569.0, total_loss = 0.95647, pg_loss = -20.254, baseline_loss = 25.872, entropy_loss = -4.6616, learner_queue_size = 32, train_seconds = 8497.3, _tick = 2920, _time = 1.6546e+09)
[2022-06-07 12:45:29,323][root][INFO] - Step 15255040 @ 2045.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15255040, mean_episode_return = None, mean_episode_step = 2075.7, total_loss = 243.26, pg_loss = 147.73, baseline_loss = 100.13, entropy_loss = -4.6008, learner_queue_size = 32, train_seconds = 8502.3, _tick = 2921, _time = 1.6546e+09)
[2022-06-07 12:45:34,326][root][INFO] - Step 15265280 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 15265280, mean_episode_return = None, mean_episode_step = 2126.0, total_loss = -138.78, pg_loss = -137.4, baseline_loss = 3.2894, entropy_loss = -4.6652, learner_queue_size = 32, train_seconds = 8507.3, _tick = 2922, _time = 1.6546e+09)
[2022-06-07 12:45:39,332][root][INFO] - Step 15272960 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15272960, mean_episode_return = 46.63, mean_episode_step = 1731.5, total_loss = 98.438, pg_loss = 30.407, baseline_loss = 72.831, entropy_loss = -4.7996, learner_queue_size = 32, train_seconds = 8512.3, _tick = 2924, _time = 1.6546e+09)
[2022-06-07 12:45:44,338][root][INFO] - Step 15283200 @ 2045.7 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 15283200, mean_episode_return = 90.398, mean_episode_step = 2101.7, total_loss = -36.036, pg_loss = -67.251, baseline_loss = 36.112, entropy_loss = -4.8971, learner_queue_size = 32, train_seconds = 8517.3, _tick = 2925, _time = 1.6546e+09)
[2022-06-07 12:45:49,344][root][INFO] - Step 15290880 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15290880, mean_episode_return = None, mean_episode_step = 2716.5, total_loss = 218.09, pg_loss = 131.81, baseline_loss = 91.049, entropy_loss = -4.7707, learner_queue_size = 32, train_seconds = 8522.3, _tick = 2925, _time = 1.6546e+09)
[2022-06-07 12:45:54,346][root][INFO] - Step 15301120 @ 2047.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 15301120, mean_episode_return = None, mean_episode_step = 2084.7, total_loss = 179.26, pg_loss = 111.93, baseline_loss = 72.199, entropy_loss = -4.8668, learner_queue_size = 32, train_seconds = 8527.3, _tick = 2926, _time = 1.6546e+09)
[2022-06-07 12:45:59,350][root][INFO] - Step 15308800 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15308800, mean_episode_return = None, mean_episode_step = 2351.0, total_loss = 8.9182, pg_loss = -20.204, baseline_loss = 34.063, entropy_loss = -4.9408, learner_queue_size = 32, train_seconds = 8532.3, _tick = 2927, _time = 1.6546e+09)
[2022-06-07 12:46:04,352][root][INFO] - Step 15319040 @ 2047.3 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 15319040, mean_episode_return = 52.032, mean_episode_step = 2168.0, total_loss = -54.621, pg_loss = -77.993, baseline_loss = 28.427, entropy_loss = -5.0542, learner_queue_size = 32, train_seconds = 8537.3, _tick = 2928, _time = 1.6546e+09)
[2022-06-07 12:46:09,357][root][INFO] - Step 15329280 @ 2045.9 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 15329280, mean_episode_return = None, mean_episode_step = 2690.5, total_loss = 129.37, pg_loss = 95.801, baseline_loss = 38.609, entropy_loss = -5.043, learner_queue_size = 32, train_seconds = 8542.4, _tick = 2928, _time = 1.6546e+09)
[2022-06-07 12:46:14,362][root][INFO] - Step 15336960 @ 1534.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 15336960, mean_episode_return = 58.071, mean_episode_step = 2400.2, total_loss = -80.669, pg_loss = -88.677, baseline_loss = 13.187, entropy_loss = -5.1786, learner_queue_size = 32, train_seconds = 8547.4, _tick = 2930, _time = 1.6546e+09)
[2022-06-07 12:46:19,367][root][INFO] - Step 15347200 @ 2046.0 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 15347200, mean_episode_return = None, mean_episode_step = 2597.8, total_loss = 201.41, pg_loss = 143.66, baseline_loss = 62.804, entropy_loss = -5.0584, learner_queue_size = 32, train_seconds = 8552.4, _tick = 2932, _time = 1.6546e+09)
[2022-06-07 12:46:24,370][root][INFO] - Step 15354880 @ 1535.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15354880, mean_episode_return = None, mean_episode_step = 2312.2, total_loss = -0.25675, pg_loss = -11.968, baseline_loss = 16.825, entropy_loss = -5.1137, learner_queue_size = 32, train_seconds = 8557.4, _tick = 2932, _time = 1.6546e+09)
[2022-06-07 12:46:29,376][root][INFO] - Step 15365120 @ 2045.6 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 15365120, mean_episode_return = None, mean_episode_step = 2100.0, total_loss = 115.34, pg_loss = 60.699, baseline_loss = 59.773, entropy_loss = -5.1372, learner_queue_size = 32, train_seconds = 8562.4, _tick = 2933, _time = 1.6546e+09)
[2022-06-07 12:46:34,382][root][INFO] - Step 15372800 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 15372800, mean_episode_return = 78.142, mean_episode_step = 2446.6, total_loss = -27.069, pg_loss = -62.236, baseline_loss = 40.246, entropy_loss = -5.0784, learner_queue_size = 32, train_seconds = 8567.4, _tick = 2935, _time = 1.6546e+09)
[2022-06-07 12:46:39,389][root][INFO] - Step 15383040 @ 2045.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 15383040, mean_episode_return = 106.01, mean_episode_step = 2247.4, total_loss = 23.68, pg_loss = -31.36, baseline_loss = 59.862, entropy_loss = -4.8224, learner_queue_size = 32, train_seconds = 8572.4, _tick = 2939, _time = 1.6546e+09)
[2022-06-07 12:46:44,394][root][INFO] - Step 15390720 @ 1534.3 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 15390720, mean_episode_return = 52.19, mean_episode_step = 2546.1, total_loss = 30.276, pg_loss = 13.243, baseline_loss = 21.671, entropy_loss = -4.6386, learner_queue_size = 32, train_seconds = 8577.4, _tick = 2941, _time = 1.6546e+09)
[2022-06-07 12:46:49,398][root][INFO] - Step 15400960 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 15400960, mean_episode_return = None, mean_episode_step = 1988.4, total_loss = -100.49, pg_loss = -100.66, baseline_loss = 4.9169, entropy_loss = -4.7483, learner_queue_size = 32, train_seconds = 8582.4, _tick = 2944, _time = 1.6546e+09)
[2022-06-07 12:46:54,402][root][INFO] - Step 15408640 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 15408640, mean_episode_return = 58.302, mean_episode_step = 2637.3, total_loss = -44.949, pg_loss = -80.523, baseline_loss = 40.355, entropy_loss = -4.7814, learner_queue_size = 32, train_seconds = 8587.4, _tick = 2946, _time = 1.6546e+09)
[2022-06-07 12:46:59,406][root][INFO] - Step 15418880 @ 2046.4 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 15418880, mean_episode_return = 34.547, mean_episode_step = 2254.0, total_loss = -86.887, pg_loss = -147.88, baseline_loss = 65.75, entropy_loss = -4.7575, learner_queue_size = 32, train_seconds = 8592.4, _tick = 2947, _time = 1.6546e+09)
[2022-06-07 12:47:04,410][root][INFO] - Step 15426560 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15426560, mean_episode_return = None, mean_episode_step = 2309.4, total_loss = 31.246, pg_loss = 2.106, baseline_loss = 33.876, entropy_loss = -4.7356, learner_queue_size = 32, train_seconds = 8597.4, _tick = 2948, _time = 1.6546e+09)
[2022-06-07 12:47:09,416][root][INFO] - Step 15436800 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15436800, mean_episode_return = 76.12, mean_episode_step = 2209.6, total_loss = -18.152, pg_loss = -39.895, baseline_loss = 26.601, entropy_loss = -4.8582, learner_queue_size = 32, train_seconds = 8602.4, _tick = 2951, _time = 1.6546e+09)
[2022-06-07 12:47:14,421][root][INFO] - Step 15444480 @ 1534.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15444480, mean_episode_return = 151.39, mean_episode_step = 2369.2, total_loss = 82.421, pg_loss = 29.387, baseline_loss = 57.803, entropy_loss = -4.7685, learner_queue_size = 32, train_seconds = 8607.4, _tick = 2954, _time = 1.6546e+09)
[2022-06-07 12:47:19,426][root][INFO] - Step 15454720 @ 2045.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15454720, mean_episode_return = 7.5396, mean_episode_step = 2242.8, total_loss = -80.543, pg_loss = -95.496, baseline_loss = 19.738, entropy_loss = -4.7845, learner_queue_size = 32, train_seconds = 8612.4, _tick = 2957, _time = 1.6546e+09)
[2022-06-07 12:47:24,430][root][INFO] - Step 15462400 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15462400, mean_episode_return = 52.912, mean_episode_step = 2210.4, total_loss = 113.93, pg_loss = 65.53, baseline_loss = 53.312, entropy_loss = -4.9078, learner_queue_size = 32, train_seconds = 8617.4, _tick = 2959, _time = 1.6546e+09)
[2022-06-07 12:47:29,436][root][INFO] - Step 15472640 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 15472640, mean_episode_return = None, mean_episode_step = 2115.3, total_loss = 51.33, pg_loss = 29.414, baseline_loss = 26.679, entropy_loss = -4.7629, learner_queue_size = 32, train_seconds = 8622.4, _tick = 2962, _time = 1.6546e+09)
[2022-06-07 12:47:34,442][root][INFO] - Step 15480320 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 15480320, mean_episode_return = 161.76, mean_episode_step = 1861.4, total_loss = 46.79, pg_loss = -4.7305, baseline_loss = 56.324, entropy_loss = -4.8041, learner_queue_size = 32, train_seconds = 8627.4, _tick = 2965, _time = 1.6546e+09)
[2022-06-07 12:47:39,446][root][INFO] - Step 15490560 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15490560, mean_episode_return = 18.926, mean_episode_step = 1759.1, total_loss = 54.725, pg_loss = 21.036, baseline_loss = 38.469, entropy_loss = -4.7803, learner_queue_size = 32, train_seconds = 8632.4, _tick = 2969, _time = 1.6546e+09)
[2022-06-07 12:47:44,452][root][INFO] - Step 15498240 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 15498240, mean_episode_return = 81.61, mean_episode_step = 1797.6, total_loss = 12.26, pg_loss = -33.428, baseline_loss = 50.489, entropy_loss = -4.8009, learner_queue_size = 32, train_seconds = 8637.4, _tick = 2971, _time = 1.6546e+09)
[2022-06-07 12:47:49,458][root][INFO] - Step 15508480 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 15508480, mean_episode_return = None, mean_episode_step = 1744.2, total_loss = 151.51, pg_loss = 95.922, baseline_loss = 60.297, entropy_loss = -4.7128, learner_queue_size = 32, train_seconds = 8642.5, _tick = 2973, _time = 1.6546e+09)
[2022-06-07 12:47:54,464][root][INFO] - Step 15516160 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 15516160, mean_episode_return = 94.936, mean_episode_step = 2473.3, total_loss = -11.448, pg_loss = -55.126, baseline_loss = 48.538, entropy_loss = -4.8605, learner_queue_size = 32, train_seconds = 8647.5, _tick = 2976, _time = 1.6546e+09)
[2022-06-07 12:47:59,470][root][INFO] - Step 15526400 @ 2045.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 15526400, mean_episode_return = 17.15, mean_episode_step = 2131.5, total_loss = -164.55, pg_loss = -164.35, baseline_loss = 4.6591, entropy_loss = -4.8563, learner_queue_size = 32, train_seconds = 8652.5, _tick = 2980, _time = 1.6546e+09)
[2022-06-07 12:48:04,474][root][INFO] - Step 15534080 @ 1534.9 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 15534080, mean_episode_return = -5.94, mean_episode_step = 1783.2, total_loss = 103.46, pg_loss = 28.169, baseline_loss = 80.228, entropy_loss = -4.935, learner_queue_size = 32, train_seconds = 8657.5, _tick = 2983, _time = 1.6546e+09)
[2022-06-07 12:48:09,478][root][INFO] - Step 15544320 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 15544320, mean_episode_return = 59.748, mean_episode_step = 1982.5, total_loss = 278.39, pg_loss = 175.17, baseline_loss = 108.18, entropy_loss = -4.9697, learner_queue_size = 32, train_seconds = 8662.5, _tick = 2986, _time = 1.6546e+09)
[2022-06-07 12:48:14,484][root][INFO] - Step 15552000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15552000, mean_episode_return = 88.587, mean_episode_step = 1503.8, total_loss = 24.682, pg_loss = -24.831, baseline_loss = 54.51, entropy_loss = -4.997, learner_queue_size = 32, train_seconds = 8667.5, _tick = 2988, _time = 1.6546e+09)
[2022-06-07 12:48:19,490][root][INFO] - Step 15562240 @ 2045.6 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 15562240, mean_episode_return = None, mean_episode_step = 2180.5, total_loss = -106.46, pg_loss = -109.98, baseline_loss = 8.4512, entropy_loss = -4.9231, learner_queue_size = 32, train_seconds = 8672.5, _tick = 2989, _time = 1.6546e+09)
[2022-06-07 12:48:24,496][root][INFO] - Step 15569920 @ 1534.1 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 15569920, mean_episode_return = 103.92, mean_episode_step = 1761.4, total_loss = -88.313, pg_loss = -113.72, baseline_loss = 30.436, entropy_loss = -5.0331, learner_queue_size = 32, train_seconds = 8677.5, _tick = 2992, _time = 1.6546e+09)
[2022-06-07 12:48:29,502][root][INFO] - Step 15580160 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 15580160, mean_episode_return = None, mean_episode_step = 1978.7, total_loss = 111.89, pg_loss = 75.948, baseline_loss = 40.927, entropy_loss = -4.9826, learner_queue_size = 32, train_seconds = 8682.5, _tick = 2994, _time = 1.6546e+09)
[2022-06-07 12:48:34,506][root][INFO] - Step 15587840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15587840, mean_episode_return = 113.82, mean_episode_step = 1562.7, total_loss = 151.46, pg_loss = 77.499, baseline_loss = 78.845, entropy_loss = -4.8798, learner_queue_size = 32, train_seconds = 8687.5, _tick = 2995, _time = 1.6546e+09)
[2022-06-07 12:48:39,512][root][INFO] - Step 15598080 @ 2045.5 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 15598080, mean_episode_return = None, mean_episode_step = 1541.1, total_loss = 332.31, pg_loss = 221.35, baseline_loss = 115.84, entropy_loss = -4.8885, learner_queue_size = 32, train_seconds = 8692.5, _tick = 2997, _time = 1.6546e+09)
[2022-06-07 12:48:44,518][root][INFO] - Step 15605760 @ 1534.1 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 15605760, mean_episode_return = 112.67, mean_episode_step = 1878.7, total_loss = 97.483, pg_loss = 48.694, baseline_loss = 53.65, entropy_loss = -4.861, learner_queue_size = 32, train_seconds = 8697.5, _tick = 2998, _time = 1.6546e+09)
[2022-06-07 12:48:49,522][root][INFO] - Step 15616000 @ 2046.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 15616000, mean_episode_return = None, mean_episode_step = 1639.9, total_loss = 2.381, pg_loss = -40.459, baseline_loss = 47.646, entropy_loss = -4.8062, learner_queue_size = 32, train_seconds = 8702.5, _tick = 3000, _time = 1.6546e+09)
[2022-06-07 12:48:54,526][root][INFO] - Step 15623680 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 15623680, mean_episode_return = None, mean_episode_step = 1860.3, total_loss = 154.77, pg_loss = 105.33, baseline_loss = 54.413, entropy_loss = -4.9742, learner_queue_size = 32, train_seconds = 8707.5, _tick = 3000, _time = 1.6546e+09)
[2022-06-07 12:48:59,530][root][INFO] - Step 15633920 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15633920, mean_episode_return = None, mean_episode_step = 2162.9, total_loss = -149.89, pg_loss = -150.69, baseline_loss = 5.775, entropy_loss = -4.9774, learner_queue_size = 32, train_seconds = 8712.5, _tick = 3001, _time = 1.6546e+09)
[2022-06-07 12:49:04,534][root][INFO] - Step 15641600 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 15641600, mean_episode_return = 28.97, mean_episode_step = 1967.9, total_loss = 101.3, pg_loss = 24.708, baseline_loss = 81.608, entropy_loss = -5.0159, learner_queue_size = 32, train_seconds = 8717.5, _tick = 3002, _time = 1.6546e+09)
[2022-06-07 12:49:09,540][root][INFO] - Step 15651840 @ 2045.5 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 15651840, mean_episode_return = 141.7, mean_episode_step = 2021.9, total_loss = 31.015, pg_loss = -16.808, baseline_loss = 52.829, entropy_loss = -5.0061, learner_queue_size = 32, train_seconds = 8722.5, _tick = 3003, _time = 1.6546e+09)
[2022-06-07 12:49:14,546][root][INFO] - Step 15659520 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 15659520, mean_episode_return = None, mean_episode_step = 2409.6, total_loss = -37.077, pg_loss = -55.179, baseline_loss = 23.135, entropy_loss = -5.0333, learner_queue_size = 32, train_seconds = 8727.5, _tick = 3004, _time = 1.6546e+09)
[2022-06-07 12:49:19,550][root][INFO] - Step 15669760 @ 2046.4 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 15669760, mean_episode_return = 19.635, mean_episode_step = 1818.5, total_loss = 295.77, pg_loss = 196.15, baseline_loss = 104.72, entropy_loss = -5.1081, learner_queue_size = 32, train_seconds = 8732.5, _tick = 3007, _time = 1.6546e+09)
[2022-06-07 12:49:24,554][root][INFO] - Step 15677440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 15677440, mean_episode_return = 78.494, mean_episode_step = 2408.2, total_loss = -37.456, pg_loss = -85.613, baseline_loss = 53.241, entropy_loss = -5.0844, learner_queue_size = 32, train_seconds = 8737.5, _tick = 3008, _time = 1.6546e+09)
[2022-06-07 12:49:29,558][root][INFO] - Step 15687680 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15687680, mean_episode_return = 140.66, mean_episode_step = 2062.5, total_loss = 116.41, pg_loss = 46.22, baseline_loss = 75.229, entropy_loss = -5.043, learner_queue_size = 32, train_seconds = 8742.6, _tick = 3010, _time = 1.6546e+09)
[2022-06-07 12:49:34,562][root][INFO] - Step 15697920 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 15697920, mean_episode_return = 44.911, mean_episode_step = 1539.6, total_loss = -218.5, pg_loss = -236.49, baseline_loss = 23.123, entropy_loss = -5.1327, learner_queue_size = 32, train_seconds = 8747.6, _tick = 3012, _time = 1.6546e+09)
[2022-06-07 12:49:39,568][root][INFO] - Step 15705600 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15705600, mean_episode_return = 47.416, mean_episode_step = 2068.9, total_loss = 133.35, pg_loss = 52.432, baseline_loss = 86.071, entropy_loss = -5.1506, learner_queue_size = 32, train_seconds = 8752.6, _tick = 3013, _time = 1.6546e+09)
[2022-06-07 12:49:44,574][root][INFO] - Step 15715840 @ 2045.7 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 15715840, mean_episode_return = None, mean_episode_step = 2164.6, total_loss = 60.024, pg_loss = 6.1121, baseline_loss = 58.948, entropy_loss = -5.0364, learner_queue_size = 32, train_seconds = 8757.6, _tick = 3014, _time = 1.6546e+09)
[2022-06-07 12:49:49,578][root][INFO] - Step 15723520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15723520, mean_episode_return = None, mean_episode_step = 2340.0, total_loss = 251.74, pg_loss = 168.2, baseline_loss = 88.589, entropy_loss = -5.0456, learner_queue_size = 32, train_seconds = 8762.6, _tick = 3015, _time = 1.6546e+09)
[2022-06-07 12:49:54,584][root][INFO] - Step 15733760 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15733760, mean_episode_return = None, mean_episode_step = 2145.8, total_loss = 114.02, pg_loss = 73.128, baseline_loss = 45.93, entropy_loss = -5.0434, learner_queue_size = 32, train_seconds = 8767.6, _tick = 3017, _time = 1.6546e+09)
[2022-06-07 12:49:59,590][root][INFO] - Step 15741440 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 15741440, mean_episode_return = None, mean_episode_step = 1896.6, total_loss = -55.439, pg_loss = -63.988, baseline_loss = 13.462, entropy_loss = -4.9129, learner_queue_size = 32, train_seconds = 8772.6, _tick = 3018, _time = 1.6546e+09)
[2022-06-07 12:50:04,596][root][INFO] - Step 15751680 @ 2045.5 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 15751680, mean_episode_return = 48.941, mean_episode_step = 2586.3, total_loss = -232.52, pg_loss = -253.77, baseline_loss = 26.236, entropy_loss = -4.9804, learner_queue_size = 32, train_seconds = 8777.6, _tick = 3019, _time = 1.6546e+09)
[2022-06-07 12:50:09,598][root][INFO] - Step 15759360 @ 1535.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 15759360, mean_episode_return = 80.069, mean_episode_step = 1674.4, total_loss = -130.71, pg_loss = -147.52, baseline_loss = 21.915, entropy_loss = -5.1101, learner_queue_size = 32, train_seconds = 8782.6, _tick = 3022, _time = 1.6546e+09)
[2022-06-07 12:50:14,602][root][INFO] - Step 15769600 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 15769600, mean_episode_return = 31.633, mean_episode_step = 2537.5, total_loss = -42.141, pg_loss = -76.903, baseline_loss = 39.988, entropy_loss = -5.2255, learner_queue_size = 32, train_seconds = 8787.6, _tick = 3023, _time = 1.6546e+09)
[2022-06-07 12:50:19,650][root][INFO] - Step 15777280 @ 1521.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15777280, mean_episode_return = 61.974, mean_episode_step = 2206.6, total_loss = 187.91, pg_loss = 83.384, baseline_loss = 109.69, entropy_loss = -5.1568, learner_queue_size = 32, train_seconds = 8792.6, _tick = 3024, _time = 1.6546e+09)
[2022-06-07 12:50:24,656][root][INFO] - Step 15787520 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 15787520, mean_episode_return = 57.971, mean_episode_step = 1673.4, total_loss = 6.3736, pg_loss = -37.585, baseline_loss = 49.191, entropy_loss = -5.2323, learner_queue_size = 32, train_seconds = 8797.7, _tick = 3026, _time = 1.6546e+09)
[2022-06-07 12:50:29,662][root][INFO] - Step 15795200 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 15795200, mean_episode_return = 78.616, mean_episode_step = 2258.5, total_loss = -187.5, pg_loss = -199.38, baseline_loss = 17.079, entropy_loss = -5.1971, learner_queue_size = 32, train_seconds = 8802.7, _tick = 3028, _time = 1.6546e+09)
[2022-06-07 12:50:34,666][root][INFO] - Step 15805440 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 15805440, mean_episode_return = 38.96, mean_episode_step = 1727.5, total_loss = 132.34, pg_loss = 75.307, baseline_loss = 62.221, entropy_loss = -5.1887, learner_queue_size = 32, train_seconds = 8807.7, _tick = 3031, _time = 1.6546e+09)
[2022-06-07 12:50:39,670][root][INFO] - Step 15813120 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 15813120, mean_episode_return = None, mean_episode_step = 1718.0, total_loss = 17.366, pg_loss = 5.7208, baseline_loss = 16.723, entropy_loss = -5.0775, learner_queue_size = 32, train_seconds = 8812.7, _tick = 3032, _time = 1.6546e+09)
[2022-06-07 12:50:44,674][root][INFO] - Step 15823360 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 15823360, mean_episode_return = 50.518, mean_episode_step = 2708.6, total_loss = -35.394, pg_loss = -59.976, baseline_loss = 29.756, entropy_loss = -5.1735, learner_queue_size = 32, train_seconds = 8817.7, _tick = 3035, _time = 1.6546e+09)
[2022-06-07 12:50:49,678][root][INFO] - Step 15831040 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 15831040, mean_episode_return = None, mean_episode_step = 2239.7, total_loss = -118.95, pg_loss = -124.56, baseline_loss = 10.752, entropy_loss = -5.1428, learner_queue_size = 32, train_seconds = 8822.7, _tick = 3036, _time = 1.6546e+09)
[2022-06-07 12:50:54,682][root][INFO] - Step 15841280 @ 2046.3 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 15841280, mean_episode_return = None, mean_episode_step = 2094.0, total_loss = 101.2, pg_loss = 59.745, baseline_loss = 46.642, entropy_loss = -5.184, learner_queue_size = 32, train_seconds = 8827.7, _tick = 3037, _time = 1.6546e+09)
[2022-06-07 12:50:59,686][root][INFO] - Step 15848960 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 15848960, mean_episode_return = 60.791, mean_episode_step = 1889.7, total_loss = 82.3, pg_loss = 26.57, baseline_loss = 60.937, entropy_loss = -5.206, learner_queue_size = 32, train_seconds = 8832.7, _tick = 3039, _time = 1.6546e+09)
[2022-06-07 12:51:04,692][root][INFO] - Step 15859200 @ 2045.5 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 15859200, mean_episode_return = None, mean_episode_step = 2251.0, total_loss = 53.65, pg_loss = 42.654, baseline_loss = 16.144, entropy_loss = -5.1476, learner_queue_size = 32, train_seconds = 8837.7, _tick = 3042, _time = 1.6546e+09)
[2022-06-07 12:51:09,698][root][INFO] - Step 15869440 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 15869440, mean_episode_return = None, mean_episode_step = 2303.5, total_loss = 365.86, pg_loss = 251.71, baseline_loss = 119.31, entropy_loss = -5.1601, learner_queue_size = 32, train_seconds = 8842.7, _tick = 3045, _time = 1.6546e+09)
[2022-06-07 12:51:14,704][root][INFO] - Step 15877120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 15877120, mean_episode_return = None, mean_episode_step = 1924.5, total_loss = 137.48, pg_loss = 85.981, baseline_loss = 56.612, entropy_loss = -5.1141, learner_queue_size = 32, train_seconds = 8847.7, _tick = 3046, _time = 1.6546e+09)
[2022-06-07 12:51:19,710][root][INFO] - Step 15884800 @ 1534.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 15884800, mean_episode_return = 77.338, mean_episode_step = 2532.5, total_loss = -133.02, pg_loss = -135.33, baseline_loss = 7.4501, entropy_loss = -5.132, learner_queue_size = 32, train_seconds = 8852.7, _tick = 3048, _time = 1.6546e+09)
[2022-06-07 12:51:24,714][root][INFO] - Step 15895040 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 15895040, mean_episode_return = 101.89, mean_episode_step = 1719.9, total_loss = 102.65, pg_loss = 53.748, baseline_loss = 54.031, entropy_loss = -5.13, learner_queue_size = 32, train_seconds = 8857.7, _tick = 3050, _time = 1.6546e+09)
[2022-06-07 12:51:29,720][root][INFO] - Step 15905280 @ 2045.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 15905280, mean_episode_return = None, mean_episode_step = 2299.9, total_loss = -71.076, pg_loss = -74.61, baseline_loss = 8.6545, entropy_loss = -5.1205, learner_queue_size = 32, train_seconds = 8862.7, _tick = 3050, _time = 1.6546e+09)
[2022-06-07 12:51:34,726][root][INFO] - Step 15912960 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 15912960, mean_episode_return = None, mean_episode_step = 2113.5, total_loss = -2.2485, pg_loss = -17.04, baseline_loss = 19.942, entropy_loss = -5.1498, learner_queue_size = 32, train_seconds = 8867.7, _tick = 3051, _time = 1.6546e+09)
[2022-06-07 12:51:39,730][root][INFO] - Step 15920640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15920640, mean_episode_return = 45.918, mean_episode_step = 2208.0, total_loss = -148.88, pg_loss = -151.83, baseline_loss = 8.1301, entropy_loss = -5.1728, learner_queue_size = 32, train_seconds = 8872.7, _tick = 3053, _time = 1.6546e+09)
[2022-06-07 12:51:44,734][root][INFO] - Step 15930880 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 15930880, mean_episode_return = None, mean_episode_step = 2223.5, total_loss = 16.215, pg_loss = -9.9624, baseline_loss = 31.394, entropy_loss = -5.2159, learner_queue_size = 32, train_seconds = 8877.7, _tick = 3054, _time = 1.6546e+09)
[2022-06-07 12:51:49,736][root][INFO] - Step 15938560 @ 1535.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 15938560, mean_episode_return = None, mean_episode_step = 2488.5, total_loss = -86.628, pg_loss = -109.55, baseline_loss = 28.172, entropy_loss = -5.252, learner_queue_size = 32, train_seconds = 8882.7, _tick = 3056, _time = 1.6546e+09)
[2022-06-07 12:51:54,741][root][INFO] - Step 15948800 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 15948800, mean_episode_return = None, mean_episode_step = 2151.8, total_loss = 13.15, pg_loss = -2.3817, baseline_loss = 20.787, entropy_loss = -5.255, learner_queue_size = 32, train_seconds = 8887.7, _tick = 3057, _time = 1.6546e+09)
[2022-06-07 12:51:59,748][root][INFO] - Step 15956480 @ 1534.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 15956480, mean_episode_return = 66.477, mean_episode_step = 1992.0, total_loss = -17.935, pg_loss = -32.389, baseline_loss = 19.765, entropy_loss = -5.3112, learner_queue_size = 32, train_seconds = 8892.7, _tick = 3060, _time = 1.6546e+09)
[2022-06-07 12:52:04,754][root][INFO] - Step 15966720 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 15966720, mean_episode_return = None, mean_episode_step = 2170.8, total_loss = 42.041, pg_loss = 19.832, baseline_loss = 27.426, entropy_loss = -5.2168, learner_queue_size = 32, train_seconds = 8897.7, _tick = 3061, _time = 1.6546e+09)
[2022-06-07 12:52:09,758][root][INFO] - Step 15974400 @ 1534.7 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 15974400, mean_episode_return = 27.06, mean_episode_step = 2144.6, total_loss = 218.17, pg_loss = 143.54, baseline_loss = 79.842, entropy_loss = -5.215, learner_queue_size = 32, train_seconds = 8902.8, _tick = 3064, _time = 1.6546e+09)
[2022-06-07 12:52:14,764][root][INFO] - Step 15984640 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 15984640, mean_episode_return = 112.73, mean_episode_step = 2448.9, total_loss = 119.58, pg_loss = 56.482, baseline_loss = 68.244, entropy_loss = -5.1465, learner_queue_size = 32, train_seconds = 8907.8, _tick = 3066, _time = 1.6546e+09)
[2022-06-07 12:52:19,770][root][INFO] - Step 15992320 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 15992320, mean_episode_return = 133.69, mean_episode_step = 2562.4, total_loss = -106.97, pg_loss = -109.18, baseline_loss = 7.4291, entropy_loss = -5.2219, learner_queue_size = 32, train_seconds = 8912.8, _tick = 3069, _time = 1.6546e+09)
[2022-06-07 12:52:24,774][root][INFO] - Step 16002560 @ 2046.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 16002560, mean_episode_return = 108.9, mean_episode_step = 2420.0, total_loss = 176.91, pg_loss = 119.18, baseline_loss = 62.965, entropy_loss = -5.2393, learner_queue_size = 32, train_seconds = 8917.8, _tick = 3073, _time = 1.6546e+09)
[2022-06-07 12:52:29,778][root][INFO] - Step 16010240 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 16010240, mean_episode_return = None, mean_episode_step = 1904.1, total_loss = 82.261, pg_loss = 39.271, baseline_loss = 48.251, entropy_loss = -5.2616, learner_queue_size = 32, train_seconds = 8922.8, _tick = 3074, _time = 1.6546e+09)
[2022-06-07 12:52:34,782][root][INFO] - Step 16020480 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 16020480, mean_episode_return = None, mean_episode_step = 1891.3, total_loss = 147.2, pg_loss = 100.42, baseline_loss = 51.941, entropy_loss = -5.1602, learner_queue_size = 32, train_seconds = 8927.8, _tick = 3076, _time = 1.6546e+09)
[2022-06-07 12:52:39,786][root][INFO] - Step 16030720 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 16030720, mean_episode_return = 75.729, mean_episode_step = 2068.3, total_loss = -156.91, pg_loss = -179.13, baseline_loss = 27.335, entropy_loss = -5.117, learner_queue_size = 32, train_seconds = 8932.8, _tick = 3078, _time = 1.6546e+09)
[2022-06-07 12:52:44,793][root][INFO] - Step 16038400 @ 1533.9 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 16038400, mean_episode_return = None, mean_episode_step = 2060.3, total_loss = -62.297, pg_loss = -73.186, baseline_loss = 15.886, entropy_loss = -4.9969, learner_queue_size = 32, train_seconds = 8937.8, _tick = 3079, _time = 1.6546e+09)
[2022-06-07 12:52:49,796][root][INFO] - Step 16046080 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16046080, mean_episode_return = 41.071, mean_episode_step = 2232.0, total_loss = 305.39, pg_loss = 230.15, baseline_loss = 80.169, entropy_loss = -4.9321, learner_queue_size = 32, train_seconds = 8942.8, _tick = 3081, _time = 1.6546e+09)
[2022-06-07 12:52:54,802][root][INFO] - Step 16056320 @ 2045.6 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 16056320, mean_episode_return = 114.83, mean_episode_step = 2291.9, total_loss = -7.3528, pg_loss = -64.081, baseline_loss = 61.531, entropy_loss = -4.8021, learner_queue_size = 32, train_seconds = 8947.8, _tick = 3085, _time = 1.6546e+09)
[2022-06-07 12:52:59,806][root][INFO] - Step 16066560 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 16066560, mean_episode_return = None, mean_episode_step = 1930.4, total_loss = -16.148, pg_loss = -27.728, baseline_loss = 16.242, entropy_loss = -4.6627, learner_queue_size = 32, train_seconds = 8952.8, _tick = 3087, _time = 1.6546e+09)
[2022-06-07 12:53:04,810][root][INFO] - Step 16074240 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16074240, mean_episode_return = None, mean_episode_step = 2207.3, total_loss = 96.807, pg_loss = 47.631, baseline_loss = 53.901, entropy_loss = -4.7253, learner_queue_size = 32, train_seconds = 8957.8, _tick = 3089, _time = 1.6546e+09)
[2022-06-07 12:53:09,814][root][INFO] - Step 16084480 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16084480, mean_episode_return = -32.351, mean_episode_step = 1822.2, total_loss = 261.06, pg_loss = 164.03, baseline_loss = 101.76, entropy_loss = -4.7265, learner_queue_size = 32, train_seconds = 8962.8, _tick = 3091, _time = 1.6546e+09)
[2022-06-07 12:53:14,818][root][INFO] - Step 16092160 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 16092160, mean_episode_return = 81.251, mean_episode_step = 1746.6, total_loss = 207.53, pg_loss = 123.34, baseline_loss = 88.996, entropy_loss = -4.8072, learner_queue_size = 32, train_seconds = 8967.8, _tick = 3093, _time = 1.6546e+09)
[2022-06-07 12:53:19,822][root][INFO] - Step 16102400 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16102400, mean_episode_return = 43.302, mean_episode_step = 2306.9, total_loss = -96.251, pg_loss = -120.14, baseline_loss = 28.708, entropy_loss = -4.8222, learner_queue_size = 32, train_seconds = 8972.8, _tick = 3095, _time = 1.6546e+09)
[2022-06-07 12:53:24,826][root][INFO] - Step 16110080 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16110080, mean_episode_return = 33.711, mean_episode_step = 2525.3, total_loss = -185.59, pg_loss = -187.79, baseline_loss = 7.0071, entropy_loss = -4.812, learner_queue_size = 32, train_seconds = 8977.8, _tick = 3097, _time = 1.6546e+09)
[2022-06-07 12:53:29,830][root][INFO] - Step 16120320 @ 2046.3 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 16120320, mean_episode_return = None, mean_episode_step = 1892.8, total_loss = 4.2431, pg_loss = -9.6901, baseline_loss = 18.747, entropy_loss = -4.8139, learner_queue_size = 32, train_seconds = 8982.8, _tick = 3099, _time = 1.6546e+09)
[2022-06-07 12:53:34,836][root][INFO] - Step 16128000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16128000, mean_episode_return = None, mean_episode_step = 2726.7, total_loss = 89.999, pg_loss = 47.452, baseline_loss = 47.361, entropy_loss = -4.8133, learner_queue_size = 32, train_seconds = 8987.8, _tick = 3100, _time = 1.6546e+09)
[2022-06-07 12:53:39,842][root][INFO] - Step 16138240 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 16138240, mean_episode_return = 74.139, mean_episode_step = 2140.7, total_loss = 64.956, pg_loss = -0.70098, baseline_loss = 70.425, entropy_loss = -4.7684, learner_queue_size = 32, train_seconds = 8992.8, _tick = 3103, _time = 1.6546e+09)
[2022-06-07 12:53:44,846][root][INFO] - Step 16145920 @ 1534.9 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 16145920, mean_episode_return = 46.87, mean_episode_step = 1904.8, total_loss = -38.247, pg_loss = -51.184, baseline_loss = 17.779, entropy_loss = -4.8422, learner_queue_size = 32, train_seconds = 8997.8, _tick = 3105, _time = 1.6546e+09)
[2022-06-07 12:53:49,850][root][INFO] - Step 16156160 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16156160, mean_episode_return = 2.3696, mean_episode_step = 2030.1, total_loss = -11.891, pg_loss = -31.494, baseline_loss = 24.255, entropy_loss = -4.651, learner_queue_size = 32, train_seconds = 9002.8, _tick = 3109, _time = 1.6546e+09)
[2022-06-07 12:53:54,854][root][INFO] - Step 16163840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16163840, mean_episode_return = 65.979, mean_episode_step = 2095.1, total_loss = -71.049, pg_loss = -80.82, baseline_loss = 14.429, entropy_loss = -4.6572, learner_queue_size = 32, train_seconds = 9007.8, _tick = 3111, _time = 1.6546e+09)
[2022-06-07 12:53:59,866][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 12:54:00,085][root][INFO] - Step 16174080 @ 2043.1 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 16174080, mean_episode_return = 58.189, mean_episode_step = 2381.8, total_loss = 60.126, pg_loss = 35.343, baseline_loss = 29.33, entropy_loss = -4.5465, learner_queue_size = 32, train_seconds = 9012.9, _tick = 3114, _time = 1.6546e+09)
[2022-06-07 12:54:05,090][root][INFO] - Step 16181760 @ 1470.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16181760, mean_episode_return = 87.837, mean_episode_step = 2552.1, total_loss = 388.92, pg_loss = 260.58, baseline_loss = 132.85, entropy_loss = -4.5155, learner_queue_size = 32, train_seconds = 9018.1, _tick = 3116, _time = 1.6546e+09)
[2022-06-07 12:54:10,094][root][INFO] - Step 16192000 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16192000, mean_episode_return = None, mean_episode_step = 2067.0, total_loss = 120.72, pg_loss = 81.275, baseline_loss = 43.98, entropy_loss = -4.534, learner_queue_size = 32, train_seconds = 9023.1, _tick = 3116, _time = 1.6546e+09)
[2022-06-07 12:54:15,098][root][INFO] - Step 16199680 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 16199680, mean_episode_return = 82.038, mean_episode_step = 1816.6, total_loss = 10.53, pg_loss = -43.535, baseline_loss = 58.621, entropy_loss = -4.5569, learner_queue_size = 32, train_seconds = 9028.1, _tick = 3117, _time = 1.6546e+09)
[2022-06-07 12:54:20,102][root][INFO] - Step 16209920 @ 2046.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 16209920, mean_episode_return = 68.669, mean_episode_step = 1972.4, total_loss = 121.35, pg_loss = 51.116, baseline_loss = 74.79, entropy_loss = -4.5543, learner_queue_size = 32, train_seconds = 9033.1, _tick = 3118, _time = 1.6546e+09)
[2022-06-07 12:54:25,106][root][INFO] - Step 16217600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16217600, mean_episode_return = None, mean_episode_step = 2606.7, total_loss = 75.508, pg_loss = 42.995, baseline_loss = 37.028, entropy_loss = -4.5148, learner_queue_size = 32, train_seconds = 9038.1, _tick = 3118, _time = 1.6546e+09)
[2022-06-07 12:54:30,110][root][INFO] - Step 16227840 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16227840, mean_episode_return = 25.74, mean_episode_step = 2444.2, total_loss = -138.74, pg_loss = -141.58, baseline_loss = 7.3924, entropy_loss = -4.5484, learner_queue_size = 32, train_seconds = 9043.1, _tick = 3121, _time = 1.6546e+09)
[2022-06-07 12:54:35,114][root][INFO] - Step 16235520 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16235520, mean_episode_return = None, mean_episode_step = 1658.6, total_loss = -82.264, pg_loss = -82.737, baseline_loss = 4.9762, entropy_loss = -4.5035, learner_queue_size = 32, train_seconds = 9048.1, _tick = 3123, _time = 1.6546e+09)
[2022-06-07 12:54:40,118][root][INFO] - Step 16245760 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16245760, mean_episode_return = 22.46, mean_episode_step = 2391.3, total_loss = -138.71, pg_loss = -153.31, baseline_loss = 19.164, entropy_loss = -4.5651, learner_queue_size = 32, train_seconds = 9053.1, _tick = 3125, _time = 1.6546e+09)
[2022-06-07 12:54:45,122][root][INFO] - Step 16253440 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16253440, mean_episode_return = None, mean_episode_step = 2370.6, total_loss = 97.943, pg_loss = 61.541, baseline_loss = 41.087, entropy_loss = -4.6846, learner_queue_size = 32, train_seconds = 9058.1, _tick = 3126, _time = 1.6546e+09)
[2022-06-07 12:54:50,126][root][INFO] - Step 16263680 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 16263680, mean_episode_return = 8.0098, mean_episode_step = 2182.5, total_loss = 639.12, pg_loss = 431.35, baseline_loss = 212.5, entropy_loss = -4.7263, learner_queue_size = 32, train_seconds = 9063.1, _tick = 3129, _time = 1.6546e+09)
[2022-06-07 12:54:55,130][root][INFO] - Step 16271360 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16271360, mean_episode_return = 162.56, mean_episode_step = 2039.7, total_loss = -183.24, pg_loss = -183.28, baseline_loss = 4.7641, entropy_loss = -4.7204, learner_queue_size = 32, train_seconds = 9068.1, _tick = 3131, _time = 1.6546e+09)
[2022-06-07 12:55:00,134][root][INFO] - Step 16281600 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 16281600, mean_episode_return = 81.889, mean_episode_step = 2176.6, total_loss = -63.162, pg_loss = -109.97, baseline_loss = 51.584, entropy_loss = -4.7764, learner_queue_size = 32, train_seconds = 9073.1, _tick = 3132, _time = 1.6546e+09)
[2022-06-07 12:55:05,138][root][INFO] - Step 16289280 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 16289280, mean_episode_return = None, mean_episode_step = 2400.7, total_loss = 416.29, pg_loss = 280.06, baseline_loss = 140.99, entropy_loss = -4.7581, learner_queue_size = 32, train_seconds = 9078.1, _tick = 3132, _time = 1.6546e+09)
[2022-06-07 12:55:10,142][root][INFO] - Step 16299520 @ 2046.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 16299520, mean_episode_return = 50.371, mean_episode_step = 2238.3, total_loss = -151.63, pg_loss = -156.5, baseline_loss = 9.6606, entropy_loss = -4.7918, learner_queue_size = 32, train_seconds = 9083.1, _tick = 3134, _time = 1.6546e+09)
[2022-06-07 12:55:15,148][root][INFO] - Step 16307200 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16307200, mean_episode_return = 46.493, mean_episode_step = 2252.9, total_loss = 24.243, pg_loss = -21.959, baseline_loss = 51.095, entropy_loss = -4.8935, learner_queue_size = 32, train_seconds = 9088.1, _tick = 3137, _time = 1.6546e+09)
[2022-06-07 12:55:20,154][root][INFO] - Step 16317440 @ 2045.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 16317440, mean_episode_return = None, mean_episode_step = 2554.3, total_loss = -97.915, pg_loss = -103.56, baseline_loss = 10.711, entropy_loss = -5.0617, learner_queue_size = 32, train_seconds = 9093.1, _tick = 3138, _time = 1.6546e+09)
[2022-06-07 12:55:25,158][root][INFO] - Step 16327680 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16327680, mean_episode_return = None, mean_episode_step = 2658.3, total_loss = -85.767, pg_loss = -86.039, baseline_loss = 5.3275, entropy_loss = -5.0556, learner_queue_size = 32, train_seconds = 9098.2, _tick = 3141, _time = 1.6546e+09)
[2022-06-07 12:55:30,164][root][INFO] - Step 16335360 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16335360, mean_episode_return = None, mean_episode_step = 2130.2, total_loss = 255.39, pg_loss = 179.44, baseline_loss = 80.991, entropy_loss = -5.0419, learner_queue_size = 32, train_seconds = 9103.2, _tick = 3142, _time = 1.6546e+09)
[2022-06-07 12:55:35,168][root][INFO] - Step 16343040 @ 1534.7 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 16343040, mean_episode_return = -4.5903, mean_episode_step = 2938.1, total_loss = -60.882, pg_loss = -106.69, baseline_loss = 50.91, entropy_loss = -5.1007, learner_queue_size = 32, train_seconds = 9108.2, _tick = 3144, _time = 1.6546e+09)
[2022-06-07 12:55:40,174][root][INFO] - Step 16353280 @ 2045.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16353280, mean_episode_return = None, mean_episode_step = 2091.7, total_loss = -144.0, pg_loss = -140.63, baseline_loss = 1.8734, entropy_loss = -5.2349, learner_queue_size = 32, train_seconds = 9113.2, _tick = 3145, _time = 1.6546e+09)
[2022-06-07 12:55:45,180][root][INFO] - Step 16360960 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 16360960, mean_episode_return = -4.7304, mean_episode_step = 2697.7, total_loss = -56.218, pg_loss = -84.447, baseline_loss = 33.458, entropy_loss = -5.2284, learner_queue_size = 32, train_seconds = 9118.2, _tick = 3148, _time = 1.6546e+09)
[2022-06-07 12:55:50,186][root][INFO] - Step 16371200 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16371200, mean_episode_return = None, mean_episode_step = 2066.8, total_loss = 584.22, pg_loss = 453.22, baseline_loss = 136.22, entropy_loss = -5.23, learner_queue_size = 32, train_seconds = 9123.2, _tick = 3149, _time = 1.6546e+09)
[2022-06-07 12:55:55,192][root][INFO] - Step 16381440 @ 2045.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 16381440, mean_episode_return = -9.9301, mean_episode_step = 2859.3, total_loss = -24.794, pg_loss = -64.953, baseline_loss = 45.293, entropy_loss = -5.1333, learner_queue_size = 32, train_seconds = 9128.2, _tick = 3150, _time = 1.6546e+09)
[2022-06-07 12:56:00,198][root][INFO] - Step 16389120 @ 1534.1 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 16389120, mean_episode_return = None, mean_episode_step = 2341.0, total_loss = -163.92, pg_loss = -166.32, baseline_loss = 7.502, entropy_loss = -5.1036, learner_queue_size = 32, train_seconds = 9133.2, _tick = 3151, _time = 1.6546e+09)
[2022-06-07 12:56:05,204][root][INFO] - Step 16396800 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16396800, mean_episode_return = None, mean_episode_step = 2210.0, total_loss = 237.78, pg_loss = 156.86, baseline_loss = 86.021, entropy_loss = -5.1003, learner_queue_size = 32, train_seconds = 9138.2, _tick = 3151, _time = 1.6546e+09)
[2022-06-07 12:56:10,210][root][INFO] - Step 16407040 @ 2045.6 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 16407040, mean_episode_return = None, mean_episode_step = 2251.6, total_loss = 372.65, pg_loss = 276.68, baseline_loss = 101.04, entropy_loss = -5.0707, learner_queue_size = 32, train_seconds = 9143.2, _tick = 3153, _time = 1.6546e+09)
[2022-06-07 12:56:15,216][root][INFO] - Step 16414720 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16414720, mean_episode_return = 61.692, mean_episode_step = 2551.2, total_loss = 162.0, pg_loss = 110.06, baseline_loss = 56.954, entropy_loss = -5.016, learner_queue_size = 32, train_seconds = 9148.2, _tick = 3155, _time = 1.6546e+09)
[2022-06-07 12:56:20,226][root][INFO] - Step 16424960 @ 2044.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16424960, mean_episode_return = 62.512, mean_episode_step = 2243.3, total_loss = 431.66, pg_loss = 318.92, baseline_loss = 117.69, entropy_loss = -4.952, learner_queue_size = 32, train_seconds = 9153.2, _tick = 3159, _time = 1.6546e+09)
[2022-06-07 12:56:25,232][root][INFO] - Step 16432640 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16432640, mean_episode_return = None, mean_episode_step = 2333.0, total_loss = 51.082, pg_loss = 22.714, baseline_loss = 33.339, entropy_loss = -4.9704, learner_queue_size = 32, train_seconds = 9158.2, _tick = 3160, _time = 1.6546e+09)
[2022-06-07 12:56:30,234][root][INFO] - Step 16442880 @ 2047.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 16442880, mean_episode_return = None, mean_episode_step = 2477.8, total_loss = 98.495, pg_loss = 59.055, baseline_loss = 44.453, entropy_loss = -5.0131, learner_queue_size = 32, train_seconds = 9163.2, _tick = 3160, _time = 1.6546e+09)
[2022-06-07 12:56:35,238][root][INFO] - Step 16453120 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16453120, mean_episode_return = None, mean_episode_step = 2294.3, total_loss = 97.529, pg_loss = 56.249, baseline_loss = 46.323, entropy_loss = -5.0432, learner_queue_size = 32, train_seconds = 9168.2, _tick = 3162, _time = 1.6546e+09)
[2022-06-07 12:56:40,242][root][INFO] - Step 16460800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16460800, mean_episode_return = 58.078, mean_episode_step = 2259.5, total_loss = -154.46, pg_loss = -164.01, baseline_loss = 14.65, entropy_loss = -5.0965, learner_queue_size = 32, train_seconds = 9173.2, _tick = 3163, _time = 1.6546e+09)
[2022-06-07 12:56:45,246][root][INFO] - Step 16471040 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 16471040, mean_episode_return = None, mean_episode_step = 2389.4, total_loss = -40.221, pg_loss = -48.802, baseline_loss = 13.73, entropy_loss = -5.1497, learner_queue_size = 32, train_seconds = 9178.2, _tick = 3164, _time = 1.6546e+09)
[2022-06-07 12:56:50,252][root][INFO] - Step 16478720 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16478720, mean_episode_return = 54.602, mean_episode_step = 2176.0, total_loss = -82.253, pg_loss = -102.22, baseline_loss = 25.081, entropy_loss = -5.1149, learner_queue_size = 32, train_seconds = 9183.2, _tick = 3167, _time = 1.6546e+09)
[2022-06-07 12:56:55,254][root][INFO] - Step 16488960 @ 2047.2 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 16488960, mean_episode_return = None, mean_episode_step = 2481.6, total_loss = 206.91, pg_loss = 135.34, baseline_loss = 76.666, entropy_loss = -5.0978, learner_queue_size = 32, train_seconds = 9188.2, _tick = 3169, _time = 1.6546e+09)
[2022-06-07 12:57:00,258][root][INFO] - Step 16496640 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 16496640, mean_episode_return = None, mean_episode_step = 2249.3, total_loss = 183.93, pg_loss = 109.49, baseline_loss = 79.495, entropy_loss = -5.0542, learner_queue_size = 32, train_seconds = 9193.3, _tick = 3170, _time = 1.6546e+09)
[2022-06-07 12:57:05,262][root][INFO] - Step 16506880 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 16506880, mean_episode_return = 45.44, mean_episode_step = 2401.8, total_loss = -36.211, pg_loss = -68.422, baseline_loss = 37.27, entropy_loss = -5.0587, learner_queue_size = 32, train_seconds = 9198.3, _tick = 3173, _time = 1.6546e+09)
[2022-06-07 12:57:10,268][root][INFO] - Step 16514560 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16514560, mean_episode_return = None, mean_episode_step = 1977.2, total_loss = -91.084, pg_loss = -103.96, baseline_loss = 17.925, entropy_loss = -5.049, learner_queue_size = 32, train_seconds = 9203.3, _tick = 3174, _time = 1.6546e+09)
[2022-06-07 12:57:15,270][root][INFO] - Step 16524800 @ 2047.1 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 16524800, mean_episode_return = 47.239, mean_episode_step = 2588.4, total_loss = 216.76, pg_loss = 121.8, baseline_loss = 99.996, entropy_loss = -5.0365, learner_queue_size = 32, train_seconds = 9208.3, _tick = 3177, _time = 1.6546e+09)
[2022-06-07 12:57:20,274][root][INFO] - Step 16532480 @ 1534.8 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 16532480, mean_episode_return = None, mean_episode_step = 2167.8, total_loss = 32.584, pg_loss = 3.5246, baseline_loss = 34.031, entropy_loss = -4.9716, learner_queue_size = 32, train_seconds = 9213.3, _tick = 3178, _time = 1.6546e+09)
[2022-06-07 12:57:25,280][root][INFO] - Step 16542720 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16542720, mean_episode_return = None, mean_episode_step = 2502.8, total_loss = 54.938, pg_loss = 5.2927, baseline_loss = 54.644, entropy_loss = -4.9988, learner_queue_size = 32, train_seconds = 9218.3, _tick = 3180, _time = 1.6546e+09)
[2022-06-07 12:57:30,286][root][INFO] - Step 16550400 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16550400, mean_episode_return = 77.429, mean_episode_step = 2834.1, total_loss = -36.445, pg_loss = -64.364, baseline_loss = 33.015, entropy_loss = -5.0958, learner_queue_size = 32, train_seconds = 9223.3, _tick = 3181, _time = 1.6546e+09)
[2022-06-07 12:57:35,292][root][INFO] - Step 16560640 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16560640, mean_episode_return = None, mean_episode_step = 2120.6, total_loss = -138.69, pg_loss = -135.33, baseline_loss = 1.8084, entropy_loss = -5.1642, learner_queue_size = 32, train_seconds = 9228.3, _tick = 3182, _time = 1.6546e+09)
[2022-06-07 12:57:40,296][root][INFO] - Step 16570880 @ 2046.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 16570880, mean_episode_return = None, mean_episode_step = 3106.8, total_loss = -42.39, pg_loss = -49.934, baseline_loss = 12.741, entropy_loss = -5.1974, learner_queue_size = 32, train_seconds = 9233.3, _tick = 3183, _time = 1.6546e+09)
[2022-06-07 12:57:45,301][root][INFO] - Step 16578560 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16578560, mean_episode_return = None, mean_episode_step = 2666.7, total_loss = -114.92, pg_loss = -121.85, baseline_loss = 12.133, entropy_loss = -5.203, learner_queue_size = 32, train_seconds = 9238.3, _tick = 3185, _time = 1.6546e+09)
[2022-06-07 12:57:50,306][root][INFO] - Step 16586240 @ 1534.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 16586240, mean_episode_return = None, mean_episode_step = 3143.5, total_loss = -130.73, pg_loss = -128.51, baseline_loss = 2.9628, entropy_loss = -5.1867, learner_queue_size = 32, train_seconds = 9243.3, _tick = 3187, _time = 1.6546e+09)
[2022-06-07 12:57:55,312][root][INFO] - Step 16596480 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16596480, mean_episode_return = 1.2602, mean_episode_step = 2113.9, total_loss = 245.12, pg_loss = 148.3, baseline_loss = 102.1, entropy_loss = -5.2822, learner_queue_size = 32, train_seconds = 9248.3, _tick = 3190, _time = 1.6546e+09)
[2022-06-07 12:58:00,318][root][INFO] - Step 16604160 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16604160, mean_episode_return = None, mean_episode_step = 2358.3, total_loss = 7.8944, pg_loss = -20.382, baseline_loss = 33.499, entropy_loss = -5.223, learner_queue_size = 32, train_seconds = 9253.3, _tick = 3192, _time = 1.6546e+09)
[2022-06-07 12:58:05,324][root][INFO] - Step 16614400 @ 2045.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 16614400, mean_episode_return = 57.343, mean_episode_step = 2624.7, total_loss = 14.616, pg_loss = -16.785, baseline_loss = 36.591, entropy_loss = -5.1903, learner_queue_size = 32, train_seconds = 9258.3, _tick = 3196, _time = 1.6546e+09)
[2022-06-07 12:58:10,330][root][INFO] - Step 16622080 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 16622080, mean_episode_return = 74.989, mean_episode_step = 2315.4, total_loss = 152.87, pg_loss = 91.536, baseline_loss = 66.584, entropy_loss = -5.2535, learner_queue_size = 32, train_seconds = 9263.3, _tick = 3199, _time = 1.6546e+09)
[2022-06-07 12:58:15,336][root][INFO] - Step 16632320 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16632320, mean_episode_return = None, mean_episode_step = 2164.1, total_loss = 136.14, pg_loss = 55.719, baseline_loss = 85.635, entropy_loss = -5.2096, learner_queue_size = 32, train_seconds = 9268.3, _tick = 3201, _time = 1.6546e+09)
[2022-06-07 12:58:20,342][root][INFO] - Step 16640000 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 16640000, mean_episode_return = None, mean_episode_step = 2040.0, total_loss = 201.54, pg_loss = 122.27, baseline_loss = 84.375, entropy_loss = -5.1012, learner_queue_size = 32, train_seconds = 9273.3, _tick = 3202, _time = 1.6546e+09)
[2022-06-07 12:58:25,346][root][INFO] - Step 16650240 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 16650240, mean_episode_return = None, mean_episode_step = 2675.4, total_loss = 253.33, pg_loss = 186.66, baseline_loss = 71.614, entropy_loss = -4.9448, learner_queue_size = 32, train_seconds = 9278.3, _tick = 3205, _time = 1.6546e+09)
[2022-06-07 12:58:30,350][root][INFO] - Step 16657920 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16657920, mean_episode_return = None, mean_episode_step = 2271.7, total_loss = 249.46, pg_loss = 143.13, baseline_loss = 111.36, entropy_loss = -5.0373, learner_queue_size = 32, train_seconds = 9283.3, _tick = 3206, _time = 1.6546e+09)
[2022-06-07 12:58:35,356][root][INFO] - Step 16668160 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16668160, mean_episode_return = 12.74, mean_episode_step = 2825.4, total_loss = 219.99, pg_loss = 144.84, baseline_loss = 80.198, entropy_loss = -5.0489, learner_queue_size = 32, train_seconds = 9288.4, _tick = 3209, _time = 1.6546e+09)
[2022-06-07 12:58:40,358][root][INFO] - Step 16675840 @ 1535.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16675840, mean_episode_return = None, mean_episode_step = 2661.4, total_loss = 265.88, pg_loss = 130.52, baseline_loss = 140.38, entropy_loss = -5.0238, learner_queue_size = 32, train_seconds = 9293.4, _tick = 3211, _time = 1.6546e+09)
[2022-06-07 12:58:45,362][root][INFO] - Step 16686080 @ 2046.5 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 16686080, mean_episode_return = 50.463, mean_episode_step = 2147.3, total_loss = -76.98, pg_loss = -96.201, baseline_loss = 24.368, entropy_loss = -5.1463, learner_queue_size = 32, train_seconds = 9298.4, _tick = 3215, _time = 1.6546e+09)
[2022-06-07 12:58:50,368][root][INFO] - Step 16693760 @ 1534.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16693760, mean_episode_return = 50.692, mean_episode_step = 2140.9, total_loss = -53.841, pg_loss = -82.606, baseline_loss = 33.954, entropy_loss = -5.1894, learner_queue_size = 32, train_seconds = 9303.4, _tick = 3216, _time = 1.6546e+09)
[2022-06-07 12:58:55,375][root][INFO] - Step 16704000 @ 2045.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16704000, mean_episode_return = None, mean_episode_step = 2107.8, total_loss = -87.328, pg_loss = -97.701, baseline_loss = 15.583, entropy_loss = -5.2106, learner_queue_size = 32, train_seconds = 9308.4, _tick = 3219, _time = 1.6546e+09)
[2022-06-07 12:59:00,378][root][INFO] - Step 16711680 @ 1535.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16711680, mean_episode_return = None, mean_episode_step = 2044.4, total_loss = 487.31, pg_loss = 352.67, baseline_loss = 139.82, entropy_loss = -5.1719, learner_queue_size = 32, train_seconds = 9313.4, _tick = 3219, _time = 1.6546e+09)
[2022-06-07 12:59:05,384][root][INFO] - Step 16721920 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 16721920, mean_episode_return = None, mean_episode_step = 2263.2, total_loss = -73.249, pg_loss = -93.425, baseline_loss = 25.342, entropy_loss = -5.1664, learner_queue_size = 32, train_seconds = 9318.4, _tick = 3221, _time = 1.6546e+09)
[2022-06-07 12:59:10,390][root][INFO] - Step 16729600 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 16729600, mean_episode_return = None, mean_episode_step = 3551.1, total_loss = 122.84, pg_loss = 71.746, baseline_loss = 56.181, entropy_loss = -5.091, learner_queue_size = 32, train_seconds = 9323.4, _tick = 3221, _time = 1.6546e+09)
[2022-06-07 12:59:15,394][root][INFO] - Step 16739840 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16739840, mean_episode_return = 42.934, mean_episode_step = 2202.6, total_loss = 153.78, pg_loss = 47.967, baseline_loss = 110.9, entropy_loss = -5.0833, learner_queue_size = 32, train_seconds = 9328.4, _tick = 3225, _time = 1.6546e+09)
[2022-06-07 12:59:20,398][root][INFO] - Step 16747520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16747520, mean_episode_return = None, mean_episode_step = 2138.9, total_loss = -52.781, pg_loss = -67.122, baseline_loss = 19.384, entropy_loss = -5.0437, learner_queue_size = 32, train_seconds = 9333.4, _tick = 3225, _time = 1.6546e+09)
[2022-06-07 12:59:25,402][root][INFO] - Step 16757760 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 16757760, mean_episode_return = 85.598, mean_episode_step = 2280.4, total_loss = 239.04, pg_loss = 150.67, baseline_loss = 93.352, entropy_loss = -4.9887, learner_queue_size = 32, train_seconds = 9338.4, _tick = 3227, _time = 1.6546e+09)
[2022-06-07 12:59:30,408][root][INFO] - Step 16765440 @ 1534.1 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 16765440, mean_episode_return = 21.065, mean_episode_step = 2400.1, total_loss = -156.12, pg_loss = -176.98, baseline_loss = 25.842, entropy_loss = -4.9827, learner_queue_size = 32, train_seconds = 9343.4, _tick = 3230, _time = 1.6546e+09)
[2022-06-07 12:59:35,414][root][INFO] - Step 16775680 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16775680, mean_episode_return = 26.375, mean_episode_step = 1946.3, total_loss = 26.058, pg_loss = -25.847, baseline_loss = 56.887, entropy_loss = -4.9815, learner_queue_size = 32, train_seconds = 9348.4, _tick = 3232, _time = 1.6546e+09)
[2022-06-07 12:59:40,418][root][INFO] - Step 16785920 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 16785920, mean_episode_return = None, mean_episode_step = 2184.7, total_loss = 141.23, pg_loss = 95.564, baseline_loss = 50.711, entropy_loss = -5.0477, learner_queue_size = 32, train_seconds = 9353.4, _tick = 3235, _time = 1.6546e+09)
[2022-06-07 12:59:45,424][root][INFO] - Step 16793600 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 16793600, mean_episode_return = None, mean_episode_step = 2399.2, total_loss = 79.655, pg_loss = 29.865, baseline_loss = 54.958, entropy_loss = -5.1687, learner_queue_size = 32, train_seconds = 9358.4, _tick = 3236, _time = 1.6546e+09)
[2022-06-07 12:59:50,426][root][INFO] - Step 16803840 @ 2047.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 16803840, mean_episode_return = None, mean_episode_step = 2885.6, total_loss = 174.93, pg_loss = 85.876, baseline_loss = 94.205, entropy_loss = -5.1488, learner_queue_size = 32, train_seconds = 9363.4, _tick = 3236, _time = 1.6546e+09)
[2022-06-07 12:59:55,430][root][INFO] - Step 16811520 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 16811520, mean_episode_return = None, mean_episode_step = 2240.7, total_loss = -87.019, pg_loss = -91.789, baseline_loss = 9.7677, entropy_loss = -4.9978, learner_queue_size = 32, train_seconds = 9368.4, _tick = 3237, _time = 1.6546e+09)
[2022-06-07 13:00:00,434][root][INFO] - Step 16821760 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 16821760, mean_episode_return = 52.875, mean_episode_step = 1928.2, total_loss = 43.895, pg_loss = 4.7423, baseline_loss = 44.113, entropy_loss = -4.9605, learner_queue_size = 32, train_seconds = 9373.4, _tick = 3240, _time = 1.6546e+09)
[2022-06-07 13:00:05,438][root][INFO] - Step 16829440 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16829440, mean_episode_return = None, mean_episode_step = 2231.0, total_loss = -26.053, pg_loss = -44.259, baseline_loss = 23.078, entropy_loss = -4.8716, learner_queue_size = 32, train_seconds = 9378.4, _tick = 3241, _time = 1.6546e+09)
[2022-06-07 13:00:10,442][root][INFO] - Step 16839680 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16839680, mean_episode_return = None, mean_episode_step = 2549.9, total_loss = 49.382, pg_loss = 30.198, baseline_loss = 24.009, entropy_loss = -4.8256, learner_queue_size = 32, train_seconds = 9383.4, _tick = 3243, _time = 1.6546e+09)
[2022-06-07 13:00:15,446][root][INFO] - Step 16847360 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16847360, mean_episode_return = None, mean_episode_step = 2012.7, total_loss = 167.56, pg_loss = 83.615, baseline_loss = 88.796, entropy_loss = -4.8554, learner_queue_size = 32, train_seconds = 9388.4, _tick = 3245, _time = 1.6546e+09)
[2022-06-07 13:00:20,450][root][INFO] - Step 16857600 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 16857600, mean_episode_return = 54.48, mean_episode_step = 2497.3, total_loss = -140.38, pg_loss = -155.76, baseline_loss = 20.144, entropy_loss = -4.7599, learner_queue_size = 32, train_seconds = 9393.4, _tick = 3247, _time = 1.6546e+09)
[2022-06-07 13:00:25,453][root][INFO] - Step 16865280 @ 1534.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16865280, mean_episode_return = None, mean_episode_step = 1666.9, total_loss = -24.389, pg_loss = -42.009, baseline_loss = 22.503, entropy_loss = -4.8835, learner_queue_size = 32, train_seconds = 9398.4, _tick = 3248, _time = 1.6546e+09)
[2022-06-07 13:00:30,458][root][INFO] - Step 16875520 @ 2046.2 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 16875520, mean_episode_return = 65.081, mean_episode_step = 2952.9, total_loss = 356.37, pg_loss = 221.46, baseline_loss = 139.83, entropy_loss = -4.92, learner_queue_size = 32, train_seconds = 9403.5, _tick = 3250, _time = 1.6546e+09)
[2022-06-07 13:00:35,462][root][INFO] - Step 16883200 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16883200, mean_episode_return = 21.135, mean_episode_step = 2554.8, total_loss = 287.82, pg_loss = 187.99, baseline_loss = 104.75, entropy_loss = -4.9152, learner_queue_size = 32, train_seconds = 9408.5, _tick = 3252, _time = 1.6546e+09)
[2022-06-07 13:00:40,466][root][INFO] - Step 16893440 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16893440, mean_episode_return = 45.823, mean_episode_step = 2046.7, total_loss = 337.44, pg_loss = 235.96, baseline_loss = 106.45, entropy_loss = -4.9604, learner_queue_size = 32, train_seconds = 9413.5, _tick = 3254, _time = 1.6546e+09)
[2022-06-07 13:00:45,471][root][INFO] - Step 16901120 @ 1534.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 16901120, mean_episode_return = 129.26, mean_episode_step = 1810.0, total_loss = 6.751e+04, pg_loss = 5452.7, baseline_loss = 6.2062e+04, entropy_loss = -4.8663, learner_queue_size = 32, train_seconds = 9418.5, _tick = 3256, _time = 1.6546e+09)
[2022-06-07 13:00:50,477][root][INFO] - Step 16911360 @ 2045.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 16911360, mean_episode_return = 60.531, mean_episode_step = 2476.7, total_loss = 130.65, pg_loss = 55.737, baseline_loss = 79.896, entropy_loss = -4.9843, learner_queue_size = 32, train_seconds = 9423.5, _tick = 3258, _time = 1.6546e+09)
[2022-06-07 13:00:55,481][root][INFO] - Step 16919040 @ 1534.7 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 16919040, mean_episode_return = 149.66, mean_episode_step = 2276.4, total_loss = 45.847, pg_loss = -3.2309, baseline_loss = 53.872, entropy_loss = -4.7939, learner_queue_size = 32, train_seconds = 9428.5, _tick = 3261, _time = 1.6546e+09)
[2022-06-07 13:01:00,486][root][INFO] - Step 16929280 @ 2045.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 16929280, mean_episode_return = 51.99, mean_episode_step = 2053.1, total_loss = 116.91, pg_loss = 65.401, baseline_loss = 56.453, entropy_loss = -4.9396, learner_queue_size = 32, train_seconds = 9433.5, _tick = 3262, _time = 1.6546e+09)
[2022-06-07 13:01:05,490][root][INFO] - Step 16939520 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 16939520, mean_episode_return = 3.8496, mean_episode_step = 2536.2, total_loss = -119.55, pg_loss = -125.5, baseline_loss = 10.948, entropy_loss = -5.0033, learner_queue_size = 32, train_seconds = 9438.5, _tick = 3265, _time = 1.6546e+09)
[2022-06-07 13:01:10,502][root][INFO] - Step 16947200 @ 1532.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16947200, mean_episode_return = None, mean_episode_step = 2433.9, total_loss = 2.1922, pg_loss = -13.885, baseline_loss = 20.969, entropy_loss = -4.8912, learner_queue_size = 32, train_seconds = 9443.5, _tick = 3266, _time = 1.6546e+09)
[2022-06-07 13:01:15,508][root][INFO] - Step 16954880 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16954880, mean_episode_return = 46.653, mean_episode_step = 2453.8, total_loss = 0.45073, pg_loss = -21.962, baseline_loss = 27.304, entropy_loss = -4.8914, learner_queue_size = 32, train_seconds = 9448.5, _tick = 3268, _time = 1.6546e+09)
[2022-06-07 13:01:20,514][root][INFO] - Step 16965120 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 16965120, mean_episode_return = None, mean_episode_step = 1965.9, total_loss = -147.65, pg_loss = -144.97, baseline_loss = 2.2386, entropy_loss = -4.9192, learner_queue_size = 32, train_seconds = 9453.5, _tick = 3271, _time = 1.6546e+09)
[2022-06-07 13:01:25,518][root][INFO] - Step 16975360 @ 2046.5 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 16975360, mean_episode_return = -5.2204, mean_episode_step = 2564.7, total_loss = -158.7, pg_loss = -160.95, baseline_loss = 7.2593, entropy_loss = -5.0074, learner_queue_size = 32, train_seconds = 9458.5, _tick = 3273, _time = 1.6546e+09)
[2022-06-07 13:01:30,522][root][INFO] - Step 16983040 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 16983040, mean_episode_return = 67.574, mean_episode_step = 3381.9, total_loss = -50.878, pg_loss = -94.78, baseline_loss = 49.035, entropy_loss = -5.132, learner_queue_size = 32, train_seconds = 9463.5, _tick = 3275, _time = 1.6546e+09)
[2022-06-07 13:01:35,526][root][INFO] - Step 16993280 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 16993280, mean_episode_return = 81.834, mean_episode_step = 2470.8, total_loss = 48.982, pg_loss = -0.83098, baseline_loss = 54.978, entropy_loss = -5.1654, learner_queue_size = 32, train_seconds = 9468.5, _tick = 3277, _time = 1.6546e+09)
[2022-06-07 13:01:40,530][root][INFO] - Step 17000960 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 17000960, mean_episode_return = None, mean_episode_step = 2764.6, total_loss = 99.598, pg_loss = 58.951, baseline_loss = 45.85, entropy_loss = -5.2036, learner_queue_size = 32, train_seconds = 9473.5, _tick = 3279, _time = 1.6546e+09)
[2022-06-07 13:01:45,536][root][INFO] - Step 17011200 @ 2045.6 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 17011200, mean_episode_return = None, mean_episode_step = 1915.5, total_loss = -31.643, pg_loss = -32.149, baseline_loss = 5.888, entropy_loss = -5.3822, learner_queue_size = 32, train_seconds = 9478.5, _tick = 3281, _time = 1.6546e+09)
[2022-06-07 13:01:50,542][root][INFO] - Step 17018880 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 17018880, mean_episode_return = None, mean_episode_step = 2572.9, total_loss = 30.056, pg_loss = 7.9506, baseline_loss = 27.577, entropy_loss = -5.4715, learner_queue_size = 32, train_seconds = 9483.5, _tick = 3282, _time = 1.6546e+09)
[2022-06-07 13:01:55,548][root][INFO] - Step 17029120 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17029120, mean_episode_return = None, mean_episode_step = 2702.7, total_loss = -45.077, pg_loss = -65.278, baseline_loss = 25.657, entropy_loss = -5.4566, learner_queue_size = 32, train_seconds = 9488.5, _tick = 3282, _time = 1.6546e+09)
[2022-06-07 13:02:00,554][root][INFO] - Step 17036800 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 17036800, mean_episode_return = 23.165, mean_episode_step = 2868.9, total_loss = -170.41, pg_loss = -180.74, baseline_loss = 15.806, entropy_loss = -5.4795, learner_queue_size = 32, train_seconds = 9493.5, _tick = 3284, _time = 1.6546e+09)
[2022-06-07 13:02:05,560][root][INFO] - Step 17047040 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17047040, mean_episode_return = None, mean_episode_step = 2703.7, total_loss = 126.05, pg_loss = 86.078, baseline_loss = 45.311, entropy_loss = -5.3343, learner_queue_size = 32, train_seconds = 9498.6, _tick = 3285, _time = 1.6546e+09)
[2022-06-07 13:02:10,566][root][INFO] - Step 17057280 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17057280, mean_episode_return = 82.263, mean_episode_step = 1996.6, total_loss = 278.23, pg_loss = 191.2, baseline_loss = 92.335, entropy_loss = -5.3021, learner_queue_size = 32, train_seconds = 9503.6, _tick = 3288, _time = 1.6546e+09)
[2022-06-07 13:02:15,572][root][INFO] - Step 17064960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17064960, mean_episode_return = 36.405, mean_episode_step = 1998.4, total_loss = -51.392, pg_loss = -68.632, baseline_loss = 22.508, entropy_loss = -5.2677, learner_queue_size = 32, train_seconds = 9508.6, _tick = 3290, _time = 1.6546e+09)
[2022-06-07 13:02:20,578][root][INFO] - Step 17072640 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 17072640, mean_episode_return = None, mean_episode_step = 2459.1, total_loss = 138.09, pg_loss = 87.963, baseline_loss = 55.422, entropy_loss = -5.2995, learner_queue_size = 32, train_seconds = 9513.6, _tick = 3291, _time = 1.6546e+09)
[2022-06-07 13:02:25,584][root][INFO] - Step 17082880 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17082880, mean_episode_return = None, mean_episode_step = 2907.8, total_loss = 113.01, pg_loss = 69.946, baseline_loss = 48.355, entropy_loss = -5.2956, learner_queue_size = 32, train_seconds = 9518.6, _tick = 3293, _time = 1.6546e+09)
[2022-06-07 13:02:30,590][root][INFO] - Step 17093120 @ 2045.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17093120, mean_episode_return = None, mean_episode_step = 2486.2, total_loss = 24.193, pg_loss = -1.8503, baseline_loss = 31.398, entropy_loss = -5.3548, learner_queue_size = 32, train_seconds = 9523.6, _tick = 3294, _time = 1.6546e+09)
[2022-06-07 13:02:35,596][root][INFO] - Step 17100800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17100800, mean_episode_return = None, mean_episode_step = 2525.9, total_loss = 60.638, pg_loss = 29.775, baseline_loss = 36.29, entropy_loss = -5.4274, learner_queue_size = 32, train_seconds = 9528.6, _tick = 3296, _time = 1.6546e+09)
[2022-06-07 13:02:40,602][root][INFO] - Step 17111040 @ 2045.7 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 17111040, mean_episode_return = None, mean_episode_step = 2222.7, total_loss = 197.17, pg_loss = 133.45, baseline_loss = 69.046, entropy_loss = -5.3273, learner_queue_size = 32, train_seconds = 9533.6, _tick = 3296, _time = 1.6546e+09)
[2022-06-07 13:02:45,606][root][INFO] - Step 17118720 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17118720, mean_episode_return = None, mean_episode_step = 2580.8, total_loss = -161.42, pg_loss = -157.81, baseline_loss = 1.6585, entropy_loss = -5.2614, learner_queue_size = 32, train_seconds = 9538.6, _tick = 3298, _time = 1.6546e+09)
[2022-06-07 13:02:50,610][root][INFO] - Step 17128960 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17128960, mean_episode_return = None, mean_episode_step = 2167.4, total_loss = -34.299, pg_loss = -46.477, baseline_loss = 17.478, entropy_loss = -5.3004, learner_queue_size = 32, train_seconds = 9543.6, _tick = 3300, _time = 1.6546e+09)
[2022-06-07 13:02:55,616][root][INFO] - Step 17136640 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 17136640, mean_episode_return = 63.935, mean_episode_step = 2538.9, total_loss = -62.896, pg_loss = -90.259, baseline_loss = 32.585, entropy_loss = -5.2222, learner_queue_size = 32, train_seconds = 9548.6, _tick = 3302, _time = 1.6546e+09)
[2022-06-07 13:03:00,622][root][INFO] - Step 17146880 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17146880, mean_episode_return = 360.07, mean_episode_step = 2785.3, total_loss = 50.525, pg_loss = -16.52, baseline_loss = 72.205, entropy_loss = -5.1598, learner_queue_size = 32, train_seconds = 9553.6, _tick = 3304, _time = 1.6546e+09)
[2022-06-07 13:03:05,628][root][INFO] - Step 17154560 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17154560, mean_episode_return = 59.452, mean_episode_step = 2676.8, total_loss = -40.734, pg_loss = -69.509, baseline_loss = 33.917, entropy_loss = -5.1417, learner_queue_size = 32, train_seconds = 9558.6, _tick = 3306, _time = 1.6546e+09)
[2022-06-07 13:03:10,634][root][INFO] - Step 17164800 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 17164800, mean_episode_return = None, mean_episode_step = 2029.1, total_loss = -55.159, pg_loss = -56.141, baseline_loss = 6.1436, entropy_loss = -5.1618, learner_queue_size = 32, train_seconds = 9563.6, _tick = 3306, _time = 1.6546e+09)
[2022-06-07 13:03:15,638][root][INFO] - Step 17172480 @ 1534.8 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 17172480, mean_episode_return = 98.555, mean_episode_step = 2362.6, total_loss = -49.56, pg_loss = -90.616, baseline_loss = 46.238, entropy_loss = -5.1819, learner_queue_size = 32, train_seconds = 9568.6, _tick = 3309, _time = 1.6546e+09)
[2022-06-07 13:03:20,642][root][INFO] - Step 17182720 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 17182720, mean_episode_return = None, mean_episode_step = 2336.7, total_loss = 116.68, pg_loss = 83.199, baseline_loss = 38.707, entropy_loss = -5.2254, learner_queue_size = 32, train_seconds = 9573.6, _tick = 3311, _time = 1.6546e+09)
[2022-06-07 13:03:25,654][root][INFO] - Step 17192960 @ 2043.1 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17192960, mean_episode_return = 29.24, mean_episode_step = 1963.8, total_loss = -66.387, pg_loss = -102.61, baseline_loss = 41.37, entropy_loss = -5.1497, learner_queue_size = 32, train_seconds = 9578.6, _tick = 3313, _time = 1.6546e+09)
[2022-06-07 13:03:30,660][root][INFO] - Step 17200640 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 17200640, mean_episode_return = 96.763, mean_episode_step = 2155.0, total_loss = 25.686, pg_loss = -7.2682, baseline_loss = 38.11, entropy_loss = -5.1562, learner_queue_size = 32, train_seconds = 9583.7, _tick = 3315, _time = 1.6546e+09)
[2022-06-07 13:03:35,666][root][INFO] - Step 17210880 @ 2045.6 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 17210880, mean_episode_return = None, mean_episode_step = 2163.9, total_loss = 96.902, pg_loss = 73.763, baseline_loss = 28.349, entropy_loss = -5.2098, learner_queue_size = 32, train_seconds = 9588.7, _tick = 3316, _time = 1.6546e+09)
[2022-06-07 13:03:40,672][root][INFO] - Step 17218560 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17218560, mean_episode_return = None, mean_episode_step = 1914.2, total_loss = 76.404, pg_loss = 51.63, baseline_loss = 29.922, entropy_loss = -5.1483, learner_queue_size = 32, train_seconds = 9593.7, _tick = 3317, _time = 1.6546e+09)
[2022-06-07 13:03:45,678][root][INFO] - Step 17228800 @ 2045.5 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 17228800, mean_episode_return = 64.899, mean_episode_step = 2206.3, total_loss = 129.31, pg_loss = 77.217, baseline_loss = 57.209, entropy_loss = -5.1142, learner_queue_size = 32, train_seconds = 9598.7, _tick = 3320, _time = 1.6546e+09)
[2022-06-07 13:03:50,682][root][INFO] - Step 17236480 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17236480, mean_episode_return = -42.239, mean_episode_step = 2669.5, total_loss = -37.05, pg_loss = -51.504, baseline_loss = 19.518, entropy_loss = -5.0645, learner_queue_size = 32, train_seconds = 9603.7, _tick = 3323, _time = 1.6546e+09)
[2022-06-07 13:03:55,688][root][INFO] - Step 17246720 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 17246720, mean_episode_return = None, mean_episode_step = 2481.0, total_loss = -136.3, pg_loss = -136.89, baseline_loss = 5.6915, entropy_loss = -5.1043, learner_queue_size = 32, train_seconds = 9608.7, _tick = 3326, _time = 1.6546e+09)
[2022-06-07 13:04:00,690][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 13:04:00,873][root][INFO] - Step 17254400 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17254400, mean_episode_return = 146.94, mean_episode_step = 1779.1, total_loss = 140.96, pg_loss = 80.362, baseline_loss = 65.784, entropy_loss = -5.189, learner_queue_size = 32, train_seconds = 9613.7, _tick = 3329, _time = 1.6546e+09)
[2022-06-07 13:04:05,878][root][INFO] - Step 17264640 @ 1973.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 17264640, mean_episode_return = None, mean_episode_step = 2255.7, total_loss = 202.46, pg_loss = 139.23, baseline_loss = 68.489, entropy_loss = -5.2604, learner_queue_size = 32, train_seconds = 9618.9, _tick = 3329, _time = 1.6546e+09)
[2022-06-07 13:04:10,882][root][INFO] - Step 17272320 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17272320, mean_episode_return = 62.372, mean_episode_step = 1782.2, total_loss = 254.3, pg_loss = 176.12, baseline_loss = 83.47, entropy_loss = -5.2891, learner_queue_size = 32, train_seconds = 9623.9, _tick = 3330, _time = 1.6546e+09)
[2022-06-07 13:04:15,886][root][INFO] - Step 17282560 @ 2046.4 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 17282560, mean_episode_return = 8.4798, mean_episode_step = 2221.5, total_loss = -80.952, pg_loss = -118.1, baseline_loss = 42.35, entropy_loss = -5.1983, learner_queue_size = 32, train_seconds = 9628.9, _tick = 3333, _time = 1.6546e+09)
[2022-06-07 13:04:20,890][root][INFO] - Step 17290240 @ 1534.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17290240, mean_episode_return = None, mean_episode_step = 2056.4, total_loss = 174.86, pg_loss = 85.859, baseline_loss = 94.175, entropy_loss = -5.1729, learner_queue_size = 32, train_seconds = 9633.9, _tick = 3335, _time = 1.6546e+09)
[2022-06-07 13:04:25,925][root][INFO] - Step 17300480 @ 2034.1 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 17300480, mean_episode_return = None, mean_episode_step = 2242.5, total_loss = -81.955, pg_loss = -82.702, baseline_loss = 5.9465, entropy_loss = -5.2, learner_queue_size = 32, train_seconds = 9638.9, _tick = 3338, _time = 1.6546e+09)
[2022-06-07 13:04:30,931][root][INFO] - Step 17308160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17308160, mean_episode_return = None, mean_episode_step = 2174.6, total_loss = 472.84, pg_loss = 335.38, baseline_loss = 142.72, entropy_loss = -5.264, learner_queue_size = 32, train_seconds = 9643.9, _tick = 3340, _time = 1.6546e+09)
[2022-06-07 13:04:35,934][root][INFO] - Step 17318400 @ 2046.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 17318400, mean_episode_return = 79.703, mean_episode_step = 1940.6, total_loss = 42.241, pg_loss = -11.349, baseline_loss = 58.749, entropy_loss = -5.1587, learner_queue_size = 32, train_seconds = 9648.9, _tick = 3343, _time = 1.6546e+09)
[2022-06-07 13:04:40,938][root][INFO] - Step 17326080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17326080, mean_episode_return = 79.447, mean_episode_step = 2271.7, total_loss = 101.39, pg_loss = 52.015, baseline_loss = 54.465, entropy_loss = -5.0877, learner_queue_size = 32, train_seconds = 9653.9, _tick = 3345, _time = 1.6546e+09)
[2022-06-07 13:04:45,944][root][INFO] - Step 17336320 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 17336320, mean_episode_return = 40.72, mean_episode_step = 2161.8, total_loss = 265.23, pg_loss = 122.62, baseline_loss = 147.65, entropy_loss = -5.041, learner_queue_size = 32, train_seconds = 9658.9, _tick = 3349, _time = 1.6546e+09)
[2022-06-07 13:04:50,950][root][INFO] - Step 17344000 @ 1534.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 17344000, mean_episode_return = None, mean_episode_step = 1941.4, total_loss = -144.24, pg_loss = -146.17, baseline_loss = 6.9363, entropy_loss = -5.0095, learner_queue_size = 32, train_seconds = 9663.9, _tick = 3349, _time = 1.6546e+09)
[2022-06-07 13:04:55,954][root][INFO] - Step 17354240 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 17354240, mean_episode_return = -0.45083, mean_episode_step = 2723.5, total_loss = -73.9, pg_loss = -109.37, baseline_loss = 40.405, entropy_loss = -4.9345, learner_queue_size = 32, train_seconds = 9668.9, _tick = 3351, _time = 1.6546e+09)
[2022-06-07 13:05:00,958][root][INFO] - Step 17361920 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 17361920, mean_episode_return = 75.328, mean_episode_step = 1927.1, total_loss = -53.493, pg_loss = -87.484, baseline_loss = 38.978, entropy_loss = -4.9862, learner_queue_size = 32, train_seconds = 9674.0, _tick = 3354, _time = 1.6546e+09)
[2022-06-07 13:05:05,961][root][INFO] - Step 17372160 @ 2046.7 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 17372160, mean_episode_return = None, mean_episode_step = 1992.7, total_loss = 42.865, pg_loss = 13.651, baseline_loss = 34.151, entropy_loss = -4.937, learner_queue_size = 32, train_seconds = 9679.0, _tick = 3356, _time = 1.6546e+09)
[2022-06-07 13:05:10,967][root][INFO] - Step 17379840 @ 1534.1 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 17379840, mean_episode_return = 15.857, mean_episode_step = 2094.1, total_loss = 28.343, pg_loss = -47.437, baseline_loss = 80.762, entropy_loss = -4.9827, learner_queue_size = 32, train_seconds = 9684.0, _tick = 3359, _time = 1.6546e+09)
[2022-06-07 13:05:15,970][root][INFO] - Step 17390080 @ 2046.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 17390080, mean_episode_return = 45.072, mean_episode_step = 1679.3, total_loss = 3.6691, pg_loss = -26.615, baseline_loss = 35.368, entropy_loss = -5.0838, learner_queue_size = 32, train_seconds = 9689.0, _tick = 3363, _time = 1.6546e+09)
[2022-06-07 13:05:20,976][root][INFO] - Step 17397760 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 17397760, mean_episode_return = 57.012, mean_episode_step = 2064.8, total_loss = 265.79, pg_loss = 130.93, baseline_loss = 139.92, entropy_loss = -5.0605, learner_queue_size = 32, train_seconds = 9694.0, _tick = 3366, _time = 1.6546e+09)
[2022-06-07 13:05:25,982][root][INFO] - Step 17408000 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17408000, mean_episode_return = 101.0, mean_episode_step = 1959.4, total_loss = 131.66, pg_loss = 86.284, baseline_loss = 50.433, entropy_loss = -5.0537, learner_queue_size = 32, train_seconds = 9699.0, _tick = 3370, _time = 1.6546e+09)
[2022-06-07 13:05:30,986][root][INFO] - Step 17418240 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17418240, mean_episode_return = None, mean_episode_step = 2485.2, total_loss = 9.36, pg_loss = -18.797, baseline_loss = 33.079, entropy_loss = -4.9218, learner_queue_size = 32, train_seconds = 9704.0, _tick = 3372, _time = 1.6546e+09)
[2022-06-07 13:05:35,992][root][INFO] - Step 17425920 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 17425920, mean_episode_return = None, mean_episode_step = 2069.2, total_loss = 223.37, pg_loss = 166.45, baseline_loss = 61.84, entropy_loss = -4.9226, learner_queue_size = 32, train_seconds = 9709.0, _tick = 3374, _time = 1.6546e+09)
[2022-06-07 13:05:40,998][root][INFO] - Step 17436160 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17436160, mean_episode_return = 37.9, mean_episode_step = 2040.1, total_loss = 88.889, pg_loss = 43.05, baseline_loss = 50.855, entropy_loss = -5.0166, learner_queue_size = 32, train_seconds = 9714.0, _tick = 3377, _time = 1.6546e+09)
[2022-06-07 13:05:46,002][root][INFO] - Step 17443840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17443840, mean_episode_return = None, mean_episode_step = 1964.6, total_loss = 23.438, pg_loss = -4.3642, baseline_loss = 32.934, entropy_loss = -5.1316, learner_queue_size = 32, train_seconds = 9719.0, _tick = 3377, _time = 1.6546e+09)
[2022-06-07 13:05:51,006][root][INFO] - Step 17454080 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 17454080, mean_episode_return = None, mean_episode_step = 2021.3, total_loss = -3.1607, pg_loss = -19.017, baseline_loss = 20.953, entropy_loss = -5.0967, learner_queue_size = 32, train_seconds = 9724.0, _tick = 3377, _time = 1.6546e+09)
[2022-06-07 13:05:56,008][root][INFO] - Step 17461760 @ 1535.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 17461760, mean_episode_return = 13.75, mean_episode_step = 1920.2, total_loss = 207.82, pg_loss = 135.4, baseline_loss = 77.531, entropy_loss = -5.1114, learner_queue_size = 32, train_seconds = 9729.0, _tick = 3378, _time = 1.6546e+09)
[2022-06-07 13:06:01,010][root][INFO] - Step 17472000 @ 2047.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 17472000, mean_episode_return = None, mean_episode_step = 2391.2, total_loss = -111.35, pg_loss = -118.63, baseline_loss = 12.304, entropy_loss = -5.0259, learner_queue_size = 32, train_seconds = 9734.0, _tick = 3379, _time = 1.6546e+09)
[2022-06-07 13:06:06,018][root][INFO] - Step 17479680 @ 1533.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 17479680, mean_episode_return = None, mean_episode_step = 1647.0, total_loss = -15.385, pg_loss = -24.16, baseline_loss = 13.828, entropy_loss = -5.0531, learner_queue_size = 32, train_seconds = 9739.0, _tick = 3380, _time = 1.6546e+09)
[2022-06-07 13:06:11,032][root][INFO] - Step 17489920 @ 2042.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 17489920, mean_episode_return = 34.22, mean_episode_step = 1940.8, total_loss = 76.013, pg_loss = 51.437, baseline_loss = 29.343, entropy_loss = -4.7669, learner_queue_size = 32, train_seconds = 9744.0, _tick = 3384, _time = 1.6546e+09)
[2022-06-07 13:06:16,034][root][INFO] - Step 17500160 @ 2047.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 17500160, mean_episode_return = 138.02, mean_episode_step = 2494.4, total_loss = 59.799, pg_loss = 22.424, baseline_loss = 42.157, entropy_loss = -4.7818, learner_queue_size = 32, train_seconds = 9749.0, _tick = 3388, _time = 1.6546e+09)
[2022-06-07 13:06:21,038][root][INFO] - Step 17507840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 17507840, mean_episode_return = None, mean_episode_step = 1540.9, total_loss = 575.1, pg_loss = 366.58, baseline_loss = 213.38, entropy_loss = -4.8598, learner_queue_size = 32, train_seconds = 9754.0, _tick = 3390, _time = 1.6546e+09)
[2022-06-07 13:06:26,042][root][INFO] - Step 17515520 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 17515520, mean_episode_return = None, mean_episode_step = 2768.2, total_loss = -1.2868, pg_loss = -22.089, baseline_loss = 25.693, entropy_loss = -4.8908, learner_queue_size = 32, train_seconds = 9759.0, _tick = 3392, _time = 1.6546e+09)
[2022-06-07 13:06:31,046][root][INFO] - Step 17525760 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 17525760, mean_episode_return = None, mean_episode_step = 1960.9, total_loss = 107.32, pg_loss = 43.255, baseline_loss = 69.006, entropy_loss = -4.9455, learner_queue_size = 32, train_seconds = 9764.0, _tick = 3394, _time = 1.6546e+09)
[2022-06-07 13:06:36,050][root][INFO] - Step 17533440 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 17533440, mean_episode_return = 77.03, mean_episode_step = 2255.0, total_loss = 156.92, pg_loss = 44.689, baseline_loss = 117.17, entropy_loss = -4.9421, learner_queue_size = 32, train_seconds = 9769.0, _tick = 3396, _time = 1.6546e+09)
[2022-06-07 13:06:41,056][root][INFO] - Step 17543680 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 17543680, mean_episode_return = None, mean_episode_step = 2644.6, total_loss = 216.14, pg_loss = 150.3, baseline_loss = 70.919, entropy_loss = -5.075, learner_queue_size = 32, train_seconds = 9774.1, _tick = 3396, _time = 1.6546e+09)
[2022-06-07 13:06:46,062][root][INFO] - Step 17551360 @ 1534.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 17551360, mean_episode_return = 2.4128, mean_episode_step = 2616.9, total_loss = -172.5, pg_loss = -207.86, baseline_loss = 40.425, entropy_loss = -5.0656, learner_queue_size = 32, train_seconds = 9779.1, _tick = 3399, _time = 1.6546e+09)
[2022-06-07 13:06:51,066][root][INFO] - Step 17561600 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17561600, mean_episode_return = None, mean_episode_step = 2631.2, total_loss = 203.31, pg_loss = 126.64, baseline_loss = 81.688, entropy_loss = -5.0197, learner_queue_size = 32, train_seconds = 9784.1, _tick = 3400, _time = 1.6546e+09)
[2022-06-07 13:06:56,070][root][INFO] - Step 17569280 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 17569280, mean_episode_return = 38.49, mean_episode_step = 1489.6, total_loss = 138.64, pg_loss = 79.394, baseline_loss = 64.152, entropy_loss = -4.9065, learner_queue_size = 32, train_seconds = 9789.1, _tick = 3401, _time = 1.6546e+09)
[2022-06-07 13:07:01,074][root][INFO] - Step 17579520 @ 2046.3 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 17579520, mean_episode_return = None, mean_episode_step = 2262.9, total_loss = 109.21, pg_loss = 68.252, baseline_loss = 45.836, entropy_loss = -4.8791, learner_queue_size = 32, train_seconds = 9794.1, _tick = 3402, _time = 1.6546e+09)
[2022-06-07 13:07:06,086][root][INFO] - Step 17587200 @ 1532.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 17587200, mean_episode_return = None, mean_episode_step = 1858.5, total_loss = -64.289, pg_loss = -71.622, baseline_loss = 12.235, entropy_loss = -4.9013, learner_queue_size = 32, train_seconds = 9799.1, _tick = 3402, _time = 1.6546e+09)
[2022-06-07 13:07:11,090][root][INFO] - Step 17597440 @ 2046.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 17597440, mean_episode_return = 22.032, mean_episode_step = 2334.8, total_loss = -15.644, pg_loss = -61.005, baseline_loss = 50.154, entropy_loss = -4.7929, learner_queue_size = 32, train_seconds = 9804.1, _tick = 3404, _time = 1.6546e+09)
[2022-06-07 13:07:16,094][root][INFO] - Step 17605120 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 17605120, mean_episode_return = None, mean_episode_step = 2601.9, total_loss = -42.495, pg_loss = -56.089, baseline_loss = 18.469, entropy_loss = -4.8756, learner_queue_size = 32, train_seconds = 9809.1, _tick = 3406, _time = 1.6546e+09)
[2022-06-07 13:07:21,098][root][INFO] - Step 17615360 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17615360, mean_episode_return = 97.119, mean_episode_step = 2184.9, total_loss = 1143.6, pg_loss = 389.07, baseline_loss = 759.39, entropy_loss = -4.9099, learner_queue_size = 32, train_seconds = 9814.1, _tick = 3409, _time = 1.6546e+09)
[2022-06-07 13:07:26,102][root][INFO] - Step 17623040 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 17623040, mean_episode_return = 27.791, mean_episode_step = 1700.7, total_loss = 176.17, pg_loss = 99.621, baseline_loss = 81.509, entropy_loss = -4.964, learner_queue_size = 32, train_seconds = 9819.1, _tick = 3411, _time = 1.6546e+09)
[2022-06-07 13:07:31,106][root][INFO] - Step 17633280 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 17633280, mean_episode_return = 34.95, mean_episode_step = 2496.7, total_loss = -6.4886, pg_loss = -69.669, baseline_loss = 68.082, entropy_loss = -4.9024, learner_queue_size = 32, train_seconds = 9824.1, _tick = 3414, _time = 1.6546e+09)
[2022-06-07 13:07:36,112][root][INFO] - Step 17643520 @ 2045.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 17643520, mean_episode_return = None, mean_episode_step = 2134.6, total_loss = 231.6, pg_loss = 162.24, baseline_loss = 74.346, entropy_loss = -4.987, learner_queue_size = 32, train_seconds = 9829.1, _tick = 3415, _time = 1.6546e+09)
[2022-06-07 13:07:41,118][root][INFO] - Step 17651200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17651200, mean_episode_return = None, mean_episode_step = 2575.6, total_loss = 219.08, pg_loss = 143.84, baseline_loss = 80.265, entropy_loss = -5.0243, learner_queue_size = 32, train_seconds = 9834.1, _tick = 3416, _time = 1.6546e+09)
[2022-06-07 13:07:46,122][root][INFO] - Step 17661440 @ 2046.3 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 17661440, mean_episode_return = None, mean_episode_step = 1868.5, total_loss = 241.64, pg_loss = 164.63, baseline_loss = 82.026, entropy_loss = -5.0205, learner_queue_size = 32, train_seconds = 9839.1, _tick = 3418, _time = 1.6546e+09)
[2022-06-07 13:07:51,126][root][INFO] - Step 17669120 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 17669120, mean_episode_return = None, mean_episode_step = 2098.3, total_loss = 81.366, pg_loss = 42.213, baseline_loss = 44.116, entropy_loss = -4.9631, learner_queue_size = 32, train_seconds = 9844.1, _tick = 3419, _time = 1.6546e+09)
[2022-06-07 13:07:56,130][root][INFO] - Step 17679360 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 17679360, mean_episode_return = None, mean_episode_step = 2531.7, total_loss = -29.283, pg_loss = -34.182, baseline_loss = 9.7724, entropy_loss = -4.8736, learner_queue_size = 32, train_seconds = 9849.1, _tick = 3419, _time = 1.6546e+09)
[2022-06-07 13:08:01,136][root][INFO] - Step 17687040 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17687040, mean_episode_return = 62.98, mean_episode_step = 2666.1, total_loss = 188.74, pg_loss = 137.18, baseline_loss = 56.392, entropy_loss = -4.8345, learner_queue_size = 32, train_seconds = 9854.1, _tick = 3421, _time = 1.6546e+09)
[2022-06-07 13:08:06,142][root][INFO] - Step 17697280 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 17697280, mean_episode_return = None, mean_episode_step = 2883.8, total_loss = -149.78, pg_loss = -160.34, baseline_loss = 15.415, entropy_loss = -4.8473, learner_queue_size = 32, train_seconds = 9859.1, _tick = 3422, _time = 1.6546e+09)
[2022-06-07 13:08:11,148][root][INFO] - Step 17704960 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17704960, mean_episode_return = 34.63, mean_episode_step = 1708.0, total_loss = 85.275, pg_loss = 2.8525, baseline_loss = 87.234, entropy_loss = -4.8117, learner_queue_size = 32, train_seconds = 9864.1, _tick = 3423, _time = 1.6546e+09)
[2022-06-07 13:08:16,150][root][INFO] - Step 17715200 @ 2047.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 17715200, mean_episode_return = None, mean_episode_step = 2489.5, total_loss = -83.148, pg_loss = -91.227, baseline_loss = 12.958, entropy_loss = -4.8793, learner_queue_size = 32, train_seconds = 9869.1, _tick = 3424, _time = 1.6546e+09)
[2022-06-07 13:08:21,154][root][INFO] - Step 17722880 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 17722880, mean_episode_return = None, mean_episode_step = 2156.2, total_loss = -3.6172, pg_loss = -16.61, baseline_loss = 17.848, entropy_loss = -4.8554, learner_queue_size = 32, train_seconds = 9874.1, _tick = 3425, _time = 1.6546e+09)
[2022-06-07 13:08:26,158][root][INFO] - Step 17733120 @ 2046.3 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 17733120, mean_episode_return = None, mean_episode_step = 2345.2, total_loss = -12.338, pg_loss = -21.45, baseline_loss = 13.913, entropy_loss = -4.8008, learner_queue_size = 32, train_seconds = 9879.2, _tick = 3426, _time = 1.6546e+09)
[2022-06-07 13:08:31,164][root][INFO] - Step 17740800 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17740800, mean_episode_return = 108.26, mean_episode_step = 2701.8, total_loss = -184.99, pg_loss = -187.69, baseline_loss = 7.6044, entropy_loss = -4.9052, learner_queue_size = 32, train_seconds = 9884.2, _tick = 3428, _time = 1.6546e+09)
[2022-06-07 13:08:36,170][root][INFO] - Step 17751040 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17751040, mean_episode_return = None, mean_episode_step = 2566.2, total_loss = 295.65, pg_loss = 193.94, baseline_loss = 106.64, entropy_loss = -4.9326, learner_queue_size = 32, train_seconds = 9889.2, _tick = 3430, _time = 1.6546e+09)
[2022-06-07 13:08:41,174][root][INFO] - Step 17761280 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 17761280, mean_episode_return = None, mean_episode_step = 2877.3, total_loss = 44.859, pg_loss = -2.7856, baseline_loss = 52.595, entropy_loss = -4.95, learner_queue_size = 32, train_seconds = 9894.2, _tick = 3433, _time = 1.6546e+09)
[2022-06-07 13:08:46,178][root][INFO] - Step 17768960 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17768960, mean_episode_return = 25.611, mean_episode_step = 2538.0, total_loss = -58.598, pg_loss = -77.988, baseline_loss = 24.244, entropy_loss = -4.8545, learner_queue_size = 32, train_seconds = 9899.2, _tick = 3435, _time = 1.6546e+09)
[2022-06-07 13:08:51,184][root][INFO] - Step 17779200 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17779200, mean_episode_return = None, mean_episode_step = 2149.4, total_loss = -46.568, pg_loss = -66.31, baseline_loss = 24.642, entropy_loss = -4.9001, learner_queue_size = 32, train_seconds = 9904.2, _tick = 3436, _time = 1.6546e+09)
[2022-06-07 13:08:56,190][root][INFO] - Step 17786880 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17786880, mean_episode_return = None, mean_episode_step = 2458.8, total_loss = -54.703, pg_loss = -57.472, baseline_loss = 7.6099, entropy_loss = -4.8401, learner_queue_size = 32, train_seconds = 9909.2, _tick = 3437, _time = 1.6546e+09)
[2022-06-07 13:09:01,194][root][INFO] - Step 17797120 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 17797120, mean_episode_return = None, mean_episode_step = 2133.7, total_loss = 519.65, pg_loss = 392.02, baseline_loss = 132.62, entropy_loss = -4.9927, learner_queue_size = 32, train_seconds = 9914.2, _tick = 3438, _time = 1.6546e+09)
[2022-06-07 13:09:06,200][root][INFO] - Step 17804800 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17804800, mean_episode_return = None, mean_episode_step = 2434.0, total_loss = 132.51, pg_loss = 68.202, baseline_loss = 69.338, entropy_loss = -5.0267, learner_queue_size = 32, train_seconds = 9919.2, _tick = 3438, _time = 1.6546e+09)
[2022-06-07 13:09:11,206][root][INFO] - Step 17815040 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 17815040, mean_episode_return = None, mean_episode_step = 2067.4, total_loss = 151.56, pg_loss = 79.005, baseline_loss = 77.693, entropy_loss = -5.1401, learner_queue_size = 32, train_seconds = 9924.2, _tick = 3440, _time = 1.6546e+09)
[2022-06-07 13:09:16,210][root][INFO] - Step 17822720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17822720, mean_episode_return = 42.973, mean_episode_step = 2833.6, total_loss = -222.03, pg_loss = -234.14, baseline_loss = 17.106, entropy_loss = -5.001, learner_queue_size = 32, train_seconds = 9929.2, _tick = 3442, _time = 1.6546e+09)
[2022-06-07 13:09:21,214][root][INFO] - Step 17832960 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 17832960, mean_episode_return = 136.55, mean_episode_step = 1881.7, total_loss = -100.07, pg_loss = -134.29, baseline_loss = 39.221, entropy_loss = -5.0041, learner_queue_size = 32, train_seconds = 9934.2, _tick = 3444, _time = 1.6546e+09)
[2022-06-07 13:09:26,218][root][INFO] - Step 17840640 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 17840640, mean_episode_return = None, mean_episode_step = 2502.7, total_loss = 227.23, pg_loss = 133.19, baseline_loss = 99.039, entropy_loss = -5.0023, learner_queue_size = 32, train_seconds = 9939.2, _tick = 3445, _time = 1.6546e+09)
[2022-06-07 13:09:31,224][root][INFO] - Step 17850880 @ 2045.7 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 17850880, mean_episode_return = None, mean_episode_step = 2435.7, total_loss = -6.2345, pg_loss = -38.499, baseline_loss = 37.182, entropy_loss = -4.917, learner_queue_size = 32, train_seconds = 9944.2, _tick = 3447, _time = 1.6546e+09)
[2022-06-07 13:09:36,226][root][INFO] - Step 17858560 @ 1535.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 17858560, mean_episode_return = 149.53, mean_episode_step = 2503.1, total_loss = 151.88, pg_loss = 94.452, baseline_loss = 62.19, entropy_loss = -4.758, learner_queue_size = 32, train_seconds = 9949.2, _tick = 3449, _time = 1.6546e+09)
[2022-06-07 13:09:41,230][root][INFO] - Step 17868800 @ 2046.3 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 17868800, mean_episode_return = 22.405, mean_episode_step = 2532.5, total_loss = 54.851, pg_loss = 12.657, baseline_loss = 46.891, entropy_loss = -4.6972, learner_queue_size = 32, train_seconds = 9954.2, _tick = 3453, _time = 1.6546e+09)
[2022-06-07 13:09:46,236][root][INFO] - Step 17876480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17876480, mean_episode_return = 156.77, mean_episode_step = 2136.9, total_loss = -0.065298, pg_loss = -12.983, baseline_loss = 17.521, entropy_loss = -4.6027, learner_queue_size = 32, train_seconds = 9959.2, _tick = 3456, _time = 1.6546e+09)
[2022-06-07 13:09:51,242][root][INFO] - Step 17886720 @ 2045.6 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 17886720, mean_episode_return = 32.306, mean_episode_step = 1684.5, total_loss = 230.85, pg_loss = 149.13, baseline_loss = 86.281, entropy_loss = -4.5596, learner_queue_size = 32, train_seconds = 9964.2, _tick = 3458, _time = 1.6546e+09)
[2022-06-07 13:09:56,246][root][INFO] - Step 17894400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 17894400, mean_episode_return = 8.1753, mean_episode_step = 2266.5, total_loss = 343.76, pg_loss = 245.28, baseline_loss = 103.04, entropy_loss = -4.5622, learner_queue_size = 32, train_seconds = 9969.2, _tick = 3461, _time = 1.6546e+09)
[2022-06-07 13:10:01,250][root][INFO] - Step 17904640 @ 2046.4 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 17904640, mean_episode_return = 25.047, mean_episode_step = 2165.1, total_loss = -93.282, pg_loss = -123.93, baseline_loss = 35.246, entropy_loss = -4.5994, learner_queue_size = 32, train_seconds = 9974.2, _tick = 3464, _time = 1.6546e+09)
[2022-06-07 13:10:06,256][root][INFO] - Step 17912320 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 17912320, mean_episode_return = 1.6496, mean_episode_step = 1849.7, total_loss = 66.424, pg_loss = 6.1976, baseline_loss = 64.935, entropy_loss = -4.709, learner_queue_size = 32, train_seconds = 9979.3, _tick = 3466, _time = 1.6546e+09)
[2022-06-07 13:10:11,258][root][INFO] - Step 17922560 @ 2047.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 17922560, mean_episode_return = 38.21, mean_episode_step = 2409.3, total_loss = 150.75, pg_loss = 90.264, baseline_loss = 65.22, entropy_loss = -4.7348, learner_queue_size = 32, train_seconds = 9984.3, _tick = 3470, _time = 1.6546e+09)
[2022-06-07 13:10:16,264][root][INFO] - Step 17930240 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17930240, mean_episode_return = 73.29, mean_episode_step = 2500.0, total_loss = 138.77, pg_loss = 71.942, baseline_loss = 71.634, entropy_loss = -4.8043, learner_queue_size = 32, train_seconds = 9989.3, _tick = 3473, _time = 1.6546e+09)
[2022-06-07 13:10:21,266][root][INFO] - Step 17940480 @ 2047.2 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 17940480, mean_episode_return = None, mean_episode_step = 2319.0, total_loss = 93.065, pg_loss = 35.379, baseline_loss = 62.405, entropy_loss = -4.7196, learner_queue_size = 32, train_seconds = 9994.3, _tick = 3475, _time = 1.6546e+09)
[2022-06-07 13:10:26,270][root][INFO] - Step 17948160 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17948160, mean_episode_return = None, mean_episode_step = 1646.8, total_loss = 246.07, pg_loss = 180.55, baseline_loss = 70.224, entropy_loss = -4.7063, learner_queue_size = 32, train_seconds = 9999.3, _tick = 3477, _time = 1.6546e+09)
[2022-06-07 13:10:31,275][root][INFO] - Step 17958400 @ 2046.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 17958400, mean_episode_return = None, mean_episode_step = 2053.9, total_loss = 109.51, pg_loss = 49.965, baseline_loss = 64.159, entropy_loss = -4.6143, learner_queue_size = 32, train_seconds = 1.0004e+04, _tick = 3479, _time = 1.6546e+09)
[2022-06-07 13:10:36,278][root][INFO] - Step 17966080 @ 1535.1 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 17966080, mean_episode_return = 20.622, mean_episode_step = 2299.4, total_loss = -208.96, pg_loss = -219.97, baseline_loss = 15.656, entropy_loss = -4.6519, learner_queue_size = 32, train_seconds = 1.0009e+04, _tick = 3482, _time = 1.6546e+09)
[2022-06-07 13:10:41,282][root][INFO] - Step 17976320 @ 2046.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 17976320, mean_episode_return = None, mean_episode_step = 2169.9, total_loss = 61.426, pg_loss = 19.143, baseline_loss = 46.944, entropy_loss = -4.6617, learner_queue_size = 32, train_seconds = 1.0014e+04, _tick = 3485, _time = 1.6546e+09)
[2022-06-07 13:10:46,288][root][INFO] - Step 17984000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 17984000, mean_episode_return = 24.99, mean_episode_step = 1739.6, total_loss = 90.708, pg_loss = 32.496, baseline_loss = 63.001, entropy_loss = -4.789, learner_queue_size = 32, train_seconds = 1.0019e+04, _tick = 3486, _time = 1.6546e+09)
[2022-06-07 13:10:51,294][root][INFO] - Step 17994240 @ 2045.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 17994240, mean_episode_return = None, mean_episode_step = 2256.0, total_loss = -157.06, pg_loss = -153.49, baseline_loss = 1.1858, entropy_loss = -4.7546, learner_queue_size = 32, train_seconds = 1.0024e+04, _tick = 3489, _time = 1.6546e+09)
[2022-06-07 13:10:56,300][root][INFO] - Step 18001920 @ 1534.1 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 18001920, mean_episode_return = 71.468, mean_episode_step = 2063.0, total_loss = 80.882, pg_loss = 19.625, baseline_loss = 66.107, entropy_loss = -4.8488, learner_queue_size = 32, train_seconds = 1.0029e+04, _tick = 3491, _time = 1.6546e+09)
[2022-06-07 13:11:01,306][root][INFO] - Step 18012160 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18012160, mean_episode_return = 62.498, mean_episode_step = 2278.0, total_loss = -64.645, pg_loss = -76.511, baseline_loss = 16.726, entropy_loss = -4.8606, learner_queue_size = 32, train_seconds = 1.0034e+04, _tick = 3495, _time = 1.6546e+09)
[2022-06-07 13:11:06,312][root][INFO] - Step 18019840 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18019840, mean_episode_return = None, mean_episode_step = 2131.3, total_loss = 29.339, pg_loss = 9.1331, baseline_loss = 25.058, entropy_loss = -4.8519, learner_queue_size = 32, train_seconds = 1.0039e+04, _tick = 3497, _time = 1.6546e+09)
[2022-06-07 13:11:11,318][root][INFO] - Step 18030080 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 18030080, mean_episode_return = 81.73, mean_episode_step = 2166.2, total_loss = 79.615, pg_loss = 25.987, baseline_loss = 58.447, entropy_loss = -4.8189, learner_queue_size = 32, train_seconds = 1.0044e+04, _tick = 3500, _time = 1.6546e+09)
[2022-06-07 13:11:16,322][root][INFO] - Step 18037760 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 18037760, mean_episode_return = 73.795, mean_episode_step = 2049.7, total_loss = -26.85, pg_loss = -34.204, baseline_loss = 12.028, entropy_loss = -4.6741, learner_queue_size = 32, train_seconds = 1.0049e+04, _tick = 3503, _time = 1.6546e+09)
[2022-06-07 13:11:21,326][root][INFO] - Step 18048000 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 18048000, mean_episode_return = None, mean_episode_step = 2114.8, total_loss = -95.159, pg_loss = -93.513, baseline_loss = 3.0179, entropy_loss = -4.6636, learner_queue_size = 32, train_seconds = 1.0054e+04, _tick = 3505, _time = 1.6546e+09)
[2022-06-07 13:11:26,330][root][INFO] - Step 18055680 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18055680, mean_episode_return = -9.2695, mean_episode_step = 2024.6, total_loss = -262.05, pg_loss = -282.09, baseline_loss = 24.9, entropy_loss = -4.8577, learner_queue_size = 32, train_seconds = 1.0059e+04, _tick = 3506, _time = 1.6546e+09)
[2022-06-07 13:11:31,334][root][INFO] - Step 18065920 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18065920, mean_episode_return = 27.671, mean_episode_step = 1342.4, total_loss = 283.31, pg_loss = 177.67, baseline_loss = 110.42, entropy_loss = -4.7778, learner_queue_size = 32, train_seconds = 1.0064e+04, _tick = 3509, _time = 1.6546e+09)
[2022-06-07 13:11:36,338][root][INFO] - Step 18073600 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 18073600, mean_episode_return = 31.924, mean_episode_step = 1309.9, total_loss = 122.13, pg_loss = 25.364, baseline_loss = 101.47, entropy_loss = -4.7035, learner_queue_size = 32, train_seconds = 1.0069e+04, _tick = 3510, _time = 1.6546e+09)
[2022-06-07 13:11:41,344][root][INFO] - Step 18083840 @ 2045.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 18083840, mean_episode_return = 31.225, mean_episode_step = 2072.3, total_loss = 91.82, pg_loss = 39.018, baseline_loss = 57.351, entropy_loss = -4.5483, learner_queue_size = 32, train_seconds = 1.0074e+04, _tick = 3513, _time = 1.6546e+09)
[2022-06-07 13:11:46,350][root][INFO] - Step 18094080 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18094080, mean_episode_return = 32.148, mean_episode_step = 2276.8, total_loss = -174.3, pg_loss = -233.39, baseline_loss = 63.696, entropy_loss = -4.6071, learner_queue_size = 32, train_seconds = 1.0079e+04, _tick = 3515, _time = 1.6546e+09)
[2022-06-07 13:11:51,354][root][INFO] - Step 18101760 @ 1534.9 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 18101760, mean_episode_return = 56.669, mean_episode_step = 1714.8, total_loss = 711.38, pg_loss = 472.98, baseline_loss = 242.93, entropy_loss = -4.5334, learner_queue_size = 32, train_seconds = 1.0084e+04, _tick = 3517, _time = 1.6546e+09)
[2022-06-07 13:11:56,360][root][INFO] - Step 18112000 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 18112000, mean_episode_return = None, mean_episode_step = 1757.7, total_loss = 43.973, pg_loss = 12.678, baseline_loss = 35.759, entropy_loss = -4.4635, learner_queue_size = 32, train_seconds = 1.0089e+04, _tick = 3519, _time = 1.6546e+09)
[2022-06-07 13:12:01,362][root][INFO] - Step 18119680 @ 1535.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 18119680, mean_episode_return = None, mean_episode_step = 1906.4, total_loss = 158.28, pg_loss = 72.674, baseline_loss = 90.171, entropy_loss = -4.5628, learner_queue_size = 32, train_seconds = 1.0094e+04, _tick = 3521, _time = 1.6546e+09)
[2022-06-07 13:12:06,367][root][INFO] - Step 18129920 @ 2046.0 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 18129920, mean_episode_return = 75.279, mean_episode_step = 1802.6, total_loss = -14.234, pg_loss = -41.278, baseline_loss = 31.672, entropy_loss = -4.6275, learner_queue_size = 32, train_seconds = 1.0099e+04, _tick = 3522, _time = 1.6546e+09)
[2022-06-07 13:12:11,370][root][INFO] - Step 18137600 @ 1535.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18137600, mean_episode_return = 64.792, mean_episode_step = 1875.4, total_loss = 18.303, pg_loss = -30.277, baseline_loss = 53.344, entropy_loss = -4.7641, learner_queue_size = 32, train_seconds = 1.0104e+04, _tick = 3525, _time = 1.6546e+09)
[2022-06-07 13:12:16,374][root][INFO] - Step 18145280 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 18145280, mean_episode_return = None, mean_episode_step = 1812.8, total_loss = 101.56, pg_loss = 51.017, baseline_loss = 55.357, entropy_loss = -4.8106, learner_queue_size = 32, train_seconds = 1.0109e+04, _tick = 3525, _time = 1.6546e+09)
[2022-06-07 13:12:21,378][root][INFO] - Step 18155520 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18155520, mean_episode_return = None, mean_episode_step = 1765.8, total_loss = 130.43, pg_loss = 73.043, baseline_loss = 62.113, entropy_loss = -4.7279, learner_queue_size = 32, train_seconds = 1.0114e+04, _tick = 3525, _time = 1.6546e+09)
[2022-06-07 13:12:26,382][root][INFO] - Step 18163200 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18163200, mean_episode_return = None, mean_episode_step = 2372.6, total_loss = 41.247, pg_loss = 4.5482, baseline_loss = 41.363, entropy_loss = -4.6648, learner_queue_size = 32, train_seconds = 1.0119e+04, _tick = 3525, _time = 1.6546e+09)
[2022-06-07 13:12:31,386][root][INFO] - Step 18173440 @ 2046.3 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 18173440, mean_episode_return = 119.17, mean_episode_step = 1482.2, total_loss = -62.654, pg_loss = -109.64, baseline_loss = 51.517, entropy_loss = -4.5357, learner_queue_size = 32, train_seconds = 1.0124e+04, _tick = 3529, _time = 1.6546e+09)
[2022-06-07 13:12:36,390][root][INFO] - Step 18181120 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18181120, mean_episode_return = 90.94, mean_episode_step = 1729.5, total_loss = -26.358, pg_loss = -76.131, baseline_loss = 54.206, entropy_loss = -4.4332, learner_queue_size = 32, train_seconds = 1.0129e+04, _tick = 3532, _time = 1.6546e+09)
[2022-06-07 13:12:41,394][root][INFO] - Step 18191360 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 18191360, mean_episode_return = 30.09, mean_episode_step = 1911.6, total_loss = 224.88, pg_loss = 143.18, baseline_loss = 86.322, entropy_loss = -4.6189, learner_queue_size = 32, train_seconds = 1.0134e+04, _tick = 3535, _time = 1.6546e+09)
[2022-06-07 13:12:46,400][root][INFO] - Step 18199040 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 18199040, mean_episode_return = -5.5406, mean_episode_step = 2275.1, total_loss = 3.5563, pg_loss = -56.724, baseline_loss = 65.102, entropy_loss = -4.8225, learner_queue_size = 32, train_seconds = 1.0139e+04, _tick = 3537, _time = 1.6546e+09)
[2022-06-07 13:12:51,402][root][INFO] - Step 18209280 @ 2047.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 18209280, mean_episode_return = 8.7448, mean_episode_step = 1709.0, total_loss = 97.647, pg_loss = 46.089, baseline_loss = 56.469, entropy_loss = -4.911, learner_queue_size = 32, train_seconds = 1.0144e+04, _tick = 3540, _time = 1.6546e+09)
[2022-06-07 13:12:56,408][root][INFO] - Step 18216960 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18216960, mean_episode_return = None, mean_episode_step = 2240.2, total_loss = 150.3, pg_loss = 93.161, baseline_loss = 61.97, entropy_loss = -4.8351, learner_queue_size = 32, train_seconds = 1.0149e+04, _tick = 3541, _time = 1.6546e+09)
[2022-06-07 13:13:01,414][root][INFO] - Step 18227200 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18227200, mean_episode_return = None, mean_episode_step = 1708.4, total_loss = 196.39, pg_loss = 142.52, baseline_loss = 58.762, entropy_loss = -4.8892, learner_queue_size = 32, train_seconds = 1.0154e+04, _tick = 3543, _time = 1.6546e+09)
[2022-06-07 13:13:06,418][root][INFO] - Step 18234880 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 18234880, mean_episode_return = 69.493, mean_episode_step = 2091.0, total_loss = -46.265, pg_loss = -97.629, baseline_loss = 56.036, entropy_loss = -4.6712, learner_queue_size = 32, train_seconds = 1.0159e+04, _tick = 3545, _time = 1.6546e+09)
[2022-06-07 13:13:11,422][root][INFO] - Step 18245120 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 18245120, mean_episode_return = 91.367, mean_episode_step = 1916.8, total_loss = 0.39562, pg_loss = -76.711, baseline_loss = 81.965, entropy_loss = -4.8589, learner_queue_size = 32, train_seconds = 1.0164e+04, _tick = 3547, _time = 1.6546e+09)
[2022-06-07 13:13:16,426][root][INFO] - Step 18252800 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 18252800, mean_episode_return = 80.149, mean_episode_step = 1756.4, total_loss = 39.822, pg_loss = -13.034, baseline_loss = 57.658, entropy_loss = -4.8023, learner_queue_size = 32, train_seconds = 1.0169e+04, _tick = 3550, _time = 1.6546e+09)
[2022-06-07 13:13:21,430][root][INFO] - Step 18263040 @ 2046.4 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 18263040, mean_episode_return = None, mean_episode_step = 1816.7, total_loss = 60.062, pg_loss = 30.352, baseline_loss = 34.526, entropy_loss = -4.8157, learner_queue_size = 32, train_seconds = 1.0174e+04, _tick = 3551, _time = 1.6546e+09)
[2022-06-07 13:13:26,434][root][INFO] - Step 18270720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18270720, mean_episode_return = 31.37, mean_episode_step = 2366.1, total_loss = 126.13, pg_loss = 65.075, baseline_loss = 65.848, entropy_loss = -4.794, learner_queue_size = 32, train_seconds = 1.0179e+04, _tick = 3553, _time = 1.6546e+09)
[2022-06-07 13:13:31,438][root][INFO] - Step 18280960 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 18280960, mean_episode_return = None, mean_episode_step = 2023.8, total_loss = -118.46, pg_loss = -123.55, baseline_loss = 9.9818, entropy_loss = -4.8975, learner_queue_size = 32, train_seconds = 1.0184e+04, _tick = 3554, _time = 1.6546e+09)
[2022-06-07 13:13:36,442][root][INFO] - Step 18288640 @ 1534.7 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 18288640, mean_episode_return = 72.201, mean_episode_step = 2009.3, total_loss = -125.64, pg_loss = -148.26, baseline_loss = 27.469, entropy_loss = -4.8463, learner_queue_size = 32, train_seconds = 1.0189e+04, _tick = 3556, _time = 1.6546e+09)
[2022-06-07 13:13:41,446][root][INFO] - Step 18298880 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 18298880, mean_episode_return = 101.97, mean_episode_step = 2163.2, total_loss = 71.002, pg_loss = -23.437, baseline_loss = 99.341, entropy_loss = -4.9027, learner_queue_size = 32, train_seconds = 1.0194e+04, _tick = 3558, _time = 1.6546e+09)
[2022-06-07 13:13:46,450][root][INFO] - Step 18306560 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18306560, mean_episode_return = 60.509, mean_episode_step = 1666.5, total_loss = -94.931, pg_loss = -112.11, baseline_loss = 22.105, entropy_loss = -4.9274, learner_queue_size = 32, train_seconds = 1.0199e+04, _tick = 3559, _time = 1.6546e+09)
[2022-06-07 13:13:51,456][root][INFO] - Step 18316800 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 18316800, mean_episode_return = 69.45, mean_episode_step = 2065.2, total_loss = 188.23, pg_loss = 102.48, baseline_loss = 90.585, entropy_loss = -4.8279, learner_queue_size = 32, train_seconds = 1.0204e+04, _tick = 3562, _time = 1.6546e+09)
[2022-06-07 13:13:56,462][root][INFO] - Step 18327040 @ 2045.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 18327040, mean_episode_return = None, mean_episode_step = 1808.8, total_loss = -28.299, pg_loss = -60.565, baseline_loss = 37.244, entropy_loss = -4.9773, learner_queue_size = 32, train_seconds = 1.021e+04, _tick = 3562, _time = 1.6546e+09)
[2022-06-07 13:14:01,468][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 13:14:01,692][root][INFO] - Step 18334720 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18334720, mean_episode_return = 65.01, mean_episode_step = 1655.3, total_loss = -98.402, pg_loss = -127.2, baseline_loss = 33.754, entropy_loss = -4.9511, learner_queue_size = 32, train_seconds = 1.0214e+04, _tick = 3563, _time = 1.6546e+09)
[2022-06-07 13:14:06,694][root][INFO] - Step 18344960 @ 1959.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 18344960, mean_episode_return = None, mean_episode_step = 1910.7, total_loss = 143.55, pg_loss = 59.22, baseline_loss = 89.206, entropy_loss = -4.8749, learner_queue_size = 32, train_seconds = 1.022e+04, _tick = 3566, _time = 1.6546e+09)
[2022-06-07 13:14:11,698][root][INFO] - Step 18352640 @ 1534.7 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 18352640, mean_episode_return = 105.73, mean_episode_step = 1874.1, total_loss = -94.603, pg_loss = -109.47, baseline_loss = 19.681, entropy_loss = -4.811, learner_queue_size = 32, train_seconds = 1.0225e+04, _tick = 3567, _time = 1.6546e+09)
[2022-06-07 13:14:16,704][root][INFO] - Step 18360320 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 18360320, mean_episode_return = 12.309, mean_episode_step = 1986.3, total_loss = -187.42, pg_loss = -187.67, baseline_loss = 5.015, entropy_loss = -4.766, learner_queue_size = 32, train_seconds = 1.023e+04, _tick = 3570, _time = 1.6546e+09)
[2022-06-07 13:14:21,710][root][INFO] - Step 18370560 @ 2045.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 18370560, mean_episode_return = None, mean_episode_step = 1943.4, total_loss = 212.22, pg_loss = 101.32, baseline_loss = 115.73, entropy_loss = -4.8332, learner_queue_size = 32, train_seconds = 1.0235e+04, _tick = 3571, _time = 1.6546e+09)
[2022-06-07 13:14:26,715][root][INFO] - Step 18380800 @ 2045.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18380800, mean_episode_return = 39.69, mean_episode_step = 2215.4, total_loss = -207.78, pg_loss = -210.57, baseline_loss = 7.7855, entropy_loss = -4.9965, learner_queue_size = 32, train_seconds = 1.024e+04, _tick = 3574, _time = 1.6546e+09)
[2022-06-07 13:14:31,718][root][INFO] - Step 18388480 @ 1535.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18388480, mean_episode_return = 10.79, mean_episode_step = 1803.2, total_loss = -109.58, pg_loss = -128.42, baseline_loss = 23.827, entropy_loss = -4.9832, learner_queue_size = 32, train_seconds = 1.0245e+04, _tick = 3577, _time = 1.6546e+09)
[2022-06-07 13:14:36,722][root][INFO] - Step 18398720 @ 2046.4 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 18398720, mean_episode_return = None, mean_episode_step = 2069.6, total_loss = 47.288, pg_loss = 13.7, baseline_loss = 38.602, entropy_loss = -5.0142, learner_queue_size = 32, train_seconds = 1.025e+04, _tick = 3579, _time = 1.6546e+09)
[2022-06-07 13:14:41,726][root][INFO] - Step 18406400 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18406400, mean_episode_return = 21.95, mean_episode_step = 1738.6, total_loss = 128.95, pg_loss = 67.26, baseline_loss = 66.63, entropy_loss = -4.939, learner_queue_size = 32, train_seconds = 1.0255e+04, _tick = 3581, _time = 1.6546e+09)
[2022-06-07 13:14:46,730][root][INFO] - Step 18416640 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 18416640, mean_episode_return = None, mean_episode_step = 2147.7, total_loss = 891.24, pg_loss = 637.85, baseline_loss = 258.41, entropy_loss = -5.0148, learner_queue_size = 32, train_seconds = 1.026e+04, _tick = 3584, _time = 1.6546e+09)
[2022-06-07 13:14:51,736][root][INFO] - Step 18424320 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18424320, mean_episode_return = 114.27, mean_episode_step = 1632.1, total_loss = 60.944, pg_loss = 26.236, baseline_loss = 39.64, entropy_loss = -4.9316, learner_queue_size = 32, train_seconds = 1.0265e+04, _tick = 3586, _time = 1.6546e+09)
[2022-06-07 13:14:56,752][root][INFO] - Step 18434560 @ 2041.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 18434560, mean_episode_return = None, mean_episode_step = 1900.2, total_loss = -133.84, pg_loss = -134.02, baseline_loss = 5.0361, entropy_loss = -4.8568, learner_queue_size = 32, train_seconds = 1.027e+04, _tick = 3587, _time = 1.6546e+09)
[2022-06-07 13:15:01,755][root][INFO] - Step 18442240 @ 1535.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18442240, mean_episode_return = 48.03, mean_episode_step = 2221.0, total_loss = -61.512, pg_loss = -95.241, baseline_loss = 38.602, entropy_loss = -4.8734, learner_queue_size = 32, train_seconds = 1.0275e+04, _tick = 3589, _time = 1.6546e+09)
[2022-06-07 13:15:06,758][root][INFO] - Step 18452480 @ 2046.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18452480, mean_episode_return = 120.79, mean_episode_step = 1731.2, total_loss = -29.413, pg_loss = -80.528, baseline_loss = 55.928, entropy_loss = -4.8128, learner_queue_size = 32, train_seconds = 1.028e+04, _tick = 3592, _time = 1.6546e+09)
[2022-06-07 13:15:11,762][root][INFO] - Step 18460160 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18460160, mean_episode_return = None, mean_episode_step = 2005.0, total_loss = 67.412, pg_loss = 15.453, baseline_loss = 56.745, entropy_loss = -4.7863, learner_queue_size = 32, train_seconds = 1.0285e+04, _tick = 3592, _time = 1.6546e+09)
[2022-06-07 13:15:16,766][root][INFO] - Step 18470400 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 18470400, mean_episode_return = 88.539, mean_episode_step = 2071.8, total_loss = -67.874, pg_loss = -90.451, baseline_loss = 27.335, entropy_loss = -4.7588, learner_queue_size = 32, train_seconds = 1.029e+04, _tick = 3595, _time = 1.6546e+09)
[2022-06-07 13:15:21,770][root][INFO] - Step 18480640 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 18480640, mean_episode_return = None, mean_episode_step = 1890.2, total_loss = 253.37, pg_loss = 168.7, baseline_loss = 89.525, entropy_loss = -4.8601, learner_queue_size = 32, train_seconds = 1.0295e+04, _tick = 3598, _time = 1.6546e+09)
[2022-06-07 13:15:26,774][root][INFO] - Step 18488320 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18488320, mean_episode_return = None, mean_episode_step = 2278.8, total_loss = -120.9, pg_loss = -120.41, baseline_loss = 4.3245, entropy_loss = -4.81, learner_queue_size = 32, train_seconds = 1.03e+04, _tick = 3599, _time = 1.6546e+09)
[2022-06-07 13:15:31,778][root][INFO] - Step 18496000 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 18496000, mean_episode_return = 35.225, mean_episode_step = 1812.7, total_loss = -204.63, pg_loss = -216.97, baseline_loss = 17.183, entropy_loss = -4.8487, learner_queue_size = 32, train_seconds = 1.0305e+04, _tick = 3601, _time = 1.6546e+09)
[2022-06-07 13:15:36,784][root][INFO] - Step 18506240 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18506240, mean_episode_return = None, mean_episode_step = 1814.8, total_loss = 991.15, pg_loss = 426.1, baseline_loss = 569.92, entropy_loss = -4.8758, learner_queue_size = 32, train_seconds = 1.031e+04, _tick = 3603, _time = 1.6546e+09)
[2022-06-07 13:15:41,790][root][INFO] - Step 18516480 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18516480, mean_episode_return = 16.13, mean_episode_step = 1724.6, total_loss = 258.09, pg_loss = 173.08, baseline_loss = 89.975, entropy_loss = -4.9699, learner_queue_size = 32, train_seconds = 1.0315e+04, _tick = 3605, _time = 1.6546e+09)
[2022-06-07 13:15:46,796][root][INFO] - Step 18524160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18524160, mean_episode_return = None, mean_episode_step = 2084.2, total_loss = -129.39, pg_loss = -127.76, baseline_loss = 3.359, entropy_loss = -4.9871, learner_queue_size = 32, train_seconds = 1.032e+04, _tick = 3606, _time = 1.6546e+09)
[2022-06-07 13:15:51,802][root][INFO] - Step 18531840 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18531840, mean_episode_return = 65.494, mean_episode_step = 1972.9, total_loss = -67.074, pg_loss = -98.309, baseline_loss = 36.218, entropy_loss = -4.9833, learner_queue_size = 32, train_seconds = 1.0325e+04, _tick = 3607, _time = 1.6546e+09)
[2022-06-07 13:15:56,806][root][INFO] - Step 18542080 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 18542080, mean_episode_return = None, mean_episode_step = 2043.9, total_loss = 91.702, pg_loss = 65.746, baseline_loss = 30.889, entropy_loss = -4.9333, learner_queue_size = 32, train_seconds = 1.033e+04, _tick = 3608, _time = 1.6546e+09)
[2022-06-07 13:16:01,810][root][INFO] - Step 18549760 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18549760, mean_episode_return = 36.952, mean_episode_step = 1954.8, total_loss = -207.69, pg_loss = -214.61, baseline_loss = 11.962, entropy_loss = -5.0418, learner_queue_size = 32, train_seconds = 1.0335e+04, _tick = 3609, _time = 1.6546e+09)
[2022-06-07 13:16:06,816][root][INFO] - Step 18560000 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 18560000, mean_episode_return = 22.372, mean_episode_step = 1956.0, total_loss = -91.857, pg_loss = -136.6, baseline_loss = 49.805, entropy_loss = -5.059, learner_queue_size = 32, train_seconds = 1.034e+04, _tick = 3611, _time = 1.6546e+09)
[2022-06-07 13:16:11,822][root][INFO] - Step 18567680 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 18567680, mean_episode_return = 49.403, mean_episode_step = 1819.2, total_loss = 1279.5, pg_loss = 475.42, baseline_loss = 808.98, entropy_loss = -4.9178, learner_queue_size = 32, train_seconds = 1.0345e+04, _tick = 3612, _time = 1.6546e+09)
[2022-06-07 13:16:16,826][root][INFO] - Step 18577920 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18577920, mean_episode_return = None, mean_episode_step = 2549.9, total_loss = 128.98, pg_loss = 79.059, baseline_loss = 54.901, entropy_loss = -4.9813, learner_queue_size = 32, train_seconds = 1.035e+04, _tick = 3615, _time = 1.6546e+09)
[2022-06-07 13:16:21,832][root][INFO] - Step 18588160 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 18588160, mean_episode_return = 9.1413, mean_episode_step = 1759.4, total_loss = 416.41, pg_loss = 288.96, baseline_loss = 132.55, entropy_loss = -5.1053, learner_queue_size = 32, train_seconds = 1.0355e+04, _tick = 3618, _time = 1.6546e+09)
[2022-06-07 13:16:26,838][root][INFO] - Step 18595840 @ 1534.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 18595840, mean_episode_return = None, mean_episode_step = 1741.4, total_loss = 7.3101, pg_loss = -3.161, baseline_loss = 15.537, entropy_loss = -5.0656, learner_queue_size = 32, train_seconds = 1.036e+04, _tick = 3619, _time = 1.6546e+09)
[2022-06-07 13:16:31,842][root][INFO] - Step 18606080 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 18606080, mean_episode_return = None, mean_episode_step = 1971.7, total_loss = 171.74, pg_loss = 94.056, baseline_loss = 82.858, entropy_loss = -5.1713, learner_queue_size = 32, train_seconds = 1.0365e+04, _tick = 3621, _time = 1.6546e+09)
[2022-06-07 13:16:36,848][root][INFO] - Step 18613760 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 18613760, mean_episode_return = None, mean_episode_step = 2038.6, total_loss = -144.22, pg_loss = -141.75, baseline_loss = 2.5299, entropy_loss = -4.9908, learner_queue_size = 32, train_seconds = 1.037e+04, _tick = 3623, _time = 1.6546e+09)
[2022-06-07 13:16:41,853][root][INFO] - Step 18621440 @ 1534.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18621440, mean_episode_return = None, mean_episode_step = 2261.7, total_loss = 128.19, pg_loss = 85.651, baseline_loss = 47.562, entropy_loss = -5.0189, learner_queue_size = 32, train_seconds = 1.0375e+04, _tick = 3624, _time = 1.6546e+09)
[2022-06-07 13:16:46,858][root][INFO] - Step 18631680 @ 2046.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 18631680, mean_episode_return = None, mean_episode_step = 2375.5, total_loss = 329.43, pg_loss = 227.5, baseline_loss = 106.96, entropy_loss = -5.0327, learner_queue_size = 32, train_seconds = 1.038e+04, _tick = 3626, _time = 1.6546e+09)
[2022-06-07 13:16:51,862][root][INFO] - Step 18639360 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18639360, mean_episode_return = 12.94, mean_episode_step = 1963.1, total_loss = 171.13, pg_loss = 96.669, baseline_loss = 79.548, entropy_loss = -5.0858, learner_queue_size = 32, train_seconds = 1.0385e+04, _tick = 3629, _time = 1.6546e+09)
[2022-06-07 13:16:56,866][root][INFO] - Step 18649600 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18649600, mean_episode_return = 32.16, mean_episode_step = 1837.4, total_loss = -141.34, pg_loss = -152.0, baseline_loss = 15.664, entropy_loss = -5.0013, learner_queue_size = 32, train_seconds = 1.039e+04, _tick = 3632, _time = 1.6546e+09)
[2022-06-07 13:17:01,872][root][INFO] - Step 18657280 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18657280, mean_episode_return = None, mean_episode_step = 1958.0, total_loss = 18.878, pg_loss = 1.7644, baseline_loss = 22.116, entropy_loss = -5.003, learner_queue_size = 32, train_seconds = 1.0395e+04, _tick = 3633, _time = 1.6546e+09)
[2022-06-07 13:17:06,878][root][INFO] - Step 18667520 @ 2045.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 18667520, mean_episode_return = None, mean_episode_step = 2191.0, total_loss = 348.29, pg_loss = 252.68, baseline_loss = 100.63, entropy_loss = -5.0223, learner_queue_size = 32, train_seconds = 1.04e+04, _tick = 3635, _time = 1.6546e+09)
[2022-06-07 13:17:11,884][root][INFO] - Step 18677760 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 18677760, mean_episode_return = 187.36, mean_episode_step = 2289.6, total_loss = 213.39, pg_loss = 105.01, baseline_loss = 113.37, entropy_loss = -4.9956, learner_queue_size = 32, train_seconds = 1.0405e+04, _tick = 3638, _time = 1.6546e+09)
[2022-06-07 13:17:16,890][root][INFO] - Step 18685440 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 18685440, mean_episode_return = None, mean_episode_step = 2149.8, total_loss = -127.46, pg_loss = -127.16, baseline_loss = 4.7484, entropy_loss = -5.0546, learner_queue_size = 32, train_seconds = 1.041e+04, _tick = 3638, _time = 1.6546e+09)
[2022-06-07 13:17:21,896][root][INFO] - Step 18695680 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18695680, mean_episode_return = 116.52, mean_episode_step = 2007.4, total_loss = 316.73, pg_loss = 211.19, baseline_loss = 110.56, entropy_loss = -5.022, learner_queue_size = 32, train_seconds = 1.0415e+04, _tick = 3642, _time = 1.6546e+09)
[2022-06-07 13:17:26,902][root][INFO] - Step 18703360 @ 1534.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 18703360, mean_episode_return = 106.82, mean_episode_step = 2173.0, total_loss = 51.103, pg_loss = -57.301, baseline_loss = 113.46, entropy_loss = -5.0589, learner_queue_size = 32, train_seconds = 1.042e+04, _tick = 3643, _time = 1.6546e+09)
[2022-06-07 13:17:31,906][root][INFO] - Step 18713600 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 18713600, mean_episode_return = 74.265, mean_episode_step = 2189.4, total_loss = -243.82, pg_loss = -259.93, baseline_loss = 21.166, entropy_loss = -5.0599, learner_queue_size = 32, train_seconds = 1.0425e+04, _tick = 3647, _time = 1.6546e+09)
[2022-06-07 13:17:36,910][root][INFO] - Step 18721280 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18721280, mean_episode_return = None, mean_episode_step = 2051.5, total_loss = -88.166, pg_loss = -103.64, baseline_loss = 20.588, entropy_loss = -5.113, learner_queue_size = 32, train_seconds = 1.043e+04, _tick = 3648, _time = 1.6546e+09)
[2022-06-07 13:17:41,916][root][INFO] - Step 18731520 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 18731520, mean_episode_return = None, mean_episode_step = 1999.7, total_loss = 31.011, pg_loss = -2.7927, baseline_loss = 38.916, entropy_loss = -5.1116, learner_queue_size = 32, train_seconds = 1.0435e+04, _tick = 3650, _time = 1.6546e+09)
[2022-06-07 13:17:46,918][root][INFO] - Step 18741760 @ 2047.1 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 18741760, mean_episode_return = 27.774, mean_episode_step = 1672.9, total_loss = 519.85, pg_loss = 355.63, baseline_loss = 169.34, entropy_loss = -5.1244, learner_queue_size = 32, train_seconds = 1.044e+04, _tick = 3652, _time = 1.6546e+09)
[2022-06-07 13:17:51,922][root][INFO] - Step 18749440 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 18749440, mean_episode_return = 120.74, mean_episode_step = 2655.9, total_loss = 329.8, pg_loss = 165.61, baseline_loss = 169.25, entropy_loss = -5.0666, learner_queue_size = 32, train_seconds = 1.0445e+04, _tick = 3655, _time = 1.6546e+09)
[2022-06-07 13:17:56,927][root][INFO] - Step 18759680 @ 2046.2 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 18759680, mean_episode_return = 65.621, mean_episode_step = 1913.4, total_loss = 65.886, pg_loss = 25.151, baseline_loss = 45.855, entropy_loss = -5.12, learner_queue_size = 32, train_seconds = 1.045e+04, _tick = 3659, _time = 1.6546e+09)
[2022-06-07 13:18:01,930][root][INFO] - Step 18767360 @ 1534.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18767360, mean_episode_return = 38.411, mean_episode_step = 2058.4, total_loss = 71.222, pg_loss = 23.999, baseline_loss = 52.382, entropy_loss = -5.1588, learner_queue_size = 32, train_seconds = 1.0455e+04, _tick = 3661, _time = 1.6546e+09)
[2022-06-07 13:18:06,934][root][INFO] - Step 18777600 @ 2046.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 18777600, mean_episode_return = 66.43, mean_episode_step = 1821.4, total_loss = -32.516, pg_loss = -56.58, baseline_loss = 28.947, entropy_loss = -4.8827, learner_queue_size = 32, train_seconds = 1.046e+04, _tick = 3664, _time = 1.6546e+09)
[2022-06-07 13:18:11,938][root][INFO] - Step 18785280 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 18785280, mean_episode_return = 0.87462, mean_episode_step = 1609.5, total_loss = -171.74, pg_loss = -179.42, baseline_loss = 12.611, entropy_loss = -4.9298, learner_queue_size = 32, train_seconds = 1.0465e+04, _tick = 3666, _time = 1.6546e+09)
[2022-06-07 13:18:16,942][root][INFO] - Step 18795520 @ 2046.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18795520, mean_episode_return = 15.765, mean_episode_step = 1903.0, total_loss = -220.05, pg_loss = -235.2, baseline_loss = 20.183, entropy_loss = -5.0323, learner_queue_size = 32, train_seconds = 1.047e+04, _tick = 3670, _time = 1.6546e+09)
[2022-06-07 13:18:21,948][root][INFO] - Step 18803200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18803200, mean_episode_return = 43.467, mean_episode_step = 2151.0, total_loss = -50.305, pg_loss = -89.33, baseline_loss = 44.095, entropy_loss = -5.0692, learner_queue_size = 32, train_seconds = 1.0475e+04, _tick = 3672, _time = 1.6546e+09)
[2022-06-07 13:18:26,950][root][INFO] - Step 18813440 @ 2047.0 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 18813440, mean_episode_return = None, mean_episode_step = 2120.0, total_loss = 133.76, pg_loss = 78.712, baseline_loss = 60.048, entropy_loss = -5.003, learner_queue_size = 32, train_seconds = 1.048e+04, _tick = 3672, _time = 1.6546e+09)
[2022-06-07 13:18:31,956][root][INFO] - Step 18821120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 18821120, mean_episode_return = None, mean_episode_step = 1520.3, total_loss = 183.61, pg_loss = 119.53, baseline_loss = 68.957, entropy_loss = -4.8729, learner_queue_size = 32, train_seconds = 1.0485e+04, _tick = 3674, _time = 1.6546e+09)
[2022-06-07 13:18:36,962][root][INFO] - Step 18831360 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 18831360, mean_episode_return = 76.023, mean_episode_step = 1735.1, total_loss = -84.148, pg_loss = -130.81, baseline_loss = 51.464, entropy_loss = -4.8037, learner_queue_size = 32, train_seconds = 1.049e+04, _tick = 3677, _time = 1.6546e+09)
[2022-06-07 13:18:41,968][root][INFO] - Step 18839040 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 18839040, mean_episode_return = 35.901, mean_episode_step = 2389.9, total_loss = 247.91, pg_loss = 113.95, baseline_loss = 138.69, entropy_loss = -4.7263, learner_queue_size = 32, train_seconds = 1.0495e+04, _tick = 3680, _time = 1.6546e+09)
[2022-06-07 13:18:46,974][root][INFO] - Step 18849280 @ 2045.6 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 18849280, mean_episode_return = None, mean_episode_step = 1994.6, total_loss = 17.558, pg_loss = -17.841, baseline_loss = 40.233, entropy_loss = -4.8342, learner_queue_size = 32, train_seconds = 1.05e+04, _tick = 3681, _time = 1.6546e+09)
[2022-06-07 13:18:51,980][root][INFO] - Step 18856960 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18856960, mean_episode_return = -9.9004, mean_episode_step = 1816.8, total_loss = 25.301, pg_loss = -33.815, baseline_loss = 63.873, entropy_loss = -4.7566, learner_queue_size = 32, train_seconds = 1.0505e+04, _tick = 3684, _time = 1.6546e+09)
[2022-06-07 13:18:56,986][root][INFO] - Step 18867200 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 18867200, mean_episode_return = 82.148, mean_episode_step = 2186.4, total_loss = -71.544, pg_loss = -127.16, baseline_loss = 60.403, entropy_loss = -4.7874, learner_queue_size = 32, train_seconds = 1.051e+04, _tick = 3687, _time = 1.6546e+09)
[2022-06-07 13:19:01,990][root][INFO] - Step 18874880 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18874880, mean_episode_return = 23.78, mean_episode_step = 2124.3, total_loss = -12.643, pg_loss = -42.488, baseline_loss = 34.661, entropy_loss = -4.8161, learner_queue_size = 32, train_seconds = 1.0515e+04, _tick = 3689, _time = 1.6546e+09)
[2022-06-07 13:19:06,995][root][INFO] - Step 18885120 @ 2045.9 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 18885120, mean_episode_return = 54.289, mean_episode_step = 1394.7, total_loss = -123.87, pg_loss = -170.82, baseline_loss = 51.799, entropy_loss = -4.8496, learner_queue_size = 32, train_seconds = 1.052e+04, _tick = 3692, _time = 1.6546e+09)
[2022-06-07 13:19:12,001][root][INFO] - Step 18892800 @ 1534.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 18892800, mean_episode_return = None, mean_episode_step = 2328.0, total_loss = -79.696, pg_loss = -87.553, baseline_loss = 12.756, entropy_loss = -4.8986, learner_queue_size = 32, train_seconds = 1.0525e+04, _tick = 3692, _time = 1.6546e+09)
[2022-06-07 13:19:17,006][root][INFO] - Step 18903040 @ 2046.0 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 18903040, mean_episode_return = 158.72, mean_episode_step = 1743.0, total_loss = -84.792, pg_loss = -137.19, baseline_loss = 57.284, entropy_loss = -4.8816, learner_queue_size = 32, train_seconds = 1.053e+04, _tick = 3695, _time = 1.6546e+09)
[2022-06-07 13:19:22,012][root][INFO] - Step 18910720 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 18910720, mean_episode_return = 107.27, mean_episode_step = 2141.2, total_loss = 1.4348e+04, pg_loss = 2098.3, baseline_loss = 1.2255e+04, entropy_loss = -4.8359, learner_queue_size = 32, train_seconds = 1.0535e+04, _tick = 3697, _time = 1.6546e+09)
[2022-06-07 13:19:27,018][root][INFO] - Step 18920960 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18920960, mean_episode_return = None, mean_episode_step = 1626.8, total_loss = 84.715, pg_loss = 24.07, baseline_loss = 65.442, entropy_loss = -4.7968, learner_queue_size = 32, train_seconds = 1.054e+04, _tick = 3699, _time = 1.6546e+09)
[2022-06-07 13:19:32,022][root][INFO] - Step 18928640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 18928640, mean_episode_return = None, mean_episode_step = 1599.8, total_loss = 27.512, pg_loss = -1.698, baseline_loss = 34.159, entropy_loss = -4.9491, learner_queue_size = 32, train_seconds = 1.0545e+04, _tick = 3699, _time = 1.6546e+09)
[2022-06-07 13:19:37,026][root][INFO] - Step 18938880 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 18938880, mean_episode_return = 37.415, mean_episode_step = 1938.2, total_loss = -67.21, pg_loss = -119.62, baseline_loss = 57.322, entropy_loss = -4.9082, learner_queue_size = 32, train_seconds = 1.055e+04, _tick = 3702, _time = 1.6546e+09)
[2022-06-07 13:19:42,032][root][INFO] - Step 18946560 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 18946560, mean_episode_return = None, mean_episode_step = 1492.0, total_loss = 68.934, pg_loss = 47.125, baseline_loss = 26.705, entropy_loss = -4.8959, learner_queue_size = 32, train_seconds = 1.0555e+04, _tick = 3702, _time = 1.6546e+09)
[2022-06-07 13:19:47,038][root][INFO] - Step 18956800 @ 2045.6 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 18956800, mean_episode_return = 33.0, mean_episode_step = 1701.4, total_loss = -62.609, pg_loss = -101.13, baseline_loss = 43.452, entropy_loss = -4.9297, learner_queue_size = 32, train_seconds = 1.056e+04, _tick = 3705, _time = 1.6546e+09)
[2022-06-07 13:19:52,042][root][INFO] - Step 18967040 @ 2046.3 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 18967040, mean_episode_return = 59.18, mean_episode_step = 2139.4, total_loss = -19.184, pg_loss = -52.735, baseline_loss = 38.262, entropy_loss = -4.7102, learner_queue_size = 32, train_seconds = 1.0565e+04, _tick = 3708, _time = 1.6546e+09)
[2022-06-07 13:19:57,061][root][INFO] - Step 18974720 @ 1530.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 18974720, mean_episode_return = 33.08, mean_episode_step = 1968.9, total_loss = -135.53, pg_loss = -156.86, baseline_loss = 26.126, entropy_loss = -4.7984, learner_queue_size = 32, train_seconds = 1.057e+04, _tick = 3709, _time = 1.6546e+09)
[2022-06-07 13:20:02,066][root][INFO] - Step 18984960 @ 2045.7 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 18984960, mean_episode_return = 9.0897, mean_episode_step = 1860.9, total_loss = 101.9, pg_loss = 61.267, baseline_loss = 45.253, entropy_loss = -4.6244, learner_queue_size = 32, train_seconds = 1.0575e+04, _tick = 3712, _time = 1.6546e+09)
[2022-06-07 13:20:07,070][root][INFO] - Step 18992640 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 18992640, mean_episode_return = None, mean_episode_step = 2024.2, total_loss = -33.458, pg_loss = -50.366, baseline_loss = 21.679, entropy_loss = -4.7708, learner_queue_size = 32, train_seconds = 1.058e+04, _tick = 3712, _time = 1.6546e+09)
[2022-06-07 13:20:12,076][root][INFO] - Step 19000320 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19000320, mean_episode_return = 72.707, mean_episode_step = 2610.0, total_loss = -16.03, pg_loss = -76.619, baseline_loss = 65.264, entropy_loss = -4.6756, learner_queue_size = 32, train_seconds = 1.0585e+04, _tick = 3714, _time = 1.6546e+09)
[2022-06-07 13:20:17,082][root][INFO] - Step 19010560 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 19010560, mean_episode_return = 22.06, mean_episode_step = 1931.0, total_loss = 22.224, pg_loss = -26.909, baseline_loss = 53.889, entropy_loss = -4.7562, learner_queue_size = 32, train_seconds = 1.059e+04, _tick = 3716, _time = 1.6546e+09)
[2022-06-07 13:20:22,086][root][INFO] - Step 19018240 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 19018240, mean_episode_return = 88.788, mean_episode_step = 1916.7, total_loss = 45.314, pg_loss = 7.419, baseline_loss = 42.735, entropy_loss = -4.8403, learner_queue_size = 32, train_seconds = 1.0595e+04, _tick = 3717, _time = 1.6546e+09)
[2022-06-07 13:20:27,090][root][INFO] - Step 19028480 @ 2046.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 19028480, mean_episode_return = None, mean_episode_step = 1896.5, total_loss = -69.97, pg_loss = -71.74, baseline_loss = 6.6126, entropy_loss = -4.8423, learner_queue_size = 32, train_seconds = 1.06e+04, _tick = 3719, _time = 1.6546e+09)
[2022-06-07 13:20:32,094][root][INFO] - Step 19036160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19036160, mean_episode_return = None, mean_episode_step = 2395.0, total_loss = 613.13, pg_loss = 434.18, baseline_loss = 183.79, entropy_loss = -4.838, learner_queue_size = 32, train_seconds = 1.0605e+04, _tick = 3720, _time = 1.6546e+09)
[2022-06-07 13:20:37,100][root][INFO] - Step 19046400 @ 2045.5 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 19046400, mean_episode_return = 41.31, mean_episode_step = 2232.0, total_loss = -181.23, pg_loss = -192.69, baseline_loss = 16.236, entropy_loss = -4.7764, learner_queue_size = 32, train_seconds = 1.061e+04, _tick = 3721, _time = 1.6546e+09)
[2022-06-07 13:20:42,101][root][INFO] - Step 19054080 @ 1535.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19054080, mean_episode_return = None, mean_episode_step = 1986.8, total_loss = -93.267, pg_loss = -109.99, baseline_loss = 21.486, entropy_loss = -4.7647, learner_queue_size = 32, train_seconds = 1.0615e+04, _tick = 3721, _time = 1.6546e+09)
[2022-06-07 13:20:47,106][root][INFO] - Step 19064320 @ 2046.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 19064320, mean_episode_return = None, mean_episode_step = 2375.3, total_loss = 47.967, pg_loss = 18.705, baseline_loss = 34.017, entropy_loss = -4.7547, learner_queue_size = 32, train_seconds = 1.062e+04, _tick = 3722, _time = 1.6546e+09)
[2022-06-07 13:20:52,110][root][INFO] - Step 19072000 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19072000, mean_episode_return = None, mean_episode_step = 2026.2, total_loss = 239.1, pg_loss = 174.81, baseline_loss = 69.034, entropy_loss = -4.747, learner_queue_size = 32, train_seconds = 1.0625e+04, _tick = 3722, _time = 1.6546e+09)
[2022-06-07 13:20:57,116][root][INFO] - Step 19082240 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 19082240, mean_episode_return = 50.181, mean_episode_step = 2318.5, total_loss = 51.944, pg_loss = 4.6517, baseline_loss = 52.048, entropy_loss = -4.7557, learner_queue_size = 32, train_seconds = 1.063e+04, _tick = 3725, _time = 1.6546e+09)
[2022-06-07 13:21:02,122][root][INFO] - Step 19089920 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19089920, mean_episode_return = 25.496, mean_episode_step = 1856.9, total_loss = 150.13, pg_loss = 50.099, baseline_loss = 104.9, entropy_loss = -4.8711, learner_queue_size = 32, train_seconds = 1.0635e+04, _tick = 3727, _time = 1.6546e+09)
[2022-06-07 13:21:07,126][root][INFO] - Step 19100160 @ 2046.3 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 19100160, mean_episode_return = None, mean_episode_step = 2822.5, total_loss = 19.293, pg_loss = -17.44, baseline_loss = 41.598, entropy_loss = -4.8647, learner_queue_size = 32, train_seconds = 1.064e+04, _tick = 3729, _time = 1.6546e+09)
[2022-06-07 13:21:12,130][root][INFO] - Step 19107840 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19107840, mean_episode_return = None, mean_episode_step = 2296.2, total_loss = 27.953, pg_loss = 0.64536, baseline_loss = 32.101, entropy_loss = -4.7936, learner_queue_size = 32, train_seconds = 1.0645e+04, _tick = 3731, _time = 1.6546e+09)
[2022-06-07 13:21:17,134][root][INFO] - Step 19118080 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19118080, mean_episode_return = 47.611, mean_episode_step = 1763.5, total_loss = -34.506, pg_loss = -59.304, baseline_loss = 29.541, entropy_loss = -4.7432, learner_queue_size = 32, train_seconds = 1.065e+04, _tick = 3733, _time = 1.6546e+09)
[2022-06-07 13:21:22,138][root][INFO] - Step 19125760 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 19125760, mean_episode_return = None, mean_episode_step = 2307.9, total_loss = -25.681, pg_loss = -42.217, baseline_loss = 21.05, entropy_loss = -4.5144, learner_queue_size = 32, train_seconds = 1.0655e+04, _tick = 3735, _time = 1.6546e+09)
[2022-06-07 13:21:27,144][root][INFO] - Step 19136000 @ 2045.7 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 19136000, mean_episode_return = None, mean_episode_step = 2412.9, total_loss = -68.58, pg_loss = -70.203, baseline_loss = 6.2275, entropy_loss = -4.6046, learner_queue_size = 32, train_seconds = 1.066e+04, _tick = 3738, _time = 1.6546e+09)
[2022-06-07 13:21:32,150][root][INFO] - Step 19143680 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 19143680, mean_episode_return = None, mean_episode_step = 2113.8, total_loss = 177.87, pg_loss = 117.13, baseline_loss = 65.393, entropy_loss = -4.6615, learner_queue_size = 32, train_seconds = 1.0665e+04, _tick = 3739, _time = 1.6546e+09)
[2022-06-07 13:21:37,155][root][INFO] - Step 19153920 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 19153920, mean_episode_return = None, mean_episode_step = 2269.1, total_loss = 181.07, pg_loss = 114.24, baseline_loss = 71.502, entropy_loss = -4.6651, learner_queue_size = 32, train_seconds = 1.067e+04, _tick = 3739, _time = 1.6546e+09)
[2022-06-07 13:21:42,162][root][INFO] - Step 19161600 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 19161600, mean_episode_return = 37.89, mean_episode_step = 1862.3, total_loss = -107.91, pg_loss = -124.71, baseline_loss = 21.49, entropy_loss = -4.6881, learner_queue_size = 32, train_seconds = 1.0675e+04, _tick = 3742, _time = 1.6546e+09)
[2022-06-07 13:21:47,166][root][INFO] - Step 19171840 @ 2046.2 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 19171840, mean_episode_return = 71.5, mean_episode_step = 1666.8, total_loss = 94.87, pg_loss = 43.62, baseline_loss = 56.051, entropy_loss = -4.8012, learner_queue_size = 32, train_seconds = 1.068e+04, _tick = 3744, _time = 1.6546e+09)
[2022-06-07 13:21:52,171][root][INFO] - Step 19179520 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19179520, mean_episode_return = None, mean_episode_step = 2751.0, total_loss = -177.01, pg_loss = -173.45, baseline_loss = 1.2533, entropy_loss = -4.8127, learner_queue_size = 32, train_seconds = 1.0685e+04, _tick = 3745, _time = 1.6546e+09)
[2022-06-07 13:21:57,177][root][INFO] - Step 19189760 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 19189760, mean_episode_return = 56.171, mean_episode_step = 2138.0, total_loss = -109.72, pg_loss = -117.76, baseline_loss = 12.95, entropy_loss = -4.9161, learner_queue_size = 32, train_seconds = 1.069e+04, _tick = 3747, _time = 1.6546e+09)
[2022-06-07 13:22:02,183][root][INFO] - Step 19197440 @ 1534.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 19197440, mean_episode_return = -11.56, mean_episode_step = 2570.8, total_loss = 161.58, pg_loss = 94.963, baseline_loss = 71.556, entropy_loss = -4.9403, learner_queue_size = 32, train_seconds = 1.0695e+04, _tick = 3748, _time = 1.6546e+09)
[2022-06-07 13:22:07,186][root][INFO] - Step 19207680 @ 2046.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19207680, mean_episode_return = None, mean_episode_step = 2120.0, total_loss = -94.2, pg_loss = -98.45, baseline_loss = 9.2945, entropy_loss = -5.0452, learner_queue_size = 32, train_seconds = 1.07e+04, _tick = 3748, _time = 1.6546e+09)
[2022-06-07 13:22:12,192][root][INFO] - Step 19215360 @ 1534.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 19215360, mean_episode_return = 83.24, mean_episode_step = 2631.0, total_loss = -24.221, pg_loss = -62.86, baseline_loss = 43.64, entropy_loss = -5.0006, learner_queue_size = 32, train_seconds = 1.0705e+04, _tick = 3750, _time = 1.6546e+09)
[2022-06-07 13:22:17,194][root][INFO] - Step 19225600 @ 2047.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19225600, mean_episode_return = None, mean_episode_step = 2182.2, total_loss = 54.91, pg_loss = 33.874, baseline_loss = 25.986, entropy_loss = -4.9502, learner_queue_size = 32, train_seconds = 1.071e+04, _tick = 3753, _time = 1.6546e+09)
[2022-06-07 13:22:22,200][root][INFO] - Step 19233280 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 19233280, mean_episode_return = -21.06, mean_episode_step = 2278.4, total_loss = 101.84, pg_loss = 33.666, baseline_loss = 73.132, entropy_loss = -4.9605, learner_queue_size = 32, train_seconds = 1.0715e+04, _tick = 3755, _time = 1.6546e+09)
[2022-06-07 13:22:27,206][root][INFO] - Step 19243520 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19243520, mean_episode_return = 82.831, mean_episode_step = 1891.7, total_loss = 260.63, pg_loss = 129.85, baseline_loss = 135.73, entropy_loss = -4.9484, learner_queue_size = 32, train_seconds = 1.072e+04, _tick = 3759, _time = 1.6546e+09)
[2022-06-07 13:22:32,209][root][INFO] - Step 19251200 @ 1535.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 19251200, mean_episode_return = 62.31, mean_episode_step = 2143.4, total_loss = -110.49, pg_loss = -126.83, baseline_loss = 21.29, entropy_loss = -4.9471, learner_queue_size = 32, train_seconds = 1.0725e+04, _tick = 3761, _time = 1.6546e+09)
[2022-06-07 13:22:37,214][root][INFO] - Step 19261440 @ 2046.0 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 19261440, mean_episode_return = None, mean_episode_step = 2310.8, total_loss = -126.78, pg_loss = -123.32, baseline_loss = 1.5537, entropy_loss = -5.0128, learner_queue_size = 32, train_seconds = 1.073e+04, _tick = 3764, _time = 1.6546e+09)
[2022-06-07 13:22:42,218][root][INFO] - Step 19271680 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 19271680, mean_episode_return = 46.959, mean_episode_step = 2515.3, total_loss = -25.363, pg_loss = -63.692, baseline_loss = 43.388, entropy_loss = -5.0589, learner_queue_size = 32, train_seconds = 1.0735e+04, _tick = 3767, _time = 1.6546e+09)
[2022-06-07 13:22:47,224][root][INFO] - Step 19279360 @ 1534.2 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 19279360, mean_episode_return = 51.907, mean_episode_step = 2042.1, total_loss = 371.61, pg_loss = 252.47, baseline_loss = 124.14, entropy_loss = -4.9923, learner_queue_size = 32, train_seconds = 1.074e+04, _tick = 3769, _time = 1.6546e+09)
[2022-06-07 13:22:52,226][root][INFO] - Step 19289600 @ 2047.1 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 19289600, mean_episode_return = -6.4405, mean_episode_step = 2284.3, total_loss = -96.722, pg_loss = -123.91, baseline_loss = 32.162, entropy_loss = -4.978, learner_queue_size = 32, train_seconds = 1.0745e+04, _tick = 3772, _time = 1.6546e+09)
[2022-06-07 13:22:57,230][root][INFO] - Step 19297280 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 19297280, mean_episode_return = None, mean_episode_step = 1979.9, total_loss = 10.755, pg_loss = -26.55, baseline_loss = 42.316, entropy_loss = -5.0119, learner_queue_size = 32, train_seconds = 1.075e+04, _tick = 3774, _time = 1.6546e+09)
[2022-06-07 13:23:02,236][root][INFO] - Step 19307520 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 19307520, mean_episode_return = 82.455, mean_episode_step = 2079.1, total_loss = 176.0, pg_loss = 103.2, baseline_loss = 77.873, entropy_loss = -5.0792, learner_queue_size = 32, train_seconds = 1.0755e+04, _tick = 3778, _time = 1.6546e+09)
[2022-06-07 13:23:07,242][root][INFO] - Step 19315200 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 19315200, mean_episode_return = None, mean_episode_step = 2087.7, total_loss = 53.105, pg_loss = 20.609, baseline_loss = 37.541, entropy_loss = -5.0457, learner_queue_size = 32, train_seconds = 1.076e+04, _tick = 3780, _time = 1.6546e+09)
[2022-06-07 13:23:12,246][root][INFO] - Step 19325440 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19325440, mean_episode_return = -7.0507, mean_episode_step = 2298.0, total_loss = 133.68, pg_loss = 87.339, baseline_loss = 51.346, entropy_loss = -5.0018, learner_queue_size = 32, train_seconds = 1.0765e+04, _tick = 3784, _time = 1.6546e+09)
[2022-06-07 13:23:17,250][root][INFO] - Step 19333120 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 19333120, mean_episode_return = None, mean_episode_step = 1955.8, total_loss = 360.41, pg_loss = 252.76, baseline_loss = 112.66, entropy_loss = -5.0063, learner_queue_size = 32, train_seconds = 1.077e+04, _tick = 3785, _time = 1.6546e+09)
[2022-06-07 13:23:22,254][root][INFO] - Step 19343360 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 19343360, mean_episode_return = 46.28, mean_episode_step = 2189.4, total_loss = 229.29, pg_loss = 135.05, baseline_loss = 99.22, entropy_loss = -4.9819, learner_queue_size = 32, train_seconds = 1.0775e+04, _tick = 3787, _time = 1.6546e+09)
[2022-06-07 13:23:27,259][root][INFO] - Step 19351040 @ 1534.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19351040, mean_episode_return = None, mean_episode_step = 1349.0, total_loss = -123.82, pg_loss = -123.21, baseline_loss = 4.4098, entropy_loss = -5.0152, learner_queue_size = 32, train_seconds = 1.078e+04, _tick = 3787, _time = 1.6546e+09)
[2022-06-07 13:23:32,266][root][INFO] - Step 19361280 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19361280, mean_episode_return = None, mean_episode_step = 2354.2, total_loss = 319.33, pg_loss = 218.23, baseline_loss = 106.15, entropy_loss = -5.0597, learner_queue_size = 32, train_seconds = 1.0785e+04, _tick = 3789, _time = 1.6546e+09)
[2022-06-07 13:23:37,270][root][INFO] - Step 19368960 @ 1534.6 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 19368960, mean_episode_return = None, mean_episode_step = 2184.2, total_loss = 205.81, pg_loss = 161.67, baseline_loss = 49.222, entropy_loss = -5.0824, learner_queue_size = 32, train_seconds = 1.079e+04, _tick = 3790, _time = 1.6546e+09)
[2022-06-07 13:23:42,274][root][INFO] - Step 19379200 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 19379200, mean_episode_return = 37.72, mean_episode_step = 1858.7, total_loss = -63.003, pg_loss = -82.677, baseline_loss = 24.648, entropy_loss = -4.9736, learner_queue_size = 32, train_seconds = 1.0795e+04, _tick = 3793, _time = 1.6546e+09)
[2022-06-07 13:23:47,278][root][INFO] - Step 19386880 @ 1534.8 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 19386880, mean_episode_return = None, mean_episode_step = 2183.5, total_loss = 150.08, pg_loss = 101.2, baseline_loss = 53.842, entropy_loss = -4.9565, learner_queue_size = 32, train_seconds = 1.08e+04, _tick = 3794, _time = 1.6546e+09)
[2022-06-07 13:23:52,282][root][INFO] - Step 19397120 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 19397120, mean_episode_return = 37.08, mean_episode_step = 2114.4, total_loss = 129.49, pg_loss = 36.294, baseline_loss = 98.147, entropy_loss = -4.9533, learner_queue_size = 32, train_seconds = 1.0805e+04, _tick = 3797, _time = 1.6546e+09)
[2022-06-07 13:23:57,287][root][INFO] - Step 19404800 @ 1534.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19404800, mean_episode_return = None, mean_episode_step = 1947.8, total_loss = 18.013, pg_loss = -34.495, baseline_loss = 57.461, entropy_loss = -4.9535, learner_queue_size = 32, train_seconds = 1.081e+04, _tick = 3799, _time = 1.6546e+09)
[2022-06-07 13:24:02,293][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 13:24:02,489][root][INFO] - Step 19415040 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19415040, mean_episode_return = None, mean_episode_step = 1437.6, total_loss = 94.414, pg_loss = 55.856, baseline_loss = 43.493, entropy_loss = -4.9343, learner_queue_size = 32, train_seconds = 1.0815e+04, _tick = 3802, _time = 1.6546e+09)
[2022-06-07 13:24:07,495][root][INFO] - Step 19425280 @ 1968.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 19425280, mean_episode_return = None, mean_episode_step = 1937.6, total_loss = -110.94, pg_loss = -128.4, baseline_loss = 22.377, entropy_loss = -4.9221, learner_queue_size = 32, train_seconds = 1.082e+04, _tick = 3804, _time = 1.6546e+09)
[2022-06-07 13:24:12,501][root][INFO] - Step 19432960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19432960, mean_episode_return = None, mean_episode_step = 2317.5, total_loss = 60.227, pg_loss = 24.905, baseline_loss = 40.22, entropy_loss = -4.8979, learner_queue_size = 32, train_seconds = 1.0826e+04, _tick = 3804, _time = 1.6546e+09)
[2022-06-07 13:24:17,507][root][INFO] - Step 19443200 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 19443200, mean_episode_return = 46.341, mean_episode_step = 2379.2, total_loss = 344.22, pg_loss = 228.15, baseline_loss = 121.04, entropy_loss = -4.9685, learner_queue_size = 32, train_seconds = 1.083e+04, _tick = 3807, _time = 1.6546e+09)
[2022-06-07 13:24:22,510][root][INFO] - Step 19450880 @ 1535.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 19450880, mean_episode_return = 102.63, mean_episode_step = 1531.7, total_loss = 276.45, pg_loss = 177.48, baseline_loss = 103.92, entropy_loss = -4.9462, learner_queue_size = 32, train_seconds = 1.0836e+04, _tick = 3809, _time = 1.6546e+09)
[2022-06-07 13:24:27,514][root][INFO] - Step 19461120 @ 2046.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 19461120, mean_episode_return = 4.4395, mean_episode_step = 1653.2, total_loss = -91.232, pg_loss = -110.81, baseline_loss = 24.569, entropy_loss = -4.9864, learner_queue_size = 32, train_seconds = 1.084e+04, _tick = 3812, _time = 1.6546e+09)
[2022-06-07 13:24:32,518][root][INFO] - Step 19468800 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 19468800, mean_episode_return = None, mean_episode_step = 2203.4, total_loss = 38.813, pg_loss = 8.2256, baseline_loss = 35.624, entropy_loss = -5.0373, learner_queue_size = 32, train_seconds = 1.0846e+04, _tick = 3812, _time = 1.6546e+09)
[2022-06-07 13:24:37,522][root][INFO] - Step 19479040 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 19479040, mean_episode_return = None, mean_episode_step = 1851.8, total_loss = -16.188, pg_loss = -59.219, baseline_loss = 48.069, entropy_loss = -5.0383, learner_queue_size = 32, train_seconds = 1.085e+04, _tick = 3814, _time = 1.6546e+09)
[2022-06-07 13:24:42,526][root][INFO] - Step 19486720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19486720, mean_episode_return = 13.34, mean_episode_step = 2050.7, total_loss = 43.557, pg_loss = 4.4586, baseline_loss = 43.978, entropy_loss = -4.88, learner_queue_size = 32, train_seconds = 1.0856e+04, _tick = 3817, _time = 1.6546e+09)
[2022-06-07 13:24:47,532][root][INFO] - Step 19494400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19494400, mean_episode_return = None, mean_episode_step = 1867.6, total_loss = 192.84, pg_loss = 147.66, baseline_loss = 50.031, entropy_loss = -4.8492, learner_queue_size = 32, train_seconds = 1.086e+04, _tick = 3818, _time = 1.6546e+09)
[2022-06-07 13:24:52,538][root][INFO] - Step 19504640 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19504640, mean_episode_return = 56.429, mean_episode_step = 1982.2, total_loss = 53.189, pg_loss = 12.002, baseline_loss = 46.001, entropy_loss = -4.815, learner_queue_size = 32, train_seconds = 1.0866e+04, _tick = 3821, _time = 1.6546e+09)
[2022-06-07 13:24:57,544][root][INFO] - Step 19514880 @ 2045.4 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 19514880, mean_episode_return = None, mean_episode_step = 1898.6, total_loss = 281.98, pg_loss = 178.45, baseline_loss = 108.43, entropy_loss = -4.9048, learner_queue_size = 32, train_seconds = 1.087e+04, _tick = 3822, _time = 1.6546e+09)
[2022-06-07 13:25:02,550][root][INFO] - Step 19522560 @ 1534.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 19522560, mean_episode_return = None, mean_episode_step = 2025.5, total_loss = 157.93, pg_loss = 118.59, baseline_loss = 44.262, entropy_loss = -4.9207, learner_queue_size = 32, train_seconds = 1.0876e+04, _tick = 3823, _time = 1.6546e+09)
[2022-06-07 13:25:07,554][root][INFO] - Step 19532800 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 19532800, mean_episode_return = 68.371, mean_episode_step = 1647.7, total_loss = 130.27, pg_loss = 78.965, baseline_loss = 56.344, entropy_loss = -5.0342, learner_queue_size = 32, train_seconds = 1.088e+04, _tick = 3825, _time = 1.6546e+09)
[2022-06-07 13:25:12,558][root][INFO] - Step 19540480 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19540480, mean_episode_return = None, mean_episode_step = 1940.7, total_loss = 289.89, pg_loss = 209.04, baseline_loss = 85.842, entropy_loss = -4.9947, learner_queue_size = 32, train_seconds = 1.0886e+04, _tick = 3827, _time = 1.6546e+09)
[2022-06-07 13:25:17,562][root][INFO] - Step 19550720 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 19550720, mean_episode_return = None, mean_episode_step = 1947.8, total_loss = 231.96, pg_loss = 156.09, baseline_loss = 80.844, entropy_loss = -4.9736, learner_queue_size = 32, train_seconds = 1.0891e+04, _tick = 3827, _time = 1.6546e+09)
[2022-06-07 13:25:22,566][root][INFO] - Step 19558400 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 19558400, mean_episode_return = None, mean_episode_step = 2049.7, total_loss = 193.21, pg_loss = 123.69, baseline_loss = 74.467, entropy_loss = -4.9393, learner_queue_size = 32, train_seconds = 1.0896e+04, _tick = 3827, _time = 1.6546e+09)
[2022-06-07 13:25:27,570][root][INFO] - Step 19568640 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19568640, mean_episode_return = None, mean_episode_step = 2076.2, total_loss = 206.15, pg_loss = 145.18, baseline_loss = 65.846, entropy_loss = -4.8772, learner_queue_size = 32, train_seconds = 1.0901e+04, _tick = 3827, _time = 1.6546e+09)
[2022-06-07 13:25:32,574][root][INFO] - Step 19576320 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 19576320, mean_episode_return = None, mean_episode_step = 1712.7, total_loss = 32.1, pg_loss = -0.94779, baseline_loss = 38.025, entropy_loss = -4.9766, learner_queue_size = 32, train_seconds = 1.0906e+04, _tick = 3828, _time = 1.6546e+09)
[2022-06-07 13:25:37,578][root][INFO] - Step 19586560 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 19586560, mean_episode_return = -8.8005, mean_episode_step = 2497.3, total_loss = -100.78, pg_loss = -115.01, baseline_loss = 19.274, entropy_loss = -5.0425, learner_queue_size = 32, train_seconds = 1.0911e+04, _tick = 3830, _time = 1.6546e+09)
[2022-06-07 13:25:42,584][root][INFO] - Step 19594240 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19594240, mean_episode_return = 97.46, mean_episode_step = 2022.0, total_loss = 405.8, pg_loss = 225.17, baseline_loss = 185.72, entropy_loss = -5.0913, learner_queue_size = 32, train_seconds = 1.0916e+04, _tick = 3832, _time = 1.6546e+09)
[2022-06-07 13:25:47,590][root][INFO] - Step 19604480 @ 2045.6 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 19604480, mean_episode_return = 43.365, mean_episode_step = 2113.6, total_loss = -105.74, pg_loss = -142.43, baseline_loss = 41.805, entropy_loss = -5.1146, learner_queue_size = 32, train_seconds = 1.0921e+04, _tick = 3836, _time = 1.6546e+09)
[2022-06-07 13:25:52,596][root][INFO] - Step 19612160 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19612160, mean_episode_return = None, mean_episode_step = 2032.8, total_loss = -15.108, pg_loss = -18.577, baseline_loss = 8.5624, entropy_loss = -5.0935, learner_queue_size = 32, train_seconds = 1.0926e+04, _tick = 3838, _time = 1.6546e+09)
[2022-06-07 13:25:57,602][root][INFO] - Step 19622400 @ 2045.6 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 19622400, mean_episode_return = 0.029704, mean_episode_step = 2478.9, total_loss = -18.523, pg_loss = -49.359, baseline_loss = 35.848, entropy_loss = -5.0118, learner_queue_size = 32, train_seconds = 1.0931e+04, _tick = 3841, _time = 1.6546e+09)
[2022-06-07 13:26:02,606][root][INFO] - Step 19630080 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 19630080, mean_episode_return = None, mean_episode_step = 2075.8, total_loss = 111.11, pg_loss = 55.214, baseline_loss = 61.005, entropy_loss = -5.1095, learner_queue_size = 32, train_seconds = 1.0936e+04, _tick = 3843, _time = 1.6546e+09)
[2022-06-07 13:26:07,612][root][INFO] - Step 19640320 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19640320, mean_episode_return = 51.94, mean_episode_step = 1836.2, total_loss = 23.559, pg_loss = -44.875, baseline_loss = 73.524, entropy_loss = -5.09, learner_queue_size = 32, train_seconds = 1.0941e+04, _tick = 3847, _time = 1.6546e+09)
[2022-06-07 13:26:12,618][root][INFO] - Step 19648000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19648000, mean_episode_return = 50.324, mean_episode_step = 2119.8, total_loss = -139.5, pg_loss = -165.37, baseline_loss = 30.941, entropy_loss = -5.0634, learner_queue_size = 32, train_seconds = 1.0946e+04, _tick = 3848, _time = 1.6546e+09)
[2022-06-07 13:26:17,624][root][INFO] - Step 19658240 @ 2045.5 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 19658240, mean_episode_return = 61.248, mean_episode_step = 2215.5, total_loss = 135.96, pg_loss = 69.171, baseline_loss = 71.779, entropy_loss = -4.9889, learner_queue_size = 32, train_seconds = 1.0951e+04, _tick = 3850, _time = 1.6546e+09)
[2022-06-07 13:26:22,630][root][INFO] - Step 19665920 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 19665920, mean_episode_return = None, mean_episode_step = 2187.5, total_loss = -51.828, pg_loss = -62.287, baseline_loss = 15.377, entropy_loss = -4.9177, learner_queue_size = 32, train_seconds = 1.0956e+04, _tick = 3851, _time = 1.6546e+09)
[2022-06-07 13:26:27,634][root][INFO] - Step 19676160 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19676160, mean_episode_return = 15.06, mean_episode_step = 1885.7, total_loss = 360.92, pg_loss = 246.45, baseline_loss = 119.37, entropy_loss = -4.9015, learner_queue_size = 32, train_seconds = 1.0961e+04, _tick = 3853, _time = 1.6546e+09)
[2022-06-07 13:26:32,640][root][INFO] - Step 19683840 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19683840, mean_episode_return = 15.645, mean_episode_step = 2271.2, total_loss = -174.09, pg_loss = -183.08, baseline_loss = 13.915, entropy_loss = -4.9278, learner_queue_size = 32, train_seconds = 1.0966e+04, _tick = 3855, _time = 1.6546e+09)
[2022-06-07 13:26:37,646][root][INFO] - Step 19694080 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19694080, mean_episode_return = None, mean_episode_step = 2076.4, total_loss = -91.959, pg_loss = -102.98, baseline_loss = 15.908, entropy_loss = -4.8825, learner_queue_size = 32, train_seconds = 1.0971e+04, _tick = 3856, _time = 1.6546e+09)
[2022-06-07 13:26:42,650][root][INFO] - Step 19704320 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 19704320, mean_episode_return = None, mean_episode_step = 2619.7, total_loss = -188.58, pg_loss = -185.89, baseline_loss = 2.2155, entropy_loss = -4.906, learner_queue_size = 32, train_seconds = 1.0976e+04, _tick = 3856, _time = 1.6546e+09)
[2022-06-07 13:26:47,654][root][INFO] - Step 19712000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19712000, mean_episode_return = 23.91, mean_episode_step = 2217.7, total_loss = -71.495, pg_loss = -99.809, baseline_loss = 33.209, entropy_loss = -4.8954, learner_queue_size = 32, train_seconds = 1.0981e+04, _tick = 3858, _time = 1.6546e+09)
[2022-06-07 13:26:52,658][root][INFO] - Step 19722240 @ 2046.4 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 19722240, mean_episode_return = 64.989, mean_episode_step = 2448.6, total_loss = -95.934, pg_loss = -119.03, baseline_loss = 27.931, entropy_loss = -4.8362, learner_queue_size = 32, train_seconds = 1.0986e+04, _tick = 3862, _time = 1.6546e+09)
[2022-06-07 13:26:57,662][root][INFO] - Step 19729920 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 19729920, mean_episode_return = -7.1653, mean_episode_step = 2135.9, total_loss = 363.74, pg_loss = 237.67, baseline_loss = 130.96, entropy_loss = -4.8839, learner_queue_size = 32, train_seconds = 1.0991e+04, _tick = 3863, _time = 1.6546e+09)
[2022-06-07 13:27:02,666][root][INFO] - Step 19740160 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 19740160, mean_episode_return = -2.2204, mean_episode_step = 2562.8, total_loss = 426.24, pg_loss = 299.06, baseline_loss = 132.0, entropy_loss = -4.8243, learner_queue_size = 32, train_seconds = 1.0996e+04, _tick = 3864, _time = 1.6546e+09)
[2022-06-07 13:27:07,673][root][INFO] - Step 19747840 @ 1533.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19747840, mean_episode_return = None, mean_episode_step = 2204.8, total_loss = -121.18, pg_loss = -125.3, baseline_loss = 8.9744, entropy_loss = -4.8523, learner_queue_size = 32, train_seconds = 1.1001e+04, _tick = 3866, _time = 1.6546e+09)
[2022-06-07 13:27:12,679][root][INFO] - Step 19758080 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 19758080, mean_episode_return = None, mean_episode_step = 2308.7, total_loss = 109.24, pg_loss = 64.898, baseline_loss = 49.274, entropy_loss = -4.9303, learner_queue_size = 32, train_seconds = 1.1006e+04, _tick = 3869, _time = 1.6546e+09)
[2022-06-07 13:27:17,685][root][INFO] - Step 19768320 @ 2045.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 19768320, mean_episode_return = 91.087, mean_episode_step = 2132.4, total_loss = -32.26, pg_loss = -72.096, baseline_loss = 44.725, entropy_loss = -4.8889, learner_queue_size = 32, train_seconds = 1.1011e+04, _tick = 3873, _time = 1.6546e+09)
[2022-06-07 13:27:22,690][root][INFO] - Step 19776000 @ 1534.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19776000, mean_episode_return = None, mean_episode_step = 1916.5, total_loss = 537.67, pg_loss = 328.7, baseline_loss = 213.9, entropy_loss = -4.9331, learner_queue_size = 32, train_seconds = 1.1016e+04, _tick = 3875, _time = 1.6546e+09)
[2022-06-07 13:27:27,694][root][INFO] - Step 19783680 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19783680, mean_episode_return = None, mean_episode_step = 2320.5, total_loss = 250.66, pg_loss = 181.4, baseline_loss = 74.159, entropy_loss = -4.902, learner_queue_size = 32, train_seconds = 1.1021e+04, _tick = 3877, _time = 1.6546e+09)
[2022-06-07 13:27:32,698][root][INFO] - Step 19793920 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 19793920, mean_episode_return = 26.49, mean_episode_step = 2069.4, total_loss = 62.739, pg_loss = -0.065592, baseline_loss = 67.775, entropy_loss = -4.971, learner_queue_size = 32, train_seconds = 1.1026e+04, _tick = 3881, _time = 1.6546e+09)
[2022-06-07 13:27:37,704][root][INFO] - Step 19801600 @ 1534.2 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 19801600, mean_episode_return = None, mean_episode_step = 2183.8, total_loss = -59.769, pg_loss = -66.901, baseline_loss = 12.128, entropy_loss = -4.9959, learner_queue_size = 32, train_seconds = 1.1031e+04, _tick = 3882, _time = 1.6546e+09)
[2022-06-07 13:27:42,710][root][INFO] - Step 19811840 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 19811840, mean_episode_return = None, mean_episode_step = 2432.1, total_loss = -7.813, pg_loss = -25.315, baseline_loss = 22.521, entropy_loss = -5.0189, learner_queue_size = 32, train_seconds = 1.1036e+04, _tick = 3884, _time = 1.6546e+09)
[2022-06-07 13:27:47,716][root][INFO] - Step 19822080 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 19822080, mean_episode_return = 54.351, mean_episode_step = 2297.2, total_loss = 41.749, pg_loss = -12.497, baseline_loss = 59.282, entropy_loss = -5.0353, learner_queue_size = 32, train_seconds = 1.1041e+04, _tick = 3886, _time = 1.6546e+09)
[2022-06-07 13:27:52,718][root][INFO] - Step 19829760 @ 1535.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 19829760, mean_episode_return = 9.66, mean_episode_step = 1952.9, total_loss = 143.16, pg_loss = 51.021, baseline_loss = 97.148, entropy_loss = -5.0133, learner_queue_size = 32, train_seconds = 1.1046e+04, _tick = 3888, _time = 1.6546e+09)
[2022-06-07 13:27:57,725][root][INFO] - Step 19840000 @ 2045.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 19840000, mean_episode_return = 32.93, mean_episode_step = 2106.3, total_loss = 2.4707, pg_loss = -22.794, baseline_loss = 30.295, entropy_loss = -5.0301, learner_queue_size = 32, train_seconds = 1.1051e+04, _tick = 3889, _time = 1.6546e+09)
[2022-06-07 13:28:02,730][root][INFO] - Step 19847680 @ 1534.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 19847680, mean_episode_return = None, mean_episode_step = 2204.7, total_loss = -50.17, pg_loss = -52.47, baseline_loss = 7.2687, entropy_loss = -4.9683, learner_queue_size = 32, train_seconds = 1.1056e+04, _tick = 3891, _time = 1.6546e+09)
[2022-06-07 13:28:07,734][root][INFO] - Step 19857920 @ 2046.3 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 19857920, mean_episode_return = 29.799, mean_episode_step = 2242.7, total_loss = 348.51, pg_loss = 221.24, baseline_loss = 132.18, entropy_loss = -4.9111, learner_queue_size = 32, train_seconds = 1.1061e+04, _tick = 3894, _time = 1.6546e+09)
[2022-06-07 13:28:12,738][root][INFO] - Step 19865600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19865600, mean_episode_return = 30.132, mean_episode_step = 1942.1, total_loss = -167.89, pg_loss = -175.73, baseline_loss = 12.677, entropy_loss = -4.8388, learner_queue_size = 32, train_seconds = 1.1066e+04, _tick = 3897, _time = 1.6546e+09)
[2022-06-07 13:28:17,742][root][INFO] - Step 19875840 @ 2046.3 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 19875840, mean_episode_return = 35.254, mean_episode_step = 1687.8, total_loss = 115.54, pg_loss = 41.601, baseline_loss = 78.819, entropy_loss = -4.8848, learner_queue_size = 32, train_seconds = 1.1071e+04, _tick = 3901, _time = 1.6546e+09)
[2022-06-07 13:28:22,748][root][INFO] - Step 19883520 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19883520, mean_episode_return = 48.581, mean_episode_step = 2354.7, total_loss = -19.992, pg_loss = -42.933, baseline_loss = 27.824, entropy_loss = -4.8833, learner_queue_size = 32, train_seconds = 1.1076e+04, _tick = 3904, _time = 1.6546e+09)
[2022-06-07 13:28:27,754][root][INFO] - Step 19893760 @ 2045.6 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 19893760, mean_episode_return = None, mean_episode_step = 2189.8, total_loss = -73.076, pg_loss = -74.995, baseline_loss = 6.7568, entropy_loss = -4.8368, learner_queue_size = 32, train_seconds = 1.1081e+04, _tick = 3907, _time = 1.6546e+09)
[2022-06-07 13:28:32,758][root][INFO] - Step 19901440 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 19901440, mean_episode_return = None, mean_episode_step = 1802.4, total_loss = -136.93, pg_loss = -133.36, baseline_loss = 1.2479, entropy_loss = -4.8252, learner_queue_size = 32, train_seconds = 1.1086e+04, _tick = 3909, _time = 1.6546e+09)
[2022-06-07 13:28:37,762][root][INFO] - Step 19911680 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 19911680, mean_episode_return = 19.917, mean_episode_step = 2130.1, total_loss = 84.937, pg_loss = 5.712, baseline_loss = 83.955, entropy_loss = -4.7299, learner_queue_size = 32, train_seconds = 1.1091e+04, _tick = 3911, _time = 1.6546e+09)
[2022-06-07 13:28:42,766][root][INFO] - Step 19919360 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 19919360, mean_episode_return = None, mean_episode_step = 2437.3, total_loss = -54.955, pg_loss = -75.561, baseline_loss = 25.306, entropy_loss = -4.699, learner_queue_size = 32, train_seconds = 1.1096e+04, _tick = 3913, _time = 1.6546e+09)
[2022-06-07 13:28:47,770][root][INFO] - Step 19929600 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 19929600, mean_episode_return = None, mean_episode_step = 1755.8, total_loss = 69.382, pg_loss = -9.8012, baseline_loss = 83.77, entropy_loss = -4.5865, learner_queue_size = 32, train_seconds = 1.1101e+04, _tick = 3915, _time = 1.6546e+09)
[2022-06-07 13:28:52,774][root][INFO] - Step 19937280 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19937280, mean_episode_return = None, mean_episode_step = 2495.1, total_loss = -87.339, pg_loss = -84.807, baseline_loss = 1.8933, entropy_loss = -4.4258, learner_queue_size = 32, train_seconds = 1.1106e+04, _tick = 3917, _time = 1.6546e+09)
[2022-06-07 13:28:57,780][root][INFO] - Step 19947520 @ 2045.6 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 19947520, mean_episode_return = None, mean_episode_step = 1900.9, total_loss = 5.522, pg_loss = -9.6168, baseline_loss = 19.572, entropy_loss = -4.4336, learner_queue_size = 32, train_seconds = 1.1111e+04, _tick = 3918, _time = 1.6546e+09)
[2022-06-07 13:29:02,782][root][INFO] - Step 19955200 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 19955200, mean_episode_return = None, mean_episode_step = 1854.3, total_loss = -24.459, pg_loss = -50.077, baseline_loss = 30.158, entropy_loss = -4.5398, learner_queue_size = 32, train_seconds = 1.1116e+04, _tick = 3920, _time = 1.6546e+09)
[2022-06-07 13:29:07,787][root][INFO] - Step 19965440 @ 2045.7 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 19965440, mean_episode_return = None, mean_episode_step = 2186.7, total_loss = 236.62, pg_loss = 157.14, baseline_loss = 83.99, entropy_loss = -4.51, learner_queue_size = 32, train_seconds = 1.1121e+04, _tick = 3921, _time = 1.6546e+09)
[2022-06-07 13:29:12,790][root][INFO] - Step 19973120 @ 1535.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 19973120, mean_episode_return = None, mean_episode_step = 2253.8, total_loss = 48.033, pg_loss = 14.516, baseline_loss = 38.065, entropy_loss = -4.5479, learner_queue_size = 32, train_seconds = 1.1126e+04, _tick = 3923, _time = 1.6546e+09)
[2022-06-07 13:29:17,794][root][INFO] - Step 19983360 @ 2046.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 19983360, mean_episode_return = 41.263, mean_episode_step = 2967.3, total_loss = -12.313, pg_loss = -26.275, baseline_loss = 18.62, entropy_loss = -4.6579, learner_queue_size = 32, train_seconds = 1.1131e+04, _tick = 3925, _time = 1.6546e+09)
[2022-06-07 13:29:22,798][root][INFO] - Step 19991040 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 19991040, mean_episode_return = 70.258, mean_episode_step = 1955.3, total_loss = 128.09, pg_loss = 31.458, baseline_loss = 101.26, entropy_loss = -4.6317, learner_queue_size = 32, train_seconds = 1.1136e+04, _tick = 3927, _time = 1.6546e+09)
[2022-06-07 13:29:27,814][root][INFO] - Step 20001280 @ 2041.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20001280, mean_episode_return = 124.17, mean_episode_step = 2224.5, total_loss = -133.85, pg_loss = -136.33, baseline_loss = 7.0855, entropy_loss = -4.5986, learner_queue_size = 32, train_seconds = 1.1141e+04, _tick = 3929, _time = 1.6546e+09)
[2022-06-07 13:29:32,820][root][INFO] - Step 20008960 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 20008960, mean_episode_return = 40.04, mean_episode_step = 2250.2, total_loss = 36.655, pg_loss = 2.8931, baseline_loss = 38.334, entropy_loss = -4.5719, learner_queue_size = 32, train_seconds = 1.1146e+04, _tick = 3932, _time = 1.6546e+09)
[2022-06-07 13:29:37,826][root][INFO] - Step 20019200 @ 2045.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20019200, mean_episode_return = -0.081122, mean_episode_step = 2185.9, total_loss = -0.6579, pg_loss = -29.692, baseline_loss = 33.763, entropy_loss = -4.7287, learner_queue_size = 32, train_seconds = 1.1151e+04, _tick = 3936, _time = 1.6546e+09)
[2022-06-07 13:29:42,830][root][INFO] - Step 20029440 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 20029440, mean_episode_return = 94.294, mean_episode_step = 1740.6, total_loss = -1.4423, pg_loss = -40.163, baseline_loss = 43.43, entropy_loss = -4.7096, learner_queue_size = 32, train_seconds = 1.1156e+04, _tick = 3938, _time = 1.6546e+09)
[2022-06-07 13:29:47,836][root][INFO] - Step 20037120 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 20037120, mean_episode_return = None, mean_episode_step = 2212.4, total_loss = 250.31, pg_loss = 174.98, baseline_loss = 79.982, entropy_loss = -4.6505, learner_queue_size = 32, train_seconds = 1.1161e+04, _tick = 3939, _time = 1.6546e+09)
[2022-06-07 13:29:52,842][root][INFO] - Step 20047360 @ 2045.5 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 20047360, mean_episode_return = None, mean_episode_step = 1838.2, total_loss = 308.02, pg_loss = 171.31, baseline_loss = 141.43, entropy_loss = -4.7185, learner_queue_size = 32, train_seconds = 1.1166e+04, _tick = 3941, _time = 1.6546e+09)
[2022-06-07 13:29:57,846][root][INFO] - Step 20055040 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20055040, mean_episode_return = 62.099, mean_episode_step = 1888.8, total_loss = -121.85, pg_loss = -123.19, baseline_loss = 6.0942, entropy_loss = -4.7558, learner_queue_size = 32, train_seconds = 1.1171e+04, _tick = 3942, _time = 1.6546e+09)
[2022-06-07 13:30:02,852][root][INFO] - Step 20065280 @ 2045.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20065280, mean_episode_return = None, mean_episode_step = 2678.4, total_loss = -73.363, pg_loss = -74.555, baseline_loss = 6.0441, entropy_loss = -4.8522, learner_queue_size = 32, train_seconds = 1.1176e+04, _tick = 3943, _time = 1.6546e+09)
[2022-06-07 13:30:07,858][root][INFO] - Step 20072960 @ 1534.2 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 20072960, mean_episode_return = 20.78, mean_episode_step = 1529.0, total_loss = 47.225, pg_loss = 19.904, baseline_loss = 32.131, entropy_loss = -4.8099, learner_queue_size = 32, train_seconds = 1.1181e+04, _tick = 3945, _time = 1.6546e+09)
[2022-06-07 13:30:12,863][root][INFO] - Step 20080640 @ 1534.2 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 20080640, mean_episode_return = 21.91, mean_episode_step = 2114.4, total_loss = 12.357, pg_loss = -35.529, baseline_loss = 52.79, entropy_loss = -4.904, learner_queue_size = 32, train_seconds = 1.1186e+04, _tick = 3948, _time = 1.6546e+09)
[2022-06-07 13:30:17,868][root][INFO] - Step 20090880 @ 2046.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 20090880, mean_episode_return = None, mean_episode_step = 1950.9, total_loss = -39.31, pg_loss = -43.817, baseline_loss = 9.4073, entropy_loss = -4.9001, learner_queue_size = 32, train_seconds = 1.1191e+04, _tick = 3950, _time = 1.6546e+09)
[2022-06-07 13:30:22,870][root][INFO] - Step 20098560 @ 1535.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20098560, mean_episode_return = None, mean_episode_step = 2387.7, total_loss = -93.114, pg_loss = -91.171, baseline_loss = 2.9787, entropy_loss = -4.9217, learner_queue_size = 32, train_seconds = 1.1196e+04, _tick = 3950, _time = 1.6546e+09)
[2022-06-07 13:30:27,874][root][INFO] - Step 20108800 @ 2046.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 20108800, mean_episode_return = None, mean_episode_step = 1701.7, total_loss = -15.793, pg_loss = -34.736, baseline_loss = 23.843, entropy_loss = -4.9005, learner_queue_size = 32, train_seconds = 1.1201e+04, _tick = 3951, _time = 1.6546e+09)
[2022-06-07 13:30:32,878][root][INFO] - Step 20116480 @ 1534.7 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 20116480, mean_episode_return = 49.611, mean_episode_step = 2100.5, total_loss = -23.077, pg_loss = -49.354, baseline_loss = 31.15, entropy_loss = -4.8728, learner_queue_size = 32, train_seconds = 1.1206e+04, _tick = 3953, _time = 1.6546e+09)
[2022-06-07 13:30:37,882][root][INFO] - Step 20126720 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20126720, mean_episode_return = None, mean_episode_step = 2087.1, total_loss = -72.106, pg_loss = -72.815, baseline_loss = 5.5887, entropy_loss = -4.8799, learner_queue_size = 32, train_seconds = 1.1211e+04, _tick = 3954, _time = 1.6546e+09)
[2022-06-07 13:30:42,888][root][INFO] - Step 20134400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20134400, mean_episode_return = None, mean_episode_step = 2025.9, total_loss = 193.42, pg_loss = 125.65, baseline_loss = 72.67, entropy_loss = -4.8949, learner_queue_size = 32, train_seconds = 1.1216e+04, _tick = 3956, _time = 1.6546e+09)
[2022-06-07 13:30:47,894][root][INFO] - Step 20144640 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20144640, mean_episode_return = 47.852, mean_episode_step = 2332.8, total_loss = -135.98, pg_loss = -137.2, baseline_loss = 6.244, entropy_loss = -5.0264, learner_queue_size = 32, train_seconds = 1.1221e+04, _tick = 3959, _time = 1.6546e+09)
[2022-06-07 13:30:52,900][root][INFO] - Step 20152320 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20152320, mean_episode_return = None, mean_episode_step = 2352.3, total_loss = -116.12, pg_loss = -112.73, baseline_loss = 1.6331, entropy_loss = -5.0263, learner_queue_size = 32, train_seconds = 1.1226e+04, _tick = 3959, _time = 1.6546e+09)
[2022-06-07 13:30:57,906][root][INFO] - Step 20162560 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 20162560, mean_episode_return = None, mean_episode_step = 2487.6, total_loss = -84.772, pg_loss = -87.971, baseline_loss = 8.2468, entropy_loss = -5.0481, learner_queue_size = 32, train_seconds = 1.1231e+04, _tick = 3961, _time = 1.6546e+09)
[2022-06-07 13:31:02,910][root][INFO] - Step 20170240 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 20170240, mean_episode_return = 50.741, mean_episode_step = 1976.9, total_loss = 81.56, pg_loss = 28.083, baseline_loss = 58.629, entropy_loss = -5.1526, learner_queue_size = 32, train_seconds = 1.1236e+04, _tick = 3962, _time = 1.6546e+09)
[2022-06-07 13:31:07,914][root][INFO] - Step 20180480 @ 2046.3 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 20180480, mean_episode_return = None, mean_episode_step = 2030.5, total_loss = 115.66, pg_loss = 65.038, baseline_loss = 55.718, entropy_loss = -5.0993, learner_queue_size = 32, train_seconds = 1.1241e+04, _tick = 3964, _time = 1.6546e+09)
[2022-06-07 13:31:12,918][root][INFO] - Step 20188160 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20188160, mean_episode_return = None, mean_episode_step = 2267.7, total_loss = -50.357, pg_loss = -56.266, baseline_loss = 10.992, entropy_loss = -5.0833, learner_queue_size = 32, train_seconds = 1.1246e+04, _tick = 3966, _time = 1.6546e+09)
[2022-06-07 13:31:17,922][root][INFO] - Step 20198400 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20198400, mean_episode_return = None, mean_episode_step = 1893.1, total_loss = 117.77, pg_loss = 73.673, baseline_loss = 49.149, entropy_loss = -5.0553, learner_queue_size = 32, train_seconds = 1.1251e+04, _tick = 3967, _time = 1.6546e+09)
[2022-06-07 13:31:22,928][root][INFO] - Step 20206080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20206080, mean_episode_return = None, mean_episode_step = 2302.5, total_loss = 41.905, pg_loss = 18.558, baseline_loss = 28.436, entropy_loss = -5.0898, learner_queue_size = 32, train_seconds = 1.1256e+04, _tick = 3968, _time = 1.6546e+09)
[2022-06-07 13:31:27,934][root][INFO] - Step 20216320 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 20216320, mean_episode_return = 44.829, mean_episode_step = 2164.2, total_loss = -228.87, pg_loss = -229.99, baseline_loss = 6.3552, entropy_loss = -5.2344, learner_queue_size = 32, train_seconds = 1.1261e+04, _tick = 3972, _time = 1.6546e+09)
[2022-06-07 13:31:32,940][root][INFO] - Step 20224000 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 20224000, mean_episode_return = None, mean_episode_step = 2063.6, total_loss = -26.205, pg_loss = -37.07, baseline_loss = 16.039, entropy_loss = -5.1746, learner_queue_size = 32, train_seconds = 1.1266e+04, _tick = 3972, _time = 1.6546e+09)
[2022-06-07 13:31:37,946][root][INFO] - Step 20234240 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 20234240, mean_episode_return = 10.92, mean_episode_step = 2187.1, total_loss = 151.93, pg_loss = 95.007, baseline_loss = 62.057, entropy_loss = -5.1311, learner_queue_size = 32, train_seconds = 1.1271e+04, _tick = 3974, _time = 1.6546e+09)
[2022-06-07 13:31:42,952][root][INFO] - Step 20241920 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 20241920, mean_episode_return = 73.696, mean_episode_step = 2458.4, total_loss = -134.01, pg_loss = -160.77, baseline_loss = 31.912, entropy_loss = -5.1564, learner_queue_size = 32, train_seconds = 1.1276e+04, _tick = 3976, _time = 1.6546e+09)
[2022-06-07 13:31:47,954][root][INFO] - Step 20252160 @ 2047.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 20252160, mean_episode_return = None, mean_episode_step = 2259.3, total_loss = 224.65, pg_loss = 157.0, baseline_loss = 72.887, entropy_loss = -5.2386, learner_queue_size = 32, train_seconds = 1.1281e+04, _tick = 3979, _time = 1.6546e+09)
[2022-06-07 13:31:52,960][root][INFO] - Step 20259840 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20259840, mean_episode_return = 22.022, mean_episode_step = 2479.0, total_loss = 321.84, pg_loss = 233.74, baseline_loss = 93.28, entropy_loss = -5.1797, learner_queue_size = 32, train_seconds = 1.1286e+04, _tick = 3981, _time = 1.6546e+09)
[2022-06-07 13:31:57,962][root][INFO] - Step 20270080 @ 2047.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20270080, mean_episode_return = 72.755, mean_episode_step = 2186.0, total_loss = 26.468, pg_loss = -32.428, baseline_loss = 64.048, entropy_loss = -5.1515, learner_queue_size = 32, train_seconds = 1.1291e+04, _tick = 3983, _time = 1.6546e+09)
[2022-06-07 13:32:02,967][root][INFO] - Step 20277760 @ 1534.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 20277760, mean_episode_return = 44.08, mean_episode_step = 1977.3, total_loss = -157.97, pg_loss = -156.38, baseline_loss = 3.5133, entropy_loss = -5.1105, learner_queue_size = 32, train_seconds = 1.1296e+04, _tick = 3985, _time = 1.6546e+09)
[2022-06-07 13:32:07,970][root][INFO] - Step 20288000 @ 2046.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20288000, mean_episode_return = -9.0501, mean_episode_step = 1911.4, total_loss = 28.505, pg_loss = 1.7362, baseline_loss = 31.85, entropy_loss = -5.0822, learner_queue_size = 32, train_seconds = 1.1301e+04, _tick = 3988, _time = 1.6546e+09)
[2022-06-07 13:32:12,974][root][INFO] - Step 20298240 @ 2046.3 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 20298240, mean_episode_return = None, mean_episode_step = 2381.1, total_loss = 33.739, pg_loss = 19.93, baseline_loss = 18.89, entropy_loss = -5.081, learner_queue_size = 32, train_seconds = 1.1306e+04, _tick = 3990, _time = 1.6546e+09)
[2022-06-07 13:32:17,980][root][INFO] - Step 20305920 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 20305920, mean_episode_return = None, mean_episode_step = 2084.8, total_loss = 350.54, pg_loss = 245.69, baseline_loss = 110.0, entropy_loss = -5.1486, learner_queue_size = 32, train_seconds = 1.1311e+04, _tick = 3991, _time = 1.6546e+09)
[2022-06-07 13:32:22,987][root][INFO] - Step 20316160 @ 2045.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 20316160, mean_episode_return = -6.8601, mean_episode_step = 2492.4, total_loss = 279.07, pg_loss = 191.3, baseline_loss = 92.79, entropy_loss = -5.0264, learner_queue_size = 32, train_seconds = 1.1316e+04, _tick = 3993, _time = 1.6546e+09)
[2022-06-07 13:32:27,993][root][INFO] - Step 20323840 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20323840, mean_episode_return = 68.869, mean_episode_step = 2075.9, total_loss = 49.4, pg_loss = -1.1437, baseline_loss = 55.526, entropy_loss = -4.9829, learner_queue_size = 32, train_seconds = 1.1321e+04, _tick = 3995, _time = 1.6546e+09)
[2022-06-07 13:32:32,998][root][INFO] - Step 20331520 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 20331520, mean_episode_return = None, mean_episode_step = 2631.9, total_loss = -5.1831, pg_loss = -28.956, baseline_loss = 28.707, entropy_loss = -4.9338, learner_queue_size = 32, train_seconds = 1.1326e+04, _tick = 3996, _time = 1.6546e+09)
[2022-06-07 13:32:38,002][root][INFO] - Step 20341760 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 20341760, mean_episode_return = None, mean_episode_step = 2544.7, total_loss = 38.602, pg_loss = 7.5102, baseline_loss = 36.026, entropy_loss = -4.9345, learner_queue_size = 32, train_seconds = 1.1331e+04, _tick = 3997, _time = 1.6546e+09)
[2022-06-07 13:32:43,006][root][INFO] - Step 20349440 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 20349440, mean_episode_return = None, mean_episode_step = 2257.0, total_loss = -150.74, pg_loss = -148.8, baseline_loss = 3.0131, entropy_loss = -4.9492, learner_queue_size = 32, train_seconds = 1.1336e+04, _tick = 3999, _time = 1.6546e+09)
[2022-06-07 13:32:48,010][root][INFO] - Step 20359680 @ 2046.4 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 20359680, mean_episode_return = 34.499, mean_episode_step = 1780.3, total_loss = -189.34, pg_loss = -214.01, baseline_loss = 29.551, entropy_loss = -4.8754, learner_queue_size = 32, train_seconds = 1.1341e+04, _tick = 4001, _time = 1.6546e+09)
[2022-06-07 13:32:53,014][root][INFO] - Step 20367360 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 20367360, mean_episode_return = -32.911, mean_episode_step = 2118.3, total_loss = 364.39, pg_loss = 217.44, baseline_loss = 151.9, entropy_loss = -4.9396, learner_queue_size = 32, train_seconds = 1.1346e+04, _tick = 4004, _time = 1.6546e+09)
[2022-06-07 13:32:58,020][root][INFO] - Step 20377600 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 20377600, mean_episode_return = None, mean_episode_step = 2500.4, total_loss = 225.15, pg_loss = 168.06, baseline_loss = 61.995, entropy_loss = -4.9065, learner_queue_size = 32, train_seconds = 1.1351e+04, _tick = 4005, _time = 1.6546e+09)
[2022-06-07 13:33:03,026][root][INFO] - Step 20385280 @ 1534.1 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 20385280, mean_episode_return = 16.295, mean_episode_step = 2227.0, total_loss = 374.1, pg_loss = 239.23, baseline_loss = 139.76, entropy_loss = -4.8959, learner_queue_size = 32, train_seconds = 1.1356e+04, _tick = 4007, _time = 1.6546e+09)
[2022-06-07 13:33:08,030][root][INFO] - Step 20395520 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 20395520, mean_episode_return = None, mean_episode_step = 2098.7, total_loss = 62.692, pg_loss = 8.6628, baseline_loss = 58.93, entropy_loss = -4.9014, learner_queue_size = 32, train_seconds = 1.1361e+04, _tick = 4010, _time = 1.6546e+09)
[2022-06-07 13:33:13,036][root][INFO] - Step 20403200 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 20403200, mean_episode_return = None, mean_episode_step = 2180.0, total_loss = 17.895, pg_loss = -19.227, baseline_loss = 41.992, entropy_loss = -4.8702, learner_queue_size = 32, train_seconds = 1.1366e+04, _tick = 4010, _time = 1.6546e+09)
[2022-06-07 13:33:18,038][root][INFO] - Step 20413440 @ 2047.1 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 20413440, mean_episode_return = None, mean_episode_step = 2308.6, total_loss = 218.13, pg_loss = 135.29, baseline_loss = 87.702, entropy_loss = -4.8611, learner_queue_size = 32, train_seconds = 1.1371e+04, _tick = 4012, _time = 1.6546e+09)
[2022-06-07 13:33:23,042][root][INFO] - Step 20423680 @ 2046.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 20423680, mean_episode_return = -2.74, mean_episode_step = 2052.5, total_loss = 19.928, pg_loss = -4.3572, baseline_loss = 29.176, entropy_loss = -4.8911, learner_queue_size = 32, train_seconds = 1.1376e+04, _tick = 4014, _time = 1.6546e+09)
[2022-06-07 13:33:28,046][root][INFO] - Step 20431360 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20431360, mean_episode_return = 47.613, mean_episode_step = 2409.6, total_loss = -44.503, pg_loss = -85.941, baseline_loss = 46.32, entropy_loss = -4.8828, learner_queue_size = 32, train_seconds = 1.1381e+04, _tick = 4017, _time = 1.6546e+09)
[2022-06-07 13:33:33,048][root][INFO] - Step 20439040 @ 1535.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 20439040, mean_episode_return = 21.34, mean_episode_step = 2065.5, total_loss = -88.214, pg_loss = -109.34, baseline_loss = 26.041, entropy_loss = -4.9144, learner_queue_size = 32, train_seconds = 1.1386e+04, _tick = 4019, _time = 1.6546e+09)
[2022-06-07 13:33:38,056][root][INFO] - Step 20449280 @ 2044.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20449280, mean_episode_return = None, mean_episode_step = 2523.7, total_loss = 356.9, pg_loss = 260.88, baseline_loss = 100.94, entropy_loss = -4.918, learner_queue_size = 32, train_seconds = 1.1391e+04, _tick = 4020, _time = 1.6546e+09)
[2022-06-07 13:33:43,062][root][INFO] - Step 20456960 @ 1534.1 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 20456960, mean_episode_return = None, mean_episode_step = 2152.0, total_loss = -46.777, pg_loss = -61.343, baseline_loss = 19.464, entropy_loss = -4.898, learner_queue_size = 32, train_seconds = 1.1396e+04, _tick = 4021, _time = 1.6546e+09)
[2022-06-07 13:33:48,066][root][INFO] - Step 20467200 @ 2046.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 20467200, mean_episode_return = None, mean_episode_step = 2435.1, total_loss = 98.243, pg_loss = 63.422, baseline_loss = 39.771, entropy_loss = -4.9503, learner_queue_size = 32, train_seconds = 1.1401e+04, _tick = 4022, _time = 1.6546e+09)
[2022-06-07 13:33:53,072][root][INFO] - Step 20474880 @ 1534.1 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 20474880, mean_episode_return = None, mean_episode_step = 2433.2, total_loss = 3.4002, pg_loss = -29.478, baseline_loss = 37.818, entropy_loss = -4.9393, learner_queue_size = 32, train_seconds = 1.1406e+04, _tick = 4023, _time = 1.6546e+09)
[2022-06-07 13:33:58,074][root][INFO] - Step 20485120 @ 2047.2 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 20485120, mean_episode_return = None, mean_episode_step = 2711.5, total_loss = 28.192, pg_loss = 3.5063, baseline_loss = 29.65, entropy_loss = -4.9635, learner_queue_size = 32, train_seconds = 1.1411e+04, _tick = 4023, _time = 1.6546e+09)
[2022-06-07 13:34:03,080][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 13:34:03,283][root][INFO] - Step 20495360 @ 2045.5 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 20495360, mean_episode_return = -50.847, mean_episode_step = 2784.4, total_loss = -223.75, pg_loss = -227.8, baseline_loss = 8.9132, entropy_loss = -4.8667, learner_queue_size = 32, train_seconds = 1.1416e+04, _tick = 4025, _time = 1.6546e+09)
[2022-06-07 13:34:08,286][root][INFO] - Step 20503040 @ 1475.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20503040, mean_episode_return = None, mean_episode_step = 2377.6, total_loss = 74.87, pg_loss = 49.573, baseline_loss = 30.213, entropy_loss = -4.916, learner_queue_size = 32, train_seconds = 1.1421e+04, _tick = 4025, _time = 1.6546e+09)
[2022-06-07 13:34:13,290][root][INFO] - Step 20513280 @ 2046.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 20513280, mean_episode_return = 66.99, mean_episode_step = 2700.9, total_loss = -148.69, pg_loss = -158.43, baseline_loss = 14.675, entropy_loss = -4.9371, learner_queue_size = 32, train_seconds = 1.1426e+04, _tick = 4026, _time = 1.6546e+09)
[2022-06-07 13:34:18,294][root][INFO] - Step 20520960 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20520960, mean_episode_return = None, mean_episode_step = 2383.7, total_loss = 64.646, pg_loss = 22.432, baseline_loss = 47.153, entropy_loss = -4.9393, learner_queue_size = 32, train_seconds = 1.1431e+04, _tick = 4026, _time = 1.6546e+09)
[2022-06-07 13:34:23,300][root][INFO] - Step 20531200 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20531200, mean_episode_return = 41.073, mean_episode_step = 2319.2, total_loss = -119.52, pg_loss = -120.55, baseline_loss = 6.0228, entropy_loss = -4.9887, learner_queue_size = 32, train_seconds = 1.1436e+04, _tick = 4030, _time = 1.6546e+09)
[2022-06-07 13:34:28,306][root][INFO] - Step 20538880 @ 1534.1 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 20538880, mean_episode_return = -10.53, mean_episode_step = 2297.6, total_loss = 72.631, pg_loss = 22.096, baseline_loss = 55.544, entropy_loss = -5.0089, learner_queue_size = 32, train_seconds = 1.1441e+04, _tick = 4032, _time = 1.6546e+09)
[2022-06-07 13:34:33,312][root][INFO] - Step 20549120 @ 2045.5 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 20549120, mean_episode_return = 67.728, mean_episode_step = 2288.0, total_loss = 53.684, pg_loss = -1.6326, baseline_loss = 60.305, entropy_loss = -4.9885, learner_queue_size = 32, train_seconds = 1.1446e+04, _tick = 4035, _time = 1.6546e+09)
[2022-06-07 13:34:38,318][root][INFO] - Step 20556800 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 20556800, mean_episode_return = None, mean_episode_step = 2466.6, total_loss = -41.218, pg_loss = -59.076, baseline_loss = 22.793, entropy_loss = -4.9352, learner_queue_size = 32, train_seconds = 1.1451e+04, _tick = 4036, _time = 1.6546e+09)
[2022-06-07 13:34:43,322][root][INFO] - Step 20567040 @ 2046.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20567040, mean_episode_return = 54.73, mean_episode_step = 2160.0, total_loss = 105.62, pg_loss = 65.819, baseline_loss = 44.754, entropy_loss = -4.9569, learner_queue_size = 32, train_seconds = 1.1456e+04, _tick = 4038, _time = 1.6546e+09)
[2022-06-07 13:34:48,326][root][INFO] - Step 20577280 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 20577280, mean_episode_return = None, mean_episode_step = 2087.5, total_loss = -44.579, pg_loss = -47.306, baseline_loss = 7.7465, entropy_loss = -5.0193, learner_queue_size = 32, train_seconds = 1.1461e+04, _tick = 4040, _time = 1.6546e+09)
[2022-06-07 13:34:53,332][root][INFO] - Step 20584960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20584960, mean_episode_return = 91.897, mean_episode_step = 2421.9, total_loss = -92.348, pg_loss = -96.802, baseline_loss = 9.5362, entropy_loss = -5.0814, learner_queue_size = 32, train_seconds = 1.1466e+04, _tick = 4043, _time = 1.6546e+09)
[2022-06-07 13:34:58,338][root][INFO] - Step 20595200 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20595200, mean_episode_return = None, mean_episode_step = 2222.6, total_loss = 70.586, pg_loss = 3.4294, baseline_loss = 72.241, entropy_loss = -5.0841, learner_queue_size = 32, train_seconds = 1.1471e+04, _tick = 4045, _time = 1.6546e+09)
[2022-06-07 13:35:03,344][root][INFO] - Step 20602880 @ 1534.0 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 20602880, mean_episode_return = 52.15, mean_episode_step = 2389.8, total_loss = 89.313, pg_loss = 47.169, baseline_loss = 47.209, entropy_loss = -5.0645, learner_queue_size = 32, train_seconds = 1.1476e+04, _tick = 4047, _time = 1.6546e+09)
[2022-06-07 13:35:08,350][root][INFO] - Step 20613120 @ 2045.7 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 20613120, mean_episode_return = None, mean_episode_step = 2156.0, total_loss = 266.62, pg_loss = 201.22, baseline_loss = 70.402, entropy_loss = -5.0006, learner_queue_size = 32, train_seconds = 1.1481e+04, _tick = 4047, _time = 1.6546e+09)
[2022-06-07 13:35:13,356][root][INFO] - Step 20620800 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20620800, mean_episode_return = None, mean_episode_step = 2686.9, total_loss = 78.516, pg_loss = 41.594, baseline_loss = 41.935, entropy_loss = -5.0134, learner_queue_size = 32, train_seconds = 1.1486e+04, _tick = 4049, _time = 1.6546e+09)
[2022-06-07 13:35:18,362][root][INFO] - Step 20631040 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 20631040, mean_episode_return = None, mean_episode_step = 2174.1, total_loss = -40.984, pg_loss = -57.91, baseline_loss = 21.905, entropy_loss = -4.98, learner_queue_size = 32, train_seconds = 1.1491e+04, _tick = 4050, _time = 1.6546e+09)
[2022-06-07 13:35:23,368][root][INFO] - Step 20638720 @ 1534.1 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 20638720, mean_episode_return = None, mean_episode_step = 2204.1, total_loss = 403.06, pg_loss = 276.15, baseline_loss = 131.94, entropy_loss = -5.0297, learner_queue_size = 32, train_seconds = 1.1496e+04, _tick = 4050, _time = 1.6546e+09)
[2022-06-07 13:35:28,374][root][INFO] - Step 20648960 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 20648960, mean_episode_return = None, mean_episode_step = 2321.5, total_loss = 194.41, pg_loss = 143.14, baseline_loss = 56.273, entropy_loss = -4.9998, learner_queue_size = 32, train_seconds = 1.1501e+04, _tick = 4051, _time = 1.6546e+09)
[2022-06-07 13:35:33,380][root][INFO] - Step 20656640 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 20656640, mean_episode_return = None, mean_episode_step = 2016.4, total_loss = 144.2, pg_loss = 110.6, baseline_loss = 38.62, entropy_loss = -5.0184, learner_queue_size = 32, train_seconds = 1.1506e+04, _tick = 4051, _time = 1.6546e+09)
[2022-06-07 13:35:38,386][root][INFO] - Step 20666880 @ 2045.7 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 20666880, mean_episode_return = None, mean_episode_step = 2342.7, total_loss = 235.08, pg_loss = 158.35, baseline_loss = 81.682, entropy_loss = -4.9555, learner_queue_size = 32, train_seconds = 1.1511e+04, _tick = 4053, _time = 1.6546e+09)
[2022-06-07 13:35:43,390][root][INFO] - Step 20677120 @ 2046.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 20677120, mean_episode_return = 18.446, mean_episode_step = 2310.1, total_loss = 312.54, pg_loss = 179.3, baseline_loss = 138.16, entropy_loss = -4.9264, learner_queue_size = 32, train_seconds = 1.1516e+04, _tick = 4055, _time = 1.6546e+09)
[2022-06-07 13:35:48,394][root][INFO] - Step 20684800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20684800, mean_episode_return = None, mean_episode_step = 2335.4, total_loss = -80.22, pg_loss = -85.499, baseline_loss = 10.195, entropy_loss = -4.916, learner_queue_size = 32, train_seconds = 1.1521e+04, _tick = 4057, _time = 1.6546e+09)
[2022-06-07 13:35:53,398][root][INFO] - Step 20695040 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 20695040, mean_episode_return = None, mean_episode_step = 2047.7, total_loss = -32.586, pg_loss = -47.0, baseline_loss = 19.402, entropy_loss = -4.988, learner_queue_size = 32, train_seconds = 1.1526e+04, _tick = 4058, _time = 1.6546e+09)
[2022-06-07 13:35:58,402][root][INFO] - Step 20702720 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20702720, mean_episode_return = 14.565, mean_episode_step = 2645.2, total_loss = -17.48, pg_loss = -49.548, baseline_loss = 37.08, entropy_loss = -5.0113, learner_queue_size = 32, train_seconds = 1.1531e+04, _tick = 4060, _time = 1.6546e+09)
[2022-06-07 13:36:03,406][root][INFO] - Step 20712960 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 20712960, mean_episode_return = 46.71, mean_episode_step = 2249.6, total_loss = 265.7, pg_loss = 157.5, baseline_loss = 113.26, entropy_loss = -5.0531, learner_queue_size = 32, train_seconds = 1.1536e+04, _tick = 4061, _time = 1.6546e+09)
[2022-06-07 13:36:08,410][root][INFO] - Step 20720640 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 20720640, mean_episode_return = None, mean_episode_step = 2508.8, total_loss = -91.6, pg_loss = -89.876, baseline_loss = 3.2997, entropy_loss = -5.0239, learner_queue_size = 32, train_seconds = 1.1541e+04, _tick = 4062, _time = 1.6546e+09)
[2022-06-07 13:36:13,414][root][INFO] - Step 20730880 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 20730880, mean_episode_return = 52.289, mean_episode_step = 2477.9, total_loss = 41.501, pg_loss = -11.498, baseline_loss = 58.012, entropy_loss = -5.0138, learner_queue_size = 32, train_seconds = 1.1546e+04, _tick = 4064, _time = 1.6546e+09)
[2022-06-07 13:36:18,418][root][INFO] - Step 20738560 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 20738560, mean_episode_return = 88.12, mean_episode_step = 2999.1, total_loss = -29.933, pg_loss = -48.364, baseline_loss = 23.488, entropy_loss = -5.0567, learner_queue_size = 32, train_seconds = 1.1551e+04, _tick = 4065, _time = 1.6546e+09)
[2022-06-07 13:36:23,422][root][INFO] - Step 20748800 @ 2046.4 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 20748800, mean_episode_return = 33.64, mean_episode_step = 2514.5, total_loss = -147.57, pg_loss = -148.15, baseline_loss = 5.6439, entropy_loss = -5.0722, learner_queue_size = 32, train_seconds = 1.1556e+04, _tick = 4067, _time = 1.6546e+09)
[2022-06-07 13:36:28,426][root][INFO] - Step 20756480 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 20756480, mean_episode_return = 31.349, mean_episode_step = 2590.6, total_loss = -51.91, pg_loss = -66.528, baseline_loss = 19.721, entropy_loss = -5.1042, learner_queue_size = 32, train_seconds = 1.1561e+04, _tick = 4070, _time = 1.6546e+09)
[2022-06-07 13:36:33,430][root][INFO] - Step 20766720 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20766720, mean_episode_return = None, mean_episode_step = 2344.7, total_loss = 206.14, pg_loss = 143.24, baseline_loss = 67.997, entropy_loss = -5.1037, learner_queue_size = 32, train_seconds = 1.1566e+04, _tick = 4071, _time = 1.6546e+09)
[2022-06-07 13:36:38,434][root][INFO] - Step 20774400 @ 1534.7 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 20774400, mean_episode_return = 52.362, mean_episode_step = 2657.1, total_loss = 197.44, pg_loss = 113.64, baseline_loss = 88.856, entropy_loss = -5.0535, learner_queue_size = 32, train_seconds = 1.1571e+04, _tick = 4074, _time = 1.6546e+09)
[2022-06-07 13:36:43,438][root][INFO] - Step 20784640 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 20784640, mean_episode_return = 79.138, mean_episode_step = 2392.6, total_loss = -207.87, pg_loss = -210.54, baseline_loss = 7.7674, entropy_loss = -5.0961, learner_queue_size = 32, train_seconds = 1.1576e+04, _tick = 4076, _time = 1.6546e+09)
[2022-06-07 13:36:48,442][root][INFO] - Step 20792320 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 20792320, mean_episode_return = 40.551, mean_episode_step = 2025.6, total_loss = 51.095, pg_loss = 21.627, baseline_loss = 34.516, entropy_loss = -5.0481, learner_queue_size = 32, train_seconds = 1.1581e+04, _tick = 4079, _time = 1.6546e+09)
[2022-06-07 13:36:53,446][root][INFO] - Step 20802560 @ 2046.4 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 20802560, mean_episode_return = 102.58, mean_episode_step = 2215.5, total_loss = 298.41, pg_loss = 174.96, baseline_loss = 128.5, entropy_loss = -5.0523, learner_queue_size = 32, train_seconds = 1.1586e+04, _tick = 4082, _time = 1.6546e+09)
[2022-06-07 13:36:58,450][root][INFO] - Step 20810240 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 20810240, mean_episode_return = 45.762, mean_episode_step = 2474.2, total_loss = -106.0, pg_loss = -132.04, baseline_loss = 31.099, entropy_loss = -5.0618, learner_queue_size = 32, train_seconds = 1.1591e+04, _tick = 4084, _time = 1.6546e+09)
[2022-06-07 13:37:03,454][root][INFO] - Step 20820480 @ 2046.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 20820480, mean_episode_return = None, mean_episode_step = 2069.4, total_loss = 113.66, pg_loss = 83.472, baseline_loss = 35.209, entropy_loss = -5.0206, learner_queue_size = 32, train_seconds = 1.1596e+04, _tick = 4087, _time = 1.6546e+09)
[2022-06-07 13:37:08,458][root][INFO] - Step 20828160 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 20828160, mean_episode_return = 73.471, mean_episode_step = 2130.3, total_loss = -137.75, pg_loss = -160.47, baseline_loss = 27.835, entropy_loss = -5.1093, learner_queue_size = 32, train_seconds = 1.1602e+04, _tick = 4089, _time = 1.6546e+09)
[2022-06-07 13:37:13,464][root][INFO] - Step 20838400 @ 2045.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 20838400, mean_episode_return = 87.74, mean_episode_step = 2329.9, total_loss = 35.106, pg_loss = -22.952, baseline_loss = 63.188, entropy_loss = -5.13, learner_queue_size = 32, train_seconds = 1.1606e+04, _tick = 4093, _time = 1.6546e+09)
[2022-06-07 13:37:18,470][root][INFO] - Step 20846080 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 20846080, mean_episode_return = -27.491, mean_episode_step = 2754.6, total_loss = -178.77, pg_loss = -178.91, baseline_loss = 5.355, entropy_loss = -5.2197, learner_queue_size = 32, train_seconds = 1.1612e+04, _tick = 4094, _time = 1.6546e+09)
[2022-06-07 13:37:23,474][root][INFO] - Step 20856320 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 20856320, mean_episode_return = 15.22, mean_episode_step = 1968.7, total_loss = 97.839, pg_loss = 31.47, baseline_loss = 71.595, entropy_loss = -5.2271, learner_queue_size = 32, train_seconds = 1.1616e+04, _tick = 4097, _time = 1.6546e+09)
[2022-06-07 13:37:28,478][root][INFO] - Step 20864000 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 20864000, mean_episode_return = 38.755, mean_episode_step = 1735.0, total_loss = 75.879, pg_loss = 45.303, baseline_loss = 35.786, entropy_loss = -5.2107, learner_queue_size = 32, train_seconds = 1.1622e+04, _tick = 4099, _time = 1.6546e+09)
[2022-06-07 13:37:33,482][root][INFO] - Step 20874240 @ 2046.4 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 20874240, mean_episode_return = 28.07, mean_episode_step = 1996.3, total_loss = -10.945, pg_loss = -42.589, baseline_loss = 36.891, entropy_loss = -5.2464, learner_queue_size = 32, train_seconds = 1.1626e+04, _tick = 4103, _time = 1.6546e+09)
[2022-06-07 13:37:38,488][root][INFO] - Step 20881920 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 20881920, mean_episode_return = 37.068, mean_episode_step = 2132.3, total_loss = 219.76, pg_loss = 104.38, baseline_loss = 120.63, entropy_loss = -5.2557, learner_queue_size = 32, train_seconds = 1.1632e+04, _tick = 4105, _time = 1.6546e+09)
[2022-06-07 13:37:43,494][root][INFO] - Step 20892160 @ 2045.6 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 20892160, mean_episode_return = 25.73, mean_episode_step = 1940.1, total_loss = -16.248, pg_loss = -55.521, baseline_loss = 44.546, entropy_loss = -5.2726, learner_queue_size = 32, train_seconds = 1.1636e+04, _tick = 4109, _time = 1.6546e+09)
[2022-06-07 13:37:48,498][root][INFO] - Step 20899840 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20899840, mean_episode_return = 22.13, mean_episode_step = 2396.8, total_loss = -143.09, pg_loss = -188.47, baseline_loss = 50.687, entropy_loss = -5.3069, learner_queue_size = 32, train_seconds = 1.1642e+04, _tick = 4111, _time = 1.6546e+09)
[2022-06-07 13:37:53,502][root][INFO] - Step 20910080 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 20910080, mean_episode_return = None, mean_episode_step = 1973.2, total_loss = 263.86, pg_loss = 184.69, baseline_loss = 84.466, entropy_loss = -5.2941, learner_queue_size = 32, train_seconds = 1.1646e+04, _tick = 4114, _time = 1.6546e+09)
[2022-06-07 13:37:58,506][root][INFO] - Step 20917760 @ 1534.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 20917760, mean_episode_return = 3.3896, mean_episode_step = 2028.6, total_loss = 727.56, pg_loss = 546.52, baseline_loss = 186.31, entropy_loss = -5.2706, learner_queue_size = 32, train_seconds = 1.1652e+04, _tick = 4117, _time = 1.6546e+09)
[2022-06-07 13:38:03,512][root][INFO] - Step 20928000 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20928000, mean_episode_return = 78.828, mean_episode_step = 2078.9, total_loss = -142.87, pg_loss = -188.67, baseline_loss = 51.064, entropy_loss = -5.2707, learner_queue_size = 32, train_seconds = 1.1656e+04, _tick = 4119, _time = 1.6546e+09)
[2022-06-07 13:38:08,519][root][INFO] - Step 20935680 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20935680, mean_episode_return = None, mean_episode_step = 2011.8, total_loss = 195.8, pg_loss = 157.0, baseline_loss = 44.019, entropy_loss = -5.2182, learner_queue_size = 32, train_seconds = 1.1662e+04, _tick = 4121, _time = 1.6546e+09)
[2022-06-07 13:38:13,522][root][INFO] - Step 20945920 @ 2046.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20945920, mean_episode_return = 86.837, mean_episode_step = 2128.1, total_loss = 504.78, pg_loss = 357.65, baseline_loss = 152.2, entropy_loss = -5.0768, learner_queue_size = 32, train_seconds = 1.1666e+04, _tick = 4125, _time = 1.6546e+09)
[2022-06-07 13:38:18,526][root][INFO] - Step 20953600 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 20953600, mean_episode_return = None, mean_episode_step = 2104.9, total_loss = 163.16, pg_loss = 109.46, baseline_loss = 58.805, entropy_loss = -5.1038, learner_queue_size = 32, train_seconds = 1.1672e+04, _tick = 4126, _time = 1.6546e+09)
[2022-06-07 13:38:23,532][root][INFO] - Step 20963840 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 20963840, mean_episode_return = None, mean_episode_step = 1484.9, total_loss = 249.97, pg_loss = 150.11, baseline_loss = 104.94, entropy_loss = -5.0878, learner_queue_size = 32, train_seconds = 1.1676e+04, _tick = 4128, _time = 1.6546e+09)
[2022-06-07 13:38:28,538][root][INFO] - Step 20971520 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 20971520, mean_episode_return = 37.281, mean_episode_step = 2457.3, total_loss = 295.59, pg_loss = 171.49, baseline_loss = 129.15, entropy_loss = -5.0536, learner_queue_size = 32, train_seconds = 1.1682e+04, _tick = 4130, _time = 1.6546e+09)
[2022-06-07 13:38:33,542][root][INFO] - Step 20981760 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 20981760, mean_episode_return = 80.612, mean_episode_step = 1627.9, total_loss = -249.76, pg_loss = -307.08, baseline_loss = 62.341, entropy_loss = -5.0205, learner_queue_size = 32, train_seconds = 1.1686e+04, _tick = 4132, _time = 1.6546e+09)
[2022-06-07 13:38:38,546][root][INFO] - Step 20989440 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 20989440, mean_episode_return = 25.783, mean_episode_step = 1779.5, total_loss = 272.3, pg_loss = 168.02, baseline_loss = 109.28, entropy_loss = -5.0023, learner_queue_size = 32, train_seconds = 1.1692e+04, _tick = 4133, _time = 1.6546e+09)
[2022-06-07 13:38:43,550][root][INFO] - Step 20999680 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 20999680, mean_episode_return = 35.83, mean_episode_step = 2061.8, total_loss = -37.437, pg_loss = -118.6, baseline_loss = 86.057, entropy_loss = -4.8976, learner_queue_size = 32, train_seconds = 1.1696e+04, _tick = 4137, _time = 1.6546e+09)
[2022-06-07 13:38:48,554][root][INFO] - Step 21007360 @ 1534.7 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 21007360, mean_episode_return = 97.968, mean_episode_step = 1567.8, total_loss = 131.97, pg_loss = 69.498, baseline_loss = 67.29, entropy_loss = -4.8129, learner_queue_size = 32, train_seconds = 1.1702e+04, _tick = 4140, _time = 1.6546e+09)
[2022-06-07 13:38:53,558][root][INFO] - Step 21017600 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 21017600, mean_episode_return = None, mean_episode_step = 1791.2, total_loss = 243.79, pg_loss = 152.29, baseline_loss = 96.173, entropy_loss = -4.6719, learner_queue_size = 32, train_seconds = 1.1707e+04, _tick = 4143, _time = 1.6546e+09)
[2022-06-07 13:38:58,562][root][INFO] - Step 21025280 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21025280, mean_episode_return = 116.17, mean_episode_step = 1721.2, total_loss = 55.109, pg_loss = -10.005, baseline_loss = 69.825, entropy_loss = -4.7109, learner_queue_size = 32, train_seconds = 1.1712e+04, _tick = 4145, _time = 1.6546e+09)
[2022-06-07 13:39:03,568][root][INFO] - Step 21035520 @ 2045.5 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 21035520, mean_episode_return = 106.37, mean_episode_step = 1948.8, total_loss = 448.49, pg_loss = 313.52, baseline_loss = 139.63, entropy_loss = -4.6692, learner_queue_size = 32, train_seconds = 1.1717e+04, _tick = 4147, _time = 1.6546e+09)
[2022-06-07 13:39:08,574][root][INFO] - Step 21043200 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 21043200, mean_episode_return = None, mean_episode_step = 1797.4, total_loss = 281.06, pg_loss = 167.46, baseline_loss = 118.23, entropy_loss = -4.6252, learner_queue_size = 32, train_seconds = 1.1722e+04, _tick = 4148, _time = 1.6546e+09)
[2022-06-07 13:39:13,578][root][INFO] - Step 21053440 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 21053440, mean_episode_return = None, mean_episode_step = 1491.9, total_loss = 117.57, pg_loss = 53.391, baseline_loss = 68.831, entropy_loss = -4.6491, learner_queue_size = 32, train_seconds = 1.1727e+04, _tick = 4149, _time = 1.6546e+09)
[2022-06-07 13:39:18,579][root][INFO] - Step 21061120 @ 1535.6 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 21061120, mean_episode_return = None, mean_episode_step = 1507.2, total_loss = 271.37, pg_loss = 146.39, baseline_loss = 129.64, entropy_loss = -4.6586, learner_queue_size = 32, train_seconds = 1.1732e+04, _tick = 4150, _time = 1.6546e+09)
[2022-06-07 13:39:23,582][root][INFO] - Step 21071360 @ 2046.9 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 21071360, mean_episode_return = None, mean_episode_step = 1855.2, total_loss = -159.94, pg_loss = -158.12, baseline_loss = 2.9052, entropy_loss = -4.7213, learner_queue_size = 32, train_seconds = 1.1737e+04, _tick = 4151, _time = 1.6546e+09)
[2022-06-07 13:39:28,588][root][INFO] - Step 21079040 @ 1534.1 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 21079040, mean_episode_return = None, mean_episode_step = 2023.8, total_loss = -125.38, pg_loss = -126.66, baseline_loss = 6.0714, entropy_loss = -4.7925, learner_queue_size = 32, train_seconds = 1.1742e+04, _tick = 4152, _time = 1.6546e+09)
[2022-06-07 13:39:33,594][root][INFO] - Step 21089280 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 21089280, mean_episode_return = None, mean_episode_step = 1385.8, total_loss = 1183.2, pg_loss = 754.57, baseline_loss = 433.45, entropy_loss = -4.7773, learner_queue_size = 32, train_seconds = 1.1747e+04, _tick = 4153, _time = 1.6546e+09)
[2022-06-07 13:39:38,600][root][INFO] - Step 21099520 @ 2045.6 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 21099520, mean_episode_return = None, mean_episode_step = 1889.0, total_loss = 175.06, pg_loss = 90.126, baseline_loss = 89.769, entropy_loss = -4.8359, learner_queue_size = 32, train_seconds = 1.1752e+04, _tick = 4155, _time = 1.6546e+09)
[2022-06-07 13:39:43,606][root][INFO] - Step 21107200 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 21107200, mean_episode_return = None, mean_episode_step = 1793.6, total_loss = -168.94, pg_loss = -165.96, baseline_loss = 1.8653, entropy_loss = -4.8414, learner_queue_size = 32, train_seconds = 1.1757e+04, _tick = 4156, _time = 1.6546e+09)
[2022-06-07 13:39:48,612][root][INFO] - Step 21117440 @ 2045.5 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 21117440, mean_episode_return = None, mean_episode_step = 1624.7, total_loss = 332.59, pg_loss = 243.84, baseline_loss = 93.492, entropy_loss = -4.7403, learner_queue_size = 32, train_seconds = 1.1762e+04, _tick = 4159, _time = 1.6546e+09)
[2022-06-07 13:39:53,618][root][INFO] - Step 21125120 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 21125120, mean_episode_return = 45.992, mean_episode_step = 2106.6, total_loss = 16.244, pg_loss = -9.7926, baseline_loss = 30.791, entropy_loss = -4.7536, learner_queue_size = 32, train_seconds = 1.1767e+04, _tick = 4161, _time = 1.6546e+09)
[2022-06-07 13:39:58,622][root][INFO] - Step 21135360 @ 2046.3 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 21135360, mean_episode_return = None, mean_episode_step = 1600.2, total_loss = 47.752, pg_loss = 10.301, baseline_loss = 42.221, entropy_loss = -4.7705, learner_queue_size = 32, train_seconds = 1.1772e+04, _tick = 4163, _time = 1.6546e+09)
[2022-06-07 13:40:03,626][root][INFO] - Step 21143040 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 21143040, mean_episode_return = 58.043, mean_episode_step = 1899.5, total_loss = 143.7, pg_loss = 89.709, baseline_loss = 58.685, entropy_loss = -4.6955, learner_queue_size = 32, train_seconds = 1.1777e+04, _tick = 4164, _time = 1.6546e+09)
[2022-06-07 13:40:08,630][root][INFO] - Step 21153280 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 21153280, mean_episode_return = 17.499, mean_episode_step = 2123.9, total_loss = -40.505, pg_loss = -60.275, baseline_loss = 24.442, entropy_loss = -4.6722, learner_queue_size = 32, train_seconds = 1.1782e+04, _tick = 4166, _time = 1.6546e+09)
[2022-06-07 13:40:13,634][root][INFO] - Step 21160960 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 21160960, mean_episode_return = 66.78, mean_episode_step = 1828.6, total_loss = 152.63, pg_loss = 85.117, baseline_loss = 72.105, entropy_loss = -4.5906, learner_queue_size = 32, train_seconds = 1.1787e+04, _tick = 4169, _time = 1.6546e+09)
[2022-06-07 13:40:18,638][root][INFO] - Step 21171200 @ 2046.3 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 21171200, mean_episode_return = None, mean_episode_step = 2272.9, total_loss = -87.956, pg_loss = -90.575, baseline_loss = 7.2218, entropy_loss = -4.6024, learner_queue_size = 32, train_seconds = 1.1792e+04, _tick = 4171, _time = 1.6546e+09)
[2022-06-07 13:40:23,644][root][INFO] - Step 21178880 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21178880, mean_episode_return = None, mean_episode_step = 1881.2, total_loss = -124.41, pg_loss = -124.61, baseline_loss = 4.777, entropy_loss = -4.5697, learner_queue_size = 32, train_seconds = 1.1797e+04, _tick = 4173, _time = 1.6546e+09)
[2022-06-07 13:40:28,650][root][INFO] - Step 21189120 @ 2045.7 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 21189120, mean_episode_return = 112.77, mean_episode_step = 1360.0, total_loss = 104.56, pg_loss = 43.65, baseline_loss = 65.47, entropy_loss = -4.5626, learner_queue_size = 32, train_seconds = 1.1802e+04, _tick = 4175, _time = 1.6546e+09)
[2022-06-07 13:40:33,654][root][INFO] - Step 21196800 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 21196800, mean_episode_return = None, mean_episode_step = 2098.7, total_loss = 105.86, pg_loss = 65.512, baseline_loss = 44.953, entropy_loss = -4.6062, learner_queue_size = 32, train_seconds = 1.1807e+04, _tick = 4175, _time = 1.6546e+09)
[2022-06-07 13:40:38,659][root][INFO] - Step 21204480 @ 1534.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21204480, mean_episode_return = 44.492, mean_episode_step = 1525.0, total_loss = 49.773, pg_loss = -12.63, baseline_loss = 67.061, entropy_loss = -4.6583, learner_queue_size = 32, train_seconds = 1.1812e+04, _tick = 4177, _time = 1.6546e+09)
[2022-06-07 13:40:43,678][root][INFO] - Step 21214720 @ 2040.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 21214720, mean_episode_return = None, mean_episode_step = 2297.2, total_loss = -98.725, pg_loss = -98.293, baseline_loss = 4.1834, entropy_loss = -4.6156, learner_queue_size = 32, train_seconds = 1.1817e+04, _tick = 4177, _time = 1.6546e+09)
[2022-06-07 13:40:48,684][root][INFO] - Step 21222400 @ 1534.1 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 21222400, mean_episode_return = None, mean_episode_step = 1855.8, total_loss = 191.03, pg_loss = 101.56, baseline_loss = 94.089, entropy_loss = -4.6162, learner_queue_size = 32, train_seconds = 1.1822e+04, _tick = 4177, _time = 1.6546e+09)
[2022-06-07 13:40:53,686][root][INFO] - Step 21232640 @ 2047.2 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 21232640, mean_episode_return = None, mean_episode_step = 2220.8, total_loss = 129.75, pg_loss = 73.685, baseline_loss = 60.74, entropy_loss = -4.679, learner_queue_size = 32, train_seconds = 1.1827e+04, _tick = 4178, _time = 1.6546e+09)
[2022-06-07 13:40:58,690][root][INFO] - Step 21242880 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 21242880, mean_episode_return = None, mean_episode_step = 1570.2, total_loss = -132.53, pg_loss = -130.73, baseline_loss = 2.9232, entropy_loss = -4.72, learner_queue_size = 32, train_seconds = 1.1832e+04, _tick = 4179, _time = 1.6546e+09)
[2022-06-07 13:41:03,694][root][INFO] - Step 21250560 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 21250560, mean_episode_return = 81.328, mean_episode_step = 2155.3, total_loss = -28.545, pg_loss = -63.121, baseline_loss = 39.31, entropy_loss = -4.7346, learner_queue_size = 32, train_seconds = 1.1837e+04, _tick = 4180, _time = 1.6546e+09)
[2022-06-07 13:41:08,700][root][INFO] - Step 21260800 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 21260800, mean_episode_return = None, mean_episode_step = 2227.6, total_loss = 372.59, pg_loss = 253.68, baseline_loss = 123.76, entropy_loss = -4.839, learner_queue_size = 32, train_seconds = 1.1842e+04, _tick = 4181, _time = 1.6546e+09)
[2022-06-07 13:41:13,702][root][INFO] - Step 21268480 @ 1535.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 21268480, mean_episode_return = None, mean_episode_step = 2047.3, total_loss = 36.462, pg_loss = -7.8621, baseline_loss = 49.141, entropy_loss = -4.8168, learner_queue_size = 32, train_seconds = 1.1847e+04, _tick = 4182, _time = 1.6546e+09)
[2022-06-07 13:41:18,706][root][INFO] - Step 21278720 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 21278720, mean_episode_return = None, mean_episode_step = 2022.9, total_loss = 554.9, pg_loss = 441.87, baseline_loss = 117.89, entropy_loss = -4.8541, learner_queue_size = 32, train_seconds = 1.1852e+04, _tick = 4184, _time = 1.6546e+09)
[2022-06-07 13:41:23,710][root][INFO] - Step 21286400 @ 1534.8 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 21286400, mean_episode_return = 92.922, mean_episode_step = 1806.3, total_loss = 50.508, pg_loss = 9.282, baseline_loss = 46.132, entropy_loss = -4.9064, learner_queue_size = 32, train_seconds = 1.1857e+04, _tick = 4187, _time = 1.6546e+09)
[2022-06-07 13:41:28,714][root][INFO] - Step 21294080 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 21294080, mean_episode_return = None, mean_episode_step = 1775.6, total_loss = 34.866, pg_loss = -2.5886, baseline_loss = 42.454, entropy_loss = -4.9989, learner_queue_size = 32, train_seconds = 1.1862e+04, _tick = 4188, _time = 1.6546e+09)
[2022-06-07 13:41:33,718][root][INFO] - Step 21304320 @ 2046.4 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 21304320, mean_episode_return = 46.044, mean_episode_step = 2292.3, total_loss = 235.13, pg_loss = 161.06, baseline_loss = 79.037, entropy_loss = -4.9667, learner_queue_size = 32, train_seconds = 1.1867e+04, _tick = 4191, _time = 1.6546e+09)
[2022-06-07 13:41:38,724][root][INFO] - Step 21312000 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 21312000, mean_episode_return = None, mean_episode_step = 1949.2, total_loss = -66.021, pg_loss = -79.805, baseline_loss = 18.781, entropy_loss = -4.9967, learner_queue_size = 32, train_seconds = 1.1872e+04, _tick = 4192, _time = 1.6546e+09)
[2022-06-07 13:41:43,730][root][INFO] - Step 21322240 @ 2045.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21322240, mean_episode_return = 83.506, mean_episode_step = 1769.6, total_loss = 251.5, pg_loss = 160.25, baseline_loss = 96.287, entropy_loss = -5.0317, learner_queue_size = 32, train_seconds = 1.1877e+04, _tick = 4193, _time = 1.6546e+09)
[2022-06-07 13:41:48,734][root][INFO] - Step 21332480 @ 2046.6 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 21332480, mean_episode_return = 94.89, mean_episode_step = 1815.4, total_loss = 461.36, pg_loss = 291.14, baseline_loss = 175.24, entropy_loss = -5.0107, learner_queue_size = 32, train_seconds = 1.1882e+04, _tick = 4194, _time = 1.6546e+09)
[2022-06-07 13:41:53,741][root][INFO] - Step 21340160 @ 1534.0 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 21340160, mean_episode_return = None, mean_episode_step = 2508.8, total_loss = 142.4, pg_loss = 98.267, baseline_loss = 49.108, entropy_loss = -4.9704, learner_queue_size = 32, train_seconds = 1.1887e+04, _tick = 4195, _time = 1.6546e+09)
[2022-06-07 13:41:58,746][root][INFO] - Step 21347840 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21347840, mean_episode_return = 14.46, mean_episode_step = 1980.7, total_loss = -65.031, pg_loss = -106.12, baseline_loss = 46.032, entropy_loss = -4.9406, learner_queue_size = 32, train_seconds = 1.1892e+04, _tick = 4196, _time = 1.6546e+09)
[2022-06-07 13:42:03,750][root][INFO] - Step 21358080 @ 2046.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 21358080, mean_episode_return = None, mean_episode_step = 2576.1, total_loss = -59.814, pg_loss = -85.069, baseline_loss = 30.148, entropy_loss = -4.8929, learner_queue_size = 32, train_seconds = 1.1897e+04, _tick = 4196, _time = 1.6546e+09)
[2022-06-07 13:42:08,756][root][INFO] - Step 21365760 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 21365760, mean_episode_return = 25.2, mean_episode_step = 2354.7, total_loss = -71.418, pg_loss = -88.365, baseline_loss = 21.818, entropy_loss = -4.8715, learner_queue_size = 32, train_seconds = 1.1902e+04, _tick = 4197, _time = 1.6546e+09)
[2022-06-07 13:42:13,762][root][INFO] - Step 21376000 @ 2045.6 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 21376000, mean_episode_return = 58.19, mean_episode_step = 1878.2, total_loss = -89.178, pg_loss = -128.71, baseline_loss = 44.395, entropy_loss = -4.8648, learner_queue_size = 32, train_seconds = 1.1907e+04, _tick = 4201, _time = 1.6546e+09)
[2022-06-07 13:42:18,766][root][INFO] - Step 21383680 @ 1534.7 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 21383680, mean_episode_return = None, mean_episode_step = 2424.8, total_loss = 158.6, pg_loss = 78.574, baseline_loss = 84.887, entropy_loss = -4.8632, learner_queue_size = 32, train_seconds = 1.1912e+04, _tick = 4202, _time = 1.6546e+09)
[2022-06-07 13:42:23,772][root][INFO] - Step 21393920 @ 2045.5 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 21393920, mean_episode_return = 20.413, mean_episode_step = 2227.6, total_loss = -49.827, pg_loss = -89.722, baseline_loss = 44.643, entropy_loss = -4.7472, learner_queue_size = 32, train_seconds = 1.1917e+04, _tick = 4204, _time = 1.6546e+09)
[2022-06-07 13:42:28,778][root][INFO] - Step 21401600 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 21401600, mean_episode_return = 42.251, mean_episode_step = 1893.5, total_loss = -139.13, pg_loss = -141.54, baseline_loss = 7.1105, entropy_loss = -4.7001, learner_queue_size = 32, train_seconds = 1.1922e+04, _tick = 4206, _time = 1.6546e+09)
[2022-06-07 13:42:33,782][root][INFO] - Step 21411840 @ 2046.3 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 21411840, mean_episode_return = None, mean_episode_step = 2712.2, total_loss = -65.555, pg_loss = -72.324, baseline_loss = 11.444, entropy_loss = -4.6746, learner_queue_size = 32, train_seconds = 1.1927e+04, _tick = 4208, _time = 1.6546e+09)
[2022-06-07 13:42:38,788][root][INFO] - Step 21419520 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21419520, mean_episode_return = None, mean_episode_step = 1820.2, total_loss = -24.211, pg_loss = -39.097, baseline_loss = 19.568, entropy_loss = -4.6814, learner_queue_size = 32, train_seconds = 1.1932e+04, _tick = 4210, _time = 1.6546e+09)
[2022-06-07 13:42:43,790][root][INFO] - Step 21429760 @ 2047.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 21429760, mean_episode_return = None, mean_episode_step = 2511.0, total_loss = -48.234, pg_loss = -65.476, baseline_loss = 21.942, entropy_loss = -4.6992, learner_queue_size = 32, train_seconds = 1.1937e+04, _tick = 4211, _time = 1.6546e+09)
[2022-06-07 13:42:48,796][root][INFO] - Step 21440000 @ 2045.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 21440000, mean_episode_return = 32.292, mean_episode_step = 2236.7, total_loss = -70.301, pg_loss = -78.54, baseline_loss = 13.165, entropy_loss = -4.9254, learner_queue_size = 32, train_seconds = 1.1942e+04, _tick = 4214, _time = 1.6546e+09)
[2022-06-07 13:42:53,798][root][INFO] - Step 21447680 @ 1535.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 21447680, mean_episode_return = None, mean_episode_step = 2095.5, total_loss = 51.184, pg_loss = 12.135, baseline_loss = 44.03, entropy_loss = -4.9811, learner_queue_size = 32, train_seconds = 1.1947e+04, _tick = 4215, _time = 1.6546e+09)
[2022-06-07 13:42:58,802][root][INFO] - Step 21457920 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 21457920, mean_episode_return = None, mean_episode_step = 2342.7, total_loss = 57.873, pg_loss = 13.916, baseline_loss = 48.942, entropy_loss = -4.9849, learner_queue_size = 32, train_seconds = 1.1952e+04, _tick = 4216, _time = 1.6546e+09)
[2022-06-07 13:43:03,808][root][INFO] - Step 21465600 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21465600, mean_episode_return = None, mean_episode_step = 2257.0, total_loss = -99.139, pg_loss = -97.966, baseline_loss = 3.7814, entropy_loss = -4.9546, learner_queue_size = 32, train_seconds = 1.1957e+04, _tick = 4217, _time = 1.6546e+09)
[2022-06-07 13:43:08,811][root][INFO] - Step 21475840 @ 2046.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21475840, mean_episode_return = None, mean_episode_step = 2333.2, total_loss = -108.62, pg_loss = -108.76, baseline_loss = 5.102, entropy_loss = -4.9591, learner_queue_size = 32, train_seconds = 1.1962e+04, _tick = 4220, _time = 1.6546e+09)
[2022-06-07 13:43:13,814][root][INFO] - Step 21483520 @ 1535.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 21483520, mean_episode_return = 55.381, mean_episode_step = 2347.5, total_loss = -155.44, pg_loss = -186.96, baseline_loss = 36.571, entropy_loss = -5.0511, learner_queue_size = 32, train_seconds = 1.1967e+04, _tick = 4222, _time = 1.6546e+09)
[2022-06-07 13:43:18,818][root][INFO] - Step 21493760 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 21493760, mean_episode_return = 4.9855, mean_episode_step = 2305.3, total_loss = 59.843, pg_loss = -5.342, baseline_loss = 70.305, entropy_loss = -5.1207, learner_queue_size = 32, train_seconds = 1.1972e+04, _tick = 4226, _time = 1.6546e+09)
[2022-06-07 13:43:23,824][root][INFO] - Step 21501440 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 21501440, mean_episode_return = 80.221, mean_episode_step = 1955.9, total_loss = 268.5, pg_loss = 198.92, baseline_loss = 74.786, entropy_loss = -5.1999, learner_queue_size = 32, train_seconds = 1.1977e+04, _tick = 4227, _time = 1.6546e+09)
[2022-06-07 13:43:28,830][root][INFO] - Step 21511680 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 21511680, mean_episode_return = 44.781, mean_episode_step = 2441.1, total_loss = 202.24, pg_loss = 126.38, baseline_loss = 81.09, entropy_loss = -5.2303, learner_queue_size = 32, train_seconds = 1.1982e+04, _tick = 4230, _time = 1.6546e+09)
[2022-06-07 13:43:33,836][root][INFO] - Step 21519360 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21519360, mean_episode_return = None, mean_episode_step = 2308.2, total_loss = 340.19, pg_loss = 270.01, baseline_loss = 75.365, entropy_loss = -5.1902, learner_queue_size = 32, train_seconds = 1.1987e+04, _tick = 4232, _time = 1.6546e+09)
[2022-06-07 13:43:38,842][root][INFO] - Step 21529600 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 21529600, mean_episode_return = 59.97, mean_episode_step = 1796.7, total_loss = 240.8, pg_loss = 152.93, baseline_loss = 93.08, entropy_loss = -5.2055, learner_queue_size = 32, train_seconds = 1.1992e+04, _tick = 4236, _time = 1.6546e+09)
[2022-06-07 13:43:43,846][root][INFO] - Step 21537280 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 21537280, mean_episode_return = 68.619, mean_episode_step = 2254.9, total_loss = -23.122, pg_loss = -66.835, baseline_loss = 48.985, entropy_loss = -5.2718, learner_queue_size = 32, train_seconds = 1.1997e+04, _tick = 4239, _time = 1.6546e+09)
[2022-06-07 13:43:48,850][root][INFO] - Step 21547520 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21547520, mean_episode_return = 13.472, mean_episode_step = 2243.7, total_loss = -53.539, pg_loss = -78.819, baseline_loss = 30.588, entropy_loss = -5.3082, learner_queue_size = 32, train_seconds = 1.2002e+04, _tick = 4241, _time = 1.6546e+09)
[2022-06-07 13:43:53,854][root][INFO] - Step 21555200 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 21555200, mean_episode_return = 136.72, mean_episode_step = 1935.1, total_loss = -144.76, pg_loss = -145.36, baseline_loss = 5.8899, entropy_loss = -5.2924, learner_queue_size = 32, train_seconds = 1.2007e+04, _tick = 4243, _time = 1.6546e+09)
[2022-06-07 13:43:58,860][root][INFO] - Step 21565440 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 21565440, mean_episode_return = None, mean_episode_step = 2166.5, total_loss = 406.28, pg_loss = 318.6, baseline_loss = 93.058, entropy_loss = -5.375, learner_queue_size = 32, train_seconds = 1.2012e+04, _tick = 4246, _time = 1.6546e+09)
[2022-06-07 13:44:03,866][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 13:44:04,088][root][INFO] - Step 21573120 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21573120, mean_episode_return = 7.7497, mean_episode_step = 1900.3, total_loss = 303.62, pg_loss = 217.02, baseline_loss = 91.961, entropy_loss = -5.3631, learner_queue_size = 32, train_seconds = 1.2017e+04, _tick = 4249, _time = 1.6546e+09)
[2022-06-07 13:44:09,090][root][INFO] - Step 21583360 @ 1960.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 21583360, mean_episode_return = None, mean_episode_step = 2499.9, total_loss = -10.684, pg_loss = -28.229, baseline_loss = 22.828, entropy_loss = -5.2834, learner_queue_size = 32, train_seconds = 1.2022e+04, _tick = 4252, _time = 1.6546e+09)
[2022-06-07 13:44:14,096][root][INFO] - Step 21591040 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21591040, mean_episode_return = 72.625, mean_episode_step = 1552.7, total_loss = -97.643, pg_loss = -119.54, baseline_loss = 27.197, entropy_loss = -5.3036, learner_queue_size = 32, train_seconds = 1.2027e+04, _tick = 4255, _time = 1.6546e+09)
[2022-06-07 13:44:19,102][root][INFO] - Step 21601280 @ 2045.6 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 21601280, mean_episode_return = None, mean_episode_step = 2062.3, total_loss = 123.0, pg_loss = 78.479, baseline_loss = 49.773, entropy_loss = -5.2504, learner_queue_size = 32, train_seconds = 1.2032e+04, _tick = 4257, _time = 1.6546e+09)
[2022-06-07 13:44:24,108][root][INFO] - Step 21608960 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 21608960, mean_episode_return = 17.81, mean_episode_step = 1881.0, total_loss = -13.793, pg_loss = -40.086, baseline_loss = 31.497, entropy_loss = -5.204, learner_queue_size = 32, train_seconds = 1.2037e+04, _tick = 4258, _time = 1.6546e+09)
[2022-06-07 13:44:29,114][root][INFO] - Step 21619200 @ 2045.5 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 21619200, mean_episode_return = 16.907, mean_episode_step = 1857.2, total_loss = 76.886, pg_loss = 4.9437, baseline_loss = 77.144, entropy_loss = -5.2021, learner_queue_size = 32, train_seconds = 1.2042e+04, _tick = 4262, _time = 1.6546e+09)
[2022-06-07 13:44:34,118][root][INFO] - Step 21626880 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 21626880, mean_episode_return = 71.588, mean_episode_step = 1959.4, total_loss = -65.779, pg_loss = -102.22, baseline_loss = 41.577, entropy_loss = -5.1331, learner_queue_size = 32, train_seconds = 1.2047e+04, _tick = 4264, _time = 1.6546e+09)
[2022-06-07 13:44:39,124][root][INFO] - Step 21637120 @ 2045.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 21637120, mean_episode_return = 27.63, mean_episode_step = 1620.9, total_loss = 3.0599, pg_loss = -37.771, baseline_loss = 46.039, entropy_loss = -5.2075, learner_queue_size = 32, train_seconds = 1.2052e+04, _tick = 4267, _time = 1.6546e+09)
[2022-06-07 13:44:44,130][root][INFO] - Step 21644800 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 21644800, mean_episode_return = 31.792, mean_episode_step = 1583.3, total_loss = 253.23, pg_loss = 167.31, baseline_loss = 91.087, entropy_loss = -5.1647, learner_queue_size = 32, train_seconds = 1.2057e+04, _tick = 4269, _time = 1.6546e+09)
[2022-06-07 13:44:49,134][root][INFO] - Step 21655040 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 21655040, mean_episode_return = -0.020345, mean_episode_step = 1840.1, total_loss = 518.87, pg_loss = 313.91, baseline_loss = 210.1, entropy_loss = -5.1436, learner_queue_size = 32, train_seconds = 1.2062e+04, _tick = 4272, _time = 1.6546e+09)
[2022-06-07 13:44:54,138][root][INFO] - Step 21662720 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 21662720, mean_episode_return = 133.64, mean_episode_step = 2138.4, total_loss = 631.59, pg_loss = 413.27, baseline_loss = 223.46, entropy_loss = -5.15, learner_queue_size = 32, train_seconds = 1.2067e+04, _tick = 4274, _time = 1.6546e+09)
[2022-06-07 13:44:59,143][root][INFO] - Step 21672960 @ 2046.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21672960, mean_episode_return = 62.022, mean_episode_step = 1194.0, total_loss = 632.08, pg_loss = 440.15, baseline_loss = 197.05, entropy_loss = -5.1263, learner_queue_size = 32, train_seconds = 1.2072e+04, _tick = 4277, _time = 1.6546e+09)
[2022-06-07 13:45:04,149][root][INFO] - Step 21680640 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21680640, mean_episode_return = None, mean_episode_step = 1775.8, total_loss = -133.5, pg_loss = -132.91, baseline_loss = 4.5709, entropy_loss = -5.1591, learner_queue_size = 32, train_seconds = 1.2077e+04, _tick = 4278, _time = 1.6546e+09)
[2022-06-07 13:45:09,155][root][INFO] - Step 21690880 @ 2045.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 21690880, mean_episode_return = None, mean_episode_step = 1873.8, total_loss = -45.647, pg_loss = -67.212, baseline_loss = 26.725, entropy_loss = -5.1598, learner_queue_size = 32, train_seconds = 1.2082e+04, _tick = 4279, _time = 1.6546e+09)
[2022-06-07 13:45:14,161][root][INFO] - Step 21698560 @ 1534.1 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 21698560, mean_episode_return = 85.201, mean_episode_step = 1634.9, total_loss = -75.273, pg_loss = -110.79, baseline_loss = 40.758, entropy_loss = -5.2393, learner_queue_size = 32, train_seconds = 1.2087e+04, _tick = 4281, _time = 1.6546e+09)
[2022-06-07 13:45:19,166][root][INFO] - Step 21706240 @ 1534.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 21706240, mean_episode_return = None, mean_episode_step = 1757.0, total_loss = 0.23213, pg_loss = -12.853, baseline_loss = 18.213, entropy_loss = -5.1283, learner_queue_size = 32, train_seconds = 1.2092e+04, _tick = 4283, _time = 1.6546e+09)
[2022-06-07 13:45:24,170][root][INFO] - Step 21716480 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 21716480, mean_episode_return = 60.94, mean_episode_step = 1594.4, total_loss = 235.91, pg_loss = 167.23, baseline_loss = 73.8, entropy_loss = -5.1258, learner_queue_size = 32, train_seconds = 1.2097e+04, _tick = 4286, _time = 1.6546e+09)
[2022-06-07 13:45:29,176][root][INFO] - Step 21724160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21724160, mean_episode_return = 49.302, mean_episode_step = 2044.6, total_loss = 190.72, pg_loss = 95.729, baseline_loss = 100.12, entropy_loss = -5.1347, learner_queue_size = 32, train_seconds = 1.2102e+04, _tick = 4289, _time = 1.6546e+09)
[2022-06-07 13:45:34,182][root][INFO] - Step 21734400 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 21734400, mean_episode_return = None, mean_episode_step = 1755.9, total_loss = -75.642, pg_loss = -85.06, baseline_loss = 14.419, entropy_loss = -5.0012, learner_queue_size = 32, train_seconds = 1.2107e+04, _tick = 4292, _time = 1.6546e+09)
[2022-06-07 13:45:39,188][root][INFO] - Step 21742080 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21742080, mean_episode_return = None, mean_episode_step = 1817.8, total_loss = -14.401, pg_loss = -30.772, baseline_loss = 21.343, entropy_loss = -4.9722, learner_queue_size = 32, train_seconds = 1.2112e+04, _tick = 4293, _time = 1.6546e+09)
[2022-06-07 13:45:44,194][root][INFO] - Step 21752320 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21752320, mean_episode_return = 140.88, mean_episode_step = 2009.3, total_loss = 344.31, pg_loss = 237.69, baseline_loss = 111.52, entropy_loss = -4.9065, learner_queue_size = 32, train_seconds = 1.2117e+04, _tick = 4295, _time = 1.6546e+09)
[2022-06-07 13:45:49,198][root][INFO] - Step 21760000 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 21760000, mean_episode_return = None, mean_episode_step = 1757.4, total_loss = -130.51, pg_loss = -142.56, baseline_loss = 17.025, entropy_loss = -4.9721, learner_queue_size = 32, train_seconds = 1.2122e+04, _tick = 4295, _time = 1.6546e+09)
[2022-06-07 13:45:54,204][root][INFO] - Step 21770240 @ 2045.5 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 21770240, mean_episode_return = None, mean_episode_step = 1738.6, total_loss = 166.72, pg_loss = 81.328, baseline_loss = 90.336, entropy_loss = -4.9394, learner_queue_size = 32, train_seconds = 1.2127e+04, _tick = 4296, _time = 1.6546e+09)
[2022-06-07 13:45:59,210][root][INFO] - Step 21777920 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 21777920, mean_episode_return = None, mean_episode_step = 2374.3, total_loss = -115.81, pg_loss = -128.64, baseline_loss = 17.81, entropy_loss = -4.9857, learner_queue_size = 32, train_seconds = 1.2132e+04, _tick = 4296, _time = 1.6546e+09)
[2022-06-07 13:46:04,214][root][INFO] - Step 21788160 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 21788160, mean_episode_return = None, mean_episode_step = 1728.9, total_loss = 56.813, pg_loss = 21.398, baseline_loss = 40.283, entropy_loss = -4.8676, learner_queue_size = 32, train_seconds = 1.2137e+04, _tick = 4298, _time = 1.6546e+09)
[2022-06-07 13:46:09,220][root][INFO] - Step 21795840 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 21795840, mean_episode_return = 133.65, mean_episode_step = 1725.0, total_loss = -147.72, pg_loss = -180.97, baseline_loss = 38.157, entropy_loss = -4.9062, learner_queue_size = 32, train_seconds = 1.2142e+04, _tick = 4301, _time = 1.6546e+09)
[2022-06-07 13:46:14,242][root][INFO] - Step 21806080 @ 2039.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21806080, mean_episode_return = None, mean_episode_step = 1335.5, total_loss = 317.38, pg_loss = 220.64, baseline_loss = 101.72, entropy_loss = -4.9761, learner_queue_size = 32, train_seconds = 1.2147e+04, _tick = 4303, _time = 1.6546e+09)
[2022-06-07 13:46:19,248][root][INFO] - Step 21813760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21813760, mean_episode_return = 34.821, mean_episode_step = 2290.3, total_loss = 489.71, pg_loss = 309.3, baseline_loss = 185.35, entropy_loss = -4.9435, learner_queue_size = 32, train_seconds = 1.2152e+04, _tick = 4306, _time = 1.6546e+09)
[2022-06-07 13:46:24,250][root][INFO] - Step 21824000 @ 2047.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21824000, mean_episode_return = 53.128, mean_episode_step = 1676.7, total_loss = 253.37, pg_loss = 79.084, baseline_loss = 179.16, entropy_loss = -4.8744, learner_queue_size = 32, train_seconds = 1.2157e+04, _tick = 4310, _time = 1.6546e+09)
[2022-06-07 13:46:29,256][root][INFO] - Step 21831680 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 21831680, mean_episode_return = 37.43, mean_episode_step = 1954.4, total_loss = 54.811, pg_loss = 22.211, baseline_loss = 37.44, entropy_loss = -4.8393, learner_queue_size = 32, train_seconds = 1.2162e+04, _tick = 4311, _time = 1.6546e+09)
[2022-06-07 13:46:34,262][root][INFO] - Step 21841920 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 21841920, mean_episode_return = 37.96, mean_episode_step = 1638.7, total_loss = 81.566, pg_loss = 3.8377, baseline_loss = 82.657, entropy_loss = -4.9283, learner_queue_size = 32, train_seconds = 1.2167e+04, _tick = 4314, _time = 1.6546e+09)
[2022-06-07 13:46:39,266][root][INFO] - Step 21852160 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21852160, mean_episode_return = None, mean_episode_step = 1893.9, total_loss = -77.562, pg_loss = -101.09, baseline_loss = 28.34, entropy_loss = -4.8143, learner_queue_size = 32, train_seconds = 1.2172e+04, _tick = 4316, _time = 1.6546e+09)
[2022-06-07 13:46:44,270][root][INFO] - Step 21859840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 21859840, mean_episode_return = None, mean_episode_step = 2096.4, total_loss = 17.634, pg_loss = -3.9917, baseline_loss = 26.462, entropy_loss = -4.8361, learner_queue_size = 32, train_seconds = 1.2177e+04, _tick = 4317, _time = 1.6546e+09)
[2022-06-07 13:46:49,274][root][INFO] - Step 21870080 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 21870080, mean_episode_return = None, mean_episode_step = 2248.6, total_loss = 17.966, pg_loss = 3.1783, baseline_loss = 19.617, entropy_loss = -4.8292, learner_queue_size = 32, train_seconds = 1.2182e+04, _tick = 4319, _time = 1.6546e+09)
[2022-06-07 13:46:54,278][root][INFO] - Step 21877760 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21877760, mean_episode_return = 77.207, mean_episode_step = 1882.0, total_loss = 75.529, pg_loss = 16.567, baseline_loss = 63.656, entropy_loss = -4.694, learner_queue_size = 32, train_seconds = 1.2187e+04, _tick = 4322, _time = 1.6546e+09)
[2022-06-07 13:46:59,282][root][INFO] - Step 21888000 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 21888000, mean_episode_return = None, mean_episode_step = 1869.1, total_loss = -13.314, pg_loss = -44.041, baseline_loss = 35.271, entropy_loss = -4.5441, learner_queue_size = 32, train_seconds = 1.2192e+04, _tick = 4324, _time = 1.6546e+09)
[2022-06-07 13:47:04,286][root][INFO] - Step 21895680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 21895680, mean_episode_return = None, mean_episode_step = 2432.8, total_loss = 211.17, pg_loss = 111.35, baseline_loss = 104.51, entropy_loss = -4.691, learner_queue_size = 32, train_seconds = 1.2197e+04, _tick = 4325, _time = 1.6546e+09)
[2022-06-07 13:47:09,305][root][INFO] - Step 21905920 @ 2040.0 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 21905920, mean_episode_return = None, mean_episode_step = 1647.6, total_loss = 0.31967, pg_loss = -23.955, baseline_loss = 28.941, entropy_loss = -4.6661, learner_queue_size = 32, train_seconds = 1.2202e+04, _tick = 4326, _time = 1.6546e+09)
[2022-06-07 13:47:14,310][root][INFO] - Step 21913600 @ 1534.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21913600, mean_episode_return = 15.96, mean_episode_step = 1858.7, total_loss = 1129.0, pg_loss = 676.9, baseline_loss = 456.96, entropy_loss = -4.8262, learner_queue_size = 32, train_seconds = 1.2207e+04, _tick = 4327, _time = 1.6546e+09)
[2022-06-07 13:47:19,314][root][INFO] - Step 21923840 @ 2046.3 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 21923840, mean_episode_return = 95.407, mean_episode_step = 1734.0, total_loss = -168.54, pg_loss = -201.66, baseline_loss = 37.932, entropy_loss = -4.8114, learner_queue_size = 32, train_seconds = 1.2212e+04, _tick = 4329, _time = 1.6546e+09)
[2022-06-07 13:47:24,318][root][INFO] - Step 21931520 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 21931520, mean_episode_return = None, mean_episode_step = 1829.2, total_loss = -187.21, pg_loss = -187.78, baseline_loss = 5.4092, entropy_loss = -4.8419, learner_queue_size = 32, train_seconds = 1.2217e+04, _tick = 4330, _time = 1.6546e+09)
[2022-06-07 13:47:29,322][root][INFO] - Step 21941760 @ 2046.4 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 21941760, mean_episode_return = None, mean_episode_step = 1977.1, total_loss = 220.0, pg_loss = 149.75, baseline_loss = 75.178, entropy_loss = -4.9281, learner_queue_size = 32, train_seconds = 1.2222e+04, _tick = 4331, _time = 1.6546e+09)
[2022-06-07 13:47:34,326][root][INFO] - Step 21949440 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 21949440, mean_episode_return = None, mean_episode_step = 1834.8, total_loss = -122.88, pg_loss = -122.22, baseline_loss = 4.3473, entropy_loss = -5.0142, learner_queue_size = 32, train_seconds = 1.2227e+04, _tick = 4331, _time = 1.6546e+09)
[2022-06-07 13:47:39,332][root][INFO] - Step 21959680 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 21959680, mean_episode_return = None, mean_episode_step = 1982.8, total_loss = 229.2, pg_loss = 166.45, baseline_loss = 67.787, entropy_loss = -5.0402, learner_queue_size = 32, train_seconds = 1.2232e+04, _tick = 4332, _time = 1.6546e+09)
[2022-06-07 13:47:44,334][root][INFO] - Step 21967360 @ 1535.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 21967360, mean_episode_return = None, mean_episode_step = 1958.4, total_loss = 224.08, pg_loss = 113.27, baseline_loss = 115.83, entropy_loss = -5.0187, learner_queue_size = 32, train_seconds = 1.2237e+04, _tick = 4333, _time = 1.6546e+09)
[2022-06-07 13:47:49,340][root][INFO] - Step 21975040 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 21975040, mean_episode_return = None, mean_episode_step = 1657.4, total_loss = 259.29, pg_loss = 164.58, baseline_loss = 99.69, entropy_loss = -4.9811, learner_queue_size = 32, train_seconds = 1.2242e+04, _tick = 4335, _time = 1.6546e+09)
[2022-06-07 13:47:54,346][root][INFO] - Step 21985280 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 21985280, mean_episode_return = 68.959, mean_episode_step = 2068.9, total_loss = 158.81, pg_loss = 73.503, baseline_loss = 90.347, entropy_loss = -5.0415, learner_queue_size = 32, train_seconds = 1.2247e+04, _tick = 4337, _time = 1.6546e+09)
[2022-06-07 13:47:59,352][root][INFO] - Step 21992960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 21992960, mean_episode_return = 46.73, mean_episode_step = 2100.3, total_loss = -192.73, pg_loss = -202.28, baseline_loss = 14.484, entropy_loss = -4.9302, learner_queue_size = 32, train_seconds = 1.2252e+04, _tick = 4340, _time = 1.6546e+09)
[2022-06-07 13:48:04,358][root][INFO] - Step 22003200 @ 2045.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 22003200, mean_episode_return = 22.1, mean_episode_step = 1977.0, total_loss = 12.308, pg_loss = -26.605, baseline_loss = 43.904, entropy_loss = -4.9908, learner_queue_size = 32, train_seconds = 1.2257e+04, _tick = 4344, _time = 1.6546e+09)
[2022-06-07 13:48:09,364][root][INFO] - Step 22013440 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 22013440, mean_episode_return = 128.38, mean_episode_step = 2226.7, total_loss = -64.867, pg_loss = -83.612, baseline_loss = 23.573, entropy_loss = -4.8283, learner_queue_size = 32, train_seconds = 1.2262e+04, _tick = 4346, _time = 1.6546e+09)
[2022-06-07 13:48:14,370][root][INFO] - Step 22021120 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 22021120, mean_episode_return = None, mean_episode_step = 1745.2, total_loss = 74.957, pg_loss = 44.722, baseline_loss = 35.067, entropy_loss = -4.8317, learner_queue_size = 32, train_seconds = 1.2267e+04, _tick = 4348, _time = 1.6546e+09)
[2022-06-07 13:48:19,376][root][INFO] - Step 22031360 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 22031360, mean_episode_return = -3.2205, mean_episode_step = 1864.3, total_loss = -270.27, pg_loss = -282.03, baseline_loss = 16.748, entropy_loss = -4.9831, learner_queue_size = 32, train_seconds = 1.2272e+04, _tick = 4352, _time = 1.6546e+09)
[2022-06-07 13:48:24,381][root][INFO] - Step 22039040 @ 1534.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22039040, mean_episode_return = None, mean_episode_step = 1966.8, total_loss = 111.23, pg_loss = 52.239, baseline_loss = 63.936, entropy_loss = -4.9398, learner_queue_size = 32, train_seconds = 1.2277e+04, _tick = 4354, _time = 1.6546e+09)
[2022-06-07 13:48:29,386][root][INFO] - Step 22049280 @ 2045.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 22049280, mean_episode_return = 80.16, mean_episode_step = 1603.7, total_loss = 2134.7, pg_loss = 787.72, baseline_loss = 1351.9, entropy_loss = -4.9401, learner_queue_size = 32, train_seconds = 1.2282e+04, _tick = 4357, _time = 1.6546e+09)
[2022-06-07 13:48:34,392][root][INFO] - Step 22056960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22056960, mean_episode_return = 110.66, mean_episode_step = 1932.4, total_loss = 90.393, pg_loss = 32.875, baseline_loss = 62.378, entropy_loss = -4.861, learner_queue_size = 32, train_seconds = 1.2287e+04, _tick = 4359, _time = 1.6546e+09)
[2022-06-07 13:48:39,398][root][INFO] - Step 22067200 @ 2045.6 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 22067200, mean_episode_return = None, mean_episode_step = 1829.4, total_loss = 41.991, pg_loss = 16.632, baseline_loss = 30.287, entropy_loss = -4.9273, learner_queue_size = 32, train_seconds = 1.2292e+04, _tick = 4362, _time = 1.6546e+09)
[2022-06-07 13:48:44,400][root][INFO] - Step 22074880 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22074880, mean_episode_return = 15.61, mean_episode_step = 1599.1, total_loss = -44.94, pg_loss = -69.034, baseline_loss = 29.023, entropy_loss = -4.9289, learner_queue_size = 32, train_seconds = 1.2297e+04, _tick = 4364, _time = 1.6546e+09)
[2022-06-07 13:48:49,402][root][INFO] - Step 22085120 @ 2047.1 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 22085120, mean_episode_return = 50.672, mean_episode_step = 2265.1, total_loss = -80.183, pg_loss = -87.505, baseline_loss = 12.21, entropy_loss = -4.8884, learner_queue_size = 32, train_seconds = 1.2302e+04, _tick = 4368, _time = 1.6546e+09)
[2022-06-07 13:48:54,408][root][INFO] - Step 22092800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22092800, mean_episode_return = 47.191, mean_episode_step = 1962.5, total_loss = 109.13, pg_loss = 48.573, baseline_loss = 65.449, entropy_loss = -4.8879, learner_queue_size = 32, train_seconds = 1.2307e+04, _tick = 4370, _time = 1.6546e+09)
[2022-06-07 13:48:59,414][root][INFO] - Step 22103040 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 22103040, mean_episode_return = None, mean_episode_step = 1538.6, total_loss = 112.78, pg_loss = 58.714, baseline_loss = 58.928, entropy_loss = -4.8602, learner_queue_size = 32, train_seconds = 1.2312e+04, _tick = 4370, _time = 1.6546e+09)
[2022-06-07 13:49:04,418][root][INFO] - Step 22110720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22110720, mean_episode_return = None, mean_episode_step = 2252.8, total_loss = 286.71, pg_loss = 201.66, baseline_loss = 89.903, entropy_loss = -4.8512, learner_queue_size = 32, train_seconds = 1.2317e+04, _tick = 4371, _time = 1.6546e+09)
[2022-06-07 13:49:09,422][root][INFO] - Step 22120960 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 22120960, mean_episode_return = None, mean_episode_step = 2217.5, total_loss = -50.093, pg_loss = -72.047, baseline_loss = 26.812, entropy_loss = -4.8577, learner_queue_size = 32, train_seconds = 1.2322e+04, _tick = 4373, _time = 1.6546e+09)
[2022-06-07 13:49:14,426][root][INFO] - Step 22128640 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22128640, mean_episode_return = 92.814, mean_episode_step = 2024.3, total_loss = -31.839, pg_loss = -99.17, baseline_loss = 72.157, entropy_loss = -4.8264, learner_queue_size = 32, train_seconds = 1.2327e+04, _tick = 4374, _time = 1.6546e+09)
[2022-06-07 13:49:19,430][root][INFO] - Step 22138880 @ 2046.3 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 22138880, mean_episode_return = None, mean_episode_step = 2263.4, total_loss = 182.12, pg_loss = 126.04, baseline_loss = 60.821, entropy_loss = -4.7342, learner_queue_size = 32, train_seconds = 1.2332e+04, _tick = 4375, _time = 1.6546e+09)
[2022-06-07 13:49:24,436][root][INFO] - Step 22146560 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22146560, mean_episode_return = 84.911, mean_episode_step = 2033.3, total_loss = 115.0, pg_loss = 70.425, baseline_loss = 49.246, entropy_loss = -4.6713, learner_queue_size = 32, train_seconds = 1.2337e+04, _tick = 4376, _time = 1.6546e+09)
[2022-06-07 13:49:29,442][root][INFO] - Step 22156800 @ 2045.5 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 22156800, mean_episode_return = 33.512, mean_episode_step = 2400.6, total_loss = -202.67, pg_loss = -216.22, baseline_loss = 18.252, entropy_loss = -4.7027, learner_queue_size = 32, train_seconds = 1.2342e+04, _tick = 4377, _time = 1.6546e+09)
[2022-06-07 13:49:34,446][root][INFO] - Step 22164480 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 22164480, mean_episode_return = 39.961, mean_episode_step = 1674.4, total_loss = -27.263, pg_loss = -58.41, baseline_loss = 35.8, entropy_loss = -4.6534, learner_queue_size = 32, train_seconds = 1.2347e+04, _tick = 4378, _time = 1.6546e+09)
[2022-06-07 13:49:39,452][root][INFO] - Step 22174720 @ 2045.5 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 22174720, mean_episode_return = None, mean_episode_step = 1775.5, total_loss = 244.29, pg_loss = 169.69, baseline_loss = 79.349, entropy_loss = -4.7526, learner_queue_size = 32, train_seconds = 1.2352e+04, _tick = 4380, _time = 1.6546e+09)
[2022-06-07 13:49:44,458][root][INFO] - Step 22182400 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22182400, mean_episode_return = None, mean_episode_step = 2074.0, total_loss = -10.285, pg_loss = -31.985, baseline_loss = 26.483, entropy_loss = -4.7826, learner_queue_size = 32, train_seconds = 1.2358e+04, _tick = 4382, _time = 1.6546e+09)
[2022-06-07 13:49:49,463][root][INFO] - Step 22192640 @ 2046.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22192640, mean_episode_return = None, mean_episode_step = 2088.9, total_loss = 54.839, pg_loss = 25.214, baseline_loss = 34.465, entropy_loss = -4.8397, learner_queue_size = 32, train_seconds = 1.2362e+04, _tick = 4385, _time = 1.6546e+09)
[2022-06-07 13:49:54,469][root][INFO] - Step 22200320 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22200320, mean_episode_return = None, mean_episode_step = 2406.8, total_loss = -35.49, pg_loss = -47.358, baseline_loss = 16.645, entropy_loss = -4.7777, learner_queue_size = 32, train_seconds = 1.2368e+04, _tick = 4385, _time = 1.6546e+09)
[2022-06-07 13:49:59,475][root][INFO] - Step 22210560 @ 2045.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 22210560, mean_episode_return = None, mean_episode_step = 2035.9, total_loss = 66.844, pg_loss = 5.9374, baseline_loss = 65.699, entropy_loss = -4.7921, learner_queue_size = 32, train_seconds = 1.2372e+04, _tick = 4386, _time = 1.6546e+09)
[2022-06-07 13:50:04,481][root][INFO] - Step 22218240 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 22218240, mean_episode_return = None, mean_episode_step = 2061.6, total_loss = 9.9842, pg_loss = -37.043, baseline_loss = 51.663, entropy_loss = -4.6357, learner_queue_size = 32, train_seconds = 1.2378e+04, _tick = 4388, _time = 1.6546e+09)
[2022-06-07 13:50:09,487][root][INFO] - Step 22228480 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 22228480, mean_episode_return = 33.251, mean_episode_step = 1998.6, total_loss = 20.674, pg_loss = -10.458, baseline_loss = 35.695, entropy_loss = -4.5632, learner_queue_size = 32, train_seconds = 1.2382e+04, _tick = 4391, _time = 1.6546e+09)
[2022-06-07 13:50:14,494][root][INFO] - Step 22238720 @ 2045.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 22238720, mean_episode_return = 39.881, mean_episode_step = 2315.8, total_loss = 262.75, pg_loss = 141.15, baseline_loss = 126.21, entropy_loss = -4.6137, learner_queue_size = 32, train_seconds = 1.2388e+04, _tick = 4394, _time = 1.6546e+09)
[2022-06-07 13:50:19,498][root][INFO] - Step 22246400 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22246400, mean_episode_return = 58.014, mean_episode_step = 2249.6, total_loss = -41.4, pg_loss = -101.54, baseline_loss = 64.786, entropy_loss = -4.6425, learner_queue_size = 32, train_seconds = 1.2392e+04, _tick = 4397, _time = 1.6546e+09)
[2022-06-07 13:50:24,502][root][INFO] - Step 22256640 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 22256640, mean_episode_return = None, mean_episode_step = 1961.8, total_loss = 3.2672, pg_loss = -36.559, baseline_loss = 44.447, entropy_loss = -4.6214, learner_queue_size = 32, train_seconds = 1.2398e+04, _tick = 4398, _time = 1.6546e+09)
[2022-06-07 13:50:29,506][root][INFO] - Step 22264320 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 22264320, mean_episode_return = None, mean_episode_step = 1823.5, total_loss = -32.987, pg_loss = -51.262, baseline_loss = 22.832, entropy_loss = -4.5571, learner_queue_size = 32, train_seconds = 1.2402e+04, _tick = 4399, _time = 1.6546e+09)
[2022-06-07 13:50:34,510][root][INFO] - Step 22274560 @ 2046.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 22274560, mean_episode_return = None, mean_episode_step = 1652.7, total_loss = 108.22, pg_loss = 58.98, baseline_loss = 53.77, entropy_loss = -4.529, learner_queue_size = 32, train_seconds = 1.2408e+04, _tick = 4402, _time = 1.6546e+09)
[2022-06-07 13:50:39,514][root][INFO] - Step 22282240 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 22282240, mean_episode_return = None, mean_episode_step = 2683.8, total_loss = 395.02, pg_loss = 267.24, baseline_loss = 132.28, entropy_loss = -4.4986, learner_queue_size = 32, train_seconds = 1.2412e+04, _tick = 4403, _time = 1.6546e+09)
[2022-06-07 13:50:44,518][root][INFO] - Step 22292480 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 22292480, mean_episode_return = None, mean_episode_step = 1842.3, total_loss = 246.74, pg_loss = 180.63, baseline_loss = 70.66, entropy_loss = -4.5513, learner_queue_size = 32, train_seconds = 1.2418e+04, _tick = 4404, _time = 1.6546e+09)
[2022-06-07 13:50:49,522][root][INFO] - Step 22300160 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 22300160, mean_episode_return = 138.96, mean_episode_step = 2286.8, total_loss = -106.52, pg_loss = -114.66, baseline_loss = 12.721, entropy_loss = -4.5892, learner_queue_size = 32, train_seconds = 1.2422e+04, _tick = 4405, _time = 1.6546e+09)
[2022-06-07 13:50:54,526][root][INFO] - Step 22310400 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22310400, mean_episode_return = None, mean_episode_step = 1893.9, total_loss = 38.272, pg_loss = 5.7518, baseline_loss = 37.222, entropy_loss = -4.7019, learner_queue_size = 32, train_seconds = 1.2428e+04, _tick = 4405, _time = 1.6546e+09)
[2022-06-07 13:50:59,530][root][INFO] - Step 22318080 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 22318080, mean_episode_return = None, mean_episode_step = 2234.1, total_loss = 207.0, pg_loss = 122.42, baseline_loss = 89.289, entropy_loss = -4.7102, learner_queue_size = 32, train_seconds = 1.2432e+04, _tick = 4407, _time = 1.6546e+09)
[2022-06-07 13:51:04,536][root][INFO] - Step 22328320 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 22328320, mean_episode_return = 189.1, mean_episode_step = 2016.1, total_loss = 312.12, pg_loss = 216.41, baseline_loss = 100.34, entropy_loss = -4.6343, learner_queue_size = 32, train_seconds = 1.2438e+04, _tick = 4410, _time = 1.6546e+09)
[2022-06-07 13:51:09,542][root][INFO] - Step 22336000 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22336000, mean_episode_return = 66.12, mean_episode_step = 2095.6, total_loss = -113.02, pg_loss = -159.71, baseline_loss = 51.357, entropy_loss = -4.673, learner_queue_size = 32, train_seconds = 1.2442e+04, _tick = 4413, _time = 1.6546e+09)
[2022-06-07 13:51:14,548][root][INFO] - Step 22346240 @ 2045.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 22346240, mean_episode_return = 61.711, mean_episode_step = 1537.6, total_loss = 211.16, pg_loss = 134.24, baseline_loss = 81.435, entropy_loss = -4.5164, learner_queue_size = 32, train_seconds = 1.2448e+04, _tick = 4417, _time = 1.6546e+09)
[2022-06-07 13:51:19,554][root][INFO] - Step 22353920 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 22353920, mean_episode_return = None, mean_episode_step = 2409.6, total_loss = 13.008, pg_loss = -7.6512, baseline_loss = 25.164, entropy_loss = -4.5054, learner_queue_size = 32, train_seconds = 1.2452e+04, _tick = 4419, _time = 1.6546e+09)
[2022-06-07 13:51:24,558][root][INFO] - Step 22364160 @ 2046.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 22364160, mean_episode_return = None, mean_episode_step = 2117.2, total_loss = -34.1, pg_loss = -59.758, baseline_loss = 30.257, entropy_loss = -4.5989, learner_queue_size = 32, train_seconds = 1.2458e+04, _tick = 4420, _time = 1.6546e+09)
[2022-06-07 13:51:29,564][root][INFO] - Step 22371840 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 22371840, mean_episode_return = None, mean_episode_step = 2622.0, total_loss = -70.075, pg_loss = -70.629, baseline_loss = 5.1972, entropy_loss = -4.6425, learner_queue_size = 32, train_seconds = 1.2463e+04, _tick = 4422, _time = 1.6546e+09)
[2022-06-07 13:51:34,570][root][INFO] - Step 22382080 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 22382080, mean_episode_return = None, mean_episode_step = 2136.8, total_loss = 98.533, pg_loss = 51.473, baseline_loss = 51.763, entropy_loss = -4.7032, learner_queue_size = 32, train_seconds = 1.2468e+04, _tick = 4424, _time = 1.6546e+09)
[2022-06-07 13:51:39,574][root][INFO] - Step 22389760 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22389760, mean_episode_return = 62.581, mean_episode_step = 2107.8, total_loss = 12.465, pg_loss = -11.968, baseline_loss = 29.175, entropy_loss = -4.7428, learner_queue_size = 32, train_seconds = 1.2473e+04, _tick = 4425, _time = 1.6546e+09)
[2022-06-07 13:51:44,580][root][INFO] - Step 22400000 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 22400000, mean_episode_return = None, mean_episode_step = 1980.2, total_loss = 307.65, pg_loss = 233.71, baseline_loss = 78.692, entropy_loss = -4.7507, learner_queue_size = 32, train_seconds = 1.2478e+04, _tick = 4425, _time = 1.6546e+09)
[2022-06-07 13:51:49,586][root][INFO] - Step 22407680 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 22407680, mean_episode_return = 2.4296, mean_episode_step = 2356.5, total_loss = 73.736, pg_loss = 2.778, baseline_loss = 75.717, entropy_loss = -4.7583, learner_queue_size = 32, train_seconds = 1.2483e+04, _tick = 4428, _time = 1.6546e+09)
[2022-06-07 13:51:54,590][root][INFO] - Step 22417920 @ 2046.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 22417920, mean_episode_return = 90.922, mean_episode_step = 2184.7, total_loss = -103.99, pg_loss = -135.32, baseline_loss = 36.14, entropy_loss = -4.8106, learner_queue_size = 32, train_seconds = 1.2488e+04, _tick = 4431, _time = 1.6546e+09)
[2022-06-07 13:51:59,594][root][INFO] - Step 22425600 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 22425600, mean_episode_return = 19.69, mean_episode_step = 2496.7, total_loss = 282.02, pg_loss = 181.21, baseline_loss = 105.57, entropy_loss = -4.7676, learner_queue_size = 32, train_seconds = 1.2493e+04, _tick = 4434, _time = 1.6546e+09)
[2022-06-07 13:52:04,598][root][INFO] - Step 22435840 @ 2046.3 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 22435840, mean_episode_return = 50.893, mean_episode_step = 1925.0, total_loss = -42.117, pg_loss = -66.888, baseline_loss = 29.387, entropy_loss = -4.6158, learner_queue_size = 32, train_seconds = 1.2498e+04, _tick = 4437, _time = 1.6546e+09)
[2022-06-07 13:52:09,602][root][INFO] - Step 22443520 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 22443520, mean_episode_return = 23.891, mean_episode_step = 2183.6, total_loss = -66.593, pg_loss = -87.951, baseline_loss = 25.816, entropy_loss = -4.4581, learner_queue_size = 32, train_seconds = 1.2503e+04, _tick = 4439, _time = 1.6546e+09)
[2022-06-07 13:52:14,606][root][INFO] - Step 22453760 @ 2046.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 22453760, mean_episode_return = None, mean_episode_step = 2184.8, total_loss = 269.81, pg_loss = 181.43, baseline_loss = 93.037, entropy_loss = -4.657, learner_queue_size = 32, train_seconds = 1.2508e+04, _tick = 4442, _time = 1.6546e+09)
[2022-06-07 13:52:19,610][root][INFO] - Step 22464000 @ 2046.3 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 22464000, mean_episode_return = 30.8, mean_episode_step = 2086.6, total_loss = 1.624, pg_loss = -12.049, baseline_loss = 18.123, entropy_loss = -4.4501, learner_queue_size = 32, train_seconds = 1.2513e+04, _tick = 4445, _time = 1.6546e+09)
[2022-06-07 13:52:24,616][root][INFO] - Step 22471680 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22471680, mean_episode_return = 52.155, mean_episode_step = 1846.6, total_loss = -49.352, pg_loss = -81.369, baseline_loss = 36.53, entropy_loss = -4.5133, learner_queue_size = 32, train_seconds = 1.2518e+04, _tick = 4446, _time = 1.6546e+09)
[2022-06-07 13:52:29,622][root][INFO] - Step 22481920 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 22481920, mean_episode_return = 70.146, mean_episode_step = 1892.7, total_loss = 285.76, pg_loss = 213.23, baseline_loss = 76.966, entropy_loss = -4.4299, learner_queue_size = 32, train_seconds = 1.2523e+04, _tick = 4448, _time = 1.6546e+09)
[2022-06-07 13:52:34,626][root][INFO] - Step 22489600 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22489600, mean_episode_return = None, mean_episode_step = 1791.3, total_loss = -43.628, pg_loss = -67.252, baseline_loss = 28.065, entropy_loss = -4.4409, learner_queue_size = 32, train_seconds = 1.2528e+04, _tick = 4448, _time = 1.6546e+09)
[2022-06-07 13:52:39,632][root][INFO] - Step 22499840 @ 2045.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 22499840, mean_episode_return = 34.58, mean_episode_step = 1750.2, total_loss = -153.87, pg_loss = -157.29, baseline_loss = 7.8193, entropy_loss = -4.3971, learner_queue_size = 32, train_seconds = 1.2533e+04, _tick = 4450, _time = 1.6546e+09)
[2022-06-07 13:52:44,638][root][INFO] - Step 22507520 @ 1534.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 22507520, mean_episode_return = 95.616, mean_episode_step = 2440.0, total_loss = 98.886, pg_loss = 39.106, baseline_loss = 64.209, entropy_loss = -4.4287, learner_queue_size = 32, train_seconds = 1.2538e+04, _tick = 4452, _time = 1.6546e+09)
[2022-06-07 13:52:49,644][root][INFO] - Step 22517760 @ 2045.6 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 22517760, mean_episode_return = 69.741, mean_episode_step = 1795.6, total_loss = -161.61, pg_loss = -165.58, baseline_loss = 8.3196, entropy_loss = -4.3508, learner_queue_size = 32, train_seconds = 1.2543e+04, _tick = 4455, _time = 1.6546e+09)
[2022-06-07 13:52:54,650][root][INFO] - Step 22525440 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22525440, mean_episode_return = None, mean_episode_step = 2269.7, total_loss = 320.96, pg_loss = 243.23, baseline_loss = 82.131, entropy_loss = -4.4005, learner_queue_size = 32, train_seconds = 1.2548e+04, _tick = 4455, _time = 1.6546e+09)
[2022-06-07 13:52:59,654][root][INFO] - Step 22535680 @ 2046.3 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 22535680, mean_episode_return = 15.47, mean_episode_step = 2161.1, total_loss = 236.24, pg_loss = 142.47, baseline_loss = 98.162, entropy_loss = -4.3905, learner_queue_size = 32, train_seconds = 1.2553e+04, _tick = 4458, _time = 1.6546e+09)
[2022-06-07 13:53:04,658][root][INFO] - Step 22543360 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22543360, mean_episode_return = 88.049, mean_episode_step = 2057.1, total_loss = 363.96, pg_loss = 229.67, baseline_loss = 138.67, entropy_loss = -4.3807, learner_queue_size = 32, train_seconds = 1.2558e+04, _tick = 4460, _time = 1.6546e+09)
[2022-06-07 13:53:09,662][root][INFO] - Step 22553600 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 22553600, mean_episode_return = 146.81, mean_episode_step = 2527.1, total_loss = -86.749, pg_loss = -94.378, baseline_loss = 12.004, entropy_loss = -4.3745, learner_queue_size = 32, train_seconds = 1.2563e+04, _tick = 4462, _time = 1.6546e+09)
[2022-06-07 13:53:14,666][root][INFO] - Step 22563840 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 22563840, mean_episode_return = 21.738, mean_episode_step = 2222.3, total_loss = -42.505, pg_loss = -91.783, baseline_loss = 53.709, entropy_loss = -4.4318, learner_queue_size = 32, train_seconds = 1.2568e+04, _tick = 4464, _time = 1.6546e+09)
[2022-06-07 13:53:19,670][root][INFO] - Step 22571520 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 22571520, mean_episode_return = 8.9495, mean_episode_step = 2507.4, total_loss = 16.344, pg_loss = -32.382, baseline_loss = 53.242, entropy_loss = -4.5164, learner_queue_size = 32, train_seconds = 1.2573e+04, _tick = 4466, _time = 1.6546e+09)
[2022-06-07 13:53:24,674][root][INFO] - Step 22581760 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 22581760, mean_episode_return = None, mean_episode_step = 2378.0, total_loss = 18.864, pg_loss = -13.346, baseline_loss = 36.818, entropy_loss = -4.6084, learner_queue_size = 32, train_seconds = 1.2578e+04, _tick = 4468, _time = 1.6546e+09)
[2022-06-07 13:53:29,678][root][INFO] - Step 22589440 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22589440, mean_episode_return = 43.422, mean_episode_step = 1821.6, total_loss = 148.34, pg_loss = 49.521, baseline_loss = 103.32, entropy_loss = -4.5051, learner_queue_size = 32, train_seconds = 1.2583e+04, _tick = 4471, _time = 1.6546e+09)
[2022-06-07 13:53:34,682][root][INFO] - Step 22599680 @ 2046.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 22599680, mean_episode_return = 30.474, mean_episode_step = 2208.9, total_loss = 92.817, pg_loss = 25.454, baseline_loss = 71.767, entropy_loss = -4.4039, learner_queue_size = 32, train_seconds = 1.2588e+04, _tick = 4473, _time = 1.6546e+09)
[2022-06-07 13:53:39,686][root][INFO] - Step 22607360 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22607360, mean_episode_return = 16.11, mean_episode_step = 1901.6, total_loss = 10.418, pg_loss = -36.787, baseline_loss = 51.753, entropy_loss = -4.5482, learner_queue_size = 32, train_seconds = 1.2593e+04, _tick = 4474, _time = 1.6546e+09)
[2022-06-07 13:53:44,690][root][INFO] - Step 22617600 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22617600, mean_episode_return = 149.6, mean_episode_step = 2142.2, total_loss = -100.66, pg_loss = -105.55, baseline_loss = 9.4336, entropy_loss = -4.545, learner_queue_size = 32, train_seconds = 1.2598e+04, _tick = 4476, _time = 1.6546e+09)
[2022-06-07 13:53:49,694][root][INFO] - Step 22625280 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22625280, mean_episode_return = None, mean_episode_step = 2370.7, total_loss = -112.53, pg_loss = -110.57, baseline_loss = 2.6058, entropy_loss = -4.5717, learner_queue_size = 32, train_seconds = 1.2603e+04, _tick = 4478, _time = 1.6546e+09)
[2022-06-07 13:53:54,698][root][INFO] - Step 22635520 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 22635520, mean_episode_return = 111.73, mean_episode_step = 2160.0, total_loss = 113.08, pg_loss = 57.373, baseline_loss = 60.32, entropy_loss = -4.6118, learner_queue_size = 32, train_seconds = 1.2608e+04, _tick = 4479, _time = 1.6546e+09)
[2022-06-07 13:53:59,702][root][INFO] - Step 22643200 @ 1534.7 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 22643200, mean_episode_return = 12.14, mean_episode_step = 2000.3, total_loss = 33.159, pg_loss = -30.306, baseline_loss = 68.064, entropy_loss = -4.5991, learner_queue_size = 32, train_seconds = 1.2613e+04, _tick = 4481, _time = 1.6546e+09)
[2022-06-07 13:54:04,706][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 13:54:04,898][root][INFO] - Step 22653440 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 22653440, mean_episode_return = None, mean_episode_step = 1517.8, total_loss = 126.51, pg_loss = 79.266, baseline_loss = 51.901, entropy_loss = -4.6582, learner_queue_size = 32, train_seconds = 1.2618e+04, _tick = 4483, _time = 1.6546e+09)
[2022-06-07 13:54:09,902][root][INFO] - Step 22661120 @ 1478.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 22661120, mean_episode_return = None, mean_episode_step = 1778.5, total_loss = -22.153, pg_loss = -30.274, baseline_loss = 12.787, entropy_loss = -4.6671, learner_queue_size = 32, train_seconds = 1.2623e+04, _tick = 4484, _time = 1.6546e+09)
[2022-06-07 13:54:14,908][root][INFO] - Step 22671360 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22671360, mean_episode_return = 18.99, mean_episode_step = 2545.1, total_loss = -183.66, pg_loss = -189.42, baseline_loss = 10.462, entropy_loss = -4.6978, learner_queue_size = 32, train_seconds = 1.2628e+04, _tick = 4485, _time = 1.6546e+09)
[2022-06-07 13:54:19,910][root][INFO] - Step 22679040 @ 1535.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 22679040, mean_episode_return = 35.41, mean_episode_step = 2600.9, total_loss = 386.11, pg_loss = 220.2, baseline_loss = 170.67, entropy_loss = -4.7595, learner_queue_size = 32, train_seconds = 1.2633e+04, _tick = 4487, _time = 1.6546e+09)
[2022-06-07 13:54:24,916][root][INFO] - Step 22689280 @ 2045.6 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 22689280, mean_episode_return = 100.84, mean_episode_step = 1785.5, total_loss = 78.053, pg_loss = 3.275, baseline_loss = 79.672, entropy_loss = -4.8942, learner_queue_size = 32, train_seconds = 1.2638e+04, _tick = 4491, _time = 1.6546e+09)
[2022-06-07 13:54:29,918][root][INFO] - Step 22696960 @ 1535.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 22696960, mean_episode_return = 102.0, mean_episode_step = 2337.3, total_loss = 370.31, pg_loss = 211.05, baseline_loss = 164.12, entropy_loss = -4.8639, learner_queue_size = 32, train_seconds = 1.2643e+04, _tick = 4494, _time = 1.6546e+09)
[2022-06-07 13:54:34,922][root][INFO] - Step 22707200 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 22707200, mean_episode_return = None, mean_episode_step = 1965.9, total_loss = 94.384, pg_loss = 41.748, baseline_loss = 57.517, entropy_loss = -4.8802, learner_queue_size = 32, train_seconds = 1.2648e+04, _tick = 4495, _time = 1.6546e+09)
[2022-06-07 13:54:39,926][root][INFO] - Step 22714880 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 22714880, mean_episode_return = 23.366, mean_episode_step = 1966.4, total_loss = -131.31, pg_loss = -140.03, baseline_loss = 13.685, entropy_loss = -4.9662, learner_queue_size = 32, train_seconds = 1.2653e+04, _tick = 4496, _time = 1.6546e+09)
[2022-06-07 13:54:44,930][root][INFO] - Step 22725120 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22725120, mean_episode_return = None, mean_episode_step = 1913.6, total_loss = 202.69, pg_loss = 150.8, baseline_loss = 56.912, entropy_loss = -5.0178, learner_queue_size = 32, train_seconds = 1.2658e+04, _tick = 4499, _time = 1.6546e+09)
[2022-06-07 13:54:49,936][root][INFO] - Step 22732800 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22732800, mean_episode_return = None, mean_episode_step = 2638.1, total_loss = -99.489, pg_loss = -97.32, baseline_loss = 2.8969, entropy_loss = -5.0662, learner_queue_size = 32, train_seconds = 1.2663e+04, _tick = 4499, _time = 1.6546e+09)
[2022-06-07 13:54:54,942][root][INFO] - Step 22743040 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 22743040, mean_episode_return = None, mean_episode_step = 1757.3, total_loss = 334.63, pg_loss = 236.92, baseline_loss = 102.67, entropy_loss = -4.9564, learner_queue_size = 32, train_seconds = 1.2668e+04, _tick = 4500, _time = 1.6546e+09)
[2022-06-07 13:54:59,946][root][INFO] - Step 22750720 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22750720, mean_episode_return = 97.918, mean_episode_step = 2666.7, total_loss = -54.704, pg_loss = -85.672, baseline_loss = 35.93, entropy_loss = -4.9621, learner_queue_size = 32, train_seconds = 1.2673e+04, _tick = 4502, _time = 1.6546e+09)
[2022-06-07 13:55:04,952][root][INFO] - Step 22760960 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22760960, mean_episode_return = 31.392, mean_episode_step = 1677.0, total_loss = 2.0418, pg_loss = -23.398, baseline_loss = 30.421, entropy_loss = -4.9816, learner_queue_size = 32, train_seconds = 1.2678e+04, _tick = 4505, _time = 1.6546e+09)
[2022-06-07 13:55:09,954][root][INFO] - Step 22771200 @ 2047.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 22771200, mean_episode_return = None, mean_episode_step = 1927.8, total_loss = 1.3693, pg_loss = -14.075, baseline_loss = 20.402, entropy_loss = -4.9572, learner_queue_size = 32, train_seconds = 1.2683e+04, _tick = 4507, _time = 1.6546e+09)
[2022-06-07 13:55:14,959][root][INFO] - Step 22778880 @ 1534.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22778880, mean_episode_return = 45.471, mean_episode_step = 1790.7, total_loss = 456.55, pg_loss = 306.28, baseline_loss = 155.18, entropy_loss = -4.9085, learner_queue_size = 32, train_seconds = 1.2688e+04, _tick = 4509, _time = 1.6546e+09)
[2022-06-07 13:55:19,965][root][INFO] - Step 22789120 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 22789120, mean_episode_return = 119.86, mean_episode_step = 1754.9, total_loss = 265.68, pg_loss = 169.84, baseline_loss = 100.74, entropy_loss = -4.9035, learner_queue_size = 32, train_seconds = 1.2693e+04, _tick = 4512, _time = 1.6546e+09)
[2022-06-07 13:55:24,970][root][INFO] - Step 22796800 @ 1534.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 22796800, mean_episode_return = 13.859, mean_episode_step = 1703.5, total_loss = -86.04, pg_loss = -97.001, baseline_loss = 15.862, entropy_loss = -4.9011, learner_queue_size = 32, train_seconds = 1.2698e+04, _tick = 4515, _time = 1.6546e+09)
[2022-06-07 13:55:29,976][root][INFO] - Step 22807040 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 22807040, mean_episode_return = 59.101, mean_episode_step = 1928.9, total_loss = 80.944, pg_loss = 25.669, baseline_loss = 60.142, entropy_loss = -4.8677, learner_queue_size = 32, train_seconds = 1.2703e+04, _tick = 4518, _time = 1.6546e+09)
[2022-06-07 13:55:34,982][root][INFO] - Step 22814720 @ 1534.2 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 22814720, mean_episode_return = 69.8, mean_episode_step = 1290.5, total_loss = 213.49, pg_loss = 139.94, baseline_loss = 78.324, entropy_loss = -4.7785, learner_queue_size = 32, train_seconds = 1.2708e+04, _tick = 4519, _time = 1.6546e+09)
[2022-06-07 13:55:39,988][root][INFO] - Step 22824960 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22824960, mean_episode_return = None, mean_episode_step = 2372.2, total_loss = -1.8272, pg_loss = -22.657, baseline_loss = 25.566, entropy_loss = -4.7359, learner_queue_size = 32, train_seconds = 1.2713e+04, _tick = 4520, _time = 1.6546e+09)
[2022-06-07 13:55:44,990][root][INFO] - Step 22832640 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 22832640, mean_episode_return = None, mean_episode_step = 2162.6, total_loss = 126.05, pg_loss = 80.261, baseline_loss = 50.64, entropy_loss = -4.849, learner_queue_size = 32, train_seconds = 1.2718e+04, _tick = 4520, _time = 1.6546e+09)
[2022-06-07 13:55:49,994][root][INFO] - Step 22842880 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 22842880, mean_episode_return = -28.25, mean_episode_step = 2104.4, total_loss = -66.974, pg_loss = -95.584, baseline_loss = 33.532, entropy_loss = -4.9219, learner_queue_size = 32, train_seconds = 1.2723e+04, _tick = 4524, _time = 1.6546e+09)
[2022-06-07 13:55:54,998][root][INFO] - Step 22850560 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 22850560, mean_episode_return = 95.299, mean_episode_step = 2240.7, total_loss = -190.13, pg_loss = -190.33, baseline_loss = 5.2133, entropy_loss = -5.009, learner_queue_size = 32, train_seconds = 1.2728e+04, _tick = 4525, _time = 1.6546e+09)
[2022-06-07 13:56:00,002][root][INFO] - Step 22860800 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 22860800, mean_episode_return = None, mean_episode_step = 2096.6, total_loss = 472.11, pg_loss = 339.67, baseline_loss = 137.46, entropy_loss = -5.0173, learner_queue_size = 32, train_seconds = 1.2733e+04, _tick = 4527, _time = 1.6546e+09)
[2022-06-07 13:56:05,006][root][INFO] - Step 22868480 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 22868480, mean_episode_return = -21.12, mean_episode_step = 2107.3, total_loss = -202.46, pg_loss = -203.29, baseline_loss = 5.8059, entropy_loss = -4.9829, learner_queue_size = 32, train_seconds = 1.2738e+04, _tick = 4530, _time = 1.6546e+09)
[2022-06-07 13:56:10,010][root][INFO] - Step 22878720 @ 2046.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 22878720, mean_episode_return = 38.773, mean_episode_step = 2326.0, total_loss = 327.15, pg_loss = 202.55, baseline_loss = 129.55, entropy_loss = -4.9465, learner_queue_size = 32, train_seconds = 1.2743e+04, _tick = 4533, _time = 1.6546e+09)
[2022-06-07 13:56:15,014][root][INFO] - Step 22886400 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22886400, mean_episode_return = 93.21, mean_episode_step = 2343.6, total_loss = -155.56, pg_loss = -204.07, baseline_loss = 53.503, entropy_loss = -4.9889, learner_queue_size = 32, train_seconds = 1.2748e+04, _tick = 4536, _time = 1.6546e+09)
[2022-06-07 13:56:20,018][root][INFO] - Step 22896640 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 22896640, mean_episode_return = 51.644, mean_episode_step = 2270.2, total_loss = -78.197, pg_loss = -100.52, baseline_loss = 27.206, entropy_loss = -4.88, learner_queue_size = 32, train_seconds = 1.2753e+04, _tick = 4539, _time = 1.6546e+09)
[2022-06-07 13:56:25,021][root][INFO] - Step 22904320 @ 1535.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 22904320, mean_episode_return = None, mean_episode_step = 2414.2, total_loss = -86.822, pg_loss = -87.576, baseline_loss = 5.6849, entropy_loss = -4.9311, learner_queue_size = 32, train_seconds = 1.2758e+04, _tick = 4540, _time = 1.6546e+09)
[2022-06-07 13:56:30,027][root][INFO] - Step 22914560 @ 2045.5 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 22914560, mean_episode_return = 99.131, mean_episode_step = 1984.9, total_loss = -16.587, pg_loss = -53.708, baseline_loss = 41.907, entropy_loss = -4.7856, learner_queue_size = 32, train_seconds = 1.2763e+04, _tick = 4543, _time = 1.6546e+09)
[2022-06-07 13:56:35,030][root][INFO] - Step 22922240 @ 1535.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 22922240, mean_episode_return = 104.96, mean_episode_step = 2630.5, total_loss = 52.819, pg_loss = -31.49, baseline_loss = 89.08, entropy_loss = -4.771, learner_queue_size = 32, train_seconds = 1.2768e+04, _tick = 4546, _time = 1.6546e+09)
[2022-06-07 13:56:40,036][root][INFO] - Step 22932480 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22932480, mean_episode_return = 65.139, mean_episode_step = 2427.1, total_loss = 0.21741, pg_loss = -25.268, baseline_loss = 30.327, entropy_loss = -4.8412, learner_queue_size = 32, train_seconds = 1.2773e+04, _tick = 4547, _time = 1.6546e+09)
[2022-06-07 13:56:45,042][root][INFO] - Step 22940160 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 22940160, mean_episode_return = None, mean_episode_step = 2153.0, total_loss = 290.32, pg_loss = 203.38, baseline_loss = 91.88, entropy_loss = -4.9378, learner_queue_size = 32, train_seconds = 1.2778e+04, _tick = 4548, _time = 1.6546e+09)
[2022-06-07 13:56:50,046][root][INFO] - Step 22950400 @ 2046.4 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 22950400, mean_episode_return = 73.23, mean_episode_step = 2599.0, total_loss = -173.84, pg_loss = -177.6, baseline_loss = 8.6597, entropy_loss = -4.8982, learner_queue_size = 32, train_seconds = 1.2783e+04, _tick = 4550, _time = 1.6546e+09)
[2022-06-07 13:56:55,050][root][INFO] - Step 22958080 @ 1534.7 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 22958080, mean_episode_return = 1.9166, mean_episode_step = 2068.4, total_loss = 2.7947, pg_loss = -57.072, baseline_loss = 64.807, entropy_loss = -4.9404, learner_queue_size = 32, train_seconds = 1.2788e+04, _tick = 4552, _time = 1.6546e+09)
[2022-06-07 13:57:00,054][root][INFO] - Step 22968320 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 22968320, mean_episode_return = None, mean_episode_step = 2730.9, total_loss = 224.01, pg_loss = 149.16, baseline_loss = 79.835, entropy_loss = -4.9778, learner_queue_size = 32, train_seconds = 1.2793e+04, _tick = 4554, _time = 1.6546e+09)
[2022-06-07 13:57:05,058][root][INFO] - Step 22976000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 22976000, mean_episode_return = 56.88, mean_episode_step = 1762.7, total_loss = -133.26, pg_loss = -135.96, baseline_loss = 7.6295, entropy_loss = -4.9341, learner_queue_size = 32, train_seconds = 1.2798e+04, _tick = 4557, _time = 1.6546e+09)
[2022-06-07 13:57:10,061][root][INFO] - Step 22986240 @ 2046.8 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 22986240, mean_episode_return = 102.36, mean_episode_step = 2213.7, total_loss = 34.784, pg_loss = 5.2319, baseline_loss = 34.553, entropy_loss = -5.0013, learner_queue_size = 32, train_seconds = 1.2803e+04, _tick = 4560, _time = 1.6546e+09)
[2022-06-07 13:57:15,066][root][INFO] - Step 22993920 @ 1534.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 22993920, mean_episode_return = 44.98, mean_episode_step = 2688.3, total_loss = -134.04, pg_loss = -187.07, baseline_loss = 58.027, entropy_loss = -4.9899, learner_queue_size = 32, train_seconds = 1.2808e+04, _tick = 4562, _time = 1.6546e+09)
[2022-06-07 13:57:20,070][root][INFO] - Step 23004160 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23004160, mean_episode_return = 22.29, mean_episode_step = 1988.0, total_loss = -190.13, pg_loss = -191.28, baseline_loss = 6.0702, entropy_loss = -4.9207, learner_queue_size = 32, train_seconds = 1.2813e+04, _tick = 4565, _time = 1.6546e+09)
[2022-06-07 13:57:25,074][root][INFO] - Step 23014400 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 23014400, mean_episode_return = None, mean_episode_step = 2627.3, total_loss = -97.57, pg_loss = -96.435, baseline_loss = 3.8113, entropy_loss = -4.9457, learner_queue_size = 32, train_seconds = 1.2818e+04, _tick = 4566, _time = 1.6546e+09)
[2022-06-07 13:57:30,078][root][INFO] - Step 23022080 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23022080, mean_episode_return = 16.156, mean_episode_step = 2308.2, total_loss = 200.32, pg_loss = 92.714, baseline_loss = 112.61, entropy_loss = -5.0097, learner_queue_size = 32, train_seconds = 1.2823e+04, _tick = 4568, _time = 1.6546e+09)
[2022-06-07 13:57:35,084][root][INFO] - Step 23032320 @ 2045.6 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 23032320, mean_episode_return = None, mean_episode_step = 2365.8, total_loss = 354.99, pg_loss = 234.85, baseline_loss = 125.06, entropy_loss = -4.919, learner_queue_size = 32, train_seconds = 1.2828e+04, _tick = 4570, _time = 1.6546e+09)
[2022-06-07 13:57:40,086][root][INFO] - Step 23040000 @ 1535.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 23040000, mean_episode_return = None, mean_episode_step = 2685.7, total_loss = 18.103, pg_loss = -6.1694, baseline_loss = 29.179, entropy_loss = -4.9059, learner_queue_size = 32, train_seconds = 1.2833e+04, _tick = 4571, _time = 1.6546e+09)
[2022-06-07 13:57:45,090][root][INFO] - Step 23047680 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 23047680, mean_episode_return = None, mean_episode_step = 2609.2, total_loss = 84.257, pg_loss = 49.525, baseline_loss = 39.609, entropy_loss = -4.8771, learner_queue_size = 32, train_seconds = 1.2838e+04, _tick = 4573, _time = 1.6546e+09)
[2022-06-07 13:57:50,096][root][INFO] - Step 23057920 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23057920, mean_episode_return = None, mean_episode_step = 2119.3, total_loss = 232.03, pg_loss = 171.3, baseline_loss = 65.786, entropy_loss = -5.0554, learner_queue_size = 32, train_seconds = 1.2843e+04, _tick = 4574, _time = 1.6546e+09)
[2022-06-07 13:57:55,102][root][INFO] - Step 23068160 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 23068160, mean_episode_return = 6.6696, mean_episode_step = 1896.3, total_loss = 81.92, pg_loss = 30.096, baseline_loss = 56.925, entropy_loss = -5.1015, learner_queue_size = 32, train_seconds = 1.2848e+04, _tick = 4576, _time = 1.6546e+09)
[2022-06-07 13:58:00,106][root][INFO] - Step 23075840 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23075840, mean_episode_return = 9.5598, mean_episode_step = 2323.5, total_loss = 89.529, pg_loss = 34.776, baseline_loss = 59.739, entropy_loss = -4.9865, learner_queue_size = 32, train_seconds = 1.2853e+04, _tick = 4578, _time = 1.6546e+09)
[2022-06-07 13:58:05,112][root][INFO] - Step 23086080 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23086080, mean_episode_return = 89.85, mean_episode_step = 1782.5, total_loss = -54.657, pg_loss = -73.512, baseline_loss = 23.912, entropy_loss = -5.0571, learner_queue_size = 32, train_seconds = 1.2858e+04, _tick = 4581, _time = 1.6546e+09)
[2022-06-07 13:58:10,118][root][INFO] - Step 23093760 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23093760, mean_episode_return = None, mean_episode_step = 2389.6, total_loss = 299.42, pg_loss = 191.57, baseline_loss = 112.91, entropy_loss = -5.0659, learner_queue_size = 32, train_seconds = 1.2863e+04, _tick = 4583, _time = 1.6546e+09)
[2022-06-07 13:58:15,122][root][INFO] - Step 23104000 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 23104000, mean_episode_return = 83.541, mean_episode_step = 2214.1, total_loss = 253.05, pg_loss = 96.578, baseline_loss = 161.6, entropy_loss = -5.13, learner_queue_size = 32, train_seconds = 1.2868e+04, _tick = 4587, _time = 1.6546e+09)
[2022-06-07 13:58:20,126][root][INFO] - Step 23111680 @ 1534.7 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 23111680, mean_episode_return = 32.631, mean_episode_step = 2147.6, total_loss = 363.3, pg_loss = 225.21, baseline_loss = 143.29, entropy_loss = -5.1932, learner_queue_size = 32, train_seconds = 1.2873e+04, _tick = 4588, _time = 1.6546e+09)
[2022-06-07 13:58:25,132][root][INFO] - Step 23121920 @ 2045.6 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 23121920, mean_episode_return = 30.63, mean_episode_step = 2095.8, total_loss = -20.055, pg_loss = -47.018, baseline_loss = 32.161, entropy_loss = -5.1974, learner_queue_size = 32, train_seconds = 1.2878e+04, _tick = 4591, _time = 1.6546e+09)
[2022-06-07 13:58:30,138][root][INFO] - Step 23129600 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23129600, mean_episode_return = 35.771, mean_episode_step = 2211.8, total_loss = 3.6576, pg_loss = -34.448, baseline_loss = 43.193, entropy_loss = -5.0877, learner_queue_size = 32, train_seconds = 1.2883e+04, _tick = 4593, _time = 1.6546e+09)
[2022-06-07 13:58:35,144][root][INFO] - Step 23139840 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 23139840, mean_episode_return = 60.245, mean_episode_step = 2317.6, total_loss = 188.94, pg_loss = 128.81, baseline_loss = 65.314, entropy_loss = -5.182, learner_queue_size = 32, train_seconds = 1.2888e+04, _tick = 4597, _time = 1.6546e+09)
[2022-06-07 13:58:40,146][root][INFO] - Step 23147520 @ 1535.3 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 23147520, mean_episode_return = None, mean_episode_step = 2448.4, total_loss = -49.923, pg_loss = -62.881, baseline_loss = 18.121, entropy_loss = -5.163, learner_queue_size = 32, train_seconds = 1.2893e+04, _tick = 4599, _time = 1.6546e+09)
[2022-06-07 13:58:45,150][root][INFO] - Step 23157760 @ 2046.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 23157760, mean_episode_return = 56.119, mean_episode_step = 2128.2, total_loss = -77.126, pg_loss = -105.19, baseline_loss = 33.243, entropy_loss = -5.1755, learner_queue_size = 32, train_seconds = 1.2898e+04, _tick = 4600, _time = 1.6546e+09)
[2022-06-07 13:58:50,154][root][INFO] - Step 23165440 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23165440, mean_episode_return = None, mean_episode_step = 2112.9, total_loss = 586.92, pg_loss = 440.38, baseline_loss = 151.73, entropy_loss = -5.1992, learner_queue_size = 32, train_seconds = 1.2903e+04, _tick = 4600, _time = 1.6546e+09)
[2022-06-07 13:58:55,160][root][INFO] - Step 23175680 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23175680, mean_episode_return = None, mean_episode_step = 2143.1, total_loss = 103.1, pg_loss = 68.941, baseline_loss = 39.378, entropy_loss = -5.215, learner_queue_size = 32, train_seconds = 1.2908e+04, _tick = 4601, _time = 1.6546e+09)
[2022-06-07 13:59:00,166][root][INFO] - Step 23183360 @ 1534.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 23183360, mean_episode_return = None, mean_episode_step = 2043.1, total_loss = -57.054, pg_loss = -68.982, baseline_loss = 17.057, entropy_loss = -5.1291, learner_queue_size = 32, train_seconds = 1.2913e+04, _tick = 4601, _time = 1.6546e+09)
[2022-06-07 13:59:05,172][root][INFO] - Step 23193600 @ 2045.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 23193600, mean_episode_return = 20.26, mean_episode_step = 2844.8, total_loss = 115.27, pg_loss = 67.479, baseline_loss = 52.879, entropy_loss = -5.0882, learner_queue_size = 32, train_seconds = 1.2918e+04, _tick = 4604, _time = 1.6546e+09)
[2022-06-07 13:59:10,178][root][INFO] - Step 23201280 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23201280, mean_episode_return = 23.759, mean_episode_step = 2145.5, total_loss = -105.03, pg_loss = -120.85, baseline_loss = 20.947, entropy_loss = -5.1211, learner_queue_size = 32, train_seconds = 1.2923e+04, _tick = 4607, _time = 1.6546e+09)
[2022-06-07 13:59:15,182][root][INFO] - Step 23211520 @ 2046.4 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 23211520, mean_episode_return = None, mean_episode_step = 2047.5, total_loss = 32.963, pg_loss = 6.1288, baseline_loss = 31.93, entropy_loss = -5.0955, learner_queue_size = 32, train_seconds = 1.2928e+04, _tick = 4608, _time = 1.6546e+09)
[2022-06-07 13:59:20,186][root][INFO] - Step 23219200 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23219200, mean_episode_return = -11.22, mean_episode_step = 2559.7, total_loss = 0.3684, pg_loss = -62.099, baseline_loss = 67.386, entropy_loss = -4.9193, learner_queue_size = 32, train_seconds = 1.2933e+04, _tick = 4611, _time = 1.6546e+09)
[2022-06-07 13:59:25,192][root][INFO] - Step 23229440 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 23229440, mean_episode_return = None, mean_episode_step = 2259.0, total_loss = 141.37, pg_loss = 83.23, baseline_loss = 62.907, entropy_loss = -4.7713, learner_queue_size = 32, train_seconds = 1.2938e+04, _tick = 4612, _time = 1.6546e+09)
[2022-06-07 13:59:30,198][root][INFO] - Step 23237120 @ 1534.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 23237120, mean_episode_return = 38.66, mean_episode_step = 2087.9, total_loss = -119.97, pg_loss = -158.1, baseline_loss = 42.978, entropy_loss = -4.8461, learner_queue_size = 32, train_seconds = 1.2943e+04, _tick = 4614, _time = 1.6546e+09)
[2022-06-07 13:59:35,202][root][INFO] - Step 23247360 @ 2046.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 23247360, mean_episode_return = None, mean_episode_step = 1979.1, total_loss = 114.4, pg_loss = 70.441, baseline_loss = 48.93, entropy_loss = -4.9687, learner_queue_size = 32, train_seconds = 1.2948e+04, _tick = 4614, _time = 1.6546e+09)
[2022-06-07 13:59:40,206][root][INFO] - Step 23255040 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 23255040, mean_episode_return = 49.061, mean_episode_step = 2282.8, total_loss = -95.309, pg_loss = -118.76, baseline_loss = 28.464, entropy_loss = -5.0146, learner_queue_size = 32, train_seconds = 1.2953e+04, _tick = 4616, _time = 1.6546e+09)
[2022-06-07 13:59:45,210][root][INFO] - Step 23265280 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 23265280, mean_episode_return = None, mean_episode_step = 2189.6, total_loss = 230.61, pg_loss = 158.29, baseline_loss = 77.334, entropy_loss = -5.0176, learner_queue_size = 32, train_seconds = 1.2958e+04, _tick = 4616, _time = 1.6546e+09)
[2022-06-07 13:59:50,214][root][INFO] - Step 23272960 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23272960, mean_episode_return = None, mean_episode_step = 2323.9, total_loss = -60.76, pg_loss = -78.014, baseline_loss = 22.339, entropy_loss = -5.0849, learner_queue_size = 32, train_seconds = 1.2963e+04, _tick = 4617, _time = 1.6546e+09)
[2022-06-07 13:59:55,218][root][INFO] - Step 23283200 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 23283200, mean_episode_return = 74.029, mean_episode_step = 2418.1, total_loss = -108.51, pg_loss = -123.47, baseline_loss = 20.08, entropy_loss = -5.1203, learner_queue_size = 32, train_seconds = 1.2968e+04, _tick = 4619, _time = 1.6546e+09)
[2022-06-07 14:00:00,222][root][INFO] - Step 23290880 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 23290880, mean_episode_return = 42.961, mean_episode_step = 2306.5, total_loss = 54.966, pg_loss = -22.384, baseline_loss = 82.437, entropy_loss = -5.0872, learner_queue_size = 32, train_seconds = 1.2973e+04, _tick = 4621, _time = 1.6546e+09)
[2022-06-07 14:00:05,226][root][INFO] - Step 23301120 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23301120, mean_episode_return = 45.517, mean_episode_step = 2413.1, total_loss = 209.51, pg_loss = 78.592, baseline_loss = 135.96, entropy_loss = -5.0444, learner_queue_size = 32, train_seconds = 1.2978e+04, _tick = 4624, _time = 1.6546e+09)
[2022-06-07 14:00:10,230][root][INFO] - Step 23308800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23308800, mean_episode_return = None, mean_episode_step = 2282.7, total_loss = 9.5302, pg_loss = -18.002, baseline_loss = 32.549, entropy_loss = -5.0167, learner_queue_size = 32, train_seconds = 1.2983e+04, _tick = 4626, _time = 1.6546e+09)
[2022-06-07 14:00:15,236][root][INFO] - Step 23319040 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 23319040, mean_episode_return = 121.0, mean_episode_step = 2776.4, total_loss = 122.58, pg_loss = 52.248, baseline_loss = 75.273, entropy_loss = -4.9421, learner_queue_size = 32, train_seconds = 1.2988e+04, _tick = 4628, _time = 1.6546e+09)
[2022-06-07 14:00:20,242][root][INFO] - Step 23326720 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23326720, mean_episode_return = None, mean_episode_step = 2420.5, total_loss = 84.926, pg_loss = 42.934, baseline_loss = 46.939, entropy_loss = -4.9474, learner_queue_size = 32, train_seconds = 1.2993e+04, _tick = 4629, _time = 1.6546e+09)
[2022-06-07 14:00:25,248][root][INFO] - Step 23336960 @ 2045.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 23336960, mean_episode_return = None, mean_episode_step = 2342.5, total_loss = 435.15, pg_loss = 272.27, baseline_loss = 167.84, entropy_loss = -4.9589, learner_queue_size = 32, train_seconds = 1.2998e+04, _tick = 4629, _time = 1.6546e+09)
[2022-06-07 14:00:30,251][root][INFO] - Step 23347200 @ 2046.7 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 23347200, mean_episode_return = -7.8105, mean_episode_step = 1982.7, total_loss = -83.253, pg_loss = -93.617, baseline_loss = 15.29, entropy_loss = -4.926, learner_queue_size = 32, train_seconds = 1.3003e+04, _tick = 4632, _time = 1.6546e+09)
[2022-06-07 14:00:35,257][root][INFO] - Step 23354880 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23354880, mean_episode_return = 3.8998, mean_episode_step = 2170.1, total_loss = 296.09, pg_loss = 205.98, baseline_loss = 94.983, entropy_loss = -4.8724, learner_queue_size = 32, train_seconds = 1.3008e+04, _tick = 4634, _time = 1.6546e+09)
[2022-06-07 14:00:40,263][root][INFO] - Step 23365120 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 23365120, mean_episode_return = None, mean_episode_step = 2721.0, total_loss = 27.552, pg_loss = -6.3779, baseline_loss = 38.7, entropy_loss = -4.7699, learner_queue_size = 32, train_seconds = 1.3013e+04, _tick = 4635, _time = 1.6546e+09)
[2022-06-07 14:00:45,269][root][INFO] - Step 23372800 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 23372800, mean_episode_return = None, mean_episode_step = 2003.5, total_loss = 87.919, pg_loss = 58.447, baseline_loss = 34.168, entropy_loss = -4.6955, learner_queue_size = 32, train_seconds = 1.3018e+04, _tick = 4636, _time = 1.6546e+09)
[2022-06-07 14:00:50,275][root][INFO] - Step 23383040 @ 2045.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23383040, mean_episode_return = None, mean_episode_step = 3176.4, total_loss = 2.3442, pg_loss = -12.526, baseline_loss = 19.644, entropy_loss = -4.7746, learner_queue_size = 32, train_seconds = 1.3023e+04, _tick = 4638, _time = 1.6546e+09)
[2022-06-07 14:00:55,278][root][INFO] - Step 23390720 @ 1535.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 23390720, mean_episode_return = None, mean_episode_step = 2866.9, total_loss = 42.701, pg_loss = 3.3423, baseline_loss = 44.252, entropy_loss = -4.8929, learner_queue_size = 32, train_seconds = 1.3028e+04, _tick = 4638, _time = 1.6546e+09)
[2022-06-07 14:01:00,284][root][INFO] - Step 23400960 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23400960, mean_episode_return = 17.354, mean_episode_step = 2499.6, total_loss = -28.004, pg_loss = -54.961, baseline_loss = 31.93, entropy_loss = -4.9725, learner_queue_size = 32, train_seconds = 1.3033e+04, _tick = 4640, _time = 1.6546e+09)
[2022-06-07 14:01:05,290][root][INFO] - Step 23408640 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23408640, mean_episode_return = 18.49, mean_episode_step = 2564.1, total_loss = 111.52, pg_loss = 68.849, baseline_loss = 47.647, entropy_loss = -4.9747, learner_queue_size = 32, train_seconds = 1.3038e+04, _tick = 4641, _time = 1.6546e+09)
[2022-06-07 14:01:10,296][root][INFO] - Step 23416320 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23416320, mean_episode_return = -2.79, mean_episode_step = 2603.0, total_loss = -174.33, pg_loss = -174.33, baseline_loss = 4.9447, entropy_loss = -4.9438, learner_queue_size = 32, train_seconds = 1.3043e+04, _tick = 4644, _time = 1.6546e+09)
[2022-06-07 14:01:15,302][root][INFO] - Step 23426560 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23426560, mean_episode_return = 64.626, mean_episode_step = 2115.9, total_loss = 4.9889, pg_loss = -38.176, baseline_loss = 48.146, entropy_loss = -4.9811, learner_queue_size = 32, train_seconds = 1.3048e+04, _tick = 4646, _time = 1.6546e+09)
[2022-06-07 14:01:20,306][root][INFO] - Step 23434240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23434240, mean_episode_return = None, mean_episode_step = 2599.5, total_loss = 545.98, pg_loss = 273.92, baseline_loss = 277.01, entropy_loss = -4.9486, learner_queue_size = 32, train_seconds = 1.3053e+04, _tick = 4646, _time = 1.6546e+09)
[2022-06-07 14:01:25,310][root][INFO] - Step 23444480 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 23444480, mean_episode_return = 68.08, mean_episode_step = 2449.1, total_loss = -174.59, pg_loss = -180.15, baseline_loss = 10.466, entropy_loss = -4.9071, learner_queue_size = 32, train_seconds = 1.3058e+04, _tick = 4648, _time = 1.6546e+09)
[2022-06-07 14:01:30,316][root][INFO] - Step 23452160 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23452160, mean_episode_return = None, mean_episode_step = 2401.7, total_loss = 206.12, pg_loss = 140.69, baseline_loss = 70.335, entropy_loss = -4.9121, learner_queue_size = 32, train_seconds = 1.3063e+04, _tick = 4648, _time = 1.6546e+09)
[2022-06-07 14:01:35,322][root][INFO] - Step 23462400 @ 2045.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 23462400, mean_episode_return = None, mean_episode_step = 2316.3, total_loss = -112.36, pg_loss = -119.26, baseline_loss = 11.742, entropy_loss = -4.8463, learner_queue_size = 32, train_seconds = 1.3068e+04, _tick = 4649, _time = 1.6546e+09)
[2022-06-07 14:01:40,328][root][INFO] - Step 23470080 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 23470080, mean_episode_return = -1.9204, mean_episode_step = 1834.0, total_loss = 263.98, pg_loss = 157.87, baseline_loss = 110.94, entropy_loss = -4.8285, learner_queue_size = 32, train_seconds = 1.3073e+04, _tick = 4652, _time = 1.6546e+09)
[2022-06-07 14:01:45,334][root][INFO] - Step 23480320 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23480320, mean_episode_return = None, mean_episode_step = 2948.9, total_loss = 511.88, pg_loss = 389.37, baseline_loss = 127.53, entropy_loss = -5.0197, learner_queue_size = 32, train_seconds = 1.3078e+04, _tick = 4654, _time = 1.6546e+09)
[2022-06-07 14:01:50,338][root][INFO] - Step 23488000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23488000, mean_episode_return = None, mean_episode_step = 2223.0, total_loss = -52.945, pg_loss = -63.306, baseline_loss = 15.43, entropy_loss = -5.0684, learner_queue_size = 32, train_seconds = 1.3083e+04, _tick = 4655, _time = 1.6546e+09)
[2022-06-07 14:01:55,344][root][INFO] - Step 23498240 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 23498240, mean_episode_return = 8.5397, mean_episode_step = 2570.3, total_loss = 62.273, pg_loss = 19.447, baseline_loss = 47.996, entropy_loss = -5.1704, learner_queue_size = 32, train_seconds = 1.3088e+04, _tick = 4658, _time = 1.6546e+09)
[2022-06-07 14:02:00,346][root][INFO] - Step 23505920 @ 1535.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 23505920, mean_episode_return = None, mean_episode_step = 2875.0, total_loss = -126.69, pg_loss = -124.39, baseline_loss = 2.8147, entropy_loss = -5.1156, learner_queue_size = 32, train_seconds = 1.3093e+04, _tick = 4658, _time = 1.6546e+09)
[2022-06-07 14:02:05,350][root][INFO] - Step 23516160 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 23516160, mean_episode_return = 74.288, mean_episode_step = 2201.4, total_loss = 602.15, pg_loss = 444.18, baseline_loss = 163.18, entropy_loss = -5.2083, learner_queue_size = 32, train_seconds = 1.3098e+04, _tick = 4659, _time = 1.6546e+09)
[2022-06-07 14:02:10,356][root][INFO] - Step 23523840 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23523840, mean_episode_return = 55.442, mean_episode_step = 2830.5, total_loss = 55.663, pg_loss = 9.7619, baseline_loss = 51.147, entropy_loss = -5.2462, learner_queue_size = 32, train_seconds = 1.3103e+04, _tick = 4661, _time = 1.6546e+09)
[2022-06-07 14:02:15,358][root][INFO] - Step 23534080 @ 2047.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23534080, mean_episode_return = 36.992, mean_episode_step = 2347.3, total_loss = 124.11, pg_loss = 70.669, baseline_loss = 58.684, entropy_loss = -5.2393, learner_queue_size = 32, train_seconds = 1.3108e+04, _tick = 4664, _time = 1.6546e+09)
[2022-06-07 14:02:20,362][root][INFO] - Step 23541760 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23541760, mean_episode_return = 54.271, mean_episode_step = 2603.1, total_loss = 177.2, pg_loss = 116.5, baseline_loss = 65.969, entropy_loss = -5.2668, learner_queue_size = 32, train_seconds = 1.3113e+04, _tick = 4666, _time = 1.6546e+09)
[2022-06-07 14:02:25,368][root][INFO] - Step 23552000 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 23552000, mean_episode_return = 37.003, mean_episode_step = 2814.1, total_loss = 133.6, pg_loss = 87.651, baseline_loss = 51.262, entropy_loss = -5.3132, learner_queue_size = 32, train_seconds = 1.3118e+04, _tick = 4669, _time = 1.6546e+09)
[2022-06-07 14:02:30,370][root][INFO] - Step 23559680 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23559680, mean_episode_return = 27.59, mean_episode_step = 2719.4, total_loss = -76.443, pg_loss = -106.89, baseline_loss = 35.787, entropy_loss = -5.3365, learner_queue_size = 32, train_seconds = 1.3123e+04, _tick = 4672, _time = 1.6546e+09)
[2022-06-07 14:02:35,376][root][INFO] - Step 23569920 @ 2045.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 23569920, mean_episode_return = 59.429, mean_episode_step = 2104.0, total_loss = 37.961, pg_loss = -2.2401, baseline_loss = 45.524, entropy_loss = -5.3229, learner_queue_size = 32, train_seconds = 1.3128e+04, _tick = 4675, _time = 1.6546e+09)
[2022-06-07 14:02:40,382][root][INFO] - Step 23577600 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 23577600, mean_episode_return = None, mean_episode_step = 1711.4, total_loss = 443.34, pg_loss = 338.84, baseline_loss = 109.86, entropy_loss = -5.3552, learner_queue_size = 32, train_seconds = 1.3133e+04, _tick = 4677, _time = 1.6546e+09)
[2022-06-07 14:02:45,386][root][INFO] - Step 23587840 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23587840, mean_episode_return = None, mean_episode_step = 2862.5, total_loss = 246.22, pg_loss = 188.5, baseline_loss = 63.021, entropy_loss = -5.3037, learner_queue_size = 32, train_seconds = 1.3138e+04, _tick = 4680, _time = 1.6546e+09)
[2022-06-07 14:02:50,390][root][INFO] - Step 23595520 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23595520, mean_episode_return = None, mean_episode_step = 2549.0, total_loss = 42.166, pg_loss = 9.975, baseline_loss = 37.482, entropy_loss = -5.2905, learner_queue_size = 32, train_seconds = 1.3143e+04, _tick = 4682, _time = 1.6546e+09)
[2022-06-07 14:02:55,394][root][INFO] - Step 23605760 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 23605760, mean_episode_return = 40.088, mean_episode_step = 2376.9, total_loss = 613.53, pg_loss = 405.77, baseline_loss = 213.08, entropy_loss = -5.3142, learner_queue_size = 32, train_seconds = 1.3148e+04, _tick = 4685, _time = 1.6546e+09)
[2022-06-07 14:03:00,398][root][INFO] - Step 23613440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23613440, mean_episode_return = 3.5319, mean_episode_step = 2266.6, total_loss = 529.26, pg_loss = 365.49, baseline_loss = 169.05, entropy_loss = -5.2887, learner_queue_size = 32, train_seconds = 1.3153e+04, _tick = 4687, _time = 1.6546e+09)
[2022-06-07 14:03:05,402][root][INFO] - Step 23623680 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 23623680, mean_episode_return = 92.44, mean_episode_step = 2426.7, total_loss = -63.829, pg_loss = -83.787, baseline_loss = 25.305, entropy_loss = -5.3469, learner_queue_size = 32, train_seconds = 1.3158e+04, _tick = 4691, _time = 1.6546e+09)
[2022-06-07 14:03:10,406][root][INFO] - Step 23631360 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23631360, mean_episode_return = 31.842, mean_episode_step = 1496.4, total_loss = 260.38, pg_loss = 172.4, baseline_loss = 93.333, entropy_loss = -5.3558, learner_queue_size = 32, train_seconds = 1.3163e+04, _tick = 4693, _time = 1.6546e+09)
[2022-06-07 14:03:15,412][root][INFO] - Step 23641600 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 23641600, mean_episode_return = None, mean_episode_step = 2210.4, total_loss = -38.871, pg_loss = -49.148, baseline_loss = 15.572, entropy_loss = -5.296, learner_queue_size = 32, train_seconds = 1.3168e+04, _tick = 4696, _time = 1.6546e+09)
[2022-06-07 14:03:20,418][root][INFO] - Step 23649280 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 23649280, mean_episode_return = None, mean_episode_step = 1951.4, total_loss = 58.968, pg_loss = 18.413, baseline_loss = 45.831, entropy_loss = -5.2759, learner_queue_size = 32, train_seconds = 1.3173e+04, _tick = 4698, _time = 1.6546e+09)
[2022-06-07 14:03:25,422][root][INFO] - Step 23659520 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23659520, mean_episode_return = 98.904, mean_episode_step = 2181.4, total_loss = 435.13, pg_loss = 325.8, baseline_loss = 114.55, entropy_loss = -5.2189, learner_queue_size = 32, train_seconds = 1.3178e+04, _tick = 4702, _time = 1.6546e+09)
[2022-06-07 14:03:30,426][root][INFO] - Step 23667200 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23667200, mean_episode_return = 67.252, mean_episode_step = 2187.1, total_loss = 501.64, pg_loss = 341.54, baseline_loss = 165.29, entropy_loss = -5.1891, learner_queue_size = 32, train_seconds = 1.3183e+04, _tick = 4704, _time = 1.6546e+09)
[2022-06-07 14:03:35,432][root][INFO] - Step 23674880 @ 1534.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23674880, mean_episode_return = None, mean_episode_step = 2313.3, total_loss = 66.531, pg_loss = 18.895, baseline_loss = 52.776, entropy_loss = -5.1407, learner_queue_size = 32, train_seconds = 1.3188e+04, _tick = 4706, _time = 1.6546e+09)
[2022-06-07 14:03:40,439][root][INFO] - Step 23685120 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 23685120, mean_episode_return = 26.361, mean_episode_step = 1880.6, total_loss = 869.43, pg_loss = 361.92, baseline_loss = 512.56, entropy_loss = -5.0457, learner_queue_size = 32, train_seconds = 1.3193e+04, _tick = 4708, _time = 1.6546e+09)
[2022-06-07 14:03:45,445][root][INFO] - Step 23692800 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 23692800, mean_episode_return = 115.22, mean_episode_step = 2600.4, total_loss = -124.16, pg_loss = -146.38, baseline_loss = 27.341, entropy_loss = -5.1171, learner_queue_size = 32, train_seconds = 1.3198e+04, _tick = 4710, _time = 1.6546e+09)
[2022-06-07 14:03:50,450][root][INFO] - Step 23703040 @ 2045.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 23703040, mean_episode_return = 85.032, mean_episode_step = 2190.1, total_loss = 126.13, pg_loss = 19.531, baseline_loss = 111.73, entropy_loss = -5.1353, learner_queue_size = 32, train_seconds = 1.3203e+04, _tick = 4713, _time = 1.6546e+09)
[2022-06-07 14:03:55,454][root][INFO] - Step 23710720 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 23710720, mean_episode_return = None, mean_episode_step = 2268.1, total_loss = 462.25, pg_loss = 310.6, baseline_loss = 156.78, entropy_loss = -5.1248, learner_queue_size = 32, train_seconds = 1.3208e+04, _tick = 4714, _time = 1.6546e+09)
[2022-06-07 14:04:00,458][root][INFO] - Step 23720960 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23720960, mean_episode_return = None, mean_episode_step = 1977.6, total_loss = 222.84, pg_loss = 154.15, baseline_loss = 73.789, entropy_loss = -5.1014, learner_queue_size = 32, train_seconds = 1.3214e+04, _tick = 4717, _time = 1.6546e+09)
[2022-06-07 14:04:05,462][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 14:04:05,673][root][INFO] - Step 23728640 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 23728640, mean_episode_return = None, mean_episode_step = 2177.8, total_loss = 159.11, pg_loss = 79.959, baseline_loss = 84.236, entropy_loss = -5.088, learner_queue_size = 32, train_seconds = 1.3218e+04, _tick = 4719, _time = 1.6546e+09)
[2022-06-07 14:04:10,678][root][INFO] - Step 23738880 @ 1963.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 23738880, mean_episode_return = 79.507, mean_episode_step = 1994.6, total_loss = -42.161, pg_loss = -127.72, baseline_loss = 90.586, entropy_loss = -5.0304, learner_queue_size = 32, train_seconds = 1.3224e+04, _tick = 4721, _time = 1.6546e+09)
[2022-06-07 14:04:15,684][root][INFO] - Step 23749120 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23749120, mean_episode_return = 34.041, mean_episode_step = 2718.3, total_loss = 102.59, pg_loss = 35.847, baseline_loss = 71.658, entropy_loss = -4.919, learner_queue_size = 32, train_seconds = 1.3229e+04, _tick = 4725, _time = 1.6546e+09)
[2022-06-07 14:04:20,690][root][INFO] - Step 23756800 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23756800, mean_episode_return = 34.03, mean_episode_step = 1696.1, total_loss = -18.118, pg_loss = -67.176, baseline_loss = 53.934, entropy_loss = -4.877, learner_queue_size = 32, train_seconds = 1.3234e+04, _tick = 4726, _time = 1.6546e+09)
[2022-06-07 14:04:25,694][root][INFO] - Step 23767040 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23767040, mean_episode_return = None, mean_episode_step = 2137.4, total_loss = 507.31, pg_loss = 366.37, baseline_loss = 145.81, entropy_loss = -4.8643, learner_queue_size = 32, train_seconds = 1.3239e+04, _tick = 4728, _time = 1.6546e+09)
[2022-06-07 14:04:30,706][root][INFO] - Step 23774720 @ 1532.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23774720, mean_episode_return = 40.253, mean_episode_step = 1579.4, total_loss = -322.53, pg_loss = -373.49, baseline_loss = 55.739, entropy_loss = -4.7766, learner_queue_size = 32, train_seconds = 1.3244e+04, _tick = 4730, _time = 1.6546e+09)
[2022-06-07 14:04:35,712][root][INFO] - Step 23782400 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 23782400, mean_episode_return = 31.209, mean_episode_step = 1893.6, total_loss = -43.498, pg_loss = -151.21, baseline_loss = 112.49, entropy_loss = -4.7816, learner_queue_size = 32, train_seconds = 1.3249e+04, _tick = 4733, _time = 1.6546e+09)
[2022-06-07 14:04:40,722][root][INFO] - Step 23792640 @ 2044.1 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 23792640, mean_episode_return = 29.443, mean_episode_step = 2210.0, total_loss = 139.28, pg_loss = 93.275, baseline_loss = 50.999, entropy_loss = -4.9943, learner_queue_size = 32, train_seconds = 1.3254e+04, _tick = 4734, _time = 1.6546e+09)
[2022-06-07 14:04:45,728][root][INFO] - Step 23802880 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23802880, mean_episode_return = None, mean_episode_step = 1619.8, total_loss = 120.56, pg_loss = 57.754, baseline_loss = 67.814, entropy_loss = -5.0061, learner_queue_size = 32, train_seconds = 1.3259e+04, _tick = 4735, _time = 1.6546e+09)
[2022-06-07 14:04:50,730][root][INFO] - Step 23810560 @ 1535.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 23810560, mean_episode_return = None, mean_episode_step = 1856.4, total_loss = 144.11, pg_loss = 77.678, baseline_loss = 71.466, entropy_loss = -5.0298, learner_queue_size = 32, train_seconds = 1.3264e+04, _tick = 4735, _time = 1.6546e+09)
[2022-06-07 14:04:55,734][root][INFO] - Step 23820800 @ 2046.4 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 23820800, mean_episode_return = None, mean_episode_step = 2240.3, total_loss = 515.24, pg_loss = 333.46, baseline_loss = 186.7, entropy_loss = -4.9148, learner_queue_size = 32, train_seconds = 1.3269e+04, _tick = 4738, _time = 1.6546e+09)
[2022-06-07 14:05:00,738][root][INFO] - Step 23828480 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 23828480, mean_episode_return = None, mean_episode_step = 2044.9, total_loss = 54.75, pg_loss = 10.773, baseline_loss = 48.975, entropy_loss = -4.9976, learner_queue_size = 32, train_seconds = 1.3274e+04, _tick = 4738, _time = 1.6546e+09)
[2022-06-07 14:05:05,742][root][INFO] - Step 23838720 @ 2046.4 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 23838720, mean_episode_return = None, mean_episode_step = 2224.7, total_loss = 34.815, pg_loss = 5.1077, baseline_loss = 34.812, entropy_loss = -5.1044, learner_queue_size = 32, train_seconds = 1.3279e+04, _tick = 4739, _time = 1.6546e+09)
[2022-06-07 14:05:10,748][root][INFO] - Step 23846400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23846400, mean_episode_return = 78.907, mean_episode_step = 1825.6, total_loss = 108.76, pg_loss = 56.565, baseline_loss = 57.267, entropy_loss = -5.0694, learner_queue_size = 32, train_seconds = 1.3284e+04, _tick = 4742, _time = 1.6546e+09)
[2022-06-07 14:05:15,754][root][INFO] - Step 23856640 @ 2045.6 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 23856640, mean_episode_return = -11.541, mean_episode_step = 2163.6, total_loss = -87.34, pg_loss = -102.47, baseline_loss = 20.136, entropy_loss = -5.0093, learner_queue_size = 32, train_seconds = 1.3289e+04, _tick = 4746, _time = 1.6546e+09)
[2022-06-07 14:05:20,758][root][INFO] - Step 23864320 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 23864320, mean_episode_return = 62.224, mean_episode_step = 1812.4, total_loss = -133.16, pg_loss = -187.13, baseline_loss = 58.903, entropy_loss = -4.9277, learner_queue_size = 32, train_seconds = 1.3294e+04, _tick = 4749, _time = 1.6546e+09)
[2022-06-07 14:05:25,762][root][INFO] - Step 23874560 @ 2046.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 23874560, mean_episode_return = 54.878, mean_episode_step = 1889.4, total_loss = -178.16, pg_loss = -208.46, baseline_loss = 35.197, entropy_loss = -4.8977, learner_queue_size = 32, train_seconds = 1.3299e+04, _tick = 4752, _time = 1.6546e+09)
[2022-06-07 14:05:30,768][root][INFO] - Step 23882240 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23882240, mean_episode_return = None, mean_episode_step = 1367.0, total_loss = 179.68, pg_loss = 122.06, baseline_loss = 62.506, entropy_loss = -4.8844, learner_queue_size = 32, train_seconds = 1.3304e+04, _tick = 4752, _time = 1.6546e+09)
[2022-06-07 14:05:35,774][root][INFO] - Step 23892480 @ 2045.6 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 23892480, mean_episode_return = None, mean_episode_step = 2056.1, total_loss = 214.75, pg_loss = 129.38, baseline_loss = 90.302, entropy_loss = -4.9233, learner_queue_size = 32, train_seconds = 1.3309e+04, _tick = 4754, _time = 1.6546e+09)
[2022-06-07 14:05:40,780][root][INFO] - Step 23900160 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 23900160, mean_episode_return = None, mean_episode_step = 2149.7, total_loss = 109.29, pg_loss = 62.36, baseline_loss = 51.867, entropy_loss = -4.9329, learner_queue_size = 32, train_seconds = 1.3314e+04, _tick = 4754, _time = 1.6546e+09)
[2022-06-07 14:05:45,787][root][INFO] - Step 23910400 @ 2045.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 23910400, mean_episode_return = 44.603, mean_episode_step = 1887.3, total_loss = 189.73, pg_loss = 98.503, baseline_loss = 96.304, entropy_loss = -5.0732, learner_queue_size = 32, train_seconds = 1.3319e+04, _tick = 4757, _time = 1.6546e+09)
[2022-06-07 14:05:50,793][root][INFO] - Step 23918080 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 23918080, mean_episode_return = None, mean_episode_step = 1537.2, total_loss = 59.647, pg_loss = 29.851, baseline_loss = 34.865, entropy_loss = -5.0693, learner_queue_size = 32, train_seconds = 1.3324e+04, _tick = 4757, _time = 1.6546e+09)
[2022-06-07 14:05:55,798][root][INFO] - Step 23928320 @ 2045.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 23928320, mean_episode_return = 86.007, mean_episode_step = 1829.4, total_loss = 106.12, pg_loss = 53.063, baseline_loss = 57.897, entropy_loss = -4.8373, learner_queue_size = 32, train_seconds = 1.3329e+04, _tick = 4760, _time = 1.6546e+09)
[2022-06-07 14:06:00,800][root][INFO] - Step 23936000 @ 1535.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23936000, mean_episode_return = 122.72, mean_episode_step = 2094.3, total_loss = 49.011, pg_loss = 13.24, baseline_loss = 40.55, entropy_loss = -4.778, learner_queue_size = 32, train_seconds = 1.3334e+04, _tick = 4761, _time = 1.6546e+09)
[2022-06-07 14:06:05,806][root][INFO] - Step 23946240 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 23946240, mean_episode_return = None, mean_episode_step = 1696.8, total_loss = -40.837, pg_loss = -69.46, baseline_loss = 33.456, entropy_loss = -4.8332, learner_queue_size = 32, train_seconds = 1.3339e+04, _tick = 4762, _time = 1.6546e+09)
[2022-06-07 14:06:10,810][root][INFO] - Step 23953920 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23953920, mean_episode_return = 5.2798, mean_episode_step = 2314.7, total_loss = -78.859, pg_loss = -115.2, baseline_loss = 41.219, entropy_loss = -4.8776, learner_queue_size = 32, train_seconds = 1.3344e+04, _tick = 4764, _time = 1.6546e+09)
[2022-06-07 14:06:15,816][root][INFO] - Step 23964160 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23964160, mean_episode_return = None, mean_episode_step = 2057.2, total_loss = 65.372, pg_loss = 25.524, baseline_loss = 44.738, entropy_loss = -4.8906, learner_queue_size = 32, train_seconds = 1.3349e+04, _tick = 4765, _time = 1.6546e+09)
[2022-06-07 14:06:20,822][root][INFO] - Step 23971840 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23971840, mean_episode_return = None, mean_episode_step = 1964.7, total_loss = 28.458, pg_loss = -13.667, baseline_loss = 47.055, entropy_loss = -4.93, learner_queue_size = 32, train_seconds = 1.3354e+04, _tick = 4767, _time = 1.6546e+09)
[2022-06-07 14:06:25,825][root][INFO] - Step 23979520 @ 1535.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 23979520, mean_episode_return = 32.56, mean_episode_step = 1919.5, total_loss = 390.11, pg_loss = 164.77, baseline_loss = 230.16, entropy_loss = -4.8281, learner_queue_size = 32, train_seconds = 1.3359e+04, _tick = 4768, _time = 1.6546e+09)
[2022-06-07 14:06:30,830][root][INFO] - Step 23989760 @ 2046.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 23989760, mean_episode_return = None, mean_episode_step = 2161.6, total_loss = 9.4078, pg_loss = -12.302, baseline_loss = 26.492, entropy_loss = -4.7824, learner_queue_size = 32, train_seconds = 1.3364e+04, _tick = 4770, _time = 1.6546e+09)
[2022-06-07 14:06:35,834][root][INFO] - Step 23997440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 23997440, mean_episode_return = None, mean_episode_step = 1372.4, total_loss = -137.1, pg_loss = -141.5, baseline_loss = 9.2363, entropy_loss = -4.837, learner_queue_size = 32, train_seconds = 1.3369e+04, _tick = 4772, _time = 1.6546e+09)
[2022-06-07 14:06:40,840][root][INFO] - Step 24007680 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 24007680, mean_episode_return = None, mean_episode_step = 2411.5, total_loss = 135.77, pg_loss = 62.965, baseline_loss = 77.666, entropy_loss = -4.862, learner_queue_size = 32, train_seconds = 1.3374e+04, _tick = 4772, _time = 1.6546e+09)
[2022-06-07 14:06:45,846][root][INFO] - Step 24015360 @ 1534.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 24015360, mean_episode_return = 52.671, mean_episode_step = 2026.4, total_loss = -133.88, pg_loss = -139.27, baseline_loss = 10.299, entropy_loss = -4.9091, learner_queue_size = 32, train_seconds = 1.3379e+04, _tick = 4774, _time = 1.6546e+09)
[2022-06-07 14:06:50,850][root][INFO] - Step 24025600 @ 2046.3 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 24025600, mean_episode_return = None, mean_episode_step = 2256.8, total_loss = -12.314, pg_loss = -23.653, baseline_loss = 16.229, entropy_loss = -4.8902, learner_queue_size = 32, train_seconds = 1.3384e+04, _tick = 4776, _time = 1.6546e+09)
[2022-06-07 14:06:55,853][root][INFO] - Step 24033280 @ 1535.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24033280, mean_episode_return = None, mean_episode_step = 2524.0, total_loss = 435.01, pg_loss = 325.71, baseline_loss = 114.25, entropy_loss = -4.9599, learner_queue_size = 32, train_seconds = 1.3389e+04, _tick = 4777, _time = 1.6546e+09)
[2022-06-07 14:07:00,858][root][INFO] - Step 24043520 @ 2046.0 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 24043520, mean_episode_return = None, mean_episode_step = 1893.9, total_loss = 37.469, pg_loss = 2.9098, baseline_loss = 39.606, entropy_loss = -5.0464, learner_queue_size = 32, train_seconds = 1.3394e+04, _tick = 4778, _time = 1.6546e+09)
[2022-06-07 14:07:05,862][root][INFO] - Step 24053760 @ 2046.4 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 24053760, mean_episode_return = -20.18, mean_episode_step = 1983.3, total_loss = -161.15, pg_loss = -161.39, baseline_loss = 5.3163, entropy_loss = -5.08, learner_queue_size = 32, train_seconds = 1.3399e+04, _tick = 4780, _time = 1.6546e+09)
[2022-06-07 14:07:10,866][root][INFO] - Step 24061440 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24061440, mean_episode_return = None, mean_episode_step = 2277.1, total_loss = -16.672, pg_loss = -29.682, baseline_loss = 18.15, entropy_loss = -5.1396, learner_queue_size = 32, train_seconds = 1.3404e+04, _tick = 4781, _time = 1.6546e+09)
[2022-06-07 14:07:15,872][root][INFO] - Step 24071680 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 24071680, mean_episode_return = -26.201, mean_episode_step = 2432.7, total_loss = 408.89, pg_loss = 293.37, baseline_loss = 120.88, entropy_loss = -5.3693, learner_queue_size = 32, train_seconds = 1.3409e+04, _tick = 4784, _time = 1.6546e+09)
[2022-06-07 14:07:20,878][root][INFO] - Step 24079360 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24079360, mean_episode_return = None, mean_episode_step = 1978.1, total_loss = -78.331, pg_loss = -89.321, baseline_loss = 16.324, entropy_loss = -5.3334, learner_queue_size = 32, train_seconds = 1.3414e+04, _tick = 4785, _time = 1.6546e+09)
[2022-06-07 14:07:25,882][root][INFO] - Step 24089600 @ 2046.4 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 24089600, mean_episode_return = 36.582, mean_episode_step = 2670.9, total_loss = -44.984, pg_loss = -69.697, baseline_loss = 30.059, entropy_loss = -5.3458, learner_queue_size = 32, train_seconds = 1.3419e+04, _tick = 4787, _time = 1.6546e+09)
[2022-06-07 14:07:30,886][root][INFO] - Step 24097280 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24097280, mean_episode_return = 12.93, mean_episode_step = 2148.2, total_loss = 181.03, pg_loss = 119.66, baseline_loss = 66.789, entropy_loss = -5.4275, learner_queue_size = 32, train_seconds = 1.3424e+04, _tick = 4789, _time = 1.6546e+09)
[2022-06-07 14:07:35,893][root][INFO] - Step 24104960 @ 1534.0 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24104960, mean_episode_return = 50.473, mean_episode_step = 2696.5, total_loss = -161.98, pg_loss = -161.12, baseline_loss = 4.5051, entropy_loss = -5.3718, learner_queue_size = 32, train_seconds = 1.3429e+04, _tick = 4791, _time = 1.6546e+09)
[2022-06-07 14:07:40,898][root][INFO] - Step 24115200 @ 2045.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 24115200, mean_episode_return = 58.18, mean_episode_step = 2541.1, total_loss = 69.672, pg_loss = 29.22, baseline_loss = 45.908, entropy_loss = -5.4566, learner_queue_size = 32, train_seconds = 1.3434e+04, _tick = 4795, _time = 1.6546e+09)
[2022-06-07 14:07:45,902][root][INFO] - Step 24122880 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 24122880, mean_episode_return = 70.265, mean_episode_step = 2106.3, total_loss = -37.01, pg_loss = -76.715, baseline_loss = 45.057, entropy_loss = -5.3517, learner_queue_size = 32, train_seconds = 1.3439e+04, _tick = 4798, _time = 1.6546e+09)
[2022-06-07 14:07:50,908][root][INFO] - Step 24133120 @ 2045.5 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 24133120, mean_episode_return = 44.073, mean_episode_step = 2801.9, total_loss = 8.9358, pg_loss = -23.127, baseline_loss = 37.528, entropy_loss = -5.465, learner_queue_size = 32, train_seconds = 1.3444e+04, _tick = 4802, _time = 1.6546e+09)
[2022-06-07 14:07:55,914][root][INFO] - Step 24140800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24140800, mean_episode_return = None, mean_episode_step = 1865.7, total_loss = 639.93, pg_loss = 445.54, baseline_loss = 199.83, entropy_loss = -5.4378, learner_queue_size = 32, train_seconds = 1.3449e+04, _tick = 4804, _time = 1.6546e+09)
[2022-06-07 14:08:00,918][root][INFO] - Step 24151040 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 24151040, mean_episode_return = None, mean_episode_step = 2128.0, total_loss = 14.461, pg_loss = -17.046, baseline_loss = 36.968, entropy_loss = -5.4616, learner_queue_size = 32, train_seconds = 1.3454e+04, _tick = 4804, _time = 1.6546e+09)
[2022-06-07 14:08:05,922][root][INFO] - Step 24158720 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24158720, mean_episode_return = None, mean_episode_step = 2315.8, total_loss = -15.368, pg_loss = -38.867, baseline_loss = 28.856, entropy_loss = -5.3568, learner_queue_size = 32, train_seconds = 1.3459e+04, _tick = 4806, _time = 1.6546e+09)
[2022-06-07 14:08:10,926][root][INFO] - Step 24168960 @ 2046.3 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 24168960, mean_episode_return = 121.6, mean_episode_step = 2146.7, total_loss = -0.68731, pg_loss = -18.017, baseline_loss = 22.763, entropy_loss = -5.433, learner_queue_size = 32, train_seconds = 1.3464e+04, _tick = 4809, _time = 1.6546e+09)
[2022-06-07 14:08:15,930][root][INFO] - Step 24176640 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24176640, mean_episode_return = None, mean_episode_step = 2241.4, total_loss = 388.65, pg_loss = 287.91, baseline_loss = 106.2, entropy_loss = -5.4547, learner_queue_size = 32, train_seconds = 1.3469e+04, _tick = 4810, _time = 1.6546e+09)
[2022-06-07 14:08:20,934][root][INFO] - Step 24186880 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 24186880, mean_episode_return = None, mean_episode_step = 2396.2, total_loss = 50.695, pg_loss = 27.648, baseline_loss = 28.572, entropy_loss = -5.5247, learner_queue_size = 32, train_seconds = 1.3474e+04, _tick = 4812, _time = 1.6546e+09)
[2022-06-07 14:08:25,940][root][INFO] - Step 24197120 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24197120, mean_episode_return = 10.28, mean_episode_step = 2369.9, total_loss = 155.64, pg_loss = 90.353, baseline_loss = 70.819, entropy_loss = -5.5297, learner_queue_size = 32, train_seconds = 1.3479e+04, _tick = 4815, _time = 1.6546e+09)
[2022-06-07 14:08:30,946][root][INFO] - Step 24204800 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24204800, mean_episode_return = 22.42, mean_episode_step = 1817.4, total_loss = -185.77, pg_loss = -191.09, baseline_loss = 10.795, entropy_loss = -5.4774, learner_queue_size = 32, train_seconds = 1.3484e+04, _tick = 4818, _time = 1.6546e+09)
[2022-06-07 14:08:35,952][root][INFO] - Step 24212480 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24212480, mean_episode_return = None, mean_episode_step = 2051.4, total_loss = 90.052, pg_loss = 62.382, baseline_loss = 33.09, entropy_loss = -5.4201, learner_queue_size = 32, train_seconds = 1.3489e+04, _tick = 4819, _time = 1.6546e+09)
[2022-06-07 14:08:40,954][root][INFO] - Step 24222720 @ 2047.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 24222720, mean_episode_return = 100.84, mean_episode_step = 2030.9, total_loss = 151.26, pg_loss = 98.422, baseline_loss = 58.251, entropy_loss = -5.4131, learner_queue_size = 32, train_seconds = 1.3494e+04, _tick = 4823, _time = 1.6546e+09)
[2022-06-07 14:08:45,960][root][INFO] - Step 24230400 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 24230400, mean_episode_return = 40.04, mean_episode_step = 1960.9, total_loss = 249.78, pg_loss = 158.31, baseline_loss = 96.936, entropy_loss = -5.469, learner_queue_size = 32, train_seconds = 1.3499e+04, _tick = 4826, _time = 1.6546e+09)
[2022-06-07 14:08:50,966][root][INFO] - Step 24240640 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 24240640, mean_episode_return = None, mean_episode_step = 1910.7, total_loss = 154.01, pg_loss = 69.125, baseline_loss = 90.393, entropy_loss = -5.5046, learner_queue_size = 32, train_seconds = 1.3504e+04, _tick = 4829, _time = 1.6546e+09)
[2022-06-07 14:08:55,970][root][INFO] - Step 24248320 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 24248320, mean_episode_return = 55.792, mean_episode_step = 2513.4, total_loss = 76.966, pg_loss = 28.071, baseline_loss = 54.361, entropy_loss = -5.4658, learner_queue_size = 32, train_seconds = 1.3509e+04, _tick = 4831, _time = 1.6546e+09)
[2022-06-07 14:09:00,974][root][INFO] - Step 24258560 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24258560, mean_episode_return = None, mean_episode_step = 1380.7, total_loss = 18.107, pg_loss = -8.0706, baseline_loss = 31.66, entropy_loss = -5.4819, learner_queue_size = 32, train_seconds = 1.3514e+04, _tick = 4834, _time = 1.6546e+09)
[2022-06-07 14:09:05,978][root][INFO] - Step 24268800 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24268800, mean_episode_return = 125.69, mean_episode_step = 2446.9, total_loss = -175.76, pg_loss = -180.48, baseline_loss = 10.209, entropy_loss = -5.4854, learner_queue_size = 32, train_seconds = 1.3519e+04, _tick = 4836, _time = 1.6546e+09)
[2022-06-07 14:09:10,982][root][INFO] - Step 24276480 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24276480, mean_episode_return = None, mean_episode_step = 1939.0, total_loss = 135.4, pg_loss = 75.595, baseline_loss = 65.26, entropy_loss = -5.4576, learner_queue_size = 32, train_seconds = 1.3524e+04, _tick = 4836, _time = 1.6546e+09)
[2022-06-07 14:09:15,988][root][INFO] - Step 24286720 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 24286720, mean_episode_return = None, mean_episode_step = 2806.8, total_loss = 41.553, pg_loss = -34.446, baseline_loss = 81.415, entropy_loss = -5.4163, learner_queue_size = 32, train_seconds = 1.3529e+04, _tick = 4839, _time = 1.6546e+09)
[2022-06-07 14:09:20,994][root][INFO] - Step 24294400 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24294400, mean_episode_return = None, mean_episode_step = 2048.4, total_loss = 31.161, pg_loss = 4.883, baseline_loss = 31.657, entropy_loss = -5.3789, learner_queue_size = 32, train_seconds = 1.3534e+04, _tick = 4839, _time = 1.6546e+09)
[2022-06-07 14:09:26,000][root][INFO] - Step 24304640 @ 2045.5 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 24304640, mean_episode_return = None, mean_episode_step = 1896.9, total_loss = 69.539, pg_loss = 35.395, baseline_loss = 39.407, entropy_loss = -5.2633, learner_queue_size = 32, train_seconds = 1.3539e+04, _tick = 4840, _time = 1.6546e+09)
[2022-06-07 14:09:31,006][root][INFO] - Step 24312320 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 24312320, mean_episode_return = None, mean_episode_step = 1764.2, total_loss = 59.452, pg_loss = 28.838, baseline_loss = 35.852, entropy_loss = -5.2383, learner_queue_size = 32, train_seconds = 1.3544e+04, _tick = 4841, _time = 1.6546e+09)
[2022-06-07 14:09:36,010][root][INFO] - Step 24322560 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 24322560, mean_episode_return = 30.305, mean_episode_step = 1988.6, total_loss = 458.92, pg_loss = 342.87, baseline_loss = 121.47, entropy_loss = -5.4224, learner_queue_size = 32, train_seconds = 1.3549e+04, _tick = 4845, _time = 1.6546e+09)
[2022-06-07 14:09:41,014][root][INFO] - Step 24330240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24330240, mean_episode_return = 35.342, mean_episode_step = 2299.4, total_loss = 177.74, pg_loss = 117.49, baseline_loss = 65.62, entropy_loss = -5.3639, learner_queue_size = 32, train_seconds = 1.3554e+04, _tick = 4847, _time = 1.6546e+09)
[2022-06-07 14:09:46,020][root][INFO] - Step 24340480 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 24340480, mean_episode_return = 71.485, mean_episode_step = 2261.7, total_loss = 124.46, pg_loss = 66.767, baseline_loss = 63.007, entropy_loss = -5.3138, learner_queue_size = 32, train_seconds = 1.3559e+04, _tick = 4851, _time = 1.6546e+09)
[2022-06-07 14:09:51,026][root][INFO] - Step 24348160 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24348160, mean_episode_return = 62.748, mean_episode_step = 1908.8, total_loss = 19.215, pg_loss = -25.054, baseline_loss = 49.478, entropy_loss = -5.2092, learner_queue_size = 32, train_seconds = 1.3564e+04, _tick = 4853, _time = 1.6546e+09)
[2022-06-07 14:09:56,030][root][INFO] - Step 24358400 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 24358400, mean_episode_return = None, mean_episode_step = 2063.8, total_loss = -129.81, pg_loss = -128.31, baseline_loss = 3.6741, entropy_loss = -5.1706, learner_queue_size = 32, train_seconds = 1.3569e+04, _tick = 4856, _time = 1.6546e+09)
[2022-06-07 14:10:01,036][root][INFO] - Step 24366080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24366080, mean_episode_return = None, mean_episode_step = 2476.4, total_loss = -31.898, pg_loss = -61.376, baseline_loss = 34.678, entropy_loss = -5.2004, learner_queue_size = 32, train_seconds = 1.3574e+04, _tick = 4858, _time = 1.6546e+09)
[2022-06-07 14:10:06,042][root][INFO] - Step 24376320 @ 2045.6 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 24376320, mean_episode_return = None, mean_episode_step = 1683.6, total_loss = 251.56, pg_loss = 164.06, baseline_loss = 92.6, entropy_loss = -5.0993, learner_queue_size = 32, train_seconds = 1.3579e+04, _tick = 4858, _time = 1.6546e+09)
[2022-06-07 14:10:11,046][root][INFO] - Step 24384000 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 24384000, mean_episode_return = None, mean_episode_step = 2023.5, total_loss = 470.17, pg_loss = 359.93, baseline_loss = 115.37, entropy_loss = -5.1392, learner_queue_size = 32, train_seconds = 1.3584e+04, _tick = 4859, _time = 1.6546e+09)
[2022-06-07 14:10:16,050][root][INFO] - Step 24394240 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24394240, mean_episode_return = None, mean_episode_step = 1949.5, total_loss = -157.91, pg_loss = -155.31, baseline_loss = 2.4764, entropy_loss = -5.0789, learner_queue_size = 32, train_seconds = 1.3589e+04, _tick = 4860, _time = 1.6546e+09)
[2022-06-07 14:10:21,054][root][INFO] - Step 24401920 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 24401920, mean_episode_return = None, mean_episode_step = 1885.2, total_loss = -13.648, pg_loss = -43.532, baseline_loss = 34.99, entropy_loss = -5.1052, learner_queue_size = 32, train_seconds = 1.3594e+04, _tick = 4861, _time = 1.6546e+09)
[2022-06-07 14:10:26,058][root][INFO] - Step 24412160 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24412160, mean_episode_return = 74.419, mean_episode_step = 1560.4, total_loss = -96.957, pg_loss = -99.184, baseline_loss = 7.298, entropy_loss = -5.0708, learner_queue_size = 32, train_seconds = 1.3599e+04, _tick = 4863, _time = 1.6546e+09)
[2022-06-07 14:10:31,062][root][INFO] - Step 24419840 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24419840, mean_episode_return = 69.598, mean_episode_step = 2408.6, total_loss = 3.6219, pg_loss = -24.411, baseline_loss = 33.126, entropy_loss = -5.0928, learner_queue_size = 32, train_seconds = 1.3604e+04, _tick = 4864, _time = 1.6546e+09)
[2022-06-07 14:10:36,068][root][INFO] - Step 24430080 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 24430080, mean_episode_return = -3.7809, mean_episode_step = 2068.4, total_loss = 287.23, pg_loss = 167.25, baseline_loss = 125.05, entropy_loss = -5.065, learner_queue_size = 32, train_seconds = 1.3609e+04, _tick = 4866, _time = 1.6546e+09)
[2022-06-07 14:10:41,074][root][INFO] - Step 24437760 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 24437760, mean_episode_return = None, mean_episode_step = 2413.4, total_loss = -137.7, pg_loss = -136.4, baseline_loss = 3.7593, entropy_loss = -5.0627, learner_queue_size = 32, train_seconds = 1.3614e+04, _tick = 4867, _time = 1.6546e+09)
[2022-06-07 14:10:46,078][root][INFO] - Step 24448000 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24448000, mean_episode_return = None, mean_episode_step = 2144.2, total_loss = -43.979, pg_loss = -57.804, baseline_loss = 18.958, entropy_loss = -5.133, learner_queue_size = 32, train_seconds = 1.3619e+04, _tick = 4869, _time = 1.6546e+09)
[2022-06-07 14:10:51,082][root][INFO] - Step 24455680 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 24455680, mean_episode_return = None, mean_episode_step = 2248.0, total_loss = 119.06, pg_loss = 53.531, baseline_loss = 70.731, entropy_loss = -5.2049, learner_queue_size = 32, train_seconds = 1.3624e+04, _tick = 4871, _time = 1.6546e+09)
[2022-06-07 14:10:56,086][root][INFO] - Step 24465920 @ 2046.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 24465920, mean_episode_return = None, mean_episode_step = 2150.3, total_loss = -145.56, pg_loss = -142.48, baseline_loss = 2.0014, entropy_loss = -5.0854, learner_queue_size = 32, train_seconds = 1.3629e+04, _tick = 4873, _time = 1.6546e+09)
[2022-06-07 14:11:01,090][root][INFO] - Step 24473600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24473600, mean_episode_return = -7.0503, mean_episode_step = 1941.5, total_loss = 39.264, pg_loss = 14.984, baseline_loss = 29.391, entropy_loss = -5.1111, learner_queue_size = 32, train_seconds = 1.3634e+04, _tick = 4875, _time = 1.6546e+09)
[2022-06-07 14:11:06,091][root][INFO] - Step 24483840 @ 2047.5 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 24483840, mean_episode_return = None, mean_episode_step = 1620.2, total_loss = 148.95, pg_loss = 92.138, baseline_loss = 61.963, entropy_loss = -5.1509, learner_queue_size = 32, train_seconds = 1.3639e+04, _tick = 4877, _time = 1.6546e+09)
[2022-06-07 14:11:11,097][root][INFO] - Step 24491520 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 24491520, mean_episode_return = 61.35, mean_episode_step = 2716.1, total_loss = 256.35, pg_loss = 85.349, baseline_loss = 176.15, entropy_loss = -5.1439, learner_queue_size = 32, train_seconds = 1.3644e+04, _tick = 4879, _time = 1.6546e+09)
[2022-06-07 14:11:16,102][root][INFO] - Step 24501760 @ 2046.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 24501760, mean_episode_return = None, mean_episode_step = 2043.5, total_loss = -71.146, pg_loss = -87.531, baseline_loss = 21.483, entropy_loss = -5.0977, learner_queue_size = 32, train_seconds = 1.3649e+04, _tick = 4880, _time = 1.6546e+09)
[2022-06-07 14:11:21,106][root][INFO] - Step 24509440 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 24509440, mean_episode_return = 114.09, mean_episode_step = 2012.6, total_loss = -143.05, pg_loss = -148.85, baseline_loss = 10.785, entropy_loss = -4.9889, learner_queue_size = 32, train_seconds = 1.3654e+04, _tick = 4882, _time = 1.6546e+09)
[2022-06-07 14:11:26,112][root][INFO] - Step 24519680 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24519680, mean_episode_return = None, mean_episode_step = 2027.4, total_loss = -94.162, pg_loss = -102.32, baseline_loss = 13.109, entropy_loss = -4.946, learner_queue_size = 32, train_seconds = 1.3659e+04, _tick = 4884, _time = 1.6546e+09)
[2022-06-07 14:11:31,118][root][INFO] - Step 24527360 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24527360, mean_episode_return = -6.4804, mean_episode_step = 2127.1, total_loss = -298.44, pg_loss = -314.18, baseline_loss = 20.759, entropy_loss = -5.0175, learner_queue_size = 32, train_seconds = 1.3664e+04, _tick = 4886, _time = 1.6546e+09)
[2022-06-07 14:11:36,125][root][INFO] - Step 24537600 @ 2045.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 24537600, mean_episode_return = None, mean_episode_step = 2355.0, total_loss = -0.46713, pg_loss = -23.843, baseline_loss = 28.426, entropy_loss = -5.0499, learner_queue_size = 32, train_seconds = 1.3669e+04, _tick = 4887, _time = 1.6546e+09)
[2022-06-07 14:11:41,131][root][INFO] - Step 24547840 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 24547840, mean_episode_return = 77.566, mean_episode_step = 2720.5, total_loss = -89.107, pg_loss = -102.4, baseline_loss = 18.432, entropy_loss = -5.1414, learner_queue_size = 32, train_seconds = 1.3674e+04, _tick = 4888, _time = 1.6546e+09)
[2022-06-07 14:11:46,134][root][INFO] - Step 24555520 @ 1535.0 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 24555520, mean_episode_return = None, mean_episode_step = 2132.8, total_loss = 50.326, pg_loss = 21.555, baseline_loss = 33.892, entropy_loss = -5.1215, learner_queue_size = 32, train_seconds = 1.3679e+04, _tick = 4888, _time = 1.6546e+09)
[2022-06-07 14:11:51,146][root][INFO] - Step 24565760 @ 2042.9 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 24565760, mean_episode_return = None, mean_episode_step = 1873.9, total_loss = -63.236, pg_loss = -63.552, baseline_loss = 5.402, entropy_loss = -5.0857, learner_queue_size = 32, train_seconds = 1.3684e+04, _tick = 4890, _time = 1.6546e+09)
[2022-06-07 14:11:56,153][root][INFO] - Step 24573440 @ 1534.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 24573440, mean_episode_return = 29.807, mean_episode_step = 2312.9, total_loss = 24.142, pg_loss = -29.887, baseline_loss = 59.171, entropy_loss = -5.1421, learner_queue_size = 32, train_seconds = 1.3689e+04, _tick = 4892, _time = 1.6546e+09)
[2022-06-07 14:12:01,158][root][INFO] - Step 24583680 @ 2045.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 24583680, mean_episode_return = 43.06, mean_episode_step = 1774.2, total_loss = 2.1572, pg_loss = -40.249, baseline_loss = 47.514, entropy_loss = -5.1079, learner_queue_size = 32, train_seconds = 1.3694e+04, _tick = 4894, _time = 1.6546e+09)
[2022-06-07 14:12:06,162][root][INFO] - Step 24591360 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24591360, mean_episode_return = None, mean_episode_step = 2006.9, total_loss = 81.673, pg_loss = 38.51, baseline_loss = 48.358, entropy_loss = -5.1956, learner_queue_size = 32, train_seconds = 1.3699e+04, _tick = 4894, _time = 1.6546e+09)
[2022-06-07 14:12:11,166][root][INFO] - Step 24599040 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24599040, mean_episode_return = 49.136, mean_episode_step = 2279.8, total_loss = -76.549, pg_loss = -112.27, baseline_loss = 40.967, entropy_loss = -5.2443, learner_queue_size = 32, train_seconds = 1.3704e+04, _tick = 4895, _time = 1.6546e+09)
[2022-06-07 14:12:16,170][root][INFO] - Step 24609280 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 24609280, mean_episode_return = None, mean_episode_step = 2093.9, total_loss = -46.176, pg_loss = -48.197, baseline_loss = 7.326, entropy_loss = -5.3058, learner_queue_size = 32, train_seconds = 1.3709e+04, _tick = 4897, _time = 1.6546e+09)
[2022-06-07 14:12:21,176][root][INFO] - Step 24616960 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 24616960, mean_episode_return = 26.519, mean_episode_step = 2252.6, total_loss = -69.335, pg_loss = -86.829, baseline_loss = 22.803, entropy_loss = -5.3091, learner_queue_size = 32, train_seconds = 1.3714e+04, _tick = 4898, _time = 1.6546e+09)
[2022-06-07 14:12:26,182][root][INFO] - Step 24627200 @ 2045.6 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 24627200, mean_episode_return = None, mean_episode_step = 2384.1, total_loss = 55.051, pg_loss = 36.859, baseline_loss = 23.56, entropy_loss = -5.3681, learner_queue_size = 32, train_seconds = 1.3719e+04, _tick = 4900, _time = 1.6546e+09)
[2022-06-07 14:12:31,191][root][INFO] - Step 24634880 @ 1533.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24634880, mean_episode_return = None, mean_episode_step = 1591.8, total_loss = -42.103, pg_loss = -52.714, baseline_loss = 15.951, entropy_loss = -5.3398, learner_queue_size = 32, train_seconds = 1.3724e+04, _tick = 4901, _time = 1.6546e+09)
[2022-06-07 14:12:36,194][root][INFO] - Step 24645120 @ 2046.7 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 24645120, mean_episode_return = 72.628, mean_episode_step = 2276.6, total_loss = -15.649, pg_loss = -41.731, baseline_loss = 31.435, entropy_loss = -5.3527, learner_queue_size = 32, train_seconds = 1.3729e+04, _tick = 4903, _time = 1.6546e+09)
[2022-06-07 14:12:41,198][root][INFO] - Step 24652800 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 24652800, mean_episode_return = 38.033, mean_episode_step = 2358.4, total_loss = 180.97, pg_loss = 116.33, baseline_loss = 69.919, entropy_loss = -5.2842, learner_queue_size = 32, train_seconds = 1.3734e+04, _tick = 4904, _time = 1.6546e+09)
[2022-06-07 14:12:46,202][root][INFO] - Step 24663040 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 24663040, mean_episode_return = -2.8005, mean_episode_step = 1976.4, total_loss = 217.94, pg_loss = 159.01, baseline_loss = 64.141, entropy_loss = -5.2071, learner_queue_size = 32, train_seconds = 1.3739e+04, _tick = 4906, _time = 1.6546e+09)
[2022-06-07 14:12:51,208][root][INFO] - Step 24670720 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24670720, mean_episode_return = 9.4694, mean_episode_step = 2202.9, total_loss = 163.35, pg_loss = 69.852, baseline_loss = 98.73, entropy_loss = -5.2329, learner_queue_size = 32, train_seconds = 1.3744e+04, _tick = 4908, _time = 1.6546e+09)
[2022-06-07 14:12:56,214][root][INFO] - Step 24680960 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24680960, mean_episode_return = None, mean_episode_step = 2110.0, total_loss = -161.46, pg_loss = -157.73, baseline_loss = 1.4975, entropy_loss = -5.2252, learner_queue_size = 32, train_seconds = 1.3749e+04, _tick = 4908, _time = 1.6546e+09)
[2022-06-07 14:13:01,218][root][INFO] - Step 24688640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24688640, mean_episode_return = None, mean_episode_step = 2234.8, total_loss = -56.537, pg_loss = -63.284, baseline_loss = 11.971, entropy_loss = -5.2233, learner_queue_size = 32, train_seconds = 1.3754e+04, _tick = 4908, _time = 1.6546e+09)
[2022-06-07 14:13:06,222][root][INFO] - Step 24698880 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 24698880, mean_episode_return = None, mean_episode_step = 2001.3, total_loss = 154.51, pg_loss = 113.49, baseline_loss = 46.22, entropy_loss = -5.1983, learner_queue_size = 32, train_seconds = 1.3759e+04, _tick = 4910, _time = 1.6546e+09)
[2022-06-07 14:13:11,226][root][INFO] - Step 24706560 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24706560, mean_episode_return = None, mean_episode_step = 2121.8, total_loss = 113.71, pg_loss = 48.801, baseline_loss = 70.138, entropy_loss = -5.2329, learner_queue_size = 32, train_seconds = 1.3764e+04, _tick = 4911, _time = 1.6546e+09)
[2022-06-07 14:13:16,230][root][INFO] - Step 24716800 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 24716800, mean_episode_return = None, mean_episode_step = 2156.9, total_loss = -134.08, pg_loss = -136.37, baseline_loss = 7.5042, entropy_loss = -5.2197, learner_queue_size = 32, train_seconds = 1.3769e+04, _tick = 4912, _time = 1.6546e+09)
[2022-06-07 14:13:21,236][root][INFO] - Step 24724480 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24724480, mean_episode_return = None, mean_episode_step = 2082.4, total_loss = 56.307, pg_loss = 29.214, baseline_loss = 32.324, entropy_loss = -5.2305, learner_queue_size = 32, train_seconds = 1.3774e+04, _tick = 4913, _time = 1.6546e+09)
[2022-06-07 14:13:26,242][root][INFO] - Step 24734720 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 24734720, mean_episode_return = 39.913, mean_episode_step = 2689.9, total_loss = 130.91, pg_loss = 88.55, baseline_loss = 47.594, entropy_loss = -5.2392, learner_queue_size = 32, train_seconds = 1.3779e+04, _tick = 4917, _time = 1.6546e+09)
[2022-06-07 14:13:31,246][root][INFO] - Step 24742400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24742400, mean_episode_return = None, mean_episode_step = 1864.2, total_loss = -4.3558, pg_loss = -24.065, baseline_loss = 24.988, entropy_loss = -5.2788, learner_queue_size = 32, train_seconds = 1.3784e+04, _tick = 4918, _time = 1.6546e+09)
[2022-06-07 14:13:36,250][root][INFO] - Step 24752640 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 24752640, mean_episode_return = None, mean_episode_step = 1835.4, total_loss = 169.64, pg_loss = 96.551, baseline_loss = 78.36, entropy_loss = -5.2744, learner_queue_size = 32, train_seconds = 1.3789e+04, _tick = 4919, _time = 1.6546e+09)
[2022-06-07 14:13:41,254][root][INFO] - Step 24760320 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 24760320, mean_episode_return = None, mean_episode_step = 2849.2, total_loss = -126.83, pg_loss = -124.5, baseline_loss = 2.9704, entropy_loss = -5.2994, learner_queue_size = 32, train_seconds = 1.3794e+04, _tick = 4920, _time = 1.6546e+09)
[2022-06-07 14:13:46,258][root][INFO] - Step 24770560 @ 2046.4 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 24770560, mean_episode_return = None, mean_episode_step = 2474.9, total_loss = -153.33, pg_loss = -151.55, baseline_loss = 3.5326, entropy_loss = -5.319, learner_queue_size = 32, train_seconds = 1.3799e+04, _tick = 4922, _time = 1.6546e+09)
[2022-06-07 14:13:51,264][root][INFO] - Step 24778240 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 24778240, mean_episode_return = 5.6304, mean_episode_step = 2323.5, total_loss = 487.55, pg_loss = 361.66, baseline_loss = 131.17, entropy_loss = -5.2821, learner_queue_size = 32, train_seconds = 1.3804e+04, _tick = 4924, _time = 1.6546e+09)
[2022-06-07 14:13:56,270][root][INFO] - Step 24788480 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 24788480, mean_episode_return = None, mean_episode_step = 2404.4, total_loss = -124.07, pg_loss = -121.97, baseline_loss = 3.1905, entropy_loss = -5.287, learner_queue_size = 32, train_seconds = 1.3809e+04, _tick = 4926, _time = 1.6546e+09)
[2022-06-07 14:14:01,274][root][INFO] - Step 24796160 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24796160, mean_episode_return = None, mean_episode_step = 3403.7, total_loss = -44.886, pg_loss = -60.861, baseline_loss = 21.246, entropy_loss = -5.2708, learner_queue_size = 32, train_seconds = 1.3814e+04, _tick = 4926, _time = 1.6546e+09)
[2022-06-07 14:14:06,278][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 14:14:06,495][root][INFO] - Step 24806400 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 24806400, mean_episode_return = None, mean_episode_step = 2083.2, total_loss = 87.202, pg_loss = 44.43, baseline_loss = 48.05, entropy_loss = -5.2779, learner_queue_size = 32, train_seconds = 1.3819e+04, _tick = 4928, _time = 1.6546e+09)
[2022-06-07 14:14:11,498][root][INFO] - Step 24814080 @ 1471.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24814080, mean_episode_return = None, mean_episode_step = 2298.0, total_loss = 70.064, pg_loss = 28.416, baseline_loss = 46.987, entropy_loss = -5.3387, learner_queue_size = 32, train_seconds = 1.3824e+04, _tick = 4929, _time = 1.6546e+09)
[2022-06-07 14:14:16,502][root][INFO] - Step 24824320 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24824320, mean_episode_return = 75.999, mean_episode_step = 2584.9, total_loss = 3.8695, pg_loss = -37.108, baseline_loss = 46.387, entropy_loss = -5.4094, learner_queue_size = 32, train_seconds = 1.383e+04, _tick = 4932, _time = 1.6546e+09)
[2022-06-07 14:14:21,508][root][INFO] - Step 24834560 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24834560, mean_episode_return = None, mean_episode_step = 2418.2, total_loss = 38.344, pg_loss = 9.2374, baseline_loss = 34.531, entropy_loss = -5.4245, learner_queue_size = 32, train_seconds = 1.3834e+04, _tick = 4933, _time = 1.6546e+09)
[2022-06-07 14:14:26,514][root][INFO] - Step 24842240 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 24842240, mean_episode_return = 4.7901, mean_episode_step = 1790.5, total_loss = -38.344, pg_loss = -79.964, baseline_loss = 47.053, entropy_loss = -5.4322, learner_queue_size = 32, train_seconds = 1.384e+04, _tick = 4935, _time = 1.6546e+09)
[2022-06-07 14:14:31,518][root][INFO] - Step 24852480 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 24852480, mean_episode_return = -2.6403, mean_episode_step = 2512.2, total_loss = -32.592, pg_loss = -66.783, baseline_loss = 39.638, entropy_loss = -5.4465, learner_queue_size = 32, train_seconds = 1.3844e+04, _tick = 4938, _time = 1.6546e+09)
[2022-06-07 14:14:36,524][root][INFO] - Step 24860160 @ 1534.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 24860160, mean_episode_return = 92.445, mean_episode_step = 2607.2, total_loss = 195.52, pg_loss = 109.37, baseline_loss = 91.541, entropy_loss = -5.3977, learner_queue_size = 32, train_seconds = 1.385e+04, _tick = 4941, _time = 1.6546e+09)
[2022-06-07 14:14:41,530][root][INFO] - Step 24870400 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24870400, mean_episode_return = 71.707, mean_episode_step = 2216.0, total_loss = -222.96, pg_loss = -233.3, baseline_loss = 15.672, entropy_loss = -5.3346, learner_queue_size = 32, train_seconds = 1.3854e+04, _tick = 4944, _time = 1.6546e+09)
[2022-06-07 14:14:46,534][root][INFO] - Step 24878080 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 24878080, mean_episode_return = 22.189, mean_episode_step = 2194.7, total_loss = 224.05, pg_loss = 160.5, baseline_loss = 68.898, entropy_loss = -5.353, learner_queue_size = 32, train_seconds = 1.386e+04, _tick = 4945, _time = 1.6546e+09)
[2022-06-07 14:14:51,539][root][INFO] - Step 24885760 @ 1534.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24885760, mean_episode_return = 65.229, mean_episode_step = 2388.9, total_loss = 318.63, pg_loss = 205.26, baseline_loss = 118.66, entropy_loss = -5.2891, learner_queue_size = 32, train_seconds = 1.3864e+04, _tick = 4946, _time = 1.6546e+09)
[2022-06-07 14:14:56,545][root][INFO] - Step 24896000 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24896000, mean_episode_return = None, mean_episode_step = 2806.8, total_loss = 82.155, pg_loss = 35.974, baseline_loss = 51.431, entropy_loss = -5.2494, learner_queue_size = 32, train_seconds = 1.387e+04, _tick = 4947, _time = 1.6546e+09)
[2022-06-07 14:15:01,556][root][INFO] - Step 24903680 @ 1532.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 24903680, mean_episode_return = None, mean_episode_step = 2372.6, total_loss = 165.5, pg_loss = 113.07, baseline_loss = 57.682, entropy_loss = -5.2536, learner_queue_size = 32, train_seconds = 1.3875e+04, _tick = 4949, _time = 1.6546e+09)
[2022-06-07 14:15:06,562][root][INFO] - Step 24913920 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24913920, mean_episode_return = None, mean_episode_step = 2779.4, total_loss = 177.08, pg_loss = 132.17, baseline_loss = 50.157, entropy_loss = -5.2436, learner_queue_size = 32, train_seconds = 1.388e+04, _tick = 4952, _time = 1.6546e+09)
[2022-06-07 14:15:11,568][root][INFO] - Step 24924160 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24924160, mean_episode_return = 86.233, mean_episode_step = 1820.3, total_loss = 148.61, pg_loss = 90.993, baseline_loss = 62.875, entropy_loss = -5.2607, learner_queue_size = 32, train_seconds = 1.3885e+04, _tick = 4956, _time = 1.6546e+09)
[2022-06-07 14:15:16,574][root][INFO] - Step 24931840 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 24931840, mean_episode_return = None, mean_episode_step = 2128.9, total_loss = 27.296, pg_loss = -0.38577, baseline_loss = 32.926, entropy_loss = -5.2449, learner_queue_size = 32, train_seconds = 1.389e+04, _tick = 4957, _time = 1.6546e+09)
[2022-06-07 14:15:21,580][root][INFO] - Step 24942080 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 24942080, mean_episode_return = 44.618, mean_episode_step = 1946.2, total_loss = 130.21, pg_loss = 28.868, baseline_loss = 106.6, entropy_loss = -5.2622, learner_queue_size = 32, train_seconds = 1.3895e+04, _tick = 4959, _time = 1.6546e+09)
[2022-06-07 14:15:26,586][root][INFO] - Step 24949760 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 24949760, mean_episode_return = None, mean_episode_step = 2390.5, total_loss = -32.392, pg_loss = -46.652, baseline_loss = 19.586, entropy_loss = -5.3265, learner_queue_size = 32, train_seconds = 1.39e+04, _tick = 4960, _time = 1.6546e+09)
[2022-06-07 14:15:31,592][root][INFO] - Step 24960000 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 24960000, mean_episode_return = 44.621, mean_episode_step = 2243.4, total_loss = -81.314, pg_loss = -102.98, baseline_loss = 26.99, entropy_loss = -5.3186, learner_queue_size = 32, train_seconds = 1.3905e+04, _tick = 4964, _time = 1.6546e+09)
[2022-06-07 14:15:36,598][root][INFO] - Step 24967680 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 24967680, mean_episode_return = 16.56, mean_episode_step = 2529.7, total_loss = 624.66, pg_loss = 367.56, baseline_loss = 262.42, entropy_loss = -5.3205, learner_queue_size = 32, train_seconds = 1.391e+04, _tick = 4966, _time = 1.6546e+09)
[2022-06-07 14:15:41,602][root][INFO] - Step 24977920 @ 2046.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 24977920, mean_episode_return = 44.393, mean_episode_step = 2043.6, total_loss = 214.07, pg_loss = 123.61, baseline_loss = 95.802, entropy_loss = -5.348, learner_queue_size = 32, train_seconds = 1.3915e+04, _tick = 4969, _time = 1.6546e+09)
[2022-06-07 14:15:46,608][root][INFO] - Step 24985600 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 24985600, mean_episode_return = 36.77, mean_episode_step = 2454.8, total_loss = -70.912, pg_loss = -100.4, baseline_loss = 34.819, entropy_loss = -5.3276, learner_queue_size = 32, train_seconds = 1.392e+04, _tick = 4972, _time = 1.6546e+09)
[2022-06-07 14:15:51,614][root][INFO] - Step 24995840 @ 2045.6 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 24995840, mean_episode_return = 7.1396, mean_episode_step = 2792.1, total_loss = -32.023, pg_loss = -62.326, baseline_loss = 35.638, entropy_loss = -5.3349, learner_queue_size = 32, train_seconds = 1.3925e+04, _tick = 4975, _time = 1.6546e+09)
[2022-06-07 14:15:56,615][root][INFO] - Step 25003520 @ 1535.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 25003520, mean_episode_return = None, mean_episode_step = 1797.0, total_loss = -41.316, pg_loss = -65.801, baseline_loss = 29.772, entropy_loss = -5.2871, learner_queue_size = 32, train_seconds = 1.393e+04, _tick = 4977, _time = 1.6546e+09)
[2022-06-07 14:16:01,618][root][INFO] - Step 25013760 @ 2046.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 25013760, mean_episode_return = -35.43, mean_episode_step = 1932.4, total_loss = 80.982, pg_loss = 46.449, baseline_loss = 39.855, entropy_loss = -5.3215, learner_queue_size = 32, train_seconds = 1.3935e+04, _tick = 4980, _time = 1.6546e+09)
[2022-06-07 14:16:06,624][root][INFO] - Step 25021440 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 25021440, mean_episode_return = None, mean_episode_step = 2161.0, total_loss = 701.99, pg_loss = 389.92, baseline_loss = 317.37, entropy_loss = -5.3018, learner_queue_size = 32, train_seconds = 1.394e+04, _tick = 4981, _time = 1.6546e+09)
[2022-06-07 14:16:11,626][root][INFO] - Step 25031680 @ 2047.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 25031680, mean_episode_return = 43.801, mean_episode_step = 2024.3, total_loss = 419.01, pg_loss = 229.01, baseline_loss = 195.23, entropy_loss = -5.222, learner_queue_size = 32, train_seconds = 1.3945e+04, _tick = 4985, _time = 1.6546e+09)
[2022-06-07 14:16:16,632][root][INFO] - Step 25039360 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25039360, mean_episode_return = 77.428, mean_episode_step = 2096.6, total_loss = -105.39, pg_loss = -118.06, baseline_loss = 17.899, entropy_loss = -5.2292, learner_queue_size = 32, train_seconds = 1.395e+04, _tick = 4988, _time = 1.6546e+09)
[2022-06-07 14:16:21,638][root][INFO] - Step 25049600 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 25049600, mean_episode_return = 56.542, mean_episode_step = 1970.4, total_loss = 289.26, pg_loss = 201.78, baseline_loss = 92.766, entropy_loss = -5.2814, learner_queue_size = 32, train_seconds = 1.3955e+04, _tick = 4989, _time = 1.6546e+09)
[2022-06-07 14:16:26,642][root][INFO] - Step 25057280 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25057280, mean_episode_return = None, mean_episode_step = 2967.9, total_loss = 493.6, pg_loss = 315.5, baseline_loss = 183.3, entropy_loss = -5.2055, learner_queue_size = 32, train_seconds = 1.396e+04, _tick = 4989, _time = 1.6546e+09)
[2022-06-07 14:16:31,646][root][INFO] - Step 25067520 @ 2046.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 25067520, mean_episode_return = 21.701, mean_episode_step = 2420.1, total_loss = -104.22, pg_loss = -164.36, baseline_loss = 65.317, entropy_loss = -5.1789, learner_queue_size = 32, train_seconds = 1.3965e+04, _tick = 4992, _time = 1.6546e+09)
[2022-06-07 14:16:36,650][root][INFO] - Step 25077760 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25077760, mean_episode_return = None, mean_episode_step = 1752.4, total_loss = 24.934, pg_loss = -12.544, baseline_loss = 42.643, entropy_loss = -5.165, learner_queue_size = 32, train_seconds = 1.397e+04, _tick = 4994, _time = 1.6546e+09)
[2022-06-07 14:16:41,657][root][INFO] - Step 25085440 @ 1533.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25085440, mean_episode_return = None, mean_episode_step = 2139.8, total_loss = -54.292, pg_loss = -61.985, baseline_loss = 12.843, entropy_loss = -5.1504, learner_queue_size = 32, train_seconds = 1.3975e+04, _tick = 4995, _time = 1.6546e+09)
[2022-06-07 14:16:46,662][root][INFO] - Step 25095680 @ 2046.0 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 25095680, mean_episode_return = None, mean_episode_step = 2029.8, total_loss = 199.48, pg_loss = 152.15, baseline_loss = 52.549, entropy_loss = -5.2218, learner_queue_size = 32, train_seconds = 1.398e+04, _tick = 4995, _time = 1.6546e+09)
[2022-06-07 14:16:51,668][root][INFO] - Step 25103360 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 25103360, mean_episode_return = 4.3897, mean_episode_step = 2258.7, total_loss = -5.0656, pg_loss = -63.984, baseline_loss = 64.131, entropy_loss = -5.2133, learner_queue_size = 32, train_seconds = 1.3985e+04, _tick = 4996, _time = 1.6546e+09)
[2022-06-07 14:16:56,674][root][INFO] - Step 25113600 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 25113600, mean_episode_return = -22.381, mean_episode_step = 2014.9, total_loss = -161.64, pg_loss = -170.64, baseline_loss = 14.278, entropy_loss = -5.2739, learner_queue_size = 32, train_seconds = 1.399e+04, _tick = 5000, _time = 1.6546e+09)
[2022-06-07 14:17:01,680][root][INFO] - Step 25121280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25121280, mean_episode_return = None, mean_episode_step = 1991.8, total_loss = 442.19, pg_loss = 311.69, baseline_loss = 135.74, entropy_loss = -5.2432, learner_queue_size = 32, train_seconds = 1.3995e+04, _tick = 5001, _time = 1.6546e+09)
[2022-06-07 14:17:06,682][root][INFO] - Step 25131520 @ 2047.2 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 25131520, mean_episode_return = None, mean_episode_step = 1673.5, total_loss = 581.73, pg_loss = 355.66, baseline_loss = 231.25, entropy_loss = -5.1843, learner_queue_size = 32, train_seconds = 1.4e+04, _tick = 5004, _time = 1.6546e+09)
[2022-06-07 14:17:11,688][root][INFO] - Step 25139200 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25139200, mean_episode_return = 12.779, mean_episode_step = 2357.4, total_loss = 81.128, pg_loss = 4.2146, baseline_loss = 82.06, entropy_loss = -5.1461, learner_queue_size = 32, train_seconds = 1.4005e+04, _tick = 5006, _time = 1.6546e+09)
[2022-06-07 14:17:16,694][root][INFO] - Step 25149440 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25149440, mean_episode_return = 70.42, mean_episode_step = 2566.4, total_loss = -91.631, pg_loss = -118.45, baseline_loss = 31.894, entropy_loss = -5.0767, learner_queue_size = 32, train_seconds = 1.401e+04, _tick = 5009, _time = 1.6546e+09)
[2022-06-07 14:17:21,698][root][INFO] - Step 25157120 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25157120, mean_episode_return = 115.24, mean_episode_step = 1750.4, total_loss = 228.2, pg_loss = 140.95, baseline_loss = 92.427, entropy_loss = -5.1747, learner_queue_size = 32, train_seconds = 1.4015e+04, _tick = 5010, _time = 1.6546e+09)
[2022-06-07 14:17:26,702][root][INFO] - Step 25167360 @ 2046.4 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 25167360, mean_episode_return = None, mean_episode_step = 2237.7, total_loss = 143.13, pg_loss = 91.794, baseline_loss = 56.526, entropy_loss = -5.1892, learner_queue_size = 32, train_seconds = 1.402e+04, _tick = 5012, _time = 1.6546e+09)
[2022-06-07 14:17:31,706][root][INFO] - Step 25175040 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25175040, mean_episode_return = None, mean_episode_step = 2466.3, total_loss = -143.0, pg_loss = -140.08, baseline_loss = 2.1905, entropy_loss = -5.1139, learner_queue_size = 32, train_seconds = 1.4025e+04, _tick = 5014, _time = 1.6546e+09)
[2022-06-07 14:17:36,712][root][INFO] - Step 25185280 @ 2045.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 25185280, mean_episode_return = None, mean_episode_step = 1582.8, total_loss = 47.71, pg_loss = 19.033, baseline_loss = 33.869, entropy_loss = -5.1922, learner_queue_size = 32, train_seconds = 1.403e+04, _tick = 5017, _time = 1.6546e+09)
[2022-06-07 14:17:41,718][root][INFO] - Step 25192960 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25192960, mean_episode_return = 10.95, mean_episode_step = 1771.8, total_loss = 21.242, pg_loss = 6.3676, baseline_loss = 19.915, entropy_loss = -5.0403, learner_queue_size = 32, train_seconds = 1.4035e+04, _tick = 5019, _time = 1.6546e+09)
[2022-06-07 14:17:46,722][root][INFO] - Step 25203200 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 25203200, mean_episode_return = None, mean_episode_step = 1907.6, total_loss = 265.65, pg_loss = 184.39, baseline_loss = 86.189, entropy_loss = -4.9318, learner_queue_size = 32, train_seconds = 1.404e+04, _tick = 5021, _time = 1.6546e+09)
[2022-06-07 14:17:51,726][root][INFO] - Step 25210880 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25210880, mean_episode_return = None, mean_episode_step = 1673.9, total_loss = 359.61, pg_loss = 265.26, baseline_loss = 99.216, entropy_loss = -4.8635, learner_queue_size = 32, train_seconds = 1.4045e+04, _tick = 5022, _time = 1.6546e+09)
[2022-06-07 14:17:56,730][root][INFO] - Step 25221120 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 25221120, mean_episode_return = None, mean_episode_step = 1981.1, total_loss = 72.793, pg_loss = 30.833, baseline_loss = 46.812, entropy_loss = -4.8531, learner_queue_size = 32, train_seconds = 1.405e+04, _tick = 5025, _time = 1.6546e+09)
[2022-06-07 14:18:01,734][root][INFO] - Step 25228800 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25228800, mean_episode_return = 33.922, mean_episode_step = 2129.9, total_loss = -36.289, pg_loss = -54.199, baseline_loss = 22.649, entropy_loss = -4.7397, learner_queue_size = 32, train_seconds = 1.4055e+04, _tick = 5026, _time = 1.6546e+09)
[2022-06-07 14:18:06,738][root][INFO] - Step 25239040 @ 2046.4 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 25239040, mean_episode_return = -21.18, mean_episode_step = 2557.3, total_loss = -8.4633, pg_loss = -40.266, baseline_loss = 36.568, entropy_loss = -4.7656, learner_queue_size = 32, train_seconds = 1.406e+04, _tick = 5028, _time = 1.6546e+09)
[2022-06-07 14:18:11,742][root][INFO] - Step 25246720 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 25246720, mean_episode_return = None, mean_episode_step = 1692.2, total_loss = -139.05, pg_loss = -138.34, baseline_loss = 4.0378, entropy_loss = -4.7451, learner_queue_size = 32, train_seconds = 1.4065e+04, _tick = 5029, _time = 1.6546e+09)
[2022-06-07 14:18:16,746][root][INFO] - Step 25256960 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 25256960, mean_episode_return = None, mean_episode_step = 2080.1, total_loss = -114.78, pg_loss = -114.43, baseline_loss = 4.4588, entropy_loss = -4.8097, learner_queue_size = 32, train_seconds = 1.407e+04, _tick = 5029, _time = 1.6546e+09)
[2022-06-07 14:18:21,752][root][INFO] - Step 25264640 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25264640, mean_episode_return = None, mean_episode_step = 1936.5, total_loss = 405.05, pg_loss = 310.82, baseline_loss = 98.988, entropy_loss = -4.7614, learner_queue_size = 32, train_seconds = 1.4075e+04, _tick = 5031, _time = 1.6546e+09)
[2022-06-07 14:18:26,758][root][INFO] - Step 25274880 @ 2045.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 25274880, mean_episode_return = None, mean_episode_step = 2302.3, total_loss = -134.44, pg_loss = -135.16, baseline_loss = 5.4714, entropy_loss = -4.7534, learner_queue_size = 32, train_seconds = 1.408e+04, _tick = 5033, _time = 1.6546e+09)
[2022-06-07 14:18:31,762][root][INFO] - Step 25282560 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25282560, mean_episode_return = 42.04, mean_episode_step = 2130.0, total_loss = -222.09, pg_loss = -226.81, baseline_loss = 9.5207, entropy_loss = -4.7965, learner_queue_size = 32, train_seconds = 1.4085e+04, _tick = 5036, _time = 1.6546e+09)
[2022-06-07 14:18:36,768][root][INFO] - Step 25292800 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25292800, mean_episode_return = 115.59, mean_episode_step = 2051.9, total_loss = -70.485, pg_loss = -85.407, baseline_loss = 19.757, entropy_loss = -4.8347, learner_queue_size = 32, train_seconds = 1.409e+04, _tick = 5039, _time = 1.6546e+09)
[2022-06-07 14:18:41,770][root][INFO] - Step 25300480 @ 1535.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25300480, mean_episode_return = None, mean_episode_step = 1850.4, total_loss = 237.45, pg_loss = 161.94, baseline_loss = 80.27, entropy_loss = -4.7645, learner_queue_size = 32, train_seconds = 1.4095e+04, _tick = 5039, _time = 1.6546e+09)
[2022-06-07 14:18:46,776][root][INFO] - Step 25310720 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25310720, mean_episode_return = None, mean_episode_step = 2645.8, total_loss = 85.327, pg_loss = 49.778, baseline_loss = 40.305, entropy_loss = -4.7561, learner_queue_size = 32, train_seconds = 1.41e+04, _tick = 5039, _time = 1.6546e+09)
[2022-06-07 14:18:51,782][root][INFO] - Step 25318400 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25318400, mean_episode_return = 69.368, mean_episode_step = 2149.1, total_loss = 61.439, pg_loss = 0.88812, baseline_loss = 65.19, entropy_loss = -4.6387, learner_queue_size = 32, train_seconds = 1.4105e+04, _tick = 5040, _time = 1.6546e+09)
[2022-06-07 14:18:56,786][root][INFO] - Step 25328640 @ 2046.4 SPS. Inference batcher size: 52. Learner queue size: 32. Other stats: (step = 25328640, mean_episode_return = 34.62, mean_episode_step = 1670.2, total_loss = 145.6, pg_loss = 74.743, baseline_loss = 75.505, entropy_loss = -4.6485, learner_queue_size = 32, train_seconds = 1.411e+04, _tick = 5042, _time = 1.6546e+09)
[2022-06-07 14:19:01,790][root][INFO] - Step 25336320 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25336320, mean_episode_return = None, mean_episode_step = 1687.2, total_loss = 244.58, pg_loss = 195.67, baseline_loss = 53.661, entropy_loss = -4.7511, learner_queue_size = 32, train_seconds = 1.4115e+04, _tick = 5043, _time = 1.6546e+09)
[2022-06-07 14:19:06,796][root][INFO] - Step 25346560 @ 2045.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 25346560, mean_episode_return = None, mean_episode_step = 2389.2, total_loss = -23.317, pg_loss = -55.115, baseline_loss = 36.493, entropy_loss = -4.695, learner_queue_size = 32, train_seconds = 1.412e+04, _tick = 5046, _time = 1.6546e+09)
[2022-06-07 14:19:11,798][root][INFO] - Step 25354240 @ 1535.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25354240, mean_episode_return = None, mean_episode_step = 2821.9, total_loss = 95.384, pg_loss = 50.421, baseline_loss = 49.695, entropy_loss = -4.7316, learner_queue_size = 32, train_seconds = 1.4125e+04, _tick = 5047, _time = 1.6546e+09)
[2022-06-07 14:19:16,804][root][INFO] - Step 25364480 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 25364480, mean_episode_return = None, mean_episode_step = 2245.9, total_loss = 374.94, pg_loss = 270.8, baseline_loss = 108.86, entropy_loss = -4.7229, learner_queue_size = 32, train_seconds = 1.413e+04, _tick = 5048, _time = 1.6546e+09)
[2022-06-07 14:19:21,810][root][INFO] - Step 25372160 @ 1534.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25372160, mean_episode_return = 79.306, mean_episode_step = 2675.6, total_loss = 166.48, pg_loss = 115.65, baseline_loss = 55.582, entropy_loss = -4.7438, learner_queue_size = 32, train_seconds = 1.4135e+04, _tick = 5050, _time = 1.6546e+09)
[2022-06-07 14:19:26,814][root][INFO] - Step 25382400 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25382400, mean_episode_return = -4.8537, mean_episode_step = 1970.6, total_loss = 24.545, pg_loss = -20.375, baseline_loss = 49.696, entropy_loss = -4.7752, learner_queue_size = 32, train_seconds = 1.414e+04, _tick = 5051, _time = 1.6546e+09)
[2022-06-07 14:19:31,820][root][INFO] - Step 25390080 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25390080, mean_episode_return = None, mean_episode_step = 1993.8, total_loss = 136.43, pg_loss = 96.809, baseline_loss = 44.384, entropy_loss = -4.7634, learner_queue_size = 32, train_seconds = 1.4145e+04, _tick = 5053, _time = 1.6546e+09)
[2022-06-07 14:19:36,822][root][INFO] - Step 25400320 @ 2047.2 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 25400320, mean_episode_return = None, mean_episode_step = 1757.1, total_loss = 54.27, pg_loss = 10.061, baseline_loss = 48.933, entropy_loss = -4.7228, learner_queue_size = 32, train_seconds = 1.415e+04, _tick = 5055, _time = 1.6546e+09)
[2022-06-07 14:19:41,826][root][INFO] - Step 25408000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25408000, mean_episode_return = None, mean_episode_step = 2639.0, total_loss = -202.69, pg_loss = -199.67, baseline_loss = 1.7483, entropy_loss = -4.7713, learner_queue_size = 32, train_seconds = 1.4155e+04, _tick = 5056, _time = 1.6546e+09)
[2022-06-07 14:19:46,832][root][INFO] - Step 25418240 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 25418240, mean_episode_return = -47.588, mean_episode_step = 2679.9, total_loss = -38.836, pg_loss = -109.68, baseline_loss = 75.587, entropy_loss = -4.7429, learner_queue_size = 32, train_seconds = 1.416e+04, _tick = 5060, _time = 1.6546e+09)
[2022-06-07 14:19:51,838][root][INFO] - Step 25425920 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 25425920, mean_episode_return = None, mean_episode_step = 1897.6, total_loss = -24.115, pg_loss = -48.885, baseline_loss = 29.516, entropy_loss = -4.746, learner_queue_size = 32, train_seconds = 1.4165e+04, _tick = 5061, _time = 1.6546e+09)
[2022-06-07 14:19:56,842][root][INFO] - Step 25436160 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 25436160, mean_episode_return = None, mean_episode_step = 1999.4, total_loss = -74.678, pg_loss = -77.523, baseline_loss = 7.6135, entropy_loss = -4.7694, learner_queue_size = 32, train_seconds = 1.417e+04, _tick = 5064, _time = 1.6546e+09)
[2022-06-07 14:20:01,848][root][INFO] - Step 25443840 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25443840, mean_episode_return = None, mean_episode_step = 2233.8, total_loss = 82.334, pg_loss = 53.542, baseline_loss = 33.63, entropy_loss = -4.8372, learner_queue_size = 32, train_seconds = 1.4175e+04, _tick = 5065, _time = 1.6546e+09)
[2022-06-07 14:20:06,854][root][INFO] - Step 25454080 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25454080, mean_episode_return = None, mean_episode_step = 1873.7, total_loss = 469.18, pg_loss = 243.95, baseline_loss = 230.08, entropy_loss = -4.8499, learner_queue_size = 32, train_seconds = 1.418e+04, _tick = 5067, _time = 1.6546e+09)
[2022-06-07 14:20:11,858][root][INFO] - Step 25461760 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25461760, mean_episode_return = 93.036, mean_episode_step = 1853.7, total_loss = 245.34, pg_loss = 134.64, baseline_loss = 115.57, entropy_loss = -4.8728, learner_queue_size = 32, train_seconds = 1.4185e+04, _tick = 5068, _time = 1.6546e+09)
[2022-06-07 14:20:16,862][root][INFO] - Step 25472000 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25472000, mean_episode_return = 11.38, mean_episode_step = 1858.4, total_loss = -212.27, pg_loss = -218.22, baseline_loss = 10.789, entropy_loss = -4.8317, learner_queue_size = 32, train_seconds = 1.419e+04, _tick = 5070, _time = 1.6546e+09)
[2022-06-07 14:20:21,867][root][INFO] - Step 25479680 @ 1534.3 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 25479680, mean_episode_return = None, mean_episode_step = 2348.6, total_loss = 134.3, pg_loss = 73.317, baseline_loss = 65.729, entropy_loss = -4.7447, learner_queue_size = 32, train_seconds = 1.4195e+04, _tick = 5071, _time = 1.6546e+09)
[2022-06-07 14:20:26,870][root][INFO] - Step 25489920 @ 2047.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 25489920, mean_episode_return = 53.154, mean_episode_step = 1991.9, total_loss = -22.613, pg_loss = -93.726, baseline_loss = 75.863, entropy_loss = -4.7493, learner_queue_size = 32, train_seconds = 1.42e+04, _tick = 5074, _time = 1.6546e+09)
[2022-06-07 14:20:31,874][root][INFO] - Step 25500160 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25500160, mean_episode_return = 32.06, mean_episode_step = 2351.2, total_loss = -27.139, pg_loss = -41.345, baseline_loss = 18.97, entropy_loss = -4.7636, learner_queue_size = 32, train_seconds = 1.4205e+04, _tick = 5078, _time = 1.6546e+09)
[2022-06-07 14:20:36,878][root][INFO] - Step 25507840 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25507840, mean_episode_return = 41.476, mean_episode_step = 2449.0, total_loss = -177.09, pg_loss = -189.57, baseline_loss = 17.175, entropy_loss = -4.6909, learner_queue_size = 32, train_seconds = 1.421e+04, _tick = 5081, _time = 1.6546e+09)
[2022-06-07 14:20:41,882][root][INFO] - Step 25518080 @ 2046.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 25518080, mean_episode_return = None, mean_episode_step = 1956.8, total_loss = 130.01, pg_loss = 77.6, baseline_loss = 57.054, entropy_loss = -4.6486, learner_queue_size = 32, train_seconds = 1.4215e+04, _tick = 5083, _time = 1.6546e+09)
[2022-06-07 14:20:46,888][root][INFO] - Step 25525760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25525760, mean_episode_return = 42.755, mean_episode_step = 1812.5, total_loss = 382.53, pg_loss = 165.62, baseline_loss = 221.61, entropy_loss = -4.6949, learner_queue_size = 32, train_seconds = 1.422e+04, _tick = 5084, _time = 1.6546e+09)
[2022-06-07 14:20:51,894][root][INFO] - Step 25536000 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25536000, mean_episode_return = 66.357, mean_episode_step = 2285.5, total_loss = 391.53, pg_loss = 233.91, baseline_loss = 162.3, entropy_loss = -4.6839, learner_queue_size = 32, train_seconds = 1.4225e+04, _tick = 5088, _time = 1.6546e+09)
[2022-06-07 14:20:56,900][root][INFO] - Step 25543680 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 25543680, mean_episode_return = 76.359, mean_episode_step = 1615.9, total_loss = 636.59, pg_loss = 373.72, baseline_loss = 267.57, entropy_loss = -4.6958, learner_queue_size = 32, train_seconds = 1.423e+04, _tick = 5089, _time = 1.6546e+09)
[2022-06-07 14:21:01,906][root][INFO] - Step 25553920 @ 2045.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 25553920, mean_episode_return = 28.69, mean_episode_step = 1722.1, total_loss = 33.824, pg_loss = -30.096, baseline_loss = 68.517, entropy_loss = -4.5964, learner_queue_size = 32, train_seconds = 1.4235e+04, _tick = 5093, _time = 1.6546e+09)
[2022-06-07 14:21:06,910][root][INFO] - Step 25561600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25561600, mean_episode_return = 34.82, mean_episode_step = 2296.6, total_loss = 101.61, pg_loss = 44.85, baseline_loss = 61.356, entropy_loss = -4.595, learner_queue_size = 32, train_seconds = 1.424e+04, _tick = 5095, _time = 1.6546e+09)
[2022-06-07 14:21:11,914][root][INFO] - Step 25571840 @ 2046.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 25571840, mean_episode_return = 84.866, mean_episode_step = 1657.8, total_loss = -12.72, pg_loss = -38.932, baseline_loss = 30.833, entropy_loss = -4.6207, learner_queue_size = 32, train_seconds = 1.4245e+04, _tick = 5098, _time = 1.6546e+09)
[2022-06-07 14:21:16,918][root][INFO] - Step 25579520 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25579520, mean_episode_return = None, mean_episode_step = 1973.8, total_loss = -95.491, pg_loss = -94.847, baseline_loss = 4.0314, entropy_loss = -4.6751, learner_queue_size = 32, train_seconds = 1.425e+04, _tick = 5100, _time = 1.6546e+09)
[2022-06-07 14:21:21,922][root][INFO] - Step 25589760 @ 2046.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 25589760, mean_episode_return = 72.771, mean_episode_step = 1957.3, total_loss = 361.27, pg_loss = 181.82, baseline_loss = 184.21, entropy_loss = -4.7616, learner_queue_size = 32, train_seconds = 1.4255e+04, _tick = 5102, _time = 1.6546e+09)
[2022-06-07 14:21:26,926][root][INFO] - Step 25597440 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25597440, mean_episode_return = None, mean_episode_step = 2153.4, total_loss = 42.473, pg_loss = 15.53, baseline_loss = 31.641, entropy_loss = -4.6983, learner_queue_size = 32, train_seconds = 1.426e+04, _tick = 5103, _time = 1.6546e+09)
[2022-06-07 14:21:31,932][root][INFO] - Step 25607680 @ 2045.7 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 25607680, mean_episode_return = 25.021, mean_episode_step = 2502.6, total_loss = 158.14, pg_loss = 85.353, baseline_loss = 77.538, entropy_loss = -4.7518, learner_queue_size = 32, train_seconds = 1.4265e+04, _tick = 5106, _time = 1.6546e+09)
[2022-06-07 14:21:36,936][root][INFO] - Step 25615360 @ 1534.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 25615360, mean_episode_return = 108.07, mean_episode_step = 1814.5, total_loss = 45.841, pg_loss = -10.215, baseline_loss = 60.792, entropy_loss = -4.7359, learner_queue_size = 32, train_seconds = 1.427e+04, _tick = 5108, _time = 1.6546e+09)
[2022-06-07 14:21:41,942][root][INFO] - Step 25625600 @ 2045.6 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 25625600, mean_episode_return = None, mean_episode_step = 2177.0, total_loss = 332.41, pg_loss = 152.53, baseline_loss = 184.52, entropy_loss = -4.6379, learner_queue_size = 32, train_seconds = 1.4275e+04, _tick = 5110, _time = 1.6546e+09)
[2022-06-07 14:21:46,946][root][INFO] - Step 25635840 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 25635840, mean_episode_return = None, mean_episode_step = 2055.1, total_loss = -86.561, pg_loss = -87.482, baseline_loss = 5.6106, entropy_loss = -4.6897, learner_queue_size = 32, train_seconds = 1.428e+04, _tick = 5111, _time = 1.6546e+09)
[2022-06-07 14:21:51,950][root][INFO] - Step 25643520 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 25643520, mean_episode_return = None, mean_episode_step = 2198.6, total_loss = 354.7, pg_loss = 215.48, baseline_loss = 143.96, entropy_loss = -4.7381, learner_queue_size = 32, train_seconds = 1.4285e+04, _tick = 5113, _time = 1.6546e+09)
[2022-06-07 14:21:56,956][root][INFO] - Step 25653760 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 25653760, mean_episode_return = 94.44, mean_episode_step = 2253.9, total_loss = 306.11, pg_loss = 169.69, baseline_loss = 141.08, entropy_loss = -4.6667, learner_queue_size = 32, train_seconds = 1.429e+04, _tick = 5116, _time = 1.6546e+09)
[2022-06-07 14:22:01,962][root][INFO] - Step 25661440 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25661440, mean_episode_return = None, mean_episode_step = 2001.8, total_loss = 163.57, pg_loss = 77.066, baseline_loss = 91.13, entropy_loss = -4.6263, learner_queue_size = 32, train_seconds = 1.4295e+04, _tick = 5117, _time = 1.6546e+09)
[2022-06-07 14:22:06,968][root][INFO] - Step 25671680 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 25671680, mean_episode_return = None, mean_episode_step = 2088.0, total_loss = 82.127, pg_loss = 35.247, baseline_loss = 51.421, entropy_loss = -4.5412, learner_queue_size = 32, train_seconds = 1.43e+04, _tick = 5117, _time = 1.6546e+09)
[2022-06-07 14:22:11,974][root][INFO] - Step 25679360 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 25679360, mean_episode_return = 77.009, mean_episode_step = 2139.0, total_loss = 465.99, pg_loss = 324.01, baseline_loss = 146.54, entropy_loss = -4.5589, learner_queue_size = 32, train_seconds = 1.4305e+04, _tick = 5120, _time = 1.6546e+09)
[2022-06-07 14:22:16,978][root][INFO] - Step 25689600 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 25689600, mean_episode_return = 153.49, mean_episode_step = 2124.7, total_loss = -33.175, pg_loss = -63.376, baseline_loss = 34.795, entropy_loss = -4.595, learner_queue_size = 32, train_seconds = 1.431e+04, _tick = 5123, _time = 1.6546e+09)
[2022-06-07 14:22:21,984][root][INFO] - Step 25697280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25697280, mean_episode_return = None, mean_episode_step = 2341.3, total_loss = 77.807, pg_loss = 32.84, baseline_loss = 49.63, entropy_loss = -4.6619, learner_queue_size = 32, train_seconds = 1.4315e+04, _tick = 5124, _time = 1.6546e+09)
[2022-06-07 14:22:26,990][root][INFO] - Step 25707520 @ 2045.6 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 25707520, mean_episode_return = None, mean_episode_step = 2270.9, total_loss = 250.84, pg_loss = 142.48, baseline_loss = 113.05, entropy_loss = -4.6889, learner_queue_size = 32, train_seconds = 1.432e+04, _tick = 5125, _time = 1.6546e+09)
[2022-06-07 14:22:31,994][root][INFO] - Step 25715200 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 25715200, mean_episode_return = None, mean_episode_step = 2338.7, total_loss = 30.405, pg_loss = -6.7813, baseline_loss = 41.92, entropy_loss = -4.7335, learner_queue_size = 32, train_seconds = 1.4325e+04, _tick = 5126, _time = 1.6546e+09)
[2022-06-07 14:22:37,007][root][INFO] - Step 25725440 @ 2042.7 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 25725440, mean_episode_return = None, mean_episode_step = 2702.8, total_loss = -61.414, pg_loss = -73.105, baseline_loss = 16.22, entropy_loss = -4.5299, learner_queue_size = 32, train_seconds = 1.433e+04, _tick = 5127, _time = 1.6546e+09)
[2022-06-07 14:22:42,013][root][INFO] - Step 25735680 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 25735680, mean_episode_return = 31.701, mean_episode_step = 2481.9, total_loss = -175.8, pg_loss = -187.22, baseline_loss = 15.907, entropy_loss = -4.4894, learner_queue_size = 32, train_seconds = 1.4335e+04, _tick = 5129, _time = 1.6546e+09)
[2022-06-07 14:22:47,019][root][INFO] - Step 25743360 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25743360, mean_episode_return = None, mean_episode_step = 1967.4, total_loss = -97.554, pg_loss = -100.88, baseline_loss = 7.7738, entropy_loss = -4.4437, learner_queue_size = 32, train_seconds = 1.434e+04, _tick = 5129, _time = 1.6546e+09)
[2022-06-07 14:22:52,025][root][INFO] - Step 25753600 @ 2045.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 25753600, mean_episode_return = None, mean_episode_step = 2162.2, total_loss = 325.21, pg_loss = 210.61, baseline_loss = 119.05, entropy_loss = -4.4437, learner_queue_size = 32, train_seconds = 1.4345e+04, _tick = 5131, _time = 1.6546e+09)
[2022-06-07 14:22:57,031][root][INFO] - Step 25761280 @ 1534.2 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 25761280, mean_episode_return = 81.668, mean_episode_step = 2180.3, total_loss = -144.53, pg_loss = -152.81, baseline_loss = 12.742, entropy_loss = -4.459, learner_queue_size = 32, train_seconds = 1.435e+04, _tick = 5133, _time = 1.6546e+09)
[2022-06-07 14:23:02,037][root][INFO] - Step 25771520 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 25771520, mean_episode_return = 53.506, mean_episode_step = 2133.1, total_loss = 75.55, pg_loss = -20.416, baseline_loss = 100.47, entropy_loss = -4.5076, learner_queue_size = 32, train_seconds = 1.4355e+04, _tick = 5135, _time = 1.6546e+09)
[2022-06-07 14:23:07,043][root][INFO] - Step 25779200 @ 1534.1 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 25779200, mean_episode_return = 27.974, mean_episode_step = 2571.7, total_loss = 223.09, pg_loss = 142.24, baseline_loss = 85.323, entropy_loss = -4.4736, learner_queue_size = 32, train_seconds = 1.436e+04, _tick = 5138, _time = 1.6546e+09)
[2022-06-07 14:23:12,046][root][INFO] - Step 25789440 @ 2046.9 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 25789440, mean_episode_return = 50.311, mean_episode_step = 2361.3, total_loss = -88.514, pg_loss = -118.89, baseline_loss = 34.745, entropy_loss = -4.3686, learner_queue_size = 32, train_seconds = 1.4365e+04, _tick = 5141, _time = 1.6546e+09)
[2022-06-07 14:23:17,050][root][INFO] - Step 25797120 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 25797120, mean_episode_return = None, mean_episode_step = 2511.2, total_loss = -23.029, pg_loss = -43.657, baseline_loss = 25.026, entropy_loss = -4.3977, learner_queue_size = 32, train_seconds = 1.437e+04, _tick = 5143, _time = 1.6546e+09)
[2022-06-07 14:23:22,056][root][INFO] - Step 25807360 @ 2045.5 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 25807360, mean_episode_return = 96.729, mean_episode_step = 1851.8, total_loss = 112.42, pg_loss = 35.289, baseline_loss = 81.5, entropy_loss = -4.371, learner_queue_size = 32, train_seconds = 1.4375e+04, _tick = 5147, _time = 1.6546e+09)
[2022-06-07 14:23:27,062][root][INFO] - Step 25817600 @ 2045.6 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 25817600, mean_episode_return = 9.1394, mean_episode_step = 1895.5, total_loss = -5.968, pg_loss = -52.787, baseline_loss = 51.235, entropy_loss = -4.4163, learner_queue_size = 32, train_seconds = 1.438e+04, _tick = 5151, _time = 1.6546e+09)
[2022-06-07 14:23:32,066][root][INFO] - Step 25825280 @ 1534.7 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 25825280, mean_episode_return = None, mean_episode_step = 2364.8, total_loss = 272.42, pg_loss = 145.43, baseline_loss = 131.39, entropy_loss = -4.3903, learner_queue_size = 32, train_seconds = 1.4385e+04, _tick = 5152, _time = 1.6546e+09)
[2022-06-07 14:23:37,070][root][INFO] - Step 25832960 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 25832960, mean_episode_return = 30.41, mean_episode_step = 2084.3, total_loss = -55.287, pg_loss = -91.211, baseline_loss = 40.294, entropy_loss = -4.3702, learner_queue_size = 32, train_seconds = 1.439e+04, _tick = 5155, _time = 1.6546e+09)
[2022-06-07 14:23:42,076][root][INFO] - Step 25843200 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 25843200, mean_episode_return = None, mean_episode_step = 2134.4, total_loss = 77.574, pg_loss = 39.214, baseline_loss = 42.704, entropy_loss = -4.3435, learner_queue_size = 32, train_seconds = 1.4395e+04, _tick = 5157, _time = 1.6546e+09)
[2022-06-07 14:23:47,082][root][INFO] - Step 25850880 @ 1534.2 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 25850880, mean_episode_return = None, mean_episode_step = 2940.6, total_loss = -62.43, pg_loss = -64.979, baseline_loss = 6.9594, entropy_loss = -4.4104, learner_queue_size = 32, train_seconds = 1.44e+04, _tick = 5157, _time = 1.6546e+09)
[2022-06-07 14:23:52,088][root][INFO] - Step 25861120 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 25861120, mean_episode_return = 27.88, mean_episode_step = 1633.1, total_loss = 24.279, pg_loss = -9.5382, baseline_loss = 38.337, entropy_loss = -4.5205, learner_queue_size = 32, train_seconds = 1.4405e+04, _tick = 5159, _time = 1.6546e+09)
[2022-06-07 14:23:57,094][root][INFO] - Step 25868800 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 25868800, mean_episode_return = 68.779, mean_episode_step = 2564.1, total_loss = -10.023, pg_loss = -43.669, baseline_loss = 38.113, entropy_loss = -4.4672, learner_queue_size = 32, train_seconds = 1.441e+04, _tick = 5162, _time = 1.6546e+09)
[2022-06-07 14:24:02,098][root][INFO] - Step 25879040 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 25879040, mean_episode_return = 77.65, mean_episode_step = 2047.4, total_loss = 50.956, pg_loss = 14.392, baseline_loss = 41.05, entropy_loss = -4.486, learner_queue_size = 32, train_seconds = 1.4415e+04, _tick = 5165, _time = 1.6546e+09)
[2022-06-07 14:24:07,102][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 14:24:07,303][root][INFO] - Step 25886720 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25886720, mean_episode_return = 45.138, mean_episode_step = 1813.0, total_loss = -35.593, pg_loss = -68.38, baseline_loss = 37.28, entropy_loss = -4.4925, learner_queue_size = 32, train_seconds = 1.442e+04, _tick = 5167, _time = 1.6546e+09)
[2022-06-07 14:24:12,306][root][INFO] - Step 25896960 @ 1967.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 25896960, mean_episode_return = 47.031, mean_episode_step = 1813.9, total_loss = -29.403, pg_loss = -49.205, baseline_loss = 24.201, entropy_loss = -4.3991, learner_queue_size = 32, train_seconds = 1.4425e+04, _tick = 5169, _time = 1.6546e+09)
[2022-06-07 14:24:17,310][root][INFO] - Step 25904640 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 25904640, mean_episode_return = None, mean_episode_step = 1821.3, total_loss = 79.138, pg_loss = 47.152, baseline_loss = 36.395, entropy_loss = -4.4091, learner_queue_size = 32, train_seconds = 1.443e+04, _tick = 5171, _time = 1.6546e+09)
[2022-06-07 14:24:22,316][root][INFO] - Step 25914880 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25914880, mean_episode_return = 43.111, mean_episode_step = 2414.2, total_loss = 195.35, pg_loss = 117.64, baseline_loss = 82.004, entropy_loss = -4.2932, learner_queue_size = 32, train_seconds = 1.4435e+04, _tick = 5174, _time = 1.6546e+09)
[2022-06-07 14:24:27,322][root][INFO] - Step 25922560 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25922560, mean_episode_return = 35.941, mean_episode_step = 1827.6, total_loss = -126.13, pg_loss = -131.36, baseline_loss = 9.5559, entropy_loss = -4.3245, learner_queue_size = 32, train_seconds = 1.444e+04, _tick = 5176, _time = 1.6546e+09)
[2022-06-07 14:24:32,328][root][INFO] - Step 25932800 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 25932800, mean_episode_return = 32.781, mean_episode_step = 2857.2, total_loss = -25.072, pg_loss = -52.428, baseline_loss = 31.739, entropy_loss = -4.3833, learner_queue_size = 32, train_seconds = 1.4445e+04, _tick = 5179, _time = 1.6546e+09)
[2022-06-07 14:24:37,334][root][INFO] - Step 25940480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25940480, mean_episode_return = None, mean_episode_step = 2523.1, total_loss = 74.239, pg_loss = 20.614, baseline_loss = 58.016, entropy_loss = -4.3914, learner_queue_size = 32, train_seconds = 1.445e+04, _tick = 5179, _time = 1.6546e+09)
[2022-06-07 14:24:42,340][root][INFO] - Step 25950720 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 25950720, mean_episode_return = 127.31, mean_episode_step = 1803.6, total_loss = 114.41, pg_loss = 36.848, baseline_loss = 81.822, entropy_loss = -4.2576, learner_queue_size = 32, train_seconds = 1.4455e+04, _tick = 5182, _time = 1.6546e+09)
[2022-06-07 14:24:47,346][root][INFO] - Step 25960960 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 25960960, mean_episode_return = 57.382, mean_episode_step = 2034.1, total_loss = -29.772, pg_loss = -51.382, baseline_loss = 25.809, entropy_loss = -4.2, learner_queue_size = 32, train_seconds = 1.446e+04, _tick = 5186, _time = 1.6546e+09)
[2022-06-07 14:24:52,350][root][INFO] - Step 25968640 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 25968640, mean_episode_return = None, mean_episode_step = 2383.0, total_loss = 126.56, pg_loss = 52.184, baseline_loss = 78.528, entropy_loss = -4.1543, learner_queue_size = 32, train_seconds = 1.4465e+04, _tick = 5188, _time = 1.6546e+09)
[2022-06-07 14:24:57,354][root][INFO] - Step 25978880 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 25978880, mean_episode_return = 72.768, mean_episode_step = 2011.4, total_loss = 160.7, pg_loss = 68.855, baseline_loss = 96.014, entropy_loss = -4.1695, learner_queue_size = 32, train_seconds = 1.447e+04, _tick = 5190, _time = 1.6546e+09)
[2022-06-07 14:25:02,358][root][INFO] - Step 25986560 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 25986560, mean_episode_return = None, mean_episode_step = 1913.0, total_loss = -56.798, pg_loss = -58.557, baseline_loss = 5.8849, entropy_loss = -4.1261, learner_queue_size = 32, train_seconds = 1.4475e+04, _tick = 5190, _time = 1.6546e+09)
[2022-06-07 14:25:07,364][root][INFO] - Step 25996800 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 25996800, mean_episode_return = None, mean_episode_step = 2433.7, total_loss = -12.31, pg_loss = -25.917, baseline_loss = 17.787, entropy_loss = -4.1799, learner_queue_size = 32, train_seconds = 1.448e+04, _tick = 5191, _time = 1.6546e+09)
[2022-06-07 14:25:12,366][root][INFO] - Step 26004480 @ 1535.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26004480, mean_episode_return = None, mean_episode_step = 2354.4, total_loss = 27.872, pg_loss = -12.235, baseline_loss = 44.372, entropy_loss = -4.2645, learner_queue_size = 32, train_seconds = 1.4485e+04, _tick = 5191, _time = 1.6546e+09)
[2022-06-07 14:25:17,372][root][INFO] - Step 26014720 @ 2045.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 26014720, mean_episode_return = None, mean_episode_step = 2301.5, total_loss = -111.97, pg_loss = -112.13, baseline_loss = 4.3596, entropy_loss = -4.2007, learner_queue_size = 32, train_seconds = 1.449e+04, _tick = 5192, _time = 1.6546e+09)
[2022-06-07 14:25:22,379][root][INFO] - Step 26022400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26022400, mean_episode_return = None, mean_episode_step = 2182.2, total_loss = 180.62, pg_loss = 131.18, baseline_loss = 53.573, entropy_loss = -4.1339, learner_queue_size = 32, train_seconds = 1.4495e+04, _tick = 5192, _time = 1.6546e+09)
[2022-06-07 14:25:27,382][root][INFO] - Step 26032640 @ 2046.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26032640, mean_episode_return = 3.81, mean_episode_step = 2429.5, total_loss = -28.377, pg_loss = -70.86, baseline_loss = 46.639, entropy_loss = -4.1556, learner_queue_size = 32, train_seconds = 1.45e+04, _tick = 5194, _time = 1.6546e+09)
[2022-06-07 14:25:32,388][root][INFO] - Step 26040320 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26040320, mean_episode_return = None, mean_episode_step = 2166.4, total_loss = 69.998, pg_loss = 39.852, baseline_loss = 34.328, entropy_loss = -4.1822, learner_queue_size = 32, train_seconds = 1.4505e+04, _tick = 5194, _time = 1.6546e+09)
[2022-06-07 14:25:37,394][root][INFO] - Step 26050560 @ 2045.6 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 26050560, mean_episode_return = None, mean_episode_step = 2318.5, total_loss = -125.52, pg_loss = -122.6, baseline_loss = 1.3431, entropy_loss = -4.2707, learner_queue_size = 32, train_seconds = 1.451e+04, _tick = 5195, _time = 1.6546e+09)
[2022-06-07 14:25:42,398][root][INFO] - Step 26058240 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26058240, mean_episode_return = None, mean_episode_step = 2354.2, total_loss = -2.7756, pg_loss = -14.063, baseline_loss = 15.61, entropy_loss = -4.3219, learner_queue_size = 32, train_seconds = 1.4515e+04, _tick = 5196, _time = 1.6546e+09)
[2022-06-07 14:25:47,404][root][INFO] - Step 26068480 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26068480, mean_episode_return = None, mean_episode_step = 2490.3, total_loss = 96.174, pg_loss = 73.327, baseline_loss = 27.367, entropy_loss = -4.52, learner_queue_size = 32, train_seconds = 1.452e+04, _tick = 5198, _time = 1.6546e+09)
[2022-06-07 14:25:52,410][root][INFO] - Step 26076160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26076160, mean_episode_return = 112.44, mean_episode_step = 2068.0, total_loss = -29.6, pg_loss = -45.966, baseline_loss = 20.963, entropy_loss = -4.5972, learner_queue_size = 32, train_seconds = 1.4525e+04, _tick = 5200, _time = 1.6546e+09)
[2022-06-07 14:25:57,416][root][INFO] - Step 26086400 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 26086400, mean_episode_return = None, mean_episode_step = 2427.0, total_loss = 318.15, pg_loss = 178.48, baseline_loss = 144.17, entropy_loss = -4.5057, learner_queue_size = 32, train_seconds = 1.453e+04, _tick = 5201, _time = 1.6546e+09)
[2022-06-07 14:26:02,422][root][INFO] - Step 26094080 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 26094080, mean_episode_return = None, mean_episode_step = 1529.0, total_loss = 335.06, pg_loss = 226.33, baseline_loss = 113.24, entropy_loss = -4.5128, learner_queue_size = 32, train_seconds = 1.4535e+04, _tick = 5203, _time = 1.6546e+09)
[2022-06-07 14:26:07,426][root][INFO] - Step 26104320 @ 2046.4 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 26104320, mean_episode_return = None, mean_episode_step = 2147.4, total_loss = 146.59, pg_loss = 97.65, baseline_loss = 53.481, entropy_loss = -4.5365, learner_queue_size = 32, train_seconds = 1.454e+04, _tick = 5205, _time = 1.6546e+09)
[2022-06-07 14:26:12,432][root][INFO] - Step 26112000 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 26112000, mean_episode_return = 121.48, mean_episode_step = 2163.3, total_loss = 103.81, pg_loss = 41.708, baseline_loss = 66.62, entropy_loss = -4.5165, learner_queue_size = 32, train_seconds = 1.4545e+04, _tick = 5207, _time = 1.6546e+09)
[2022-06-07 14:26:17,434][root][INFO] - Step 26122240 @ 2047.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 26122240, mean_episode_return = None, mean_episode_step = 2372.7, total_loss = 67.615, pg_loss = 31.995, baseline_loss = 40.226, entropy_loss = -4.6066, learner_queue_size = 32, train_seconds = 1.455e+04, _tick = 5208, _time = 1.6546e+09)
[2022-06-07 14:26:22,438][root][INFO] - Step 26129920 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 26129920, mean_episode_return = 70.069, mean_episode_step = 1831.6, total_loss = -49.31, pg_loss = -78.888, baseline_loss = 34.217, entropy_loss = -4.639, learner_queue_size = 32, train_seconds = 1.4555e+04, _tick = 5210, _time = 1.6546e+09)
[2022-06-07 14:26:27,442][root][INFO] - Step 26140160 @ 2046.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 26140160, mean_episode_return = 16.461, mean_episode_step = 2482.6, total_loss = -112.7, pg_loss = -134.56, baseline_loss = 26.579, entropy_loss = -4.7216, learner_queue_size = 32, train_seconds = 1.456e+04, _tick = 5212, _time = 1.6546e+09)
[2022-06-07 14:26:32,446][root][INFO] - Step 26147840 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26147840, mean_episode_return = 100.05, mean_episode_step = 2104.5, total_loss = 176.54, pg_loss = 97.401, baseline_loss = 83.786, entropy_loss = -4.6431, learner_queue_size = 32, train_seconds = 1.4565e+04, _tick = 5214, _time = 1.6546e+09)
[2022-06-07 14:26:37,452][root][INFO] - Step 26158080 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26158080, mean_episode_return = 20.151, mean_episode_step = 2328.0, total_loss = 333.99, pg_loss = 217.59, baseline_loss = 121.06, entropy_loss = -4.6674, learner_queue_size = 32, train_seconds = 1.457e+04, _tick = 5215, _time = 1.6546e+09)
[2022-06-07 14:26:42,458][root][INFO] - Step 26165760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26165760, mean_episode_return = None, mean_episode_step = 2259.3, total_loss = 329.97, pg_loss = 211.24, baseline_loss = 123.38, entropy_loss = -4.6524, learner_queue_size = 32, train_seconds = 1.4576e+04, _tick = 5217, _time = 1.6546e+09)
[2022-06-07 14:26:47,462][root][INFO] - Step 26176000 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26176000, mean_episode_return = None, mean_episode_step = 2442.9, total_loss = -65.296, pg_loss = -83.14, baseline_loss = 22.59, entropy_loss = -4.7456, learner_queue_size = 32, train_seconds = 1.458e+04, _tick = 5218, _time = 1.6546e+09)
[2022-06-07 14:26:52,466][root][INFO] - Step 26183680 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26183680, mean_episode_return = 89.29, mean_episode_step = 2802.0, total_loss = 82.956, pg_loss = 40.3, baseline_loss = 47.428, entropy_loss = -4.7723, learner_queue_size = 32, train_seconds = 1.4586e+04, _tick = 5220, _time = 1.6546e+09)
[2022-06-07 14:26:57,470][root][INFO] - Step 26193920 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 26193920, mean_episode_return = 58.931, mean_episode_step = 2304.0, total_loss = 303.53, pg_loss = 197.5, baseline_loss = 110.88, entropy_loss = -4.8526, learner_queue_size = 32, train_seconds = 1.459e+04, _tick = 5222, _time = 1.6546e+09)
[2022-06-07 14:27:02,474][root][INFO] - Step 26201600 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 26201600, mean_episode_return = 33.651, mean_episode_step = 2020.9, total_loss = 126.89, pg_loss = 62.343, baseline_loss = 69.443, entropy_loss = -4.8916, learner_queue_size = 32, train_seconds = 1.4596e+04, _tick = 5224, _time = 1.6546e+09)
[2022-06-07 14:27:07,478][root][INFO] - Step 26211840 @ 2046.3 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 26211840, mean_episode_return = 113.51, mean_episode_step = 2652.0, total_loss = 52.28, pg_loss = 9.6858, baseline_loss = 47.549, entropy_loss = -4.9552, learner_queue_size = 32, train_seconds = 1.46e+04, _tick = 5228, _time = 1.6546e+09)
[2022-06-07 14:27:12,482][root][INFO] - Step 26219520 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 26219520, mean_episode_return = 123.97, mean_episode_step = 2865.4, total_loss = 355.56, pg_loss = 155.38, baseline_loss = 205.2, entropy_loss = -5.0145, learner_queue_size = 32, train_seconds = 1.4606e+04, _tick = 5231, _time = 1.6546e+09)
[2022-06-07 14:27:17,486][root][INFO] - Step 26229760 @ 2046.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 26229760, mean_episode_return = 45.3, mean_episode_step = 2077.9, total_loss = -63.037, pg_loss = -75.449, baseline_loss = 17.465, entropy_loss = -5.0527, learner_queue_size = 32, train_seconds = 1.461e+04, _tick = 5235, _time = 1.6546e+09)
[2022-06-07 14:27:22,490][root][INFO] - Step 26237440 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 26237440, mean_episode_return = None, mean_episode_step = 2573.4, total_loss = 146.06, pg_loss = 86.853, baseline_loss = 64.248, entropy_loss = -5.036, learner_queue_size = 32, train_seconds = 1.4616e+04, _tick = 5236, _time = 1.6546e+09)
[2022-06-07 14:27:27,494][root][INFO] - Step 26247680 @ 2046.4 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 26247680, mean_episode_return = None, mean_episode_step = 2408.5, total_loss = -87.757, pg_loss = -88.449, baseline_loss = 5.7163, entropy_loss = -5.0243, learner_queue_size = 32, train_seconds = 1.462e+04, _tick = 5238, _time = 1.6546e+09)
[2022-06-07 14:27:32,498][root][INFO] - Step 26255360 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26255360, mean_episode_return = 58.852, mean_episode_step = 2587.9, total_loss = 129.98, pg_loss = 64.587, baseline_loss = 70.402, entropy_loss = -5.0116, learner_queue_size = 32, train_seconds = 1.4626e+04, _tick = 5241, _time = 1.6546e+09)
[2022-06-07 14:27:37,502][root][INFO] - Step 26265600 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26265600, mean_episode_return = None, mean_episode_step = 2489.4, total_loss = 50.874, pg_loss = 20.069, baseline_loss = 35.784, entropy_loss = -4.9789, learner_queue_size = 32, train_seconds = 1.463e+04, _tick = 5244, _time = 1.6546e+09)
[2022-06-07 14:27:42,506][root][INFO] - Step 26273280 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 26273280, mean_episode_return = None, mean_episode_step = 1624.9, total_loss = 711.26, pg_loss = 541.22, baseline_loss = 174.93, entropy_loss = -4.8977, learner_queue_size = 32, train_seconds = 1.4636e+04, _tick = 5244, _time = 1.6546e+09)
[2022-06-07 14:27:47,510][root][INFO] - Step 26283520 @ 2046.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 26283520, mean_episode_return = 27.01, mean_episode_step = 2114.1, total_loss = 738.41, pg_loss = 409.1, baseline_loss = 334.17, entropy_loss = -4.8643, learner_queue_size = 32, train_seconds = 1.464e+04, _tick = 5247, _time = 1.6546e+09)
[2022-06-07 14:27:52,514][root][INFO] - Step 26293760 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26293760, mean_episode_return = 95.619, mean_episode_step = 2327.2, total_loss = -95.469, pg_loss = -115.94, baseline_loss = 25.381, entropy_loss = -4.9067, learner_queue_size = 32, train_seconds = 1.4646e+04, _tick = 5248, _time = 1.6546e+09)
[2022-06-07 14:27:57,518][root][INFO] - Step 26301440 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 26301440, mean_episode_return = None, mean_episode_step = 2193.2, total_loss = -161.5, pg_loss = -159.34, baseline_loss = 2.8062, entropy_loss = -4.9661, learner_queue_size = 32, train_seconds = 1.465e+04, _tick = 5249, _time = 1.6546e+09)
[2022-06-07 14:28:02,522][root][INFO] - Step 26311680 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 26311680, mean_episode_return = 97.279, mean_episode_step = 2096.8, total_loss = 186.61, pg_loss = 119.6, baseline_loss = 72.103, entropy_loss = -5.0861, learner_queue_size = 32, train_seconds = 1.4656e+04, _tick = 5252, _time = 1.6546e+09)
[2022-06-07 14:28:07,526][root][INFO] - Step 26319360 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26319360, mean_episode_return = None, mean_episode_step = 2193.5, total_loss = -16.186, pg_loss = -39.943, baseline_loss = 28.842, entropy_loss = -5.0853, learner_queue_size = 32, train_seconds = 1.466e+04, _tick = 5253, _time = 1.6546e+09)
[2022-06-07 14:28:12,530][root][INFO] - Step 26327040 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 26327040, mean_episode_return = 28.05, mean_episode_step = 2022.8, total_loss = 524.91, pg_loss = 387.33, baseline_loss = 142.71, entropy_loss = -5.1274, learner_queue_size = 32, train_seconds = 1.4666e+04, _tick = 5255, _time = 1.6546e+09)
[2022-06-07 14:28:17,536][root][INFO] - Step 26337280 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26337280, mean_episode_return = 26.954, mean_episode_step = 2429.6, total_loss = 386.74, pg_loss = 257.71, baseline_loss = 134.14, entropy_loss = -5.1181, learner_queue_size = 32, train_seconds = 1.467e+04, _tick = 5259, _time = 1.6546e+09)
[2022-06-07 14:28:22,542][root][INFO] - Step 26347520 @ 2045.6 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 26347520, mean_episode_return = 29.362, mean_episode_step = 1585.3, total_loss = 220.28, pg_loss = 138.48, baseline_loss = 86.961, entropy_loss = -5.1585, learner_queue_size = 32, train_seconds = 1.4676e+04, _tick = 5262, _time = 1.6546e+09)
[2022-06-07 14:28:27,548][root][INFO] - Step 26355200 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 26355200, mean_episode_return = 37.818, mean_episode_step = 1961.0, total_loss = 19.95, pg_loss = -50.18, baseline_loss = 75.323, entropy_loss = -5.1929, learner_queue_size = 32, train_seconds = 1.468e+04, _tick = 5264, _time = 1.6546e+09)
[2022-06-07 14:28:32,554][root][INFO] - Step 26365440 @ 2045.6 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 26365440, mean_episode_return = 7.9549, mean_episode_step = 1895.2, total_loss = -125.27, pg_loss = -151.57, baseline_loss = 31.527, entropy_loss = -5.2259, learner_queue_size = 32, train_seconds = 1.4686e+04, _tick = 5267, _time = 1.6546e+09)
[2022-06-07 14:28:37,558][root][INFO] - Step 26373120 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 26373120, mean_episode_return = None, mean_episode_step = 2317.2, total_loss = 136.5, pg_loss = 83.511, baseline_loss = 58.244, entropy_loss = -5.2561, learner_queue_size = 32, train_seconds = 1.4691e+04, _tick = 5269, _time = 1.6546e+09)
[2022-06-07 14:28:42,562][root][INFO] - Step 26383360 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26383360, mean_episode_return = None, mean_episode_step = 1726.9, total_loss = 196.3, pg_loss = 122.52, baseline_loss = 78.962, entropy_loss = -5.1827, learner_queue_size = 32, train_seconds = 1.4696e+04, _tick = 5272, _time = 1.6546e+09)
[2022-06-07 14:28:47,566][root][INFO] - Step 26391040 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26391040, mean_episode_return = 2.9596, mean_episode_step = 1980.3, total_loss = 223.49, pg_loss = 148.5, baseline_loss = 80.139, entropy_loss = -5.1522, learner_queue_size = 32, train_seconds = 1.4701e+04, _tick = 5274, _time = 1.6546e+09)
[2022-06-07 14:28:52,570][root][INFO] - Step 26401280 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 26401280, mean_episode_return = None, mean_episode_step = 1451.2, total_loss = 405.98, pg_loss = 282.43, baseline_loss = 128.73, entropy_loss = -5.1825, learner_queue_size = 32, train_seconds = 1.4706e+04, _tick = 5276, _time = 1.6546e+09)
[2022-06-07 14:28:57,574][root][INFO] - Step 26408960 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26408960, mean_episode_return = None, mean_episode_step = 1884.0, total_loss = 315.01, pg_loss = 234.39, baseline_loss = 85.777, entropy_loss = -5.1573, learner_queue_size = 32, train_seconds = 1.4711e+04, _tick = 5278, _time = 1.6546e+09)
[2022-06-07 14:29:02,578][root][INFO] - Step 26419200 @ 2046.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 26419200, mean_episode_return = 42.861, mean_episode_step = 2507.8, total_loss = 83.279, pg_loss = -3.0973, baseline_loss = 91.466, entropy_loss = -5.0901, learner_queue_size = 32, train_seconds = 1.4716e+04, _tick = 5280, _time = 1.6546e+09)
[2022-06-07 14:29:07,582][root][INFO] - Step 26426880 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26426880, mean_episode_return = None, mean_episode_step = 1998.1, total_loss = -91.978, pg_loss = -92.009, baseline_loss = 5.1433, entropy_loss = -5.1123, learner_queue_size = 32, train_seconds = 1.4721e+04, _tick = 5282, _time = 1.6546e+09)
[2022-06-07 14:29:12,586][root][INFO] - Step 26437120 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 26437120, mean_episode_return = 50.503, mean_episode_step = 2395.5, total_loss = 148.22, pg_loss = 54.138, baseline_loss = 99.231, entropy_loss = -5.1453, learner_queue_size = 32, train_seconds = 1.4726e+04, _tick = 5285, _time = 1.6546e+09)
[2022-06-07 14:29:17,590][root][INFO] - Step 26444800 @ 1534.7 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 26444800, mean_episode_return = 29.57, mean_episode_step = 2279.4, total_loss = 187.87, pg_loss = 128.22, baseline_loss = 64.773, entropy_loss = -5.1139, learner_queue_size = 32, train_seconds = 1.4731e+04, _tick = 5288, _time = 1.6546e+09)
[2022-06-07 14:29:22,594][root][INFO] - Step 26455040 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 26455040, mean_episode_return = None, mean_episode_step = 2282.9, total_loss = -140.62, pg_loss = -145.26, baseline_loss = 9.8687, entropy_loss = -5.2318, learner_queue_size = 32, train_seconds = 1.4736e+04, _tick = 5289, _time = 1.6546e+09)
[2022-06-07 14:29:27,600][root][INFO] - Step 26462720 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26462720, mean_episode_return = 35.346, mean_episode_step = 1857.3, total_loss = -14.327, pg_loss = -66.177, baseline_loss = 57.134, entropy_loss = -5.2839, learner_queue_size = 32, train_seconds = 1.4741e+04, _tick = 5290, _time = 1.6546e+09)
[2022-06-07 14:29:32,606][root][INFO] - Step 26472960 @ 2045.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 26472960, mean_episode_return = None, mean_episode_step = 1981.0, total_loss = 198.53, pg_loss = 134.49, baseline_loss = 69.28, entropy_loss = -5.2472, learner_queue_size = 32, train_seconds = 1.4746e+04, _tick = 5292, _time = 1.6546e+09)
[2022-06-07 14:29:37,610][root][INFO] - Step 26480640 @ 1534.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 26480640, mean_episode_return = 62.182, mean_episode_step = 2276.9, total_loss = 525.83, pg_loss = 322.14, baseline_loss = 208.98, entropy_loss = -5.2836, learner_queue_size = 32, train_seconds = 1.4751e+04, _tick = 5294, _time = 1.6546e+09)
[2022-06-07 14:29:42,614][root][INFO] - Step 26490880 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 26490880, mean_episode_return = None, mean_episode_step = 2067.0, total_loss = -50.679, pg_loss = -60.282, baseline_loss = 14.809, entropy_loss = -5.2061, learner_queue_size = 32, train_seconds = 1.4756e+04, _tick = 5295, _time = 1.6546e+09)
[2022-06-07 14:29:47,620][root][INFO] - Step 26498560 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26498560, mean_episode_return = 30.382, mean_episode_step = 1828.0, total_loss = 464.97, pg_loss = 317.12, baseline_loss = 153.08, entropy_loss = -5.2351, learner_queue_size = 32, train_seconds = 1.4761e+04, _tick = 5296, _time = 1.6546e+09)
[2022-06-07 14:29:52,626][root][INFO] - Step 26508800 @ 2045.6 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 26508800, mean_episode_return = None, mean_episode_step = 2622.6, total_loss = -134.62, pg_loss = -152.06, baseline_loss = 22.571, entropy_loss = -5.1262, learner_queue_size = 32, train_seconds = 1.4766e+04, _tick = 5297, _time = 1.6546e+09)
[2022-06-07 14:29:57,632][root][INFO] - Step 26516480 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 26516480, mean_episode_return = 1.9196, mean_episode_step = 2062.5, total_loss = 280.87, pg_loss = 109.88, baseline_loss = 176.1, entropy_loss = -5.1126, learner_queue_size = 32, train_seconds = 1.4771e+04, _tick = 5299, _time = 1.6546e+09)
[2022-06-07 14:30:02,634][root][INFO] - Step 26526720 @ 2047.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 26526720, mean_episode_return = -0.53, mean_episode_step = 1774.1, total_loss = -233.16, pg_loss = -246.62, baseline_loss = 18.634, entropy_loss = -5.1741, learner_queue_size = 32, train_seconds = 1.4776e+04, _tick = 5300, _time = 1.6546e+09)
[2022-06-07 14:30:07,640][root][INFO] - Step 26536960 @ 2045.6 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 26536960, mean_episode_return = None, mean_episode_step = 2706.6, total_loss = -159.8, pg_loss = -156.34, baseline_loss = 1.8257, entropy_loss = -5.2882, learner_queue_size = 32, train_seconds = 1.4781e+04, _tick = 5302, _time = 1.6546e+09)
[2022-06-07 14:30:12,642][root][INFO] - Step 26544640 @ 1535.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 26544640, mean_episode_return = 25.28, mean_episode_step = 2575.1, total_loss = 56.1, pg_loss = 1.0452, baseline_loss = 60.323, entropy_loss = -5.2684, learner_queue_size = 32, train_seconds = 1.4786e+04, _tick = 5304, _time = 1.6546e+09)
[2022-06-07 14:30:17,648][root][INFO] - Step 26554880 @ 2045.6 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 26554880, mean_episode_return = 35.782, mean_episode_step = 2509.8, total_loss = 102.86, pg_loss = 23.069, baseline_loss = 85.064, entropy_loss = -5.2704, learner_queue_size = 32, train_seconds = 1.4791e+04, _tick = 5307, _time = 1.6546e+09)
[2022-06-07 14:30:22,654][root][INFO] - Step 26562560 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26562560, mean_episode_return = 28.269, mean_episode_step = 2053.3, total_loss = 185.27, pg_loss = 71.933, baseline_loss = 118.71, entropy_loss = -5.3776, learner_queue_size = 32, train_seconds = 1.4796e+04, _tick = 5309, _time = 1.6546e+09)
[2022-06-07 14:30:27,658][root][INFO] - Step 26572800 @ 2046.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26572800, mean_episode_return = 58.763, mean_episode_step = 2368.3, total_loss = 47.544, pg_loss = -6.7192, baseline_loss = 59.576, entropy_loss = -5.3127, learner_queue_size = 32, train_seconds = 1.4801e+04, _tick = 5312, _time = 1.6546e+09)
[2022-06-07 14:30:32,662][root][INFO] - Step 26580480 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26580480, mean_episode_return = 76.379, mean_episode_step = 2032.3, total_loss = 12.335, pg_loss = -45.114, baseline_loss = 62.785, entropy_loss = -5.336, learner_queue_size = 32, train_seconds = 1.4806e+04, _tick = 5314, _time = 1.6546e+09)
[2022-06-07 14:30:37,668][root][INFO] - Step 26590720 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26590720, mean_episode_return = None, mean_episode_step = 2401.2, total_loss = 268.86, pg_loss = 181.32, baseline_loss = 92.83, entropy_loss = -5.2889, learner_queue_size = 32, train_seconds = 1.4811e+04, _tick = 5315, _time = 1.6546e+09)
[2022-06-07 14:30:42,674][root][INFO] - Step 26598400 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26598400, mean_episode_return = 37.821, mean_episode_step = 1846.5, total_loss = 132.07, pg_loss = 64.225, baseline_loss = 73.128, entropy_loss = -5.2786, learner_queue_size = 32, train_seconds = 1.4816e+04, _tick = 5317, _time = 1.6546e+09)
[2022-06-07 14:30:47,680][root][INFO] - Step 26608640 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26608640, mean_episode_return = None, mean_episode_step = 1706.8, total_loss = 156.44, pg_loss = 101.27, baseline_loss = 60.471, entropy_loss = -5.3027, learner_queue_size = 32, train_seconds = 1.4821e+04, _tick = 5318, _time = 1.6546e+09)
[2022-06-07 14:30:52,686][root][INFO] - Step 26616320 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26616320, mean_episode_return = 62.091, mean_episode_step = 2276.0, total_loss = 30.97, pg_loss = 9.4572, baseline_loss = 26.81, entropy_loss = -5.2975, learner_queue_size = 32, train_seconds = 1.4826e+04, _tick = 5320, _time = 1.6546e+09)
[2022-06-07 14:30:57,690][root][INFO] - Step 26624000 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 26624000, mean_episode_return = 40.94, mean_episode_step = 2199.3, total_loss = -185.06, pg_loss = -194.01, baseline_loss = 14.241, entropy_loss = -5.2853, learner_queue_size = 32, train_seconds = 1.4831e+04, _tick = 5322, _time = 1.6546e+09)
[2022-06-07 14:31:02,695][root][INFO] - Step 26634240 @ 2045.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 26634240, mean_episode_return = 125.74, mean_episode_step = 1960.8, total_loss = 93.852, pg_loss = -30.596, baseline_loss = 129.75, entropy_loss = -5.3067, learner_queue_size = 32, train_seconds = 1.4836e+04, _tick = 5326, _time = 1.6546e+09)
[2022-06-07 14:31:07,706][root][INFO] - Step 26644480 @ 2043.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 26644480, mean_episode_return = None, mean_episode_step = 1843.0, total_loss = 27.735, pg_loss = 2.6718, baseline_loss = 30.377, entropy_loss = -5.3142, learner_queue_size = 32, train_seconds = 1.4841e+04, _tick = 5327, _time = 1.6546e+09)
[2022-06-07 14:31:12,710][root][INFO] - Step 26652160 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26652160, mean_episode_return = 168.39, mean_episode_step = 2412.3, total_loss = 142.15, pg_loss = 69.874, baseline_loss = 77.475, entropy_loss = -5.2001, learner_queue_size = 32, train_seconds = 1.4846e+04, _tick = 5329, _time = 1.6546e+09)
[2022-06-07 14:31:17,714][root][INFO] - Step 26662400 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 26662400, mean_episode_return = 53.749, mean_episode_step = 2090.1, total_loss = -284.14, pg_loss = -322.94, baseline_loss = 44.09, entropy_loss = -5.2869, learner_queue_size = 32, train_seconds = 1.4851e+04, _tick = 5333, _time = 1.6546e+09)
[2022-06-07 14:31:22,718][root][INFO] - Step 26670080 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 26670080, mean_episode_return = None, mean_episode_step = 2186.5, total_loss = 436.68, pg_loss = 336.65, baseline_loss = 105.3, entropy_loss = -5.261, learner_queue_size = 32, train_seconds = 1.4856e+04, _tick = 5335, _time = 1.6546e+09)
[2022-06-07 14:31:27,722][root][INFO] - Step 26677760 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 26677760, mean_episode_return = 5.9791, mean_episode_step = 2020.5, total_loss = -153.11, pg_loss = -168.38, baseline_loss = 20.536, entropy_loss = -5.2595, learner_queue_size = 32, train_seconds = 1.4861e+04, _tick = 5338, _time = 1.6546e+09)
[2022-06-07 14:31:32,726][root][INFO] - Step 26688000 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26688000, mean_episode_return = None, mean_episode_step = 1523.5, total_loss = -135.67, pg_loss = -133.76, baseline_loss = 3.313, entropy_loss = -5.2248, learner_queue_size = 32, train_seconds = 1.4866e+04, _tick = 5339, _time = 1.6546e+09)
[2022-06-07 14:31:37,730][root][INFO] - Step 26695680 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 26695680, mean_episode_return = 35.71, mean_episode_step = 2486.3, total_loss = -53.326, pg_loss = -84.261, baseline_loss = 36.151, entropy_loss = -5.2171, learner_queue_size = 32, train_seconds = 1.4871e+04, _tick = 5342, _time = 1.6546e+09)
[2022-06-07 14:31:42,734][root][INFO] - Step 26705920 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26705920, mean_episode_return = 52.832, mean_episode_step = 2087.2, total_loss = 374.24, pg_loss = 272.49, baseline_loss = 106.91, entropy_loss = -5.1636, learner_queue_size = 32, train_seconds = 1.4876e+04, _tick = 5345, _time = 1.6546e+09)
[2022-06-07 14:31:47,738][root][INFO] - Step 26713600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26713600, mean_episode_return = 38.955, mean_episode_step = 2238.0, total_loss = 59.397, pg_loss = -0.18052, baseline_loss = 64.777, entropy_loss = -5.1991, learner_queue_size = 32, train_seconds = 1.4881e+04, _tick = 5347, _time = 1.6546e+09)
[2022-06-07 14:31:52,742][root][INFO] - Step 26723840 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26723840, mean_episode_return = 71.633, mean_episode_step = 1765.1, total_loss = 36.657, pg_loss = -49.943, baseline_loss = 91.783, entropy_loss = -5.1826, learner_queue_size = 32, train_seconds = 1.4886e+04, _tick = 5350, _time = 1.6546e+09)
[2022-06-07 14:31:57,746][root][INFO] - Step 26731520 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26731520, mean_episode_return = None, mean_episode_step = 1985.8, total_loss = 98.32, pg_loss = 70.573, baseline_loss = 32.937, entropy_loss = -5.1894, learner_queue_size = 32, train_seconds = 1.4891e+04, _tick = 5352, _time = 1.6546e+09)
[2022-06-07 14:32:02,752][root][INFO] - Step 26741760 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 26741760, mean_episode_return = None, mean_episode_step = 1559.2, total_loss = 432.06, pg_loss = 332.4, baseline_loss = 104.92, entropy_loss = -5.2591, learner_queue_size = 32, train_seconds = 1.4896e+04, _tick = 5354, _time = 1.6546e+09)
[2022-06-07 14:32:07,758][root][INFO] - Step 26749440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26749440, mean_episode_return = None, mean_episode_step = 2113.4, total_loss = -166.14, pg_loss = -164.22, baseline_loss = 3.286, entropy_loss = -5.2102, learner_queue_size = 32, train_seconds = 1.4901e+04, _tick = 5355, _time = 1.6546e+09)
[2022-06-07 14:32:12,762][root][INFO] - Step 26759680 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 26759680, mean_episode_return = 37.693, mean_episode_step = 1951.8, total_loss = 443.71, pg_loss = 272.72, baseline_loss = 176.16, entropy_loss = -5.17, learner_queue_size = 32, train_seconds = 1.4906e+04, _tick = 5357, _time = 1.6546e+09)
[2022-06-07 14:32:17,766][root][INFO] - Step 26767360 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 26767360, mean_episode_return = None, mean_episode_step = 2042.4, total_loss = -19.776, pg_loss = -43.49, baseline_loss = 28.895, entropy_loss = -5.181, learner_queue_size = 32, train_seconds = 1.4911e+04, _tick = 5358, _time = 1.6546e+09)
[2022-06-07 14:32:22,770][root][INFO] - Step 26777600 @ 2046.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 26777600, mean_episode_return = 25.095, mean_episode_step = 1805.8, total_loss = 339.34, pg_loss = 201.17, baseline_loss = 143.39, entropy_loss = -5.2134, learner_queue_size = 32, train_seconds = 1.4916e+04, _tick = 5361, _time = 1.6546e+09)
[2022-06-07 14:32:27,776][root][INFO] - Step 26785280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26785280, mean_episode_return = -1.3201, mean_episode_step = 2202.8, total_loss = 63.435, pg_loss = -25.811, baseline_loss = 94.435, entropy_loss = -5.1894, learner_queue_size = 32, train_seconds = 1.4921e+04, _tick = 5363, _time = 1.6546e+09)
[2022-06-07 14:32:32,782][root][INFO] - Step 26795520 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 26795520, mean_episode_return = 81.444, mean_episode_step = 1889.2, total_loss = 73.115, pg_loss = 30.929, baseline_loss = 47.433, entropy_loss = -5.2482, learner_queue_size = 32, train_seconds = 1.4926e+04, _tick = 5367, _time = 1.6546e+09)
[2022-06-07 14:32:37,788][root][INFO] - Step 26803200 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 26803200, mean_episode_return = 66.246, mean_episode_step = 1771.2, total_loss = 62.115, pg_loss = 11.515, baseline_loss = 55.943, entropy_loss = -5.3426, learner_queue_size = 32, train_seconds = 1.4931e+04, _tick = 5369, _time = 1.6546e+09)
[2022-06-07 14:32:42,794][root][INFO] - Step 26813440 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 26813440, mean_episode_return = 44.692, mean_episode_step = 2020.2, total_loss = 375.53, pg_loss = 249.79, baseline_loss = 131.03, entropy_loss = -5.2916, learner_queue_size = 32, train_seconds = 1.4936e+04, _tick = 5373, _time = 1.6546e+09)
[2022-06-07 14:32:47,798][root][INFO] - Step 26821120 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26821120, mean_episode_return = None, mean_episode_step = 2693.8, total_loss = -143.18, pg_loss = -140.74, baseline_loss = 2.9032, entropy_loss = -5.348, learner_queue_size = 32, train_seconds = 1.4941e+04, _tick = 5374, _time = 1.6546e+09)
[2022-06-07 14:32:52,802][root][INFO] - Step 26831360 @ 2046.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 26831360, mean_episode_return = None, mean_episode_step = 1943.8, total_loss = 162.97, pg_loss = 114.24, baseline_loss = 54.074, entropy_loss = -5.3432, learner_queue_size = 32, train_seconds = 1.4946e+04, _tick = 5376, _time = 1.6546e+09)
[2022-06-07 14:32:57,806][root][INFO] - Step 26839040 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 26839040, mean_episode_return = 45.523, mean_episode_step = 2122.0, total_loss = -65.925, pg_loss = -119.81, baseline_loss = 59.191, entropy_loss = -5.3056, learner_queue_size = 32, train_seconds = 1.4951e+04, _tick = 5379, _time = 1.6546e+09)
[2022-06-07 14:33:02,818][root][INFO] - Step 26849280 @ 2043.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 26849280, mean_episode_return = None, mean_episode_step = 2069.2, total_loss = 194.51, pg_loss = 96.226, baseline_loss = 103.57, entropy_loss = -5.2877, learner_queue_size = 32, train_seconds = 1.4956e+04, _tick = 5381, _time = 1.6546e+09)
[2022-06-07 14:33:07,822][root][INFO] - Step 26856960 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 26856960, mean_episode_return = 18.481, mean_episode_step = 1427.1, total_loss = 71.912, pg_loss = -32.367, baseline_loss = 109.61, entropy_loss = -5.3254, learner_queue_size = 32, train_seconds = 1.4961e+04, _tick = 5383, _time = 1.6546e+09)
[2022-06-07 14:33:12,828][root][INFO] - Step 26864640 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26864640, mean_episode_return = 21.06, mean_episode_step = 2073.1, total_loss = -90.322, pg_loss = -117.61, baseline_loss = 32.641, entropy_loss = -5.3535, learner_queue_size = 32, train_seconds = 1.4966e+04, _tick = 5385, _time = 1.6546e+09)
[2022-06-07 14:33:17,830][root][INFO] - Step 26874880 @ 2047.2 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 26874880, mean_episode_return = -36.88, mean_episode_step = 1542.0, total_loss = -4.0812, pg_loss = -27.579, baseline_loss = 28.821, entropy_loss = -5.3238, learner_queue_size = 32, train_seconds = 1.4971e+04, _tick = 5388, _time = 1.6546e+09)
[2022-06-07 14:33:22,836][root][INFO] - Step 26882560 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26882560, mean_episode_return = 17.16, mean_episode_step = 1582.4, total_loss = 262.26, pg_loss = 163.15, baseline_loss = 104.39, entropy_loss = -5.2768, learner_queue_size = 32, train_seconds = 1.4976e+04, _tick = 5390, _time = 1.6546e+09)
[2022-06-07 14:33:27,842][root][INFO] - Step 26892800 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 26892800, mean_episode_return = 74.7, mean_episode_step = 1453.9, total_loss = 137.93, pg_loss = 49.492, baseline_loss = 93.639, entropy_loss = -5.202, learner_queue_size = 32, train_seconds = 1.4981e+04, _tick = 5394, _time = 1.6546e+09)
[2022-06-07 14:33:32,848][root][INFO] - Step 26900480 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 26900480, mean_episode_return = 52.511, mean_episode_step = 1309.0, total_loss = 28.629, pg_loss = -12.384, baseline_loss = 46.15, entropy_loss = -5.1373, learner_queue_size = 32, train_seconds = 1.4986e+04, _tick = 5396, _time = 1.6546e+09)
[2022-06-07 14:33:37,854][root][INFO] - Step 26910720 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 26910720, mean_episode_return = 56.082, mean_episode_step = 1841.8, total_loss = -214.99, pg_loss = -222.8, baseline_loss = 12.913, entropy_loss = -5.1055, learner_queue_size = 32, train_seconds = 1.4991e+04, _tick = 5399, _time = 1.6546e+09)
[2022-06-07 14:33:42,861][root][INFO] - Step 26920960 @ 2045.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 26920960, mean_episode_return = None, mean_episode_step = 1565.8, total_loss = 368.36, pg_loss = 255.22, baseline_loss = 118.28, entropy_loss = -5.1348, learner_queue_size = 32, train_seconds = 1.4996e+04, _tick = 5400, _time = 1.6546e+09)
[2022-06-07 14:33:47,866][root][INFO] - Step 26928640 @ 1534.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 26928640, mean_episode_return = 42.371, mean_episode_step = 1425.8, total_loss = 188.01, pg_loss = 116.46, baseline_loss = 76.608, entropy_loss = -5.0647, learner_queue_size = 32, train_seconds = 1.5001e+04, _tick = 5403, _time = 1.6546e+09)
[2022-06-07 14:33:52,870][root][INFO] - Step 26938880 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 26938880, mean_episode_return = None, mean_episode_step = 1994.0, total_loss = -241.79, pg_loss = -238.65, baseline_loss = 1.9909, entropy_loss = -5.131, learner_queue_size = 32, train_seconds = 1.5006e+04, _tick = 5405, _time = 1.6546e+09)
[2022-06-07 14:33:57,874][root][INFO] - Step 26946560 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 26946560, mean_episode_return = None, mean_episode_step = 1680.9, total_loss = -112.7, pg_loss = -115.79, baseline_loss = 8.1421, entropy_loss = -5.057, learner_queue_size = 32, train_seconds = 1.5011e+04, _tick = 5406, _time = 1.6546e+09)
[2022-06-07 14:34:02,878][root][INFO] - Step 26956800 @ 2046.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 26956800, mean_episode_return = 54.2, mean_episode_step = 1786.5, total_loss = 382.22, pg_loss = 270.3, baseline_loss = 116.96, entropy_loss = -5.0371, learner_queue_size = 32, train_seconds = 1.5016e+04, _tick = 5409, _time = 1.6546e+09)
[2022-06-07 14:34:07,882][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 14:34:08,087][root][INFO] - Step 26964480 @ 1534.7 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 26964480, mean_episode_return = 10.789, mean_episode_step = 2069.0, total_loss = -46.132, pg_loss = -117.14, baseline_loss = 75.963, entropy_loss = -4.96, learner_queue_size = 32, train_seconds = 1.5021e+04, _tick = 5412, _time = 1.6546e+09)
[2022-06-07 14:34:13,090][root][INFO] - Step 26974720 @ 1966.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 26974720, mean_episode_return = None, mean_episode_step = 1881.3, total_loss = 45.76, pg_loss = 12.977, baseline_loss = 37.777, entropy_loss = -4.9936, learner_queue_size = 32, train_seconds = 1.5026e+04, _tick = 5413, _time = 1.6546e+09)
[2022-06-07 14:34:18,094][root][INFO] - Step 26982400 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 26982400, mean_episode_return = -9.2102, mean_episode_step = 1818.3, total_loss = 72.116, pg_loss = 20.123, baseline_loss = 56.994, entropy_loss = -5.0013, learner_queue_size = 32, train_seconds = 1.5031e+04, _tick = 5415, _time = 1.6546e+09)
[2022-06-07 14:34:23,098][root][INFO] - Step 26992640 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 26992640, mean_episode_return = None, mean_episode_step = 1471.0, total_loss = -133.74, pg_loss = -134.02, baseline_loss = 5.2967, entropy_loss = -5.0177, learner_queue_size = 32, train_seconds = 1.5036e+04, _tick = 5416, _time = 1.6546e+09)
[2022-06-07 14:34:28,102][root][INFO] - Step 27000320 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27000320, mean_episode_return = None, mean_episode_step = 1859.7, total_loss = 39.257, pg_loss = 5.5663, baseline_loss = 38.742, entropy_loss = -5.0512, learner_queue_size = 32, train_seconds = 1.5041e+04, _tick = 5416, _time = 1.6546e+09)
[2022-06-07 14:34:33,106][root][INFO] - Step 27010560 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27010560, mean_episode_return = None, mean_episode_step = 1635.3, total_loss = 372.22, pg_loss = 282.37, baseline_loss = 94.863, entropy_loss = -5.0149, learner_queue_size = 32, train_seconds = 1.5046e+04, _tick = 5419, _time = 1.6546e+09)
[2022-06-07 14:34:38,110][root][INFO] - Step 27018240 @ 1534.7 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 27018240, mean_episode_return = 70.284, mean_episode_step = 1244.6, total_loss = 68.407, pg_loss = -33.57, baseline_loss = 107.04, entropy_loss = -5.0669, learner_queue_size = 32, train_seconds = 1.5051e+04, _tick = 5422, _time = 1.6546e+09)
[2022-06-07 14:34:43,116][root][INFO] - Step 27028480 @ 2045.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 27028480, mean_episode_return = 65.805, mean_episode_step = 1784.6, total_loss = 69.094, pg_loss = 11.893, baseline_loss = 62.347, entropy_loss = -5.1456, learner_queue_size = 32, train_seconds = 1.5056e+04, _tick = 5426, _time = 1.6546e+09)
[2022-06-07 14:34:48,122][root][INFO] - Step 27036160 @ 1534.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 27036160, mean_episode_return = None, mean_episode_step = 1572.2, total_loss = 111.25, pg_loss = 64.625, baseline_loss = 51.824, entropy_loss = -5.1946, learner_queue_size = 32, train_seconds = 1.5061e+04, _tick = 5428, _time = 1.6546e+09)
[2022-06-07 14:34:53,126][root][INFO] - Step 27043840 @ 1534.7 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 27043840, mean_episode_return = None, mean_episode_step = 1691.0, total_loss = 148.1, pg_loss = 93.725, baseline_loss = 59.472, entropy_loss = -5.1018, learner_queue_size = 32, train_seconds = 1.5066e+04, _tick = 5429, _time = 1.6546e+09)
[2022-06-07 14:34:58,130][root][INFO] - Step 27054080 @ 2046.3 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 27054080, mean_episode_return = 71.027, mean_episode_step = 1449.9, total_loss = 434.61, pg_loss = 285.79, baseline_loss = 153.9, entropy_loss = -5.0782, learner_queue_size = 32, train_seconds = 1.5071e+04, _tick = 5431, _time = 1.6546e+09)
[2022-06-07 14:35:03,134][root][INFO] - Step 27061760 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 27061760, mean_episode_return = None, mean_episode_step = 1563.7, total_loss = 202.09, pg_loss = 73.237, baseline_loss = 133.85, entropy_loss = -5.0024, learner_queue_size = 32, train_seconds = 1.5076e+04, _tick = 5432, _time = 1.6546e+09)
[2022-06-07 14:35:08,140][root][INFO] - Step 27072000 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27072000, mean_episode_return = 23.051, mean_episode_step = 1521.6, total_loss = 186.14, pg_loss = 101.83, baseline_loss = 89.33, entropy_loss = -5.0266, learner_queue_size = 32, train_seconds = 1.5081e+04, _tick = 5436, _time = 1.6546e+09)
[2022-06-07 14:35:13,146][root][INFO] - Step 27082240 @ 2045.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 27082240, mean_episode_return = 60.2, mean_episode_step = 1698.0, total_loss = -304.51, pg_loss = -364.09, baseline_loss = 64.676, entropy_loss = -5.0984, learner_queue_size = 32, train_seconds = 1.5086e+04, _tick = 5438, _time = 1.6546e+09)
[2022-06-07 14:35:18,152][root][INFO] - Step 27089920 @ 1534.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 27089920, mean_episode_return = None, mean_episode_step = 2141.1, total_loss = 12.809, pg_loss = -31.571, baseline_loss = 49.409, entropy_loss = -5.0291, learner_queue_size = 32, train_seconds = 1.5091e+04, _tick = 5440, _time = 1.6546e+09)
[2022-06-07 14:35:23,158][root][INFO] - Step 27097600 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27097600, mean_episode_return = 72.139, mean_episode_step = 1296.4, total_loss = 72.358, pg_loss = -4.8671, baseline_loss = 82.319, entropy_loss = -5.0945, learner_queue_size = 32, train_seconds = 1.5096e+04, _tick = 5442, _time = 1.6546e+09)
[2022-06-07 14:35:28,164][root][INFO] - Step 27107840 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27107840, mean_episode_return = None, mean_episode_step = 1600.8, total_loss = 198.23, pg_loss = 117.14, baseline_loss = 86.138, entropy_loss = -5.0459, learner_queue_size = 32, train_seconds = 1.5101e+04, _tick = 5443, _time = 1.6546e+09)
[2022-06-07 14:35:33,166][root][INFO] - Step 27118080 @ 2047.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 27118080, mean_episode_return = None, mean_episode_step = 1653.2, total_loss = 44.359, pg_loss = -0.071291, baseline_loss = 49.502, entropy_loss = -5.0722, learner_queue_size = 32, train_seconds = 1.5106e+04, _tick = 5445, _time = 1.6546e+09)
[2022-06-07 14:35:38,170][root][INFO] - Step 27125760 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 27125760, mean_episode_return = 56.23, mean_episode_step = 1816.4, total_loss = -73.006, pg_loss = -90.072, baseline_loss = 22.143, entropy_loss = -5.077, learner_queue_size = 32, train_seconds = 1.5111e+04, _tick = 5448, _time = 1.6546e+09)
[2022-06-07 14:35:43,174][root][INFO] - Step 27136000 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27136000, mean_episode_return = 0.25946, mean_episode_step = 1663.1, total_loss = -61.341, pg_loss = -98.785, baseline_loss = 42.572, entropy_loss = -5.129, learner_queue_size = 32, train_seconds = 1.5116e+04, _tick = 5452, _time = 1.6546e+09)
[2022-06-07 14:35:48,178][root][INFO] - Step 27143680 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 27143680, mean_episode_return = None, mean_episode_step = 1200.5, total_loss = -87.59, pg_loss = -91.487, baseline_loss = 8.9892, entropy_loss = -5.0927, learner_queue_size = 32, train_seconds = 1.5121e+04, _tick = 5452, _time = 1.6546e+09)
[2022-06-07 14:35:53,182][root][INFO] - Step 27153920 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 27153920, mean_episode_return = -4.1903, mean_episode_step = 1381.0, total_loss = -50.055, pg_loss = -80.496, baseline_loss = 35.589, entropy_loss = -5.1483, learner_queue_size = 32, train_seconds = 1.5126e+04, _tick = 5456, _time = 1.6546e+09)
[2022-06-07 14:35:58,186][root][INFO] - Step 27161600 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 27161600, mean_episode_return = None, mean_episode_step = 1356.9, total_loss = 75.759, pg_loss = 34.852, baseline_loss = 46.039, entropy_loss = -5.1315, learner_queue_size = 32, train_seconds = 1.5131e+04, _tick = 5456, _time = 1.6546e+09)
[2022-06-07 14:36:03,190][root][INFO] - Step 27171840 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 27171840, mean_episode_return = 0.64967, mean_episode_step = 1784.3, total_loss = 97.319, pg_loss = 8.9654, baseline_loss = 93.474, entropy_loss = -5.1205, learner_queue_size = 32, train_seconds = 1.5136e+04, _tick = 5458, _time = 1.6546e+09)
[2022-06-07 14:36:08,194][root][INFO] - Step 27179520 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 27179520, mean_episode_return = 78.292, mean_episode_step = 1910.7, total_loss = -1.3351, pg_loss = -44.459, baseline_loss = 48.303, entropy_loss = -5.1797, learner_queue_size = 32, train_seconds = 1.5141e+04, _tick = 5461, _time = 1.6546e+09)
[2022-06-07 14:36:13,198][root][INFO] - Step 27189760 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 27189760, mean_episode_return = None, mean_episode_step = 1624.5, total_loss = -42.569, pg_loss = -75.219, baseline_loss = 37.876, entropy_loss = -5.2255, learner_queue_size = 32, train_seconds = 1.5146e+04, _tick = 5463, _time = 1.6546e+09)
[2022-06-07 14:36:18,204][root][INFO] - Step 27197440 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27197440, mean_episode_return = None, mean_episode_step = 2102.2, total_loss = 298.2, pg_loss = 218.6, baseline_loss = 84.79, entropy_loss = -5.1917, learner_queue_size = 32, train_seconds = 1.5151e+04, _tick = 5465, _time = 1.6546e+09)
[2022-06-07 14:36:23,210][root][INFO] - Step 27207680 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 27207680, mean_episode_return = None, mean_episode_step = 1477.6, total_loss = 269.82, pg_loss = 166.72, baseline_loss = 108.3, entropy_loss = -5.2063, learner_queue_size = 32, train_seconds = 1.5156e+04, _tick = 5468, _time = 1.6546e+09)
[2022-06-07 14:36:28,216][root][INFO] - Step 27215360 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27215360, mean_episode_return = None, mean_episode_step = 1987.8, total_loss = -66.384, pg_loss = -77.302, baseline_loss = 16.06, entropy_loss = -5.1432, learner_queue_size = 32, train_seconds = 1.5161e+04, _tick = 5470, _time = 1.6546e+09)
[2022-06-07 14:36:33,222][root][INFO] - Step 27225600 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27225600, mean_episode_return = -16.98, mean_episode_step = 1430.5, total_loss = 261.21, pg_loss = 166.3, baseline_loss = 100.14, entropy_loss = -5.2239, learner_queue_size = 32, train_seconds = 1.5166e+04, _tick = 5471, _time = 1.6546e+09)
[2022-06-07 14:36:38,226][root][INFO] - Step 27235840 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 27235840, mean_episode_return = 57.147, mean_episode_step = 1813.2, total_loss = 107.73, pg_loss = -1.2101, baseline_loss = 114.18, entropy_loss = -5.2327, learner_queue_size = 32, train_seconds = 1.5171e+04, _tick = 5475, _time = 1.6546e+09)
[2022-06-07 14:36:43,230][root][INFO] - Step 27243520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27243520, mean_episode_return = 8.7197, mean_episode_step = 1598.6, total_loss = 141.2, pg_loss = 63.759, baseline_loss = 82.602, entropy_loss = -5.1584, learner_queue_size = 32, train_seconds = 1.5176e+04, _tick = 5478, _time = 1.6546e+09)
[2022-06-07 14:36:48,234][root][INFO] - Step 27253760 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 27253760, mean_episode_return = None, mean_episode_step = 1502.5, total_loss = 87.744, pg_loss = 62.519, baseline_loss = 30.435, entropy_loss = -5.2094, learner_queue_size = 32, train_seconds = 1.5181e+04, _tick = 5481, _time = 1.6546e+09)
[2022-06-07 14:36:53,238][root][INFO] - Step 27261440 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 27261440, mean_episode_return = None, mean_episode_step = 1737.7, total_loss = -64.062, pg_loss = -80.555, baseline_loss = 21.653, entropy_loss = -5.1598, learner_queue_size = 32, train_seconds = 1.5186e+04, _tick = 5483, _time = 1.6546e+09)
[2022-06-07 14:36:58,242][root][INFO] - Step 27271680 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27271680, mean_episode_return = 38.23, mean_episode_step = 2022.5, total_loss = -106.89, pg_loss = -125.85, baseline_loss = 24.066, entropy_loss = -5.1111, learner_queue_size = 32, train_seconds = 1.5191e+04, _tick = 5486, _time = 1.6546e+09)
[2022-06-07 14:37:03,246][root][INFO] - Step 27279360 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 27279360, mean_episode_return = 83.324, mean_episode_step = 1678.7, total_loss = -51.853, pg_loss = -96.914, baseline_loss = 50.218, entropy_loss = -5.157, learner_queue_size = 32, train_seconds = 1.5196e+04, _tick = 5488, _time = 1.6546e+09)
[2022-06-07 14:37:08,250][root][INFO] - Step 27289600 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 27289600, mean_episode_return = 60.9, mean_episode_step = 1128.0, total_loss = -80.109, pg_loss = -143.87, baseline_loss = 68.882, entropy_loss = -5.1248, learner_queue_size = 32, train_seconds = 1.5201e+04, _tick = 5492, _time = 1.6546e+09)
[2022-06-07 14:37:13,254][root][INFO] - Step 27297280 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27297280, mean_episode_return = 38.051, mean_episode_step = 1314.0, total_loss = -109.22, pg_loss = -125.88, baseline_loss = 21.727, entropy_loss = -5.0632, learner_queue_size = 32, train_seconds = 1.5206e+04, _tick = 5495, _time = 1.6546e+09)
[2022-06-07 14:37:18,258][root][INFO] - Step 27307520 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 27307520, mean_episode_return = 59.139, mean_episode_step = 1377.6, total_loss = 124.76, pg_loss = 28.427, baseline_loss = 101.46, entropy_loss = -5.1234, learner_queue_size = 32, train_seconds = 1.5211e+04, _tick = 5498, _time = 1.6546e+09)
[2022-06-07 14:37:23,262][root][INFO] - Step 27315200 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27315200, mean_episode_return = 2.6196, mean_episode_step = 2565.8, total_loss = 178.0, pg_loss = 104.09, baseline_loss = 79.041, entropy_loss = -5.137, learner_queue_size = 32, train_seconds = 1.5216e+04, _tick = 5501, _time = 1.6546e+09)
[2022-06-07 14:37:28,266][root][INFO] - Step 27325440 @ 2046.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 27325440, mean_episode_return = None, mean_episode_step = 1990.9, total_loss = -174.6, pg_loss = -175.02, baseline_loss = 5.538, entropy_loss = -5.1173, learner_queue_size = 32, train_seconds = 1.5221e+04, _tick = 5502, _time = 1.6546e+09)
[2022-06-07 14:37:33,273][root][INFO] - Step 27333120 @ 1533.9 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27333120, mean_episode_return = None, mean_episode_step = 1866.6, total_loss = 275.47, pg_loss = 181.38, baseline_loss = 99.255, entropy_loss = -5.1665, learner_queue_size = 32, train_seconds = 1.5226e+04, _tick = 5502, _time = 1.6546e+09)
[2022-06-07 14:37:38,278][root][INFO] - Step 27343360 @ 2046.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27343360, mean_episode_return = 128.81, mean_episode_step = 1813.8, total_loss = 289.24, pg_loss = 163.79, baseline_loss = 130.51, entropy_loss = -5.0503, learner_queue_size = 32, train_seconds = 1.5231e+04, _tick = 5503, _time = 1.6546e+09)
[2022-06-07 14:37:43,284][root][INFO] - Step 27351040 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 27351040, mean_episode_return = 69.969, mean_episode_step = 1774.3, total_loss = 563.64, pg_loss = 365.54, baseline_loss = 203.08, entropy_loss = -4.9814, learner_queue_size = 32, train_seconds = 1.5236e+04, _tick = 5506, _time = 1.6546e+09)
[2022-06-07 14:37:48,286][root][INFO] - Step 27361280 @ 2047.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 27361280, mean_episode_return = 39.215, mean_episode_step = 1729.6, total_loss = -147.37, pg_loss = -152.32, baseline_loss = 9.941, entropy_loss = -4.9841, learner_queue_size = 32, train_seconds = 1.5241e+04, _tick = 5509, _time = 1.6546e+09)
[2022-06-07 14:37:53,292][root][INFO] - Step 27368960 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27368960, mean_episode_return = None, mean_episode_step = 1995.9, total_loss = 513.82, pg_loss = 247.38, baseline_loss = 271.46, entropy_loss = -5.0109, learner_queue_size = 32, train_seconds = 1.5246e+04, _tick = 5511, _time = 1.6546e+09)
[2022-06-07 14:37:58,298][root][INFO] - Step 27379200 @ 2045.6 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 27379200, mean_episode_return = 65.793, mean_episode_step = 1705.2, total_loss = -31.795, pg_loss = -103.32, baseline_loss = 76.564, entropy_loss = -5.0355, learner_queue_size = 32, train_seconds = 1.5251e+04, _tick = 5513, _time = 1.6546e+09)
[2022-06-07 14:38:03,302][root][INFO] - Step 27386880 @ 1534.8 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 27386880, mean_episode_return = 70.189, mean_episode_step = 1728.1, total_loss = -99.916, pg_loss = -137.53, baseline_loss = 42.624, entropy_loss = -5.0091, learner_queue_size = 32, train_seconds = 1.5256e+04, _tick = 5514, _time = 1.6546e+09)
[2022-06-07 14:38:08,308][root][INFO] - Step 27397120 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27397120, mean_episode_return = None, mean_episode_step = 1688.2, total_loss = 18.583, pg_loss = 1.819, baseline_loss = 21.634, entropy_loss = -4.8696, learner_queue_size = 32, train_seconds = 1.5261e+04, _tick = 5517, _time = 1.6546e+09)
[2022-06-07 14:38:13,314][root][INFO] - Step 27404800 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27404800, mean_episode_return = None, mean_episode_step = 1990.5, total_loss = 46.872, pg_loss = 19.34, baseline_loss = 32.419, entropy_loss = -4.8863, learner_queue_size = 32, train_seconds = 1.5266e+04, _tick = 5518, _time = 1.6546e+09)
[2022-06-07 14:38:18,318][root][INFO] - Step 27415040 @ 2046.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27415040, mean_episode_return = 102.83, mean_episode_step = 1534.7, total_loss = 240.09, pg_loss = 148.13, baseline_loss = 96.848, entropy_loss = -4.8889, learner_queue_size = 32, train_seconds = 1.5271e+04, _tick = 5520, _time = 1.6546e+09)
[2022-06-07 14:38:23,322][root][INFO] - Step 27422720 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 27422720, mean_episode_return = None, mean_episode_step = 2477.4, total_loss = 60.602, pg_loss = 32.073, baseline_loss = 33.434, entropy_loss = -4.9052, learner_queue_size = 32, train_seconds = 1.5276e+04, _tick = 5522, _time = 1.6546e+09)
[2022-06-07 14:38:28,326][root][INFO] - Step 27432960 @ 2046.4 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 27432960, mean_episode_return = None, mean_episode_step = 1772.6, total_loss = 57.891, pg_loss = 25.254, baseline_loss = 37.556, entropy_loss = -4.9189, learner_queue_size = 32, train_seconds = 1.5281e+04, _tick = 5523, _time = 1.6546e+09)
[2022-06-07 14:38:33,330][root][INFO] - Step 27440640 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 27440640, mean_episode_return = None, mean_episode_step = 2078.6, total_loss = -29.286, pg_loss = -66.159, baseline_loss = 41.859, entropy_loss = -4.9863, learner_queue_size = 32, train_seconds = 1.5286e+04, _tick = 5524, _time = 1.6546e+09)
[2022-06-07 14:38:38,336][root][INFO] - Step 27450880 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 27450880, mean_episode_return = None, mean_episode_step = 1949.8, total_loss = -121.76, pg_loss = -118.61, baseline_loss = 1.8515, entropy_loss = -4.9982, learner_queue_size = 32, train_seconds = 1.5291e+04, _tick = 5525, _time = 1.6546e+09)
[2022-06-07 14:38:43,342][root][INFO] - Step 27458560 @ 1534.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 27458560, mean_episode_return = 42.251, mean_episode_step = 2139.1, total_loss = -114.98, pg_loss = -133.88, baseline_loss = 23.951, entropy_loss = -5.0576, learner_queue_size = 32, train_seconds = 1.5296e+04, _tick = 5527, _time = 1.6546e+09)
[2022-06-07 14:38:48,348][root][INFO] - Step 27468800 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27468800, mean_episode_return = None, mean_episode_step = 2450.0, total_loss = 49.078, pg_loss = -2.5243, baseline_loss = 56.665, entropy_loss = -5.0629, learner_queue_size = 32, train_seconds = 1.5301e+04, _tick = 5530, _time = 1.6546e+09)
[2022-06-07 14:38:53,354][root][INFO] - Step 27476480 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27476480, mean_episode_return = None, mean_episode_step = 1744.8, total_loss = 203.94, pg_loss = 125.61, baseline_loss = 83.458, entropy_loss = -5.1245, learner_queue_size = 32, train_seconds = 1.5306e+04, _tick = 5532, _time = 1.6546e+09)
[2022-06-07 14:38:58,358][root][INFO] - Step 27486720 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 27486720, mean_episode_return = 55.645, mean_episode_step = 2262.8, total_loss = 16.764, pg_loss = -40.74, baseline_loss = 62.556, entropy_loss = -5.0528, learner_queue_size = 32, train_seconds = 1.5311e+04, _tick = 5535, _time = 1.6546e+09)
[2022-06-07 14:39:03,362][root][INFO] - Step 27494400 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 27494400, mean_episode_return = None, mean_episode_step = 1366.0, total_loss = 168.61, pg_loss = 109.13, baseline_loss = 64.524, entropy_loss = -5.0493, learner_queue_size = 32, train_seconds = 1.5316e+04, _tick = 5536, _time = 1.6546e+09)
[2022-06-07 14:39:08,368][root][INFO] - Step 27504640 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27504640, mean_episode_return = 35.691, mean_episode_step = 2067.8, total_loss = -22.528, pg_loss = -57.815, baseline_loss = 40.276, entropy_loss = -4.9894, learner_queue_size = 32, train_seconds = 1.5321e+04, _tick = 5538, _time = 1.6546e+09)
[2022-06-07 14:39:13,374][root][INFO] - Step 27512320 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27512320, mean_episode_return = None, mean_episode_step = 2027.8, total_loss = 131.16, pg_loss = 74.278, baseline_loss = 61.983, entropy_loss = -5.0988, learner_queue_size = 32, train_seconds = 1.5326e+04, _tick = 5538, _time = 1.6546e+09)
[2022-06-07 14:39:18,378][root][INFO] - Step 27522560 @ 2046.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 27522560, mean_episode_return = None, mean_episode_step = 1972.6, total_loss = -71.924, pg_loss = -89.76, baseline_loss = 22.907, entropy_loss = -5.0711, learner_queue_size = 32, train_seconds = 1.5331e+04, _tick = 5538, _time = 1.6546e+09)
[2022-06-07 14:39:23,382][root][INFO] - Step 27530240 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27530240, mean_episode_return = None, mean_episode_step = 2166.5, total_loss = 35.641, pg_loss = 6.1412, baseline_loss = 34.585, entropy_loss = -5.0858, learner_queue_size = 32, train_seconds = 1.5336e+04, _tick = 5538, _time = 1.6546e+09)
[2022-06-07 14:39:28,388][root][INFO] - Step 27540480 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 27540480, mean_episode_return = None, mean_episode_step = 1931.0, total_loss = -30.803, pg_loss = -33.79, baseline_loss = 8.1168, entropy_loss = -5.13, learner_queue_size = 32, train_seconds = 1.5341e+04, _tick = 5540, _time = 1.6546e+09)
[2022-06-07 14:39:33,394][root][INFO] - Step 27548160 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27548160, mean_episode_return = 34.11, mean_episode_step = 1693.8, total_loss = -77.758, pg_loss = -123.06, baseline_loss = 50.463, entropy_loss = -5.1632, learner_queue_size = 32, train_seconds = 1.5346e+04, _tick = 5543, _time = 1.6546e+09)
[2022-06-07 14:39:38,398][root][INFO] - Step 27558400 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27558400, mean_episode_return = 71.358, mean_episode_step = 2279.3, total_loss = 282.12, pg_loss = 134.23, baseline_loss = 153.0, entropy_loss = -5.1105, learner_queue_size = 32, train_seconds = 1.5351e+04, _tick = 5546, _time = 1.6546e+09)
[2022-06-07 14:39:43,402][root][INFO] - Step 27566080 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 27566080, mean_episode_return = None, mean_episode_step = 2078.6, total_loss = 97.5, pg_loss = 51.607, baseline_loss = 50.99, entropy_loss = -5.0968, learner_queue_size = 32, train_seconds = 1.5356e+04, _tick = 5547, _time = 1.6546e+09)
[2022-06-07 14:39:48,406][root][INFO] - Step 27576320 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27576320, mean_episode_return = 51.331, mean_episode_step = 2161.5, total_loss = -105.33, pg_loss = -130.7, baseline_loss = 30.497, entropy_loss = -5.1255, learner_queue_size = 32, train_seconds = 1.5361e+04, _tick = 5550, _time = 1.6546e+09)
[2022-06-07 14:39:53,410][root][INFO] - Step 27584000 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27584000, mean_episode_return = None, mean_episode_step = 2158.9, total_loss = 119.04, pg_loss = 63.903, baseline_loss = 60.26, entropy_loss = -5.1244, learner_queue_size = 32, train_seconds = 1.5366e+04, _tick = 5551, _time = 1.6546e+09)
[2022-06-07 14:39:58,414][root][INFO] - Step 27594240 @ 2046.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 27594240, mean_episode_return = 32.211, mean_episode_step = 2054.6, total_loss = 117.06, pg_loss = 27.36, baseline_loss = 94.78, entropy_loss = -5.0835, learner_queue_size = 32, train_seconds = 1.5371e+04, _tick = 5554, _time = 1.6546e+09)
[2022-06-07 14:40:03,418][root][INFO] - Step 27601920 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27601920, mean_episode_return = None, mean_episode_step = 2163.5, total_loss = 568.36, pg_loss = 409.17, baseline_loss = 164.35, entropy_loss = -5.1535, learner_queue_size = 32, train_seconds = 1.5376e+04, _tick = 5556, _time = 1.6546e+09)
[2022-06-07 14:40:08,421][root][INFO] - Step 27612160 @ 2046.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 27612160, mean_episode_return = None, mean_episode_step = 1813.8, total_loss = 69.716, pg_loss = 44.732, baseline_loss = 30.179, entropy_loss = -5.194, learner_queue_size = 32, train_seconds = 1.5381e+04, _tick = 5558, _time = 1.6546e+09)
[2022-06-07 14:40:13,426][root][INFO] - Step 27619840 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27619840, mean_episode_return = 58.071, mean_episode_step = 1684.6, total_loss = 350.67, pg_loss = 231.76, baseline_loss = 124.1, entropy_loss = -5.1935, learner_queue_size = 32, train_seconds = 1.5386e+04, _tick = 5561, _time = 1.6546e+09)
[2022-06-07 14:40:18,432][root][INFO] - Step 27630080 @ 2045.6 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 27630080, mean_episode_return = 37.342, mean_episode_step = 1563.0, total_loss = 67.792, pg_loss = -1.4195, baseline_loss = 74.374, entropy_loss = -5.162, learner_queue_size = 32, train_seconds = 1.5391e+04, _tick = 5565, _time = 1.6546e+09)
[2022-06-07 14:40:23,436][root][INFO] - Step 27637760 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 27637760, mean_episode_return = 45.538, mean_episode_step = 1803.3, total_loss = -185.51, pg_loss = -226.99, baseline_loss = 46.678, entropy_loss = -5.1933, learner_queue_size = 32, train_seconds = 1.5396e+04, _tick = 5568, _time = 1.6546e+09)
[2022-06-07 14:40:28,441][root][INFO] - Step 27645440 @ 1534.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 27645440, mean_episode_return = -3.2503, mean_episode_step = 1717.8, total_loss = 68.728, pg_loss = 16.417, baseline_loss = 57.513, entropy_loss = -5.2019, learner_queue_size = 32, train_seconds = 1.5401e+04, _tick = 5571, _time = 1.6546e+09)
[2022-06-07 14:40:33,447][root][INFO] - Step 27655680 @ 2045.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27655680, mean_episode_return = 15.525, mean_episode_step = 1844.0, total_loss = -142.68, pg_loss = -160.29, baseline_loss = 22.686, entropy_loss = -5.0749, learner_queue_size = 32, train_seconds = 1.5406e+04, _tick = 5574, _time = 1.6546e+09)
[2022-06-07 14:40:38,454][root][INFO] - Step 27665920 @ 2045.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 27665920, mean_episode_return = 20.01, mean_episode_step = 1573.1, total_loss = -56.029, pg_loss = -69.893, baseline_loss = 18.777, entropy_loss = -4.9126, learner_queue_size = 32, train_seconds = 1.5411e+04, _tick = 5577, _time = 1.6546e+09)
[2022-06-07 14:40:43,460][root][INFO] - Step 27673600 @ 1534.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 27673600, mean_episode_return = 67.798, mean_episode_step = 2299.7, total_loss = 452.56, pg_loss = 302.93, baseline_loss = 154.57, entropy_loss = -4.9502, learner_queue_size = 32, train_seconds = 1.5416e+04, _tick = 5580, _time = 1.6546e+09)
[2022-06-07 14:40:48,466][root][INFO] - Step 27683840 @ 2045.5 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 27683840, mean_episode_return = 39.245, mean_episode_step = 1822.1, total_loss = 168.84, pg_loss = 72.768, baseline_loss = 101.07, entropy_loss = -4.9955, learner_queue_size = 32, train_seconds = 1.5422e+04, _tick = 5583, _time = 1.6546e+09)
[2022-06-07 14:40:53,470][root][INFO] - Step 27691520 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 27691520, mean_episode_return = None, mean_episode_step = 1957.4, total_loss = 26.264, pg_loss = -9.9505, baseline_loss = 41.238, entropy_loss = -5.0239, learner_queue_size = 32, train_seconds = 1.5426e+04, _tick = 5584, _time = 1.6546e+09)
[2022-06-07 14:40:58,474][root][INFO] - Step 27701760 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27701760, mean_episode_return = None, mean_episode_step = 1915.3, total_loss = 74.468, pg_loss = 15.254, baseline_loss = 64.217, entropy_loss = -5.0034, learner_queue_size = 32, train_seconds = 1.5432e+04, _tick = 5587, _time = 1.6546e+09)
[2022-06-07 14:41:03,478][root][INFO] - Step 27709440 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 27709440, mean_episode_return = None, mean_episode_step = 2047.3, total_loss = 37.124, pg_loss = -9.6143, baseline_loss = 51.688, entropy_loss = -4.9497, learner_queue_size = 32, train_seconds = 1.5436e+04, _tick = 5588, _time = 1.6546e+09)
[2022-06-07 14:41:08,482][root][INFO] - Step 27719680 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 27719680, mean_episode_return = None, mean_episode_step = 1628.0, total_loss = 361.98, pg_loss = 237.72, baseline_loss = 129.13, entropy_loss = -4.8694, learner_queue_size = 32, train_seconds = 1.5442e+04, _tick = 5591, _time = 1.6546e+09)
[2022-06-07 14:41:13,486][root][INFO] - Step 27727360 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 27727360, mean_episode_return = None, mean_episode_step = 2127.4, total_loss = 113.96, pg_loss = 46.579, baseline_loss = 72.256, entropy_loss = -4.8742, learner_queue_size = 32, train_seconds = 1.5446e+04, _tick = 5592, _time = 1.6546e+09)
[2022-06-07 14:41:18,492][root][INFO] - Step 27737600 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 27737600, mean_episode_return = 65.599, mean_episode_step = 1830.0, total_loss = 169.74, pg_loss = 107.38, baseline_loss = 67.224, entropy_loss = -4.8625, learner_queue_size = 32, train_seconds = 1.5452e+04, _tick = 5595, _time = 1.6546e+09)
[2022-06-07 14:41:23,498][root][INFO] - Step 27745280 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27745280, mean_episode_return = 53.85, mean_episode_step = 2064.5, total_loss = 153.13, pg_loss = 46.578, baseline_loss = 111.46, entropy_loss = -4.9069, learner_queue_size = 32, train_seconds = 1.5456e+04, _tick = 5597, _time = 1.6546e+09)
[2022-06-07 14:41:28,502][root][INFO] - Step 27755520 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 27755520, mean_episode_return = 39.33, mean_episode_step = 1600.0, total_loss = -37.778, pg_loss = -59.957, baseline_loss = 27.074, entropy_loss = -4.8949, learner_queue_size = 32, train_seconds = 1.5462e+04, _tick = 5599, _time = 1.6546e+09)
[2022-06-07 14:41:33,506][root][INFO] - Step 27763200 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 27763200, mean_episode_return = 87.348, mean_episode_step = 2002.3, total_loss = -127.69, pg_loss = -139.59, baseline_loss = 16.824, entropy_loss = -4.9172, learner_queue_size = 32, train_seconds = 1.5466e+04, _tick = 5602, _time = 1.6546e+09)
[2022-06-07 14:41:38,512][root][INFO] - Step 27773440 @ 2045.7 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 27773440, mean_episode_return = 70.455, mean_episode_step = 1644.7, total_loss = -79.777, pg_loss = -116.77, baseline_loss = 41.923, entropy_loss = -4.9284, learner_queue_size = 32, train_seconds = 1.5472e+04, _tick = 5606, _time = 1.6546e+09)
[2022-06-07 14:41:43,518][root][INFO] - Step 27783680 @ 2045.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 27783680, mean_episode_return = 64.813, mean_episode_step = 1998.5, total_loss = -207.23, pg_loss = -231.11, baseline_loss = 28.828, entropy_loss = -4.9442, learner_queue_size = 32, train_seconds = 1.5476e+04, _tick = 5608, _time = 1.6546e+09)
[2022-06-07 14:41:48,522][root][INFO] - Step 27791360 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 27791360, mean_episode_return = None, mean_episode_step = 1888.4, total_loss = 495.96, pg_loss = 370.67, baseline_loss = 130.26, entropy_loss = -4.9765, learner_queue_size = 32, train_seconds = 1.5482e+04, _tick = 5609, _time = 1.6546e+09)
[2022-06-07 14:41:53,529][root][INFO] - Step 27801600 @ 2045.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 27801600, mean_episode_return = 49.853, mean_episode_step = 2117.4, total_loss = -174.95, pg_loss = -184.35, baseline_loss = 14.412, entropy_loss = -5.0163, learner_queue_size = 32, train_seconds = 1.5486e+04, _tick = 5611, _time = 1.6546e+09)
[2022-06-07 14:41:58,534][root][INFO] - Step 27809280 @ 1534.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 27809280, mean_episode_return = 39.732, mean_episode_step = 2415.6, total_loss = -136.36, pg_loss = -169.82, baseline_loss = 38.477, entropy_loss = -5.0262, learner_queue_size = 32, train_seconds = 1.5492e+04, _tick = 5614, _time = 1.6546e+09)
[2022-06-07 14:42:03,538][root][INFO] - Step 27819520 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 27819520, mean_episode_return = None, mean_episode_step = 1846.0, total_loss = 319.37, pg_loss = 214.13, baseline_loss = 110.23, entropy_loss = -4.9951, learner_queue_size = 32, train_seconds = 1.5496e+04, _tick = 5615, _time = 1.6546e+09)
[2022-06-07 14:42:08,544][root][INFO] - Step 27827200 @ 1534.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 27827200, mean_episode_return = None, mean_episode_step = 2514.6, total_loss = 106.5, pg_loss = 67.294, baseline_loss = 44.186, entropy_loss = -4.9822, learner_queue_size = 32, train_seconds = 1.5502e+04, _tick = 5615, _time = 1.6546e+09)
[2022-06-07 14:42:13,550][root][INFO] - Step 27837440 @ 2045.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 27837440, mean_episode_return = None, mean_episode_step = 1876.7, total_loss = -90.322, pg_loss = -121.62, baseline_loss = 36.28, entropy_loss = -4.9784, learner_queue_size = 32, train_seconds = 1.5506e+04, _tick = 5616, _time = 1.6546e+09)
[2022-06-07 14:42:18,554][root][INFO] - Step 27845120 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 27845120, mean_episode_return = 102.35, mean_episode_step = 1780.9, total_loss = 146.0, pg_loss = 70.263, baseline_loss = 80.678, entropy_loss = -4.9416, learner_queue_size = 32, train_seconds = 1.5512e+04, _tick = 5619, _time = 1.6546e+09)
[2022-06-07 14:42:23,560][root][INFO] - Step 27855360 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27855360, mean_episode_return = 17.881, mean_episode_step = 1695.1, total_loss = 87.903, pg_loss = 20.322, baseline_loss = 72.492, entropy_loss = -4.9114, learner_queue_size = 32, train_seconds = 1.5517e+04, _tick = 5622, _time = 1.6546e+09)
[2022-06-07 14:42:28,566][root][INFO] - Step 27863040 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27863040, mean_episode_return = None, mean_episode_step = 1834.6, total_loss = 324.5, pg_loss = 237.09, baseline_loss = 92.212, entropy_loss = -4.8064, learner_queue_size = 32, train_seconds = 1.5522e+04, _tick = 5623, _time = 1.6546e+09)
[2022-06-07 14:42:33,570][root][INFO] - Step 27873280 @ 2046.4 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 27873280, mean_episode_return = None, mean_episode_step = 2004.4, total_loss = 5.9171, pg_loss = -12.758, baseline_loss = 23.475, entropy_loss = -4.8006, learner_queue_size = 32, train_seconds = 1.5527e+04, _tick = 5625, _time = 1.6546e+09)
[2022-06-07 14:42:38,574][root][INFO] - Step 27880960 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 27880960, mean_episode_return = None, mean_episode_step = 1612.2, total_loss = 36.215, pg_loss = 1.2477, baseline_loss = 39.701, entropy_loss = -4.7332, learner_queue_size = 32, train_seconds = 1.5532e+04, _tick = 5626, _time = 1.6546e+09)
[2022-06-07 14:42:43,578][root][INFO] - Step 27891200 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 27891200, mean_episode_return = 88.167, mean_episode_step = 1867.1, total_loss = 81.159, pg_loss = 39.522, baseline_loss = 46.465, entropy_loss = -4.8278, learner_queue_size = 32, train_seconds = 1.5537e+04, _tick = 5629, _time = 1.6546e+09)
[2022-06-07 14:42:48,582][root][INFO] - Step 27898880 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 27898880, mean_episode_return = None, mean_episode_step = 2201.3, total_loss = -77.348, pg_loss = -80.258, baseline_loss = 7.7595, entropy_loss = -4.8496, learner_queue_size = 32, train_seconds = 1.5542e+04, _tick = 5630, _time = 1.6546e+09)
[2022-06-07 14:42:53,587][root][INFO] - Step 27909120 @ 2045.9 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 27909120, mean_episode_return = -2.61, mean_episode_step = 2036.2, total_loss = 19.099, pg_loss = -9.2087, baseline_loss = 32.977, entropy_loss = -4.6693, learner_queue_size = 32, train_seconds = 1.5547e+04, _tick = 5631, _time = 1.6546e+09)
[2022-06-07 14:42:58,593][root][INFO] - Step 27916800 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 27916800, mean_episode_return = None, mean_episode_step = 2340.1, total_loss = -158.75, pg_loss = -160.04, baseline_loss = 6.0302, entropy_loss = -4.7494, learner_queue_size = 32, train_seconds = 1.5552e+04, _tick = 5632, _time = 1.6546e+09)
[2022-06-07 14:43:03,599][root][INFO] - Step 27927040 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27927040, mean_episode_return = 7.8098, mean_episode_step = 1744.8, total_loss = 35.73, pg_loss = -46.847, baseline_loss = 87.301, entropy_loss = -4.7236, learner_queue_size = 32, train_seconds = 1.5557e+04, _tick = 5635, _time = 1.6546e+09)
[2022-06-07 14:43:08,605][root][INFO] - Step 27934720 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 27934720, mean_episode_return = 14.513, mean_episode_step = 2136.8, total_loss = -189.4, pg_loss = -195.35, baseline_loss = 10.518, entropy_loss = -4.5649, learner_queue_size = 32, train_seconds = 1.5562e+04, _tick = 5637, _time = 1.6546e+09)
[2022-06-07 14:43:13,611][root][INFO] - Step 27944960 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 27944960, mean_episode_return = None, mean_episode_step = 1836.5, total_loss = -41.86, pg_loss = -49.493, baseline_loss = 12.174, entropy_loss = -4.5412, learner_queue_size = 32, train_seconds = 1.5567e+04, _tick = 5640, _time = 1.6546e+09)
[2022-06-07 14:43:18,617][root][INFO] - Step 27952640 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 27952640, mean_episode_return = None, mean_episode_step = 1867.9, total_loss = -93.159, pg_loss = -96.331, baseline_loss = 7.735, entropy_loss = -4.5632, learner_queue_size = 32, train_seconds = 1.5572e+04, _tick = 5640, _time = 1.6546e+09)
[2022-06-07 14:43:23,622][root][INFO] - Step 27962880 @ 2046.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 27962880, mean_episode_return = None, mean_episode_step = 2315.4, total_loss = 126.85, pg_loss = 72.869, baseline_loss = 58.521, entropy_loss = -4.543, learner_queue_size = 32, train_seconds = 1.5577e+04, _tick = 5643, _time = 1.6546e+09)
[2022-06-07 14:43:28,626][root][INFO] - Step 27970560 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27970560, mean_episode_return = None, mean_episode_step = 2297.5, total_loss = -110.43, pg_loss = -106.81, baseline_loss = 0.91798, entropy_loss = -4.5406, learner_queue_size = 32, train_seconds = 1.5582e+04, _tick = 5644, _time = 1.6546e+09)
[2022-06-07 14:43:33,630][root][INFO] - Step 27980800 @ 2046.3 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 27980800, mean_episode_return = None, mean_episode_step = 1826.9, total_loss = 569.53, pg_loss = 330.62, baseline_loss = 243.42, entropy_loss = -4.5131, learner_queue_size = 32, train_seconds = 1.5587e+04, _tick = 5644, _time = 1.6546e+09)
[2022-06-07 14:43:38,634][root][INFO] - Step 27988480 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 27988480, mean_episode_return = 36.427, mean_episode_step = 2102.0, total_loss = 3.9739, pg_loss = -48.937, baseline_loss = 57.429, entropy_loss = -4.518, learner_queue_size = 32, train_seconds = 1.5592e+04, _tick = 5645, _time = 1.6546e+09)
[2022-06-07 14:43:43,640][root][INFO] - Step 27998720 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 27998720, mean_episode_return = None, mean_episode_step = 2178.7, total_loss = 118.59, pg_loss = 72.132, baseline_loss = 51.055, entropy_loss = -4.5963, learner_queue_size = 32, train_seconds = 1.5597e+04, _tick = 5647, _time = 1.6546e+09)
[2022-06-07 14:43:48,646][root][INFO] - Step 28006400 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 28006400, mean_episode_return = None, mean_episode_step = 1784.7, total_loss = 149.18, pg_loss = 101.32, baseline_loss = 52.459, entropy_loss = -4.5979, learner_queue_size = 32, train_seconds = 1.5602e+04, _tick = 5648, _time = 1.6546e+09)
[2022-06-07 14:43:53,650][root][INFO] - Step 28016640 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 28016640, mean_episode_return = 67.279, mean_episode_step = 2018.3, total_loss = -2.8319, pg_loss = -21.866, baseline_loss = 23.777, entropy_loss = -4.7434, learner_queue_size = 32, train_seconds = 1.5607e+04, _tick = 5651, _time = 1.6546e+09)
[2022-06-07 14:43:58,654][root][INFO] - Step 28024320 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 28024320, mean_episode_return = -14.42, mean_episode_step = 2435.9, total_loss = 58.352, pg_loss = 11.345, baseline_loss = 51.745, entropy_loss = -4.738, learner_queue_size = 32, train_seconds = 1.5612e+04, _tick = 5652, _time = 1.6546e+09)
[2022-06-07 14:44:03,660][root][INFO] - Step 28034560 @ 2045.5 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 28034560, mean_episode_return = 27.682, mean_episode_step = 2016.3, total_loss = 138.56, pg_loss = 78.503, baseline_loss = 64.744, entropy_loss = -4.6876, learner_queue_size = 32, train_seconds = 1.5617e+04, _tick = 5655, _time = 1.6546e+09)
[2022-06-07 14:44:08,666][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 14:44:08,837][root][INFO] - Step 28042240 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28042240, mean_episode_return = 25.773, mean_episode_step = 2433.4, total_loss = 199.72, pg_loss = 131.77, baseline_loss = 72.705, entropy_loss = -4.7474, learner_queue_size = 32, train_seconds = 1.5622e+04, _tick = 5656, _time = 1.6546e+09)
[2022-06-07 14:44:13,842][root][INFO] - Step 28052480 @ 1978.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28052480, mean_episode_return = None, mean_episode_step = 2110.5, total_loss = 127.65, pg_loss = 78.829, baseline_loss = 53.584, entropy_loss = -4.7615, learner_queue_size = 32, train_seconds = 1.5627e+04, _tick = 5658, _time = 1.6546e+09)
[2022-06-07 14:44:18,846][root][INFO] - Step 28060160 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28060160, mean_episode_return = 44.593, mean_episode_step = 2128.7, total_loss = 89.301, pg_loss = 47.33, baseline_loss = 46.684, entropy_loss = -4.7132, learner_queue_size = 32, train_seconds = 1.5632e+04, _tick = 5661, _time = 1.6546e+09)
[2022-06-07 14:44:23,850][root][INFO] - Step 28070400 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 28070400, mean_episode_return = None, mean_episode_step = 2450.9, total_loss = 177.37, pg_loss = 105.24, baseline_loss = 76.72, entropy_loss = -4.5984, learner_queue_size = 32, train_seconds = 1.5637e+04, _tick = 5663, _time = 1.6546e+09)
[2022-06-07 14:44:28,854][root][INFO] - Step 28078080 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 28078080, mean_episode_return = None, mean_episode_step = 2127.2, total_loss = 559.64, pg_loss = 318.41, baseline_loss = 245.83, entropy_loss = -4.6049, learner_queue_size = 32, train_seconds = 1.5642e+04, _tick = 5664, _time = 1.6546e+09)
[2022-06-07 14:44:33,858][root][INFO] - Step 28088320 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28088320, mean_episode_return = 57.244, mean_episode_step = 1541.2, total_loss = -89.234, pg_loss = -146.85, baseline_loss = 62.291, entropy_loss = -4.673, learner_queue_size = 32, train_seconds = 1.5647e+04, _tick = 5666, _time = 1.6546e+09)
[2022-06-07 14:44:38,864][root][INFO] - Step 28096000 @ 1534.1 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 28096000, mean_episode_return = None, mean_episode_step = 2114.1, total_loss = 316.25, pg_loss = 176.56, baseline_loss = 144.36, entropy_loss = -4.6807, learner_queue_size = 32, train_seconds = 1.5652e+04, _tick = 5668, _time = 1.6546e+09)
[2022-06-07 14:44:43,870][root][INFO] - Step 28106240 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 28106240, mean_episode_return = 52.699, mean_episode_step = 2237.1, total_loss = -86.855, pg_loss = -158.92, baseline_loss = 76.759, entropy_loss = -4.6897, learner_queue_size = 32, train_seconds = 1.5657e+04, _tick = 5672, _time = 1.6546e+09)
[2022-06-07 14:44:48,877][root][INFO] - Step 28113920 @ 1534.0 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 28113920, mean_episode_return = None, mean_episode_step = 1956.0, total_loss = 101.91, pg_loss = 31.072, baseline_loss = 75.448, entropy_loss = -4.6059, learner_queue_size = 32, train_seconds = 1.5662e+04, _tick = 5674, _time = 1.6546e+09)
[2022-06-07 14:44:53,882][root][INFO] - Step 28124160 @ 2045.8 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 28124160, mean_episode_return = None, mean_episode_step = 2468.8, total_loss = 38.656, pg_loss = -31.686, baseline_loss = 74.982, entropy_loss = -4.6398, learner_queue_size = 32, train_seconds = 1.5667e+04, _tick = 5675, _time = 1.6546e+09)
[2022-06-07 14:44:58,886][root][INFO] - Step 28131840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28131840, mean_episode_return = -7.2703, mean_episode_step = 2505.2, total_loss = -197.0, pg_loss = -195.41, baseline_loss = 3.1262, entropy_loss = -4.721, learner_queue_size = 32, train_seconds = 1.5672e+04, _tick = 5677, _time = 1.6546e+09)
[2022-06-07 14:45:03,890][root][INFO] - Step 28142080 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 28142080, mean_episode_return = None, mean_episode_step = 2216.8, total_loss = 214.76, pg_loss = 124.14, baseline_loss = 95.347, entropy_loss = -4.7331, learner_queue_size = 32, train_seconds = 1.5677e+04, _tick = 5679, _time = 1.6546e+09)
[2022-06-07 14:45:08,896][root][INFO] - Step 28152320 @ 2045.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 28152320, mean_episode_return = None, mean_episode_step = 2054.5, total_loss = 162.87, pg_loss = 120.92, baseline_loss = 46.623, entropy_loss = -4.6789, learner_queue_size = 32, train_seconds = 1.5682e+04, _tick = 5681, _time = 1.6546e+09)
[2022-06-07 14:45:13,902][root][INFO] - Step 28160000 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 28160000, mean_episode_return = 41.182, mean_episode_step = 1949.7, total_loss = -102.48, pg_loss = -128.57, baseline_loss = 30.76, entropy_loss = -4.665, learner_queue_size = 32, train_seconds = 1.5687e+04, _tick = 5684, _time = 1.6546e+09)
[2022-06-07 14:45:18,908][root][INFO] - Step 28170240 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28170240, mean_episode_return = None, mean_episode_step = 1869.7, total_loss = -94.014, pg_loss = -95.15, baseline_loss = 5.8554, entropy_loss = -4.7202, learner_queue_size = 32, train_seconds = 1.5692e+04, _tick = 5686, _time = 1.6546e+09)
[2022-06-07 14:45:23,914][root][INFO] - Step 28177920 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 28177920, mean_episode_return = 96.119, mean_episode_step = 1774.2, total_loss = -44.444, pg_loss = -74.288, baseline_loss = 34.586, entropy_loss = -4.7415, learner_queue_size = 32, train_seconds = 1.5697e+04, _tick = 5687, _time = 1.6546e+09)
[2022-06-07 14:45:28,918][root][INFO] - Step 28188160 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 28188160, mean_episode_return = 42.493, mean_episode_step = 2387.8, total_loss = -160.98, pg_loss = -161.01, baseline_loss = 4.8276, entropy_loss = -4.8002, learner_queue_size = 32, train_seconds = 1.5702e+04, _tick = 5690, _time = 1.6546e+09)
[2022-06-07 14:45:33,924][root][INFO] - Step 28195840 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28195840, mean_episode_return = 51.191, mean_episode_step = 1970.1, total_loss = 515.34, pg_loss = 366.57, baseline_loss = 153.58, entropy_loss = -4.8076, learner_queue_size = 32, train_seconds = 1.5707e+04, _tick = 5693, _time = 1.6546e+09)
[2022-06-07 14:45:38,930][root][INFO] - Step 28206080 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28206080, mean_episode_return = 54.433, mean_episode_step = 2621.5, total_loss = 378.16, pg_loss = 227.77, baseline_loss = 155.04, entropy_loss = -4.6549, learner_queue_size = 32, train_seconds = 1.5712e+04, _tick = 5695, _time = 1.6546e+09)
[2022-06-07 14:45:43,934][root][INFO] - Step 28213760 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28213760, mean_episode_return = None, mean_episode_step = 1939.0, total_loss = 628.12, pg_loss = 430.44, baseline_loss = 202.34, entropy_loss = -4.6589, learner_queue_size = 32, train_seconds = 1.5717e+04, _tick = 5697, _time = 1.6546e+09)
[2022-06-07 14:45:48,938][root][INFO] - Step 28224000 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 28224000, mean_episode_return = 37.193, mean_episode_step = 1817.6, total_loss = 141.48, pg_loss = 84.443, baseline_loss = 61.685, entropy_loss = -4.6505, learner_queue_size = 32, train_seconds = 1.5722e+04, _tick = 5700, _time = 1.6546e+09)
[2022-06-07 14:45:53,942][root][INFO] - Step 28231680 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 28231680, mean_episode_return = 32.581, mean_episode_step = 1899.7, total_loss = 40.881, pg_loss = -11.637, baseline_loss = 57.163, entropy_loss = -4.6451, learner_queue_size = 32, train_seconds = 1.5727e+04, _tick = 5703, _time = 1.6546e+09)
[2022-06-07 14:45:58,946][root][INFO] - Step 28241920 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28241920, mean_episode_return = None, mean_episode_step = 1817.3, total_loss = 9.7135, pg_loss = -10.278, baseline_loss = 24.768, entropy_loss = -4.7762, learner_queue_size = 32, train_seconds = 1.5732e+04, _tick = 5704, _time = 1.6546e+09)
[2022-06-07 14:46:03,950][root][INFO] - Step 28249600 @ 1534.7 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 28249600, mean_episode_return = 98.307, mean_episode_step = 2174.5, total_loss = 122.55, pg_loss = 38.487, baseline_loss = 88.922, entropy_loss = -4.8576, learner_queue_size = 32, train_seconds = 1.5737e+04, _tick = 5706, _time = 1.6546e+09)
[2022-06-07 14:46:08,954][root][INFO] - Step 28259840 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 28259840, mean_episode_return = None, mean_episode_step = 1788.8, total_loss = 55.782, pg_loss = 24.643, baseline_loss = 35.999, entropy_loss = -4.8603, learner_queue_size = 32, train_seconds = 1.5742e+04, _tick = 5708, _time = 1.6546e+09)
[2022-06-07 14:46:13,958][root][INFO] - Step 28267520 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28267520, mean_episode_return = 26.713, mean_episode_step = 2039.9, total_loss = -67.595, pg_loss = -125.7, baseline_loss = 62.901, entropy_loss = -4.7921, learner_queue_size = 32, train_seconds = 1.5747e+04, _tick = 5711, _time = 1.6546e+09)
[2022-06-07 14:46:18,962][root][INFO] - Step 28277760 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28277760, mean_episode_return = None, mean_episode_step = 1903.7, total_loss = -168.68, pg_loss = -166.27, baseline_loss = 2.2789, entropy_loss = -4.6963, learner_queue_size = 32, train_seconds = 1.5752e+04, _tick = 5712, _time = 1.6546e+09)
[2022-06-07 14:46:23,966][root][INFO] - Step 28285440 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28285440, mean_episode_return = 36.49, mean_episode_step = 2040.2, total_loss = 30.866, pg_loss = -4.8041, baseline_loss = 40.342, entropy_loss = -4.6721, learner_queue_size = 32, train_seconds = 1.5757e+04, _tick = 5715, _time = 1.6546e+09)
[2022-06-07 14:46:28,970][root][INFO] - Step 28295680 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 28295680, mean_episode_return = -6.1504, mean_episode_step = 1962.5, total_loss = 41.446, pg_loss = -17.289, baseline_loss = 63.537, entropy_loss = -4.8025, learner_queue_size = 32, train_seconds = 1.5762e+04, _tick = 5718, _time = 1.6546e+09)
[2022-06-07 14:46:33,974][root][INFO] - Step 28303360 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 28303360, mean_episode_return = None, mean_episode_step = 2130.2, total_loss = 204.5, pg_loss = 121.64, baseline_loss = 87.589, entropy_loss = -4.7269, learner_queue_size = 32, train_seconds = 1.5767e+04, _tick = 5720, _time = 1.6546e+09)
[2022-06-07 14:46:38,978][root][INFO] - Step 28313600 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28313600, mean_episode_return = 31.941, mean_episode_step = 1680.8, total_loss = 76.566, pg_loss = 7.1881, baseline_loss = 74.026, entropy_loss = -4.6478, learner_queue_size = 32, train_seconds = 1.5772e+04, _tick = 5723, _time = 1.6546e+09)
[2022-06-07 14:46:43,982][root][INFO] - Step 28321280 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28321280, mean_episode_return = 34.101, mean_episode_step = 2345.8, total_loss = 388.7, pg_loss = 265.0, baseline_loss = 128.43, entropy_loss = -4.7315, learner_queue_size = 32, train_seconds = 1.5777e+04, _tick = 5726, _time = 1.6546e+09)
[2022-06-07 14:46:48,986][root][INFO] - Step 28331520 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 28331520, mean_episode_return = None, mean_episode_step = 2080.5, total_loss = -52.452, pg_loss = -70.286, baseline_loss = 22.544, entropy_loss = -4.7102, learner_queue_size = 32, train_seconds = 1.5782e+04, _tick = 5728, _time = 1.6546e+09)
[2022-06-07 14:46:53,992][root][INFO] - Step 28339200 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28339200, mean_episode_return = -34.08, mean_episode_step = 1441.6, total_loss = -103.16, pg_loss = -130.33, baseline_loss = 31.931, entropy_loss = -4.755, learner_queue_size = 32, train_seconds = 1.5787e+04, _tick = 5730, _time = 1.6546e+09)
[2022-06-07 14:46:58,994][root][INFO] - Step 28349440 @ 2047.1 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 28349440, mean_episode_return = None, mean_episode_step = 1216.5, total_loss = 198.37, pg_loss = 131.24, baseline_loss = 71.718, entropy_loss = -4.5831, learner_queue_size = 32, train_seconds = 1.5792e+04, _tick = 5731, _time = 1.6546e+09)
[2022-06-07 14:47:04,000][root][INFO] - Step 28357120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28357120, mean_episode_return = None, mean_episode_step = 1939.0, total_loss = 61.602, pg_loss = 22.906, baseline_loss = 43.279, entropy_loss = -4.5829, learner_queue_size = 32, train_seconds = 1.5797e+04, _tick = 5733, _time = 1.6546e+09)
[2022-06-07 14:47:09,006][root][INFO] - Step 28367360 @ 2045.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 28367360, mean_episode_return = None, mean_episode_step = 1353.8, total_loss = 163.8, pg_loss = 90.471, baseline_loss = 77.805, entropy_loss = -4.4795, learner_queue_size = 32, train_seconds = 1.5802e+04, _tick = 5735, _time = 1.6546e+09)
[2022-06-07 14:47:14,012][root][INFO] - Step 28377600 @ 2045.5 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28377600, mean_episode_return = 35.903, mean_episode_step = 1707.3, total_loss = -20.378, pg_loss = -65.304, baseline_loss = 49.413, entropy_loss = -4.4871, learner_queue_size = 32, train_seconds = 1.5807e+04, _tick = 5739, _time = 1.6546e+09)
[2022-06-07 14:47:19,018][root][INFO] - Step 28385280 @ 1534.2 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 28385280, mean_episode_return = None, mean_episode_step = 2134.1, total_loss = -142.16, pg_loss = -153.14, baseline_loss = 15.375, entropy_loss = -4.3938, learner_queue_size = 32, train_seconds = 1.5812e+04, _tick = 5741, _time = 1.6546e+09)
[2022-06-07 14:47:24,022][root][INFO] - Step 28395520 @ 2046.3 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 28395520, mean_episode_return = 68.82, mean_episode_step = 1973.4, total_loss = 204.64, pg_loss = 141.98, baseline_loss = 67.128, entropy_loss = -4.4727, learner_queue_size = 32, train_seconds = 1.5817e+04, _tick = 5743, _time = 1.6546e+09)
[2022-06-07 14:47:29,026][root][INFO] - Step 28403200 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 28403200, mean_episode_return = 39.8, mean_episode_step = 1495.8, total_loss = 73.974, pg_loss = 1.6123, baseline_loss = 76.784, entropy_loss = -4.4226, learner_queue_size = 32, train_seconds = 1.5822e+04, _tick = 5745, _time = 1.6546e+09)
[2022-06-07 14:47:34,030][root][INFO] - Step 28413440 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 28413440, mean_episode_return = 49.196, mean_episode_step = 1413.4, total_loss = -181.34, pg_loss = -192.02, baseline_loss = 15.051, entropy_loss = -4.3629, learner_queue_size = 32, train_seconds = 1.5827e+04, _tick = 5747, _time = 1.6546e+09)
[2022-06-07 14:47:39,036][root][INFO] - Step 28423680 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28423680, mean_episode_return = 18.42, mean_episode_step = 1658.0, total_loss = -96.34, pg_loss = -120.82, baseline_loss = 28.803, entropy_loss = -4.3217, learner_queue_size = 32, train_seconds = 1.5832e+04, _tick = 5749, _time = 1.6546e+09)
[2022-06-07 14:47:44,038][root][INFO] - Step 28431360 @ 1535.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28431360, mean_episode_return = None, mean_episode_step = 1647.8, total_loss = -68.592, pg_loss = -83.459, baseline_loss = 19.202, entropy_loss = -4.3345, learner_queue_size = 32, train_seconds = 1.5837e+04, _tick = 5750, _time = 1.6546e+09)
[2022-06-07 14:47:49,044][root][INFO] - Step 28439040 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28439040, mean_episode_return = None, mean_episode_step = 1505.2, total_loss = -28.684, pg_loss = -70.594, baseline_loss = 46.31, entropy_loss = -4.3996, learner_queue_size = 32, train_seconds = 1.5842e+04, _tick = 5751, _time = 1.6546e+09)
[2022-06-07 14:47:54,050][root][INFO] - Step 28449280 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28449280, mean_episode_return = 31.902, mean_episode_step = 2345.5, total_loss = 77.745, pg_loss = -26.204, baseline_loss = 108.4, entropy_loss = -4.4481, learner_queue_size = 32, train_seconds = 1.5847e+04, _tick = 5753, _time = 1.6546e+09)
[2022-06-07 14:47:59,054][root][INFO] - Step 28459520 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 28459520, mean_episode_return = 123.36, mean_episode_step = 2566.3, total_loss = -160.26, pg_loss = -165.43, baseline_loss = 9.6209, entropy_loss = -4.4505, learner_queue_size = 32, train_seconds = 1.5852e+04, _tick = 5756, _time = 1.6546e+09)
[2022-06-07 14:48:04,060][root][INFO] - Step 28467200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28467200, mean_episode_return = None, mean_episode_step = 1945.8, total_loss = -183.4, pg_loss = -181.51, baseline_loss = 2.5428, entropy_loss = -4.4296, learner_queue_size = 32, train_seconds = 1.5857e+04, _tick = 5756, _time = 1.6546e+09)
[2022-06-07 14:48:09,066][root][INFO] - Step 28474880 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 28474880, mean_episode_return = 2.0265, mean_episode_step = 2256.5, total_loss = 26.279, pg_loss = -25.67, baseline_loss = 56.326, entropy_loss = -4.3766, learner_queue_size = 32, train_seconds = 1.5862e+04, _tick = 5759, _time = 1.6546e+09)
[2022-06-07 14:48:14,070][root][INFO] - Step 28485120 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28485120, mean_episode_return = None, mean_episode_step = 1766.1, total_loss = 315.19, pg_loss = 203.65, baseline_loss = 115.98, entropy_loss = -4.4376, learner_queue_size = 32, train_seconds = 1.5867e+04, _tick = 5760, _time = 1.6546e+09)
[2022-06-07 14:48:19,076][root][INFO] - Step 28492800 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28492800, mean_episode_return = None, mean_episode_step = 2610.3, total_loss = -74.226, pg_loss = -78.746, baseline_loss = 9.0384, entropy_loss = -4.5183, learner_queue_size = 32, train_seconds = 1.5872e+04, _tick = 5760, _time = 1.6546e+09)
[2022-06-07 14:48:24,078][root][INFO] - Step 28503040 @ 2047.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 28503040, mean_episode_return = None, mean_episode_step = 1756.0, total_loss = -52.462, pg_loss = -87.905, baseline_loss = 40.042, entropy_loss = -4.5998, learner_queue_size = 32, train_seconds = 1.5877e+04, _tick = 5761, _time = 1.6546e+09)
[2022-06-07 14:48:29,082][root][INFO] - Step 28510720 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28510720, mean_episode_return = None, mean_episode_step = 1918.2, total_loss = -56.257, pg_loss = -70.54, baseline_loss = 18.858, entropy_loss = -4.5745, learner_queue_size = 32, train_seconds = 1.5882e+04, _tick = 5763, _time = 1.6546e+09)
[2022-06-07 14:48:34,086][root][INFO] - Step 28520960 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 28520960, mean_episode_return = 64.657, mean_episode_step = 1819.0, total_loss = 287.61, pg_loss = 176.23, baseline_loss = 115.89, entropy_loss = -4.5165, learner_queue_size = 32, train_seconds = 1.5887e+04, _tick = 5765, _time = 1.6546e+09)
[2022-06-07 14:48:39,090][root][INFO] - Step 28531200 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28531200, mean_episode_return = None, mean_episode_step = 2643.1, total_loss = -33.262, pg_loss = -49.381, baseline_loss = 20.794, entropy_loss = -4.6746, learner_queue_size = 32, train_seconds = 1.5892e+04, _tick = 5767, _time = 1.6546e+09)
[2022-06-07 14:48:44,094][root][INFO] - Step 28538880 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 28538880, mean_episode_return = None, mean_episode_step = 1720.8, total_loss = 245.25, pg_loss = 146.36, baseline_loss = 103.72, entropy_loss = -4.8292, learner_queue_size = 32, train_seconds = 1.5897e+04, _tick = 5767, _time = 1.6546e+09)
[2022-06-07 14:48:49,098][root][INFO] - Step 28549120 @ 2046.3 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 28549120, mean_episode_return = None, mean_episode_step = 2081.4, total_loss = 167.32, pg_loss = 102.88, baseline_loss = 69.354, entropy_loss = -4.918, learner_queue_size = 32, train_seconds = 1.5902e+04, _tick = 5770, _time = 1.6546e+09)
[2022-06-07 14:48:54,102][root][INFO] - Step 28556800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28556800, mean_episode_return = 33.569, mean_episode_step = 1537.8, total_loss = 94.989, pg_loss = 1.6468, baseline_loss = 98.305, entropy_loss = -4.9622, learner_queue_size = 32, train_seconds = 1.5907e+04, _tick = 5772, _time = 1.6546e+09)
[2022-06-07 14:48:59,106][root][INFO] - Step 28567040 @ 2046.3 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 28567040, mean_episode_return = None, mean_episode_step = 2686.1, total_loss = 1.6881, pg_loss = -8.3279, baseline_loss = 14.866, entropy_loss = -4.8502, learner_queue_size = 32, train_seconds = 1.5912e+04, _tick = 5774, _time = 1.6546e+09)
[2022-06-07 14:49:04,110][root][INFO] - Step 28574720 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 28574720, mean_episode_return = None, mean_episode_step = 1398.2, total_loss = 84.344, pg_loss = 57.515, baseline_loss = 31.683, entropy_loss = -4.8542, learner_queue_size = 32, train_seconds = 1.5917e+04, _tick = 5776, _time = 1.6546e+09)
[2022-06-07 14:49:09,114][root][INFO] - Step 28584960 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 28584960, mean_episode_return = 45.072, mean_episode_step = 2139.2, total_loss = -2.7878, pg_loss = -45.066, baseline_loss = 46.963, entropy_loss = -4.685, learner_queue_size = 32, train_seconds = 1.5922e+04, _tick = 5779, _time = 1.6546e+09)
[2022-06-07 14:49:14,120][root][INFO] - Step 28592640 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28592640, mean_episode_return = 25.2, mean_episode_step = 2329.4, total_loss = -128.33, pg_loss = -134.0, baseline_loss = 10.436, entropy_loss = -4.7612, learner_queue_size = 32, train_seconds = 1.5927e+04, _tick = 5782, _time = 1.6546e+09)
[2022-06-07 14:49:19,127][root][INFO] - Step 28602880 @ 2045.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 28602880, mean_episode_return = 49.739, mean_episode_step = 2197.3, total_loss = -180.64, pg_loss = -193.99, baseline_loss = 18.119, entropy_loss = -4.762, learner_queue_size = 32, train_seconds = 1.5932e+04, _tick = 5785, _time = 1.6546e+09)
[2022-06-07 14:49:24,135][root][INFO] - Step 28610560 @ 1533.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 28610560, mean_episode_return = 80.619, mean_episode_step = 2220.5, total_loss = 256.17, pg_loss = 124.33, baseline_loss = 136.6, entropy_loss = -4.7681, learner_queue_size = 32, train_seconds = 1.5937e+04, _tick = 5788, _time = 1.6546e+09)
[2022-06-07 14:49:29,141][root][INFO] - Step 28618240 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28618240, mean_episode_return = None, mean_episode_step = 1707.9, total_loss = 634.1, pg_loss = 414.6, baseline_loss = 224.24, entropy_loss = -4.7433, learner_queue_size = 32, train_seconds = 1.5942e+04, _tick = 5789, _time = 1.6546e+09)
[2022-06-07 14:49:34,146][root][INFO] - Step 28628480 @ 2046.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 28628480, mean_episode_return = 51.111, mean_episode_step = 1900.8, total_loss = 303.98, pg_loss = 170.26, baseline_loss = 138.46, entropy_loss = -4.7479, learner_queue_size = 32, train_seconds = 1.5947e+04, _tick = 5792, _time = 1.6546e+09)
[2022-06-07 14:49:39,152][root][INFO] - Step 28636160 @ 1534.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 28636160, mean_episode_return = 72.228, mean_episode_step = 2024.4, total_loss = 373.71, pg_loss = 226.79, baseline_loss = 151.66, entropy_loss = -4.7513, learner_queue_size = 32, train_seconds = 1.5952e+04, _tick = 5794, _time = 1.6546e+09)
[2022-06-07 14:49:44,157][root][INFO] - Step 28646400 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 28646400, mean_episode_return = None, mean_episode_step = 2436.4, total_loss = -36.834, pg_loss = -68.565, baseline_loss = 36.4, entropy_loss = -4.6688, learner_queue_size = 32, train_seconds = 1.5957e+04, _tick = 5796, _time = 1.6546e+09)
[2022-06-07 14:49:49,163][root][INFO] - Step 28654080 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28654080, mean_episode_return = 61.487, mean_episode_step = 2629.3, total_loss = 154.62, pg_loss = 52.014, baseline_loss = 107.14, entropy_loss = -4.5409, learner_queue_size = 32, train_seconds = 1.5962e+04, _tick = 5798, _time = 1.6546e+09)
[2022-06-07 14:49:54,167][root][INFO] - Step 28664320 @ 2046.6 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 28664320, mean_episode_return = None, mean_episode_step = 2477.4, total_loss = 69.762, pg_loss = 22.522, baseline_loss = 51.701, entropy_loss = -4.4608, learner_queue_size = 32, train_seconds = 1.5967e+04, _tick = 5800, _time = 1.6546e+09)
[2022-06-07 14:49:59,170][root][INFO] - Step 28674560 @ 2046.7 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 28674560, mean_episode_return = 104.09, mean_episode_step = 2009.0, total_loss = 42.107, pg_loss = -10.299, baseline_loss = 56.841, entropy_loss = -4.4358, learner_queue_size = 32, train_seconds = 1.5972e+04, _tick = 5802, _time = 1.6546e+09)
[2022-06-07 14:50:04,174][root][INFO] - Step 28682240 @ 1534.8 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28682240, mean_episode_return = None, mean_episode_step = 2089.2, total_loss = -95.829, pg_loss = -105.78, baseline_loss = 14.424, entropy_loss = -4.4742, learner_queue_size = 32, train_seconds = 1.5977e+04, _tick = 5802, _time = 1.6546e+09)
[2022-06-07 14:50:09,178][root][INFO] - Step 28689920 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 28689920, mean_episode_return = None, mean_episode_step = 2652.3, total_loss = -170.95, pg_loss = -169.32, baseline_loss = 2.8959, entropy_loss = -4.5235, learner_queue_size = 32, train_seconds = 1.5982e+04, _tick = 5802, _time = 1.6546e+09)
[2022-06-07 14:50:14,182][root][INFO] - Step 28700160 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 28700160, mean_episode_return = 73.746, mean_episode_step = 1874.5, total_loss = 1.0361, pg_loss = -46.375, baseline_loss = 51.991, entropy_loss = -4.5795, learner_queue_size = 32, train_seconds = 1.5987e+04, _tick = 5804, _time = 1.6546e+09)
[2022-06-07 14:50:19,186][root][INFO] - Step 28707840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28707840, mean_episode_return = 25.52, mean_episode_step = 1902.0, total_loss = -31.54, pg_loss = -64.833, baseline_loss = 37.991, entropy_loss = -4.6986, learner_queue_size = 32, train_seconds = 1.5992e+04, _tick = 5806, _time = 1.6546e+09)
[2022-06-07 14:50:24,192][root][INFO] - Step 28718080 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 28718080, mean_episode_return = 65.133, mean_episode_step = 2211.9, total_loss = 111.2, pg_loss = 33.933, baseline_loss = 81.98, entropy_loss = -4.7113, learner_queue_size = 32, train_seconds = 1.5997e+04, _tick = 5808, _time = 1.6546e+09)
[2022-06-07 14:50:29,198][root][INFO] - Step 28725760 @ 1534.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 28725760, mean_episode_return = None, mean_episode_step = 2316.8, total_loss = 201.4, pg_loss = 99.252, baseline_loss = 106.83, entropy_loss = -4.6832, learner_queue_size = 32, train_seconds = 1.6002e+04, _tick = 5808, _time = 1.6546e+09)
[2022-06-07 14:50:34,202][root][INFO] - Step 28736000 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 28736000, mean_episode_return = 19.99, mean_episode_step = 1926.6, total_loss = -72.848, pg_loss = -105.56, baseline_loss = 37.585, entropy_loss = -4.8682, learner_queue_size = 32, train_seconds = 1.6007e+04, _tick = 5810, _time = 1.6546e+09)
[2022-06-07 14:50:39,206][root][INFO] - Step 28743680 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28743680, mean_episode_return = None, mean_episode_step = 2091.8, total_loss = 825.32, pg_loss = 371.29, baseline_loss = 458.96, entropy_loss = -4.9248, learner_queue_size = 32, train_seconds = 1.6012e+04, _tick = 5811, _time = 1.6546e+09)
[2022-06-07 14:50:44,210][root][INFO] - Step 28753920 @ 2046.3 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 28753920, mean_episode_return = None, mean_episode_step = 1967.7, total_loss = 53.043, pg_loss = 27.908, baseline_loss = 29.99, entropy_loss = -4.8548, learner_queue_size = 32, train_seconds = 1.6017e+04, _tick = 5813, _time = 1.6546e+09)
[2022-06-07 14:50:49,214][root][INFO] - Step 28761600 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28761600, mean_episode_return = 37.957, mean_episode_step = 2089.8, total_loss = -156.66, pg_loss = -159.36, baseline_loss = 7.7971, entropy_loss = -5.0942, learner_queue_size = 32, train_seconds = 1.6022e+04, _tick = 5816, _time = 1.6546e+09)
[2022-06-07 14:50:54,218][root][INFO] - Step 28771840 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 28771840, mean_episode_return = 51.3, mean_episode_step = 1753.1, total_loss = -66.918, pg_loss = -107.11, baseline_loss = 45.353, entropy_loss = -5.1625, learner_queue_size = 32, train_seconds = 1.6027e+04, _tick = 5820, _time = 1.6546e+09)
[2022-06-07 14:50:59,220][root][INFO] - Step 28779520 @ 1535.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 28779520, mean_episode_return = 43.801, mean_episode_step = 2512.2, total_loss = 181.15, pg_loss = 86.834, baseline_loss = 99.356, entropy_loss = -5.0384, learner_queue_size = 32, train_seconds = 1.6032e+04, _tick = 5823, _time = 1.6546e+09)
[2022-06-07 14:51:04,226][root][INFO] - Step 28787200 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28787200, mean_episode_return = 118.94, mean_episode_step = 1685.3, total_loss = 293.67, pg_loss = 190.19, baseline_loss = 108.47, entropy_loss = -4.9919, learner_queue_size = 32, train_seconds = 1.6037e+04, _tick = 5825, _time = 1.6546e+09)
[2022-06-07 14:51:09,230][root][INFO] - Step 28797440 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28797440, mean_episode_return = 25.92, mean_episode_step = 1576.4, total_loss = -186.77, pg_loss = -194.15, baseline_loss = 12.354, entropy_loss = -4.9672, learner_queue_size = 32, train_seconds = 1.6042e+04, _tick = 5828, _time = 1.6546e+09)
[2022-06-07 14:51:14,234][root][INFO] - Step 28807680 @ 2046.3 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 28807680, mean_episode_return = 26.879, mean_episode_step = 2487.6, total_loss = -138.69, pg_loss = -158.87, baseline_loss = 25.107, entropy_loss = -4.9359, learner_queue_size = 32, train_seconds = 1.6047e+04, _tick = 5831, _time = 1.6546e+09)
[2022-06-07 14:51:19,238][root][INFO] - Step 28815360 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28815360, mean_episode_return = 86.07, mean_episode_step = 1964.5, total_loss = 138.06, pg_loss = 18.737, baseline_loss = 124.35, entropy_loss = -5.0237, learner_queue_size = 32, train_seconds = 1.6052e+04, _tick = 5833, _time = 1.6546e+09)
[2022-06-07 14:51:24,242][root][INFO] - Step 28825600 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28825600, mean_episode_return = 2.9696, mean_episode_step = 1532.6, total_loss = 65.282, pg_loss = -20.133, baseline_loss = 90.301, entropy_loss = -4.8857, learner_queue_size = 32, train_seconds = 1.6057e+04, _tick = 5836, _time = 1.6546e+09)
[2022-06-07 14:51:29,246][root][INFO] - Step 28833280 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28833280, mean_episode_return = 154.81, mean_episode_step = 2163.4, total_loss = 38.433, pg_loss = 15.589, baseline_loss = 27.541, entropy_loss = -4.6972, learner_queue_size = 32, train_seconds = 1.6062e+04, _tick = 5839, _time = 1.6546e+09)
[2022-06-07 14:51:34,250][root][INFO] - Step 28843520 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28843520, mean_episode_return = 50.002, mean_episode_step = 2276.0, total_loss = -78.807, pg_loss = -110.77, baseline_loss = 36.618, entropy_loss = -4.6591, learner_queue_size = 32, train_seconds = 1.6067e+04, _tick = 5840, _time = 1.6546e+09)
[2022-06-07 14:51:39,254][root][INFO] - Step 28851200 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 28851200, mean_episode_return = 33.842, mean_episode_step = 1960.0, total_loss = -139.36, pg_loss = -146.41, baseline_loss = 11.726, entropy_loss = -4.6743, learner_queue_size = 32, train_seconds = 1.6072e+04, _tick = 5842, _time = 1.6546e+09)
[2022-06-07 14:51:44,258][root][INFO] - Step 28861440 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 28861440, mean_episode_return = 6.6795, mean_episode_step = 1879.5, total_loss = 26.838, pg_loss = -39.963, baseline_loss = 71.372, entropy_loss = -4.5716, learner_queue_size = 32, train_seconds = 1.6077e+04, _tick = 5845, _time = 1.6546e+09)
[2022-06-07 14:51:49,262][root][INFO] - Step 28869120 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 28869120, mean_episode_return = None, mean_episode_step = 1495.7, total_loss = 125.78, pg_loss = 63.636, baseline_loss = 66.776, entropy_loss = -4.6275, learner_queue_size = 32, train_seconds = 1.6082e+04, _tick = 5846, _time = 1.6546e+09)
[2022-06-07 14:51:54,266][root][INFO] - Step 28879360 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 28879360, mean_episode_return = 65.825, mean_episode_step = 2163.4, total_loss = 17.19, pg_loss = -87.501, baseline_loss = 109.44, entropy_loss = -4.7447, learner_queue_size = 32, train_seconds = 1.6087e+04, _tick = 5847, _time = 1.6546e+09)
[2022-06-07 14:51:59,272][root][INFO] - Step 28887040 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28887040, mean_episode_return = 30.07, mean_episode_step = 2071.7, total_loss = 35.152, pg_loss = -28.379, baseline_loss = 68.293, entropy_loss = -4.7619, learner_queue_size = 32, train_seconds = 1.6092e+04, _tick = 5850, _time = 1.6546e+09)
[2022-06-07 14:52:04,286][root][INFO] - Step 28897280 @ 2042.1 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 28897280, mean_episode_return = 40.634, mean_episode_step = 2121.0, total_loss = 375.98, pg_loss = 232.58, baseline_loss = 148.17, entropy_loss = -4.7642, learner_queue_size = 32, train_seconds = 1.6097e+04, _tick = 5853, _time = 1.6546e+09)
[2022-06-07 14:52:09,292][root][INFO] - Step 28904960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 28904960, mean_episode_return = None, mean_episode_step = 2145.3, total_loss = 58.924, pg_loss = 24.246, baseline_loss = 39.439, entropy_loss = -4.7611, learner_queue_size = 32, train_seconds = 1.6102e+04, _tick = 5854, _time = 1.6546e+09)
[2022-06-07 14:52:14,298][root][INFO] - Step 28915200 @ 2045.7 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 28915200, mean_episode_return = 74.86, mean_episode_step = 1677.4, total_loss = 59.696, pg_loss = 1.0395, baseline_loss = 63.525, entropy_loss = -4.8686, learner_queue_size = 32, train_seconds = 1.6107e+04, _tick = 5858, _time = 1.6546e+09)
[2022-06-07 14:52:19,304][root][INFO] - Step 28922880 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28922880, mean_episode_return = None, mean_episode_step = 2464.9, total_loss = 25.366, pg_loss = -38.775, baseline_loss = 68.897, entropy_loss = -4.7553, learner_queue_size = 32, train_seconds = 1.6112e+04, _tick = 5859, _time = 1.6546e+09)
[2022-06-07 14:52:24,310][root][INFO] - Step 28933120 @ 2045.5 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 28933120, mean_episode_return = -7.4501, mean_episode_step = 1702.5, total_loss = 331.88, pg_loss = 192.79, baseline_loss = 143.86, entropy_loss = -4.7793, learner_queue_size = 32, train_seconds = 1.6117e+04, _tick = 5862, _time = 1.6546e+09)
[2022-06-07 14:52:29,314][root][INFO] - Step 28940800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28940800, mean_episode_return = 58.185, mean_episode_step = 2046.4, total_loss = 92.021, pg_loss = -6.036, baseline_loss = 102.8, entropy_loss = -4.7409, learner_queue_size = 32, train_seconds = 1.6122e+04, _tick = 5865, _time = 1.6546e+09)
[2022-06-07 14:52:34,318][root][INFO] - Step 28951040 @ 2046.4 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 28951040, mean_episode_return = 162.64, mean_episode_step = 1841.0, total_loss = -90.957, pg_loss = -123.79, baseline_loss = 37.579, entropy_loss = -4.7504, learner_queue_size = 32, train_seconds = 1.6127e+04, _tick = 5866, _time = 1.6546e+09)
[2022-06-07 14:52:39,320][root][INFO] - Step 28958720 @ 1535.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 28958720, mean_episode_return = None, mean_episode_step = 2207.8, total_loss = -130.18, pg_loss = -130.25, baseline_loss = 4.8433, entropy_loss = -4.7755, learner_queue_size = 32, train_seconds = 1.6132e+04, _tick = 5867, _time = 1.6546e+09)
[2022-06-07 14:52:44,326][root][INFO] - Step 28968960 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 28968960, mean_episode_return = 42.681, mean_episode_step = 1963.2, total_loss = 24.663, pg_loss = -16.678, baseline_loss = 46.126, entropy_loss = -4.7852, learner_queue_size = 32, train_seconds = 1.6137e+04, _tick = 5870, _time = 1.6546e+09)
[2022-06-07 14:52:49,332][root][INFO] - Step 28976640 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 28976640, mean_episode_return = 68.721, mean_episode_step = 1994.3, total_loss = -15.775, pg_loss = -91.878, baseline_loss = 80.862, entropy_loss = -4.7591, learner_queue_size = 32, train_seconds = 1.6142e+04, _tick = 5871, _time = 1.6546e+09)
[2022-06-07 14:52:54,338][root][INFO] - Step 28986880 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 28986880, mean_episode_return = -1.75, mean_episode_step = 1697.8, total_loss = 82.753, pg_loss = 4.2502, baseline_loss = 83.189, entropy_loss = -4.6856, learner_queue_size = 32, train_seconds = 1.6147e+04, _tick = 5873, _time = 1.6546e+09)
[2022-06-07 14:52:59,342][root][INFO] - Step 28994560 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 28994560, mean_episode_return = None, mean_episode_step = 2148.0, total_loss = 320.13, pg_loss = 201.7, baseline_loss = 123.12, entropy_loss = -4.6908, learner_queue_size = 32, train_seconds = 1.6152e+04, _tick = 5875, _time = 1.6546e+09)
[2022-06-07 14:53:04,346][root][INFO] - Step 29004800 @ 2046.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 29004800, mean_episode_return = None, mean_episode_step = 1698.4, total_loss = 52.432, pg_loss = -8.5701, baseline_loss = 65.716, entropy_loss = -4.7137, learner_queue_size = 32, train_seconds = 1.6157e+04, _tick = 5878, _time = 1.6546e+09)
[2022-06-07 14:53:09,352][root][INFO] - Step 29012480 @ 1534.0 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 29012480, mean_episode_return = 43.601, mean_episode_step = 1770.6, total_loss = 22.665, pg_loss = -15.706, baseline_loss = 43.261, entropy_loss = -4.89, learner_queue_size = 32, train_seconds = 1.6162e+04, _tick = 5879, _time = 1.6546e+09)
[2022-06-07 14:53:14,358][root][INFO] - Step 29022720 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29022720, mean_episode_return = None, mean_episode_step = 1799.7, total_loss = 158.1, pg_loss = 87.708, baseline_loss = 75.34, entropy_loss = -4.9455, learner_queue_size = 32, train_seconds = 1.6167e+04, _tick = 5880, _time = 1.6546e+09)
[2022-06-07 14:53:19,362][root][INFO] - Step 29032960 @ 2046.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 29032960, mean_episode_return = None, mean_episode_step = 2164.7, total_loss = 92.043, pg_loss = 54.311, baseline_loss = 42.634, entropy_loss = -4.9023, learner_queue_size = 32, train_seconds = 1.6172e+04, _tick = 5881, _time = 1.6546e+09)
[2022-06-07 14:53:24,366][root][INFO] - Step 29040640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29040640, mean_episode_return = None, mean_episode_step = 2087.0, total_loss = 48.403, pg_loss = 19.619, baseline_loss = 33.738, entropy_loss = -4.9548, learner_queue_size = 32, train_seconds = 1.6177e+04, _tick = 5883, _time = 1.6546e+09)
[2022-06-07 14:53:29,372][root][INFO] - Step 29050880 @ 2045.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 29050880, mean_episode_return = None, mean_episode_step = 2170.4, total_loss = 68.391, pg_loss = 31.973, baseline_loss = 41.518, entropy_loss = -5.1002, learner_queue_size = 32, train_seconds = 1.6182e+04, _tick = 5884, _time = 1.6546e+09)
[2022-06-07 14:53:34,378][root][INFO] - Step 29058560 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29058560, mean_episode_return = 60.328, mean_episode_step = 2060.2, total_loss = -8.1001, pg_loss = -39.321, baseline_loss = 36.333, entropy_loss = -5.1114, learner_queue_size = 32, train_seconds = 1.6187e+04, _tick = 5885, _time = 1.6546e+09)
[2022-06-07 14:53:39,382][root][INFO] - Step 29068800 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 29068800, mean_episode_return = 53.04, mean_episode_step = 1901.5, total_loss = -307.89, pg_loss = -326.7, baseline_loss = 23.981, entropy_loss = -5.1776, learner_queue_size = 32, train_seconds = 1.6192e+04, _tick = 5888, _time = 1.6546e+09)
[2022-06-07 14:53:44,388][root][INFO] - Step 29076480 @ 1534.1 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 29076480, mean_episode_return = None, mean_episode_step = 2354.7, total_loss = -0.23971, pg_loss = -13.243, baseline_loss = 18.14, entropy_loss = -5.1373, learner_queue_size = 32, train_seconds = 1.6197e+04, _tick = 5889, _time = 1.6546e+09)
[2022-06-07 14:53:49,394][root][INFO] - Step 29086720 @ 2045.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 29086720, mean_episode_return = None, mean_episode_step = 1983.8, total_loss = 118.68, pg_loss = 75.23, baseline_loss = 48.561, entropy_loss = -5.1063, learner_queue_size = 32, train_seconds = 1.6202e+04, _tick = 5890, _time = 1.6546e+09)
[2022-06-07 14:53:54,398][root][INFO] - Step 29094400 @ 1534.9 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29094400, mean_episode_return = None, mean_episode_step = 2176.1, total_loss = 95.964, pg_loss = 65.907, baseline_loss = 35.202, entropy_loss = -5.145, learner_queue_size = 32, train_seconds = 1.6207e+04, _tick = 5892, _time = 1.6546e+09)
[2022-06-07 14:53:59,402][root][INFO] - Step 29104640 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29104640, mean_episode_return = None, mean_episode_step = 1784.1, total_loss = 254.92, pg_loss = 173.61, baseline_loss = 86.479, entropy_loss = -5.1771, learner_queue_size = 32, train_seconds = 1.6212e+04, _tick = 5894, _time = 1.6546e+09)
[2022-06-07 14:54:04,406][root][INFO] - Step 29114880 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29114880, mean_episode_return = 36.073, mean_episode_step = 1747.1, total_loss = 22.736, pg_loss = -18.698, baseline_loss = 46.495, entropy_loss = -5.0605, learner_queue_size = 32, train_seconds = 1.6217e+04, _tick = 5896, _time = 1.6546e+09)
[2022-06-07 14:54:09,410][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 14:54:09,596][root][INFO] - Step 29122560 @ 1534.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 29122560, mean_episode_return = 55.943, mean_episode_step = 2281.6, total_loss = 267.34, pg_loss = 153.9, baseline_loss = 118.52, entropy_loss = -5.0796, learner_queue_size = 32, train_seconds = 1.6222e+04, _tick = 5899, _time = 1.6546e+09)
[2022-06-07 14:54:14,598][root][INFO] - Step 29132800 @ 1973.7 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 29132800, mean_episode_return = 19.634, mean_episode_step = 1362.7, total_loss = 340.88, pg_loss = 225.0, baseline_loss = 120.86, entropy_loss = -4.9837, learner_queue_size = 32, train_seconds = 1.6228e+04, _tick = 5903, _time = 1.6546e+09)
[2022-06-07 14:54:19,602][root][INFO] - Step 29143040 @ 2046.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 29143040, mean_episode_return = 12.52, mean_episode_step = 1819.9, total_loss = 258.34, pg_loss = 158.16, baseline_loss = 105.18, entropy_loss = -4.9901, learner_queue_size = 32, train_seconds = 1.6233e+04, _tick = 5906, _time = 1.6546e+09)
[2022-06-07 14:54:24,606][root][INFO] - Step 29150720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29150720, mean_episode_return = 4.2296, mean_episode_step = 1834.5, total_loss = 501.97, pg_loss = 365.71, baseline_loss = 141.31, entropy_loss = -5.0559, learner_queue_size = 32, train_seconds = 1.6238e+04, _tick = 5909, _time = 1.6546e+09)
[2022-06-07 14:54:29,610][root][INFO] - Step 29158400 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29158400, mean_episode_return = 65.47, mean_episode_step = 2443.5, total_loss = 405.58, pg_loss = 255.52, baseline_loss = 155.05, entropy_loss = -4.9915, learner_queue_size = 32, train_seconds = 1.6243e+04, _tick = 5910, _time = 1.6546e+09)
[2022-06-07 14:54:34,616][root][INFO] - Step 29168640 @ 2045.5 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 29168640, mean_episode_return = None, mean_episode_step = 2213.5, total_loss = 227.73, pg_loss = 165.36, baseline_loss = 67.378, entropy_loss = -5.0013, learner_queue_size = 32, train_seconds = 1.6248e+04, _tick = 5911, _time = 1.6546e+09)
[2022-06-07 14:54:39,622][root][INFO] - Step 29176320 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29176320, mean_episode_return = None, mean_episode_step = 1800.1, total_loss = -147.15, pg_loss = -147.5, baseline_loss = 5.2985, entropy_loss = -4.9467, learner_queue_size = 32, train_seconds = 1.6253e+04, _tick = 5913, _time = 1.6546e+09)
[2022-06-07 14:54:44,626][root][INFO] - Step 29186560 @ 2046.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 29186560, mean_episode_return = -23.031, mean_episode_step = 2136.6, total_loss = 159.77, pg_loss = 45.683, baseline_loss = 119.03, entropy_loss = -4.9437, learner_queue_size = 32, train_seconds = 1.6258e+04, _tick = 5915, _time = 1.6546e+09)
[2022-06-07 14:54:49,630][root][INFO] - Step 29194240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29194240, mean_episode_return = None, mean_episode_step = 2152.9, total_loss = 163.79, pg_loss = 113.99, baseline_loss = 54.843, entropy_loss = -5.0371, learner_queue_size = 32, train_seconds = 1.6263e+04, _tick = 5916, _time = 1.6546e+09)
[2022-06-07 14:54:54,636][root][INFO] - Step 29204480 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29204480, mean_episode_return = 32.65, mean_episode_step = 1546.1, total_loss = 96.737, pg_loss = 45.336, baseline_loss = 56.487, entropy_loss = -5.0859, learner_queue_size = 32, train_seconds = 1.6268e+04, _tick = 5919, _time = 1.6546e+09)
[2022-06-07 14:54:59,642][root][INFO] - Step 29212160 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29212160, mean_episode_return = None, mean_episode_step = 1773.3, total_loss = 55.191, pg_loss = 4.3043, baseline_loss = 55.902, entropy_loss = -5.0147, learner_queue_size = 32, train_seconds = 1.6273e+04, _tick = 5920, _time = 1.6546e+09)
[2022-06-07 14:55:04,648][root][INFO] - Step 29222400 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29222400, mean_episode_return = 55.613, mean_episode_step = 1684.1, total_loss = 314.76, pg_loss = 171.36, baseline_loss = 148.29, entropy_loss = -4.8823, learner_queue_size = 32, train_seconds = 1.6278e+04, _tick = 5922, _time = 1.6546e+09)
[2022-06-07 14:55:09,654][root][INFO] - Step 29230080 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 29230080, mean_episode_return = None, mean_episode_step = 1970.0, total_loss = 188.12, pg_loss = 143.95, baseline_loss = 49.037, entropy_loss = -4.8664, learner_queue_size = 32, train_seconds = 1.6283e+04, _tick = 5924, _time = 1.6546e+09)
[2022-06-07 14:55:14,658][root][INFO] - Step 29240320 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 29240320, mean_episode_return = -5.2903, mean_episode_step = 1623.1, total_loss = -93.719, pg_loss = -107.64, baseline_loss = 18.802, entropy_loss = -4.8781, learner_queue_size = 32, train_seconds = 1.6288e+04, _tick = 5928, _time = 1.6546e+09)
[2022-06-07 14:55:19,664][root][INFO] - Step 29248000 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29248000, mean_episode_return = 13.255, mean_episode_step = 1939.9, total_loss = 165.53, pg_loss = 66.251, baseline_loss = 104.25, entropy_loss = -4.9666, learner_queue_size = 32, train_seconds = 1.6293e+04, _tick = 5931, _time = 1.6546e+09)
[2022-06-07 14:55:24,670][root][INFO] - Step 29258240 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29258240, mean_episode_return = 45.973, mean_episode_step = 2028.4, total_loss = 100.42, pg_loss = 0.17103, baseline_loss = 105.29, entropy_loss = -5.0376, learner_queue_size = 32, train_seconds = 1.6298e+04, _tick = 5934, _time = 1.6546e+09)
[2022-06-07 14:55:29,674][root][INFO] - Step 29268480 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 29268480, mean_episode_return = None, mean_episode_step = 2059.6, total_loss = -14.011, pg_loss = -27.021, baseline_loss = 18.026, entropy_loss = -5.0166, learner_queue_size = 32, train_seconds = 1.6303e+04, _tick = 5937, _time = 1.6546e+09)
[2022-06-07 14:55:34,680][root][INFO] - Step 29276160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29276160, mean_episode_return = 25.9, mean_episode_step = 1674.9, total_loss = 145.41, pg_loss = 73.844, baseline_loss = 76.621, entropy_loss = -5.0548, learner_queue_size = 32, train_seconds = 1.6308e+04, _tick = 5939, _time = 1.6546e+09)
[2022-06-07 14:55:39,686][root][INFO] - Step 29286400 @ 2045.6 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 29286400, mean_episode_return = 96.849, mean_episode_step = 1504.2, total_loss = 63.79, pg_loss = 14.025, baseline_loss = 54.789, entropy_loss = -5.024, learner_queue_size = 32, train_seconds = 1.6313e+04, _tick = 5940, _time = 1.6546e+09)
[2022-06-07 14:55:44,692][root][INFO] - Step 29294080 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29294080, mean_episode_return = 22.35, mean_episode_step = 1995.0, total_loss = 64.957, pg_loss = 10.862, baseline_loss = 59.125, entropy_loss = -5.0299, learner_queue_size = 32, train_seconds = 1.6318e+04, _tick = 5942, _time = 1.6546e+09)
[2022-06-07 14:55:49,698][root][INFO] - Step 29301760 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29301760, mean_episode_return = 120.98, mean_episode_step = 2260.8, total_loss = 619.14, pg_loss = 416.29, baseline_loss = 207.8, entropy_loss = -4.9555, learner_queue_size = 32, train_seconds = 1.6323e+04, _tick = 5945, _time = 1.6546e+09)
[2022-06-07 14:55:54,704][root][INFO] - Step 29312000 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29312000, mean_episode_return = 51.362, mean_episode_step = 1999.8, total_loss = 29.203, pg_loss = -31.039, baseline_loss = 65.165, entropy_loss = -4.9231, learner_queue_size = 32, train_seconds = 1.6328e+04, _tick = 5949, _time = 1.6546e+09)
[2022-06-07 14:55:59,710][root][INFO] - Step 29319680 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29319680, mean_episode_return = None, mean_episode_step = 1899.8, total_loss = -16.537, pg_loss = -30.317, baseline_loss = 18.738, entropy_loss = -4.958, learner_queue_size = 32, train_seconds = 1.6333e+04, _tick = 5949, _time = 1.6546e+09)
[2022-06-07 14:56:04,714][root][INFO] - Step 29329920 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 29329920, mean_episode_return = None, mean_episode_step = 1529.9, total_loss = 43.515, pg_loss = 21.398, baseline_loss = 27.147, entropy_loss = -5.0298, learner_queue_size = 32, train_seconds = 1.6338e+04, _tick = 5950, _time = 1.6546e+09)
[2022-06-07 14:56:09,720][root][INFO] - Step 29337600 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 29337600, mean_episode_return = 71.799, mean_episode_step = 1700.3, total_loss = -14.832, pg_loss = -74.288, baseline_loss = 64.581, entropy_loss = -5.1248, learner_queue_size = 32, train_seconds = 1.6343e+04, _tick = 5951, _time = 1.6546e+09)
[2022-06-07 14:56:14,726][root][INFO] - Step 29347840 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 29347840, mean_episode_return = None, mean_episode_step = 1798.6, total_loss = 170.83, pg_loss = 71.335, baseline_loss = 104.51, entropy_loss = -5.0136, learner_queue_size = 32, train_seconds = 1.6348e+04, _tick = 5952, _time = 1.6546e+09)
[2022-06-07 14:56:19,732][root][INFO] - Step 29355520 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29355520, mean_episode_return = None, mean_episode_step = 1803.1, total_loss = -0.26673, pg_loss = -33.779, baseline_loss = 38.464, entropy_loss = -4.9515, learner_queue_size = 32, train_seconds = 1.6353e+04, _tick = 5953, _time = 1.6546e+09)
[2022-06-07 14:56:24,738][root][INFO] - Step 29365760 @ 2045.5 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 29365760, mean_episode_return = None, mean_episode_step = 2058.1, total_loss = 179.06, pg_loss = 57.263, baseline_loss = 126.83, entropy_loss = -5.0339, learner_queue_size = 32, train_seconds = 1.6358e+04, _tick = 5954, _time = 1.6546e+09)
[2022-06-07 14:56:29,744][root][INFO] - Step 29373440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29373440, mean_episode_return = 80.146, mean_episode_step = 2032.0, total_loss = 309.54, pg_loss = 206.94, baseline_loss = 107.65, entropy_loss = -5.0492, learner_queue_size = 32, train_seconds = 1.6363e+04, _tick = 5957, _time = 1.6546e+09)
[2022-06-07 14:56:34,750][root][INFO] - Step 29383680 @ 2045.7 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 29383680, mean_episode_return = None, mean_episode_step = 1480.9, total_loss = 519.29, pg_loss = 350.25, baseline_loss = 174.12, entropy_loss = -5.0822, learner_queue_size = 32, train_seconds = 1.6368e+04, _tick = 5957, _time = 1.6546e+09)
[2022-06-07 14:56:39,754][root][INFO] - Step 29391360 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29391360, mean_episode_return = None, mean_episode_step = 2268.1, total_loss = 1626.3, pg_loss = 445.18, baseline_loss = 1186.1, entropy_loss = -5.0629, learner_queue_size = 32, train_seconds = 1.6373e+04, _tick = 5957, _time = 1.6546e+09)
[2022-06-07 14:56:44,760][root][INFO] - Step 29401600 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 29401600, mean_episode_return = None, mean_episode_step = 2040.8, total_loss = -38.365, pg_loss = -60.395, baseline_loss = 27.08, entropy_loss = -5.0499, learner_queue_size = 32, train_seconds = 1.6378e+04, _tick = 5957, _time = 1.6546e+09)
[2022-06-07 14:56:49,766][root][INFO] - Step 29409280 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 29409280, mean_episode_return = None, mean_episode_step = 2107.4, total_loss = 52.779, pg_loss = 19.758, baseline_loss = 38.046, entropy_loss = -5.0241, learner_queue_size = 32, train_seconds = 1.6383e+04, _tick = 5957, _time = 1.6546e+09)
[2022-06-07 14:56:54,770][root][INFO] - Step 29419520 @ 2046.4 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 29419520, mean_episode_return = 3.7896, mean_episode_step = 1885.2, total_loss = -122.9, pg_loss = -161.5, baseline_loss = 43.661, entropy_loss = -5.0643, learner_queue_size = 32, train_seconds = 1.6388e+04, _tick = 5959, _time = 1.6546e+09)
[2022-06-07 14:56:59,774][root][INFO] - Step 29429760 @ 2046.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 29429760, mean_episode_return = 73.766, mean_episode_step = 2297.4, total_loss = -91.506, pg_loss = -107.91, baseline_loss = 21.438, entropy_loss = -5.0325, learner_queue_size = 32, train_seconds = 1.6393e+04, _tick = 5962, _time = 1.6546e+09)
[2022-06-07 14:57:04,778][root][INFO] - Step 29437440 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29437440, mean_episode_return = 71.953, mean_episode_step = 1720.8, total_loss = -46.181, pg_loss = -100.68, baseline_loss = 59.549, entropy_loss = -5.0526, learner_queue_size = 32, train_seconds = 1.6398e+04, _tick = 5964, _time = 1.6546e+09)
[2022-06-07 14:57:09,783][root][INFO] - Step 29447680 @ 2045.9 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 29447680, mean_episode_return = 68.623, mean_episode_step = 2045.9, total_loss = 138.95, pg_loss = 9.3277, baseline_loss = 134.74, entropy_loss = -5.1171, learner_queue_size = 32, train_seconds = 1.6403e+04, _tick = 5966, _time = 1.6546e+09)
[2022-06-07 14:57:14,786][root][INFO] - Step 29455360 @ 1535.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29455360, mean_episode_return = None, mean_episode_step = 1966.9, total_loss = 38.311, pg_loss = -21.775, baseline_loss = 65.188, entropy_loss = -5.1017, learner_queue_size = 32, train_seconds = 1.6408e+04, _tick = 5966, _time = 1.6546e+09)
[2022-06-07 14:57:19,790][root][INFO] - Step 29465600 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29465600, mean_episode_return = 47.408, mean_episode_step = 1704.4, total_loss = -43.478, pg_loss = -72.325, baseline_loss = 33.924, entropy_loss = -5.0767, learner_queue_size = 32, train_seconds = 1.6413e+04, _tick = 5969, _time = 1.6546e+09)
[2022-06-07 14:57:24,796][root][INFO] - Step 29473280 @ 1534.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29473280, mean_episode_return = 46.56, mean_episode_step = 2092.0, total_loss = 444.49, pg_loss = 249.16, baseline_loss = 200.46, entropy_loss = -5.1317, learner_queue_size = 32, train_seconds = 1.6418e+04, _tick = 5972, _time = 1.6546e+09)
[2022-06-07 14:57:29,802][root][INFO] - Step 29483520 @ 2045.7 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 29483520, mean_episode_return = 120.34, mean_episode_step = 1815.4, total_loss = -101.47, pg_loss = -114.51, baseline_loss = 18.237, entropy_loss = -5.1948, learner_queue_size = 32, train_seconds = 1.6423e+04, _tick = 5975, _time = 1.6546e+09)
[2022-06-07 14:57:34,808][root][INFO] - Step 29491200 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29491200, mean_episode_return = 80.707, mean_episode_step = 1909.1, total_loss = 495.27, pg_loss = 234.65, baseline_loss = 265.88, entropy_loss = -5.2629, learner_queue_size = 32, train_seconds = 1.6428e+04, _tick = 5978, _time = 1.6546e+09)
[2022-06-07 14:57:39,812][root][INFO] - Step 29498880 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29498880, mean_episode_return = 62.634, mean_episode_step = 1917.3, total_loss = 175.47, pg_loss = 115.87, baseline_loss = 64.876, entropy_loss = -5.2797, learner_queue_size = 32, train_seconds = 1.6433e+04, _tick = 5981, _time = 1.6546e+09)
[2022-06-07 14:57:44,818][root][INFO] - Step 29509120 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 29509120, mean_episode_return = 87.928, mean_episode_step = 1777.7, total_loss = -52.736, pg_loss = -70.204, baseline_loss = 22.569, entropy_loss = -5.1014, learner_queue_size = 32, train_seconds = 1.6438e+04, _tick = 5985, _time = 1.6546e+09)
[2022-06-07 14:57:49,824][root][INFO] - Step 29516800 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29516800, mean_episode_return = 5.0196, mean_episode_step = 2139.0, total_loss = 213.24, pg_loss = 148.26, baseline_loss = 70.121, entropy_loss = -5.142, learner_queue_size = 32, train_seconds = 1.6443e+04, _tick = 5986, _time = 1.6546e+09)
[2022-06-07 14:57:54,830][root][INFO] - Step 29527040 @ 2045.5 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 29527040, mean_episode_return = 10.4, mean_episode_step = 1668.7, total_loss = 425.09, pg_loss = 264.83, baseline_loss = 165.29, entropy_loss = -5.0311, learner_queue_size = 32, train_seconds = 1.6448e+04, _tick = 5990, _time = 1.6546e+09)
[2022-06-07 14:57:59,834][root][INFO] - Step 29534720 @ 1534.9 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 29534720, mean_episode_return = None, mean_episode_step = 1740.8, total_loss = 323.21, pg_loss = 232.34, baseline_loss = 95.839, entropy_loss = -4.9741, learner_queue_size = 32, train_seconds = 1.6453e+04, _tick = 5992, _time = 1.6546e+09)
[2022-06-07 14:58:04,840][root][INFO] - Step 29544960 @ 2045.5 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 29544960, mean_episode_return = 4.0497, mean_episode_step = 1924.0, total_loss = -112.2, pg_loss = -119.91, baseline_loss = 12.725, entropy_loss = -5.0133, learner_queue_size = 32, train_seconds = 1.6458e+04, _tick = 5996, _time = 1.6546e+09)
[2022-06-07 14:58:09,846][root][INFO] - Step 29552640 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29552640, mean_episode_return = None, mean_episode_step = 1785.5, total_loss = 325.74, pg_loss = 198.61, baseline_loss = 132.1, entropy_loss = -4.9601, learner_queue_size = 32, train_seconds = 1.6463e+04, _tick = 5998, _time = 1.6546e+09)
[2022-06-07 14:58:14,852][root][INFO] - Step 29562880 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29562880, mean_episode_return = 51.208, mean_episode_step = 1565.6, total_loss = -101.5, pg_loss = -139.74, baseline_loss = 43.082, entropy_loss = -4.845, learner_queue_size = 32, train_seconds = 1.6468e+04, _tick = 6000, _time = 1.6546e+09)
[2022-06-07 14:58:19,853][root][INFO] - Step 29570560 @ 1535.7 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 29570560, mean_episode_return = 144.04, mean_episode_step = 1724.5, total_loss = 249.04, pg_loss = 160.38, baseline_loss = 93.539, entropy_loss = -4.8827, learner_queue_size = 32, train_seconds = 1.6473e+04, _tick = 6002, _time = 1.6546e+09)
[2022-06-07 14:58:24,859][root][INFO] - Step 29580800 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29580800, mean_episode_return = 34.57, mean_episode_step = 1771.7, total_loss = 12.564, pg_loss = -59.986, baseline_loss = 77.49, entropy_loss = -4.94, learner_queue_size = 32, train_seconds = 1.6478e+04, _tick = 6005, _time = 1.6546e+09)
[2022-06-07 14:58:29,862][root][INFO] - Step 29591040 @ 2046.8 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 29591040, mean_episode_return = 21.126, mean_episode_step = 2350.6, total_loss = -167.5, pg_loss = -179.8, baseline_loss = 17.17, entropy_loss = -4.8762, learner_queue_size = 32, train_seconds = 1.6483e+04, _tick = 6008, _time = 1.6546e+09)
[2022-06-07 14:58:34,868][root][INFO] - Step 29598720 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29598720, mean_episode_return = 47.902, mean_episode_step = 1937.0, total_loss = -27.728, pg_loss = -56.201, baseline_loss = 33.378, entropy_loss = -4.9046, learner_queue_size = 32, train_seconds = 1.6488e+04, _tick = 6010, _time = 1.6546e+09)
[2022-06-07 14:58:39,874][root][INFO] - Step 29606400 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29606400, mean_episode_return = 16.856, mean_episode_step = 2127.5, total_loss = -2.1048, pg_loss = -67.816, baseline_loss = 70.671, entropy_loss = -4.9592, learner_queue_size = 32, train_seconds = 1.6493e+04, _tick = 6012, _time = 1.6546e+09)
[2022-06-07 14:58:44,878][root][INFO] - Step 29616640 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 29616640, mean_episode_return = 60.611, mean_episode_step = 2000.5, total_loss = 79.864, pg_loss = 37.187, baseline_loss = 47.742, entropy_loss = -5.0644, learner_queue_size = 32, train_seconds = 1.6498e+04, _tick = 6016, _time = 1.6546e+09)
[2022-06-07 14:58:49,883][root][INFO] - Step 29624320 @ 1534.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29624320, mean_episode_return = 56.302, mean_episode_step = 1922.4, total_loss = 555.3, pg_loss = 199.37, baseline_loss = 361.03, entropy_loss = -5.1014, learner_queue_size = 32, train_seconds = 1.6503e+04, _tick = 6019, _time = 1.6546e+09)
[2022-06-07 14:58:54,886][root][INFO] - Step 29634560 @ 2046.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29634560, mean_episode_return = 21.51, mean_episode_step = 1701.9, total_loss = 86.233, pg_loss = 46.309, baseline_loss = 44.994, entropy_loss = -5.0701, learner_queue_size = 32, train_seconds = 1.6508e+04, _tick = 6020, _time = 1.6546e+09)
[2022-06-07 14:58:59,892][root][INFO] - Step 29644800 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 29644800, mean_episode_return = 45.014, mean_episode_step = 1841.8, total_loss = 519.61, pg_loss = 366.66, baseline_loss = 158.0, entropy_loss = -5.0541, learner_queue_size = 32, train_seconds = 1.6513e+04, _tick = 6023, _time = 1.6546e+09)
[2022-06-07 14:59:04,898][root][INFO] - Step 29652480 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29652480, mean_episode_return = None, mean_episode_step = 2810.0, total_loss = 35.237, pg_loss = 8.5005, baseline_loss = 31.701, entropy_loss = -4.9641, learner_queue_size = 32, train_seconds = 1.6518e+04, _tick = 6023, _time = 1.6546e+09)
[2022-06-07 14:59:09,902][root][INFO] - Step 29662720 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 29662720, mean_episode_return = 106.21, mean_episode_step = 2437.4, total_loss = -69.346, pg_loss = -128.49, baseline_loss = 64.236, entropy_loss = -5.0954, learner_queue_size = 32, train_seconds = 1.6523e+04, _tick = 6025, _time = 1.6546e+09)
[2022-06-07 14:59:14,918][root][INFO] - Step 29670400 @ 1531.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29670400, mean_episode_return = -1.7703, mean_episode_step = 1741.9, total_loss = -51.327, pg_loss = -57.306, baseline_loss = 11.07, entropy_loss = -5.0904, learner_queue_size = 32, train_seconds = 1.6528e+04, _tick = 6026, _time = 1.6546e+09)
[2022-06-07 14:59:19,922][root][INFO] - Step 29678080 @ 1534.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29678080, mean_episode_return = None, mean_episode_step = 2370.0, total_loss = -130.07, pg_loss = -130.16, baseline_loss = 5.2374, entropy_loss = -5.1475, learner_queue_size = 32, train_seconds = 1.6533e+04, _tick = 6027, _time = 1.6546e+09)
[2022-06-07 14:59:24,928][root][INFO] - Step 29688320 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29688320, mean_episode_return = None, mean_episode_step = 1923.3, total_loss = 329.38, pg_loss = 238.79, baseline_loss = 95.757, entropy_loss = -5.1647, learner_queue_size = 32, train_seconds = 1.6538e+04, _tick = 6030, _time = 1.6546e+09)
[2022-06-07 14:59:29,934][root][INFO] - Step 29696000 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29696000, mean_episode_return = 72.428, mean_episode_step = 1913.4, total_loss = -192.92, pg_loss = -222.95, baseline_loss = 35.232, entropy_loss = -5.2076, learner_queue_size = 32, train_seconds = 1.6543e+04, _tick = 6032, _time = 1.6546e+09)
[2022-06-07 14:59:34,940][root][INFO] - Step 29706240 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29706240, mean_episode_return = 59.302, mean_episode_step = 1474.5, total_loss = 379.1, pg_loss = 286.15, baseline_loss = 98.216, entropy_loss = -5.2651, learner_queue_size = 32, train_seconds = 1.6548e+04, _tick = 6034, _time = 1.6546e+09)
[2022-06-07 14:59:39,946][root][INFO] - Step 29713920 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 29713920, mean_episode_return = 55.639, mean_episode_step = 1749.1, total_loss = 496.71, pg_loss = 340.42, baseline_loss = 161.45, entropy_loss = -5.173, learner_queue_size = 32, train_seconds = 1.6553e+04, _tick = 6036, _time = 1.6546e+09)
[2022-06-07 14:59:44,950][root][INFO] - Step 29724160 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29724160, mean_episode_return = 36.625, mean_episode_step = 1728.5, total_loss = 749.11, pg_loss = 512.64, baseline_loss = 241.58, entropy_loss = -5.1103, learner_queue_size = 32, train_seconds = 1.6558e+04, _tick = 6039, _time = 1.6546e+09)
[2022-06-07 14:59:49,954][root][INFO] - Step 29731840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29731840, mean_episode_return = 13.83, mean_episode_step = 1875.2, total_loss = 44.337, pg_loss = -7.5262, baseline_loss = 56.927, entropy_loss = -5.0635, learner_queue_size = 32, train_seconds = 1.6563e+04, _tick = 6042, _time = 1.6546e+09)
[2022-06-07 14:59:54,958][root][INFO] - Step 29742080 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 29742080, mean_episode_return = None, mean_episode_step = 1629.2, total_loss = 75.568, pg_loss = 38.351, baseline_loss = 42.272, entropy_loss = -5.0548, learner_queue_size = 32, train_seconds = 1.6568e+04, _tick = 6043, _time = 1.6546e+09)
[2022-06-07 14:59:59,962][root][INFO] - Step 29749760 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 29749760, mean_episode_return = None, mean_episode_step = 2200.1, total_loss = 142.93, pg_loss = 47.454, baseline_loss = 100.59, entropy_loss = -5.1154, learner_queue_size = 32, train_seconds = 1.6573e+04, _tick = 6045, _time = 1.6546e+09)
[2022-06-07 15:00:04,969][root][INFO] - Step 29760000 @ 2045.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 29760000, mean_episode_return = 98.13, mean_episode_step = 2073.6, total_loss = -23.694, pg_loss = -97.238, baseline_loss = 78.627, entropy_loss = -5.0829, learner_queue_size = 32, train_seconds = 1.6578e+04, _tick = 6048, _time = 1.6546e+09)
[2022-06-07 15:00:09,975][root][INFO] - Step 29767680 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29767680, mean_episode_return = None, mean_episode_step = 2022.6, total_loss = -194.7, pg_loss = -192.16, baseline_loss = 2.45, entropy_loss = -4.9964, learner_queue_size = 32, train_seconds = 1.6583e+04, _tick = 6049, _time = 1.6546e+09)
[2022-06-07 15:00:14,978][root][INFO] - Step 29775360 @ 1535.0 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 29775360, mean_episode_return = 58.769, mean_episode_step = 1933.8, total_loss = 45.063, pg_loss = -48.214, baseline_loss = 98.252, entropy_loss = -4.9754, learner_queue_size = 32, train_seconds = 1.6588e+04, _tick = 6051, _time = 1.6546e+09)
[2022-06-07 15:00:19,982][root][INFO] - Step 29785600 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 29785600, mean_episode_return = None, mean_episode_step = 1861.2, total_loss = 502.91, pg_loss = 297.02, baseline_loss = 210.72, entropy_loss = -4.8292, learner_queue_size = 32, train_seconds = 1.6593e+04, _tick = 6053, _time = 1.6546e+09)
[2022-06-07 15:00:24,984][root][INFO] - Step 29793280 @ 1535.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29793280, mean_episode_return = None, mean_episode_step = 1995.8, total_loss = -83.677, pg_loss = -97.723, baseline_loss = 18.837, entropy_loss = -4.791, learner_queue_size = 32, train_seconds = 1.6598e+04, _tick = 6055, _time = 1.6546e+09)
[2022-06-07 15:00:29,990][root][INFO] - Step 29803520 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29803520, mean_episode_return = None, mean_episode_step = 1711.2, total_loss = 88.027, pg_loss = 32.666, baseline_loss = 60.148, entropy_loss = -4.787, learner_queue_size = 32, train_seconds = 1.6603e+04, _tick = 6055, _time = 1.6546e+09)
[2022-06-07 15:00:34,995][root][INFO] - Step 29813760 @ 2045.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 29813760, mean_episode_return = 5.4596, mean_episode_step = 1640.0, total_loss = 46.578, pg_loss = -36.444, baseline_loss = 87.842, entropy_loss = -4.82, learner_queue_size = 32, train_seconds = 1.6608e+04, _tick = 6058, _time = 1.6546e+09)
[2022-06-07 15:00:40,000][root][INFO] - Step 29821440 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29821440, mean_episode_return = 43.018, mean_episode_step = 2180.3, total_loss = -243.17, pg_loss = -276.81, baseline_loss = 38.462, entropy_loss = -4.8155, learner_queue_size = 32, train_seconds = 1.6613e+04, _tick = 6060, _time = 1.6546e+09)
[2022-06-07 15:00:45,006][root][INFO] - Step 29829120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29829120, mean_episode_return = 17.54, mean_episode_step = 1767.6, total_loss = -62.34, pg_loss = -102.78, baseline_loss = 45.26, entropy_loss = -4.8172, learner_queue_size = 32, train_seconds = 1.6618e+04, _tick = 6061, _time = 1.6546e+09)
[2022-06-07 15:00:50,034][root][INFO] - Step 29839360 @ 2036.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 29839360, mean_episode_return = None, mean_episode_step = 1426.1, total_loss = 207.98, pg_loss = 95.821, baseline_loss = 117.0, entropy_loss = -4.8414, learner_queue_size = 32, train_seconds = 1.6623e+04, _tick = 6062, _time = 1.6546e+09)
[2022-06-07 15:00:55,038][root][INFO] - Step 29847040 @ 1534.9 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 29847040, mean_episode_return = 112.26, mean_episode_step = 2569.7, total_loss = -42.145, pg_loss = -86.949, baseline_loss = 49.547, entropy_loss = -4.7436, learner_queue_size = 32, train_seconds = 1.6628e+04, _tick = 6064, _time = 1.6546e+09)
[2022-06-07 15:01:00,043][root][INFO] - Step 29857280 @ 2045.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29857280, mean_episode_return = -15.361, mean_episode_step = 2258.9, total_loss = 174.1, pg_loss = 84.002, baseline_loss = 94.911, entropy_loss = -4.8125, learner_queue_size = 32, train_seconds = 1.6633e+04, _tick = 6067, _time = 1.6546e+09)
[2022-06-07 15:01:05,046][root][INFO] - Step 29867520 @ 2046.9 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 29867520, mean_episode_return = 186.29, mean_episode_step = 1562.7, total_loss = 539.52, pg_loss = 403.73, baseline_loss = 140.57, entropy_loss = -4.7846, learner_queue_size = 32, train_seconds = 1.6638e+04, _tick = 6069, _time = 1.6546e+09)
[2022-06-07 15:01:10,050][root][INFO] - Step 29875200 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 29875200, mean_episode_return = None, mean_episode_step = 2197.2, total_loss = -131.44, pg_loss = -134.37, baseline_loss = 7.7476, entropy_loss = -4.8216, learner_queue_size = 32, train_seconds = 1.6643e+04, _tick = 6069, _time = 1.6546e+09)
[2022-06-07 15:01:15,054][root][INFO] - Step 29885440 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29885440, mean_episode_return = 138.81, mean_episode_step = 2073.5, total_loss = 49.189, pg_loss = -31.641, baseline_loss = 85.675, entropy_loss = -4.8455, learner_queue_size = 32, train_seconds = 1.6648e+04, _tick = 6072, _time = 1.6546e+09)
[2022-06-07 15:01:20,060][root][INFO] - Step 29893120 @ 1534.1 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 29893120, mean_episode_return = -10.371, mean_episode_step = 1718.6, total_loss = -217.72, pg_loss = -241.38, baseline_loss = 28.488, entropy_loss = -4.8285, learner_queue_size = 32, train_seconds = 1.6653e+04, _tick = 6074, _time = 1.6546e+09)
[2022-06-07 15:01:25,066][root][INFO] - Step 29903360 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29903360, mean_episode_return = None, mean_episode_step = 2673.9, total_loss = -123.73, pg_loss = -132.32, baseline_loss = 13.476, entropy_loss = -4.8813, learner_queue_size = 32, train_seconds = 1.6658e+04, _tick = 6076, _time = 1.6546e+09)
[2022-06-07 15:01:30,070][root][INFO] - Step 29911040 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 29911040, mean_episode_return = 36.58, mean_episode_step = 1767.6, total_loss = 409.6, pg_loss = 154.34, baseline_loss = 260.17, entropy_loss = -4.9112, learner_queue_size = 32, train_seconds = 1.6663e+04, _tick = 6078, _time = 1.6546e+09)
[2022-06-07 15:01:35,076][root][INFO] - Step 29921280 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 29921280, mean_episode_return = None, mean_episode_step = 2020.5, total_loss = 197.77, pg_loss = 112.23, baseline_loss = 90.411, entropy_loss = -4.8692, learner_queue_size = 32, train_seconds = 1.6668e+04, _tick = 6080, _time = 1.6546e+09)
[2022-06-07 15:01:40,082][root][INFO] - Step 29928960 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 29928960, mean_episode_return = -14.021, mean_episode_step = 2259.0, total_loss = -6.1346, pg_loss = -62.295, baseline_loss = 60.991, entropy_loss = -4.8306, learner_queue_size = 32, train_seconds = 1.6673e+04, _tick = 6083, _time = 1.6546e+09)
[2022-06-07 15:01:45,088][root][INFO] - Step 29939200 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 29939200, mean_episode_return = None, mean_episode_step = 1419.2, total_loss = 82.476, pg_loss = 13.58, baseline_loss = 73.762, entropy_loss = -4.8667, learner_queue_size = 32, train_seconds = 1.6678e+04, _tick = 6083, _time = 1.6546e+09)
[2022-06-07 15:01:50,094][root][INFO] - Step 29946880 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29946880, mean_episode_return = None, mean_episode_step = 1942.6, total_loss = 190.97, pg_loss = 77.538, baseline_loss = 118.23, entropy_loss = -4.7955, learner_queue_size = 32, train_seconds = 1.6683e+04, _tick = 6084, _time = 1.6546e+09)
[2022-06-07 15:01:55,098][root][INFO] - Step 29957120 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 29957120, mean_episode_return = None, mean_episode_step = 1777.4, total_loss = 51.374, pg_loss = 12.395, baseline_loss = 43.793, entropy_loss = -4.8136, learner_queue_size = 32, train_seconds = 1.6688e+04, _tick = 6086, _time = 1.6546e+09)
[2022-06-07 15:02:00,102][root][INFO] - Step 29964800 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 29964800, mean_episode_return = 54.833, mean_episode_step = 2455.6, total_loss = 72.497, pg_loss = 14.064, baseline_loss = 63.255, entropy_loss = -4.8212, learner_queue_size = 32, train_seconds = 1.6693e+04, _tick = 6088, _time = 1.6546e+09)
[2022-06-07 15:02:05,106][root][INFO] - Step 29975040 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 29975040, mean_episode_return = 4.0497, mean_episode_step = 2162.2, total_loss = -234.05, pg_loss = -232.67, baseline_loss = 3.5164, entropy_loss = -4.8985, learner_queue_size = 32, train_seconds = 1.6698e+04, _tick = 6091, _time = 1.6546e+09)
[2022-06-07 15:02:10,110][root][INFO] - Step 29982720 @ 1534.7 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 29982720, mean_episode_return = 18.5, mean_episode_step = 1687.8, total_loss = -126.98, pg_loss = -131.02, baseline_loss = 8.9431, entropy_loss = -4.904, learner_queue_size = 32, train_seconds = 1.6703e+04, _tick = 6094, _time = 1.6546e+09)
[2022-06-07 15:02:15,114][root][INFO] - Step 29992960 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 29992960, mean_episode_return = 70.399, mean_episode_step = 2319.4, total_loss = -23.331, pg_loss = -76.053, baseline_loss = 57.636, entropy_loss = -4.9142, learner_queue_size = 32, train_seconds = 1.6708e+04, _tick = 6097, _time = 1.6546e+09)
[2022-06-07 15:02:20,118][root][INFO] - Step 30000640 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30000640, mean_episode_return = 45.067, mean_episode_step = 2098.0, total_loss = 109.13, pg_loss = 18.755, baseline_loss = 95.249, entropy_loss = -4.8768, learner_queue_size = 32, train_seconds = 1.6713e+04, _tick = 6099, _time = 1.6546e+09)
[2022-06-07 15:02:25,124][root][INFO] - Step 30010880 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 30010880, mean_episode_return = 71.889, mean_episode_step = 1901.9, total_loss = 154.36, pg_loss = 68.543, baseline_loss = 90.583, entropy_loss = -4.7619, learner_queue_size = 32, train_seconds = 1.6718e+04, _tick = 6102, _time = 1.6546e+09)
[2022-06-07 15:02:30,134][root][INFO] - Step 30018560 @ 1533.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30018560, mean_episode_return = None, mean_episode_step = 2123.4, total_loss = 60.17, pg_loss = 18.118, baseline_loss = 46.815, entropy_loss = -4.7628, learner_queue_size = 32, train_seconds = 1.6723e+04, _tick = 6102, _time = 1.6546e+09)
[2022-06-07 15:02:35,140][root][INFO] - Step 30028800 @ 2045.5 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 30028800, mean_episode_return = None, mean_episode_step = 1879.1, total_loss = 223.14, pg_loss = 135.89, baseline_loss = 92.018, entropy_loss = -4.7755, learner_queue_size = 32, train_seconds = 1.6728e+04, _tick = 6103, _time = 1.6546e+09)
[2022-06-07 15:02:40,146][root][INFO] - Step 30036480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30036480, mean_episode_return = 22.25, mean_episode_step = 2052.3, total_loss = -107.07, pg_loss = -121.3, baseline_loss = 19.025, entropy_loss = -4.7957, learner_queue_size = 32, train_seconds = 1.6733e+04, _tick = 6104, _time = 1.6546e+09)
[2022-06-07 15:02:45,150][root][INFO] - Step 30046720 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 30046720, mean_episode_return = None, mean_episode_step = 1569.1, total_loss = -102.59, pg_loss = -110.63, baseline_loss = 12.842, entropy_loss = -4.8034, learner_queue_size = 32, train_seconds = 1.6738e+04, _tick = 6106, _time = 1.6546e+09)
[2022-06-07 15:02:50,156][root][INFO] - Step 30054400 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30054400, mean_episode_return = 100.13, mean_episode_step = 2168.9, total_loss = 326.21, pg_loss = 207.96, baseline_loss = 123.07, entropy_loss = -4.8249, learner_queue_size = 32, train_seconds = 1.6743e+04, _tick = 6108, _time = 1.6546e+09)
[2022-06-07 15:02:55,162][root][INFO] - Step 30064640 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30064640, mean_episode_return = 64.051, mean_episode_step = 1998.6, total_loss = -234.35, pg_loss = -244.11, baseline_loss = 14.611, entropy_loss = -4.8497, learner_queue_size = 32, train_seconds = 1.6748e+04, _tick = 6110, _time = 1.6546e+09)
[2022-06-07 15:03:00,168][root][INFO] - Step 30074880 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 30074880, mean_episode_return = 46.18, mean_episode_step = 1747.2, total_loss = 745.6, pg_loss = 227.06, baseline_loss = 523.38, entropy_loss = -4.8451, learner_queue_size = 32, train_seconds = 1.6753e+04, _tick = 6112, _time = 1.6546e+09)
[2022-06-07 15:03:05,174][root][INFO] - Step 30082560 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 30082560, mean_episode_return = 70.781, mean_episode_step = 2283.1, total_loss = 55.884, pg_loss = -8.144, baseline_loss = 68.816, entropy_loss = -4.7877, learner_queue_size = 32, train_seconds = 1.6758e+04, _tick = 6114, _time = 1.6546e+09)
[2022-06-07 15:03:10,180][root][INFO] - Step 30092800 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 30092800, mean_episode_return = 49.236, mean_episode_step = 1799.6, total_loss = -235.46, pg_loss = -257.74, baseline_loss = 27.026, entropy_loss = -4.7464, learner_queue_size = 32, train_seconds = 1.6763e+04, _tick = 6116, _time = 1.6546e+09)
[2022-06-07 15:03:15,186][root][INFO] - Step 30100480 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30100480, mean_episode_return = 226.05, mean_episode_step = 2218.6, total_loss = 539.08, pg_loss = 277.38, baseline_loss = 266.47, entropy_loss = -4.7708, learner_queue_size = 32, train_seconds = 1.6768e+04, _tick = 6118, _time = 1.6546e+09)
[2022-06-07 15:03:20,188][root][INFO] - Step 30110720 @ 2047.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 30110720, mean_episode_return = None, mean_episode_step = 1759.4, total_loss = -89.873, pg_loss = -90.264, baseline_loss = 5.1607, entropy_loss = -4.7696, learner_queue_size = 32, train_seconds = 1.6773e+04, _tick = 6121, _time = 1.6546e+09)
[2022-06-07 15:03:25,197][root][INFO] - Step 30118400 @ 1533.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30118400, mean_episode_return = 28.63, mean_episode_step = 1942.4, total_loss = -201.8, pg_loss = -211.53, baseline_loss = 14.56, entropy_loss = -4.8353, learner_queue_size = 32, train_seconds = 1.6778e+04, _tick = 6123, _time = 1.6546e+09)
[2022-06-07 15:03:30,202][root][INFO] - Step 30126080 @ 1534.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30126080, mean_episode_return = None, mean_episode_step = 1768.2, total_loss = 135.28, pg_loss = 81.384, baseline_loss = 58.746, entropy_loss = -4.8542, learner_queue_size = 32, train_seconds = 1.6783e+04, _tick = 6125, _time = 1.6546e+09)
[2022-06-07 15:03:35,206][root][INFO] - Step 30136320 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 30136320, mean_episode_return = 55.472, mean_episode_step = 1966.6, total_loss = -194.82, pg_loss = -195.35, baseline_loss = 5.4624, entropy_loss = -4.93, learner_queue_size = 32, train_seconds = 1.6788e+04, _tick = 6127, _time = 1.6546e+09)
[2022-06-07 15:03:40,210][root][INFO] - Step 30144000 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 30144000, mean_episode_return = 26.89, mean_episode_step = 1975.0, total_loss = -119.23, pg_loss = -139.82, baseline_loss = 25.525, entropy_loss = -4.9292, learner_queue_size = 32, train_seconds = 1.6793e+04, _tick = 6130, _time = 1.6546e+09)
[2022-06-07 15:03:45,214][root][INFO] - Step 30154240 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 30154240, mean_episode_return = 55.66, mean_episode_step = 1613.6, total_loss = 295.86, pg_loss = 147.16, baseline_loss = 153.58, entropy_loss = -4.873, learner_queue_size = 32, train_seconds = 1.6798e+04, _tick = 6134, _time = 1.6546e+09)
[2022-06-07 15:03:50,218][root][INFO] - Step 30161920 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 30161920, mean_episode_return = 94.017, mean_episode_step = 2146.2, total_loss = -73.642, pg_loss = -107.61, baseline_loss = 38.829, entropy_loss = -4.8597, learner_queue_size = 32, train_seconds = 1.6803e+04, _tick = 6136, _time = 1.6546e+09)
[2022-06-07 15:03:55,223][root][INFO] - Step 30172160 @ 2046.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30172160, mean_episode_return = 147.07, mean_episode_step = 2293.7, total_loss = -13.021, pg_loss = -61.624, baseline_loss = 53.546, entropy_loss = -4.9425, learner_queue_size = 32, train_seconds = 1.6808e+04, _tick = 6137, _time = 1.6546e+09)
[2022-06-07 15:04:00,226][root][INFO] - Step 30179840 @ 1535.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30179840, mean_episode_return = 22.71, mean_episode_step = 2012.1, total_loss = 208.2, pg_loss = 128.99, baseline_loss = 84.114, entropy_loss = -4.9033, learner_queue_size = 32, train_seconds = 1.6813e+04, _tick = 6140, _time = 1.6546e+09)
[2022-06-07 15:04:05,230][root][INFO] - Step 30190080 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 30190080, mean_episode_return = 61.082, mean_episode_step = 2289.7, total_loss = -32.506, pg_loss = -105.53, baseline_loss = 77.955, entropy_loss = -4.9336, learner_queue_size = 32, train_seconds = 1.6818e+04, _tick = 6141, _time = 1.6546e+09)
[2022-06-07 15:04:10,234][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 15:04:10,418][root][INFO] - Step 30197760 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 30197760, mean_episode_return = None, mean_episode_step = 2061.8, total_loss = 31.22, pg_loss = 14.88, baseline_loss = 21.351, entropy_loss = -5.0117, learner_queue_size = 32, train_seconds = 1.6823e+04, _tick = 6142, _time = 1.6546e+09)
[2022-06-07 15:04:15,422][root][INFO] - Step 30208000 @ 1973.8 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 30208000, mean_episode_return = None, mean_episode_step = 1717.8, total_loss = 333.67, pg_loss = 257.58, baseline_loss = 81.104, entropy_loss = -5.0062, learner_queue_size = 32, train_seconds = 1.6828e+04, _tick = 6143, _time = 1.6546e+09)
[2022-06-07 15:04:20,426][root][INFO] - Step 30215680 @ 1534.7 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 30215680, mean_episode_return = None, mean_episode_step = 1964.3, total_loss = 24.653, pg_loss = -21.827, baseline_loss = 51.364, entropy_loss = -4.8848, learner_queue_size = 32, train_seconds = 1.6833e+04, _tick = 6145, _time = 1.6546e+09)
[2022-06-07 15:04:25,430][root][INFO] - Step 30223360 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30223360, mean_episode_return = 111.02, mean_episode_step = 2910.5, total_loss = 29.183, pg_loss = -25.287, baseline_loss = 59.341, entropy_loss = -4.8714, learner_queue_size = 32, train_seconds = 1.6838e+04, _tick = 6148, _time = 1.6546e+09)
[2022-06-07 15:04:30,443][root][INFO] - Step 30233600 @ 2042.8 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 30233600, mean_episode_return = None, mean_episode_step = 1867.9, total_loss = 235.98, pg_loss = 164.58, baseline_loss = 76.177, entropy_loss = -4.7777, learner_queue_size = 32, train_seconds = 1.6843e+04, _tick = 6148, _time = 1.6546e+09)
[2022-06-07 15:04:35,449][root][INFO] - Step 30241280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30241280, mean_episode_return = 28.07, mean_episode_step = 1597.3, total_loss = 21.465, pg_loss = -46.649, baseline_loss = 72.911, entropy_loss = -4.7959, learner_queue_size = 32, train_seconds = 1.6848e+04, _tick = 6150, _time = 1.6546e+09)
[2022-06-07 15:04:40,454][root][INFO] - Step 30251520 @ 2045.8 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 30251520, mean_episode_return = None, mean_episode_step = 2632.1, total_loss = -150.93, pg_loss = -153.21, baseline_loss = 6.9973, entropy_loss = -4.7201, learner_queue_size = 32, train_seconds = 1.6853e+04, _tick = 6151, _time = 1.6546e+09)
[2022-06-07 15:04:45,460][root][INFO] - Step 30261760 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 30261760, mean_episode_return = None, mean_episode_step = 1914.2, total_loss = 143.18, pg_loss = 79.528, baseline_loss = 68.514, entropy_loss = -4.8577, learner_queue_size = 32, train_seconds = 1.6858e+04, _tick = 6153, _time = 1.6546e+09)
[2022-06-07 15:04:50,466][root][INFO] - Step 30269440 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 30269440, mean_episode_return = None, mean_episode_step = 1894.4, total_loss = 249.64, pg_loss = 146.74, baseline_loss = 107.74, entropy_loss = -4.8371, learner_queue_size = 32, train_seconds = 1.6864e+04, _tick = 6153, _time = 1.6546e+09)
[2022-06-07 15:04:55,470][root][INFO] - Step 30277120 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30277120, mean_episode_return = None, mean_episode_step = 2020.7, total_loss = 177.64, pg_loss = 107.38, baseline_loss = 75.163, entropy_loss = -4.8952, learner_queue_size = 32, train_seconds = 1.6868e+04, _tick = 6154, _time = 1.6546e+09)
[2022-06-07 15:05:00,474][root][INFO] - Step 30287360 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30287360, mean_episode_return = None, mean_episode_step = 1738.2, total_loss = 246.61, pg_loss = 168.3, baseline_loss = 83.326, entropy_loss = -5.015, learner_queue_size = 32, train_seconds = 1.6874e+04, _tick = 6156, _time = 1.6546e+09)
[2022-06-07 15:05:05,478][root][INFO] - Step 30297600 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 30297600, mean_episode_return = 60.972, mean_episode_step = 1762.6, total_loss = 2.9515, pg_loss = -37.059, baseline_loss = 45.074, entropy_loss = -5.0632, learner_queue_size = 32, train_seconds = 1.6878e+04, _tick = 6160, _time = 1.6546e+09)
[2022-06-07 15:05:10,484][root][INFO] - Step 30305280 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30305280, mean_episode_return = -24.251, mean_episode_step = 2677.4, total_loss = -185.89, pg_loss = -192.81, baseline_loss = 11.979, entropy_loss = -5.066, learner_queue_size = 32, train_seconds = 1.6884e+04, _tick = 6163, _time = 1.6546e+09)
[2022-06-07 15:05:15,490][root][INFO] - Step 30312960 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30312960, mean_episode_return = 72.501, mean_episode_step = 2104.3, total_loss = 170.0, pg_loss = 83.585, baseline_loss = 91.467, entropy_loss = -5.0487, learner_queue_size = 32, train_seconds = 1.6888e+04, _tick = 6165, _time = 1.6546e+09)
[2022-06-07 15:05:20,496][root][INFO] - Step 30323200 @ 2045.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 30323200, mean_episode_return = 28.3, mean_episode_step = 2128.0, total_loss = 352.13, pg_loss = 242.99, baseline_loss = 114.31, entropy_loss = -5.1682, learner_queue_size = 32, train_seconds = 1.6894e+04, _tick = 6167, _time = 1.6546e+09)
[2022-06-07 15:05:25,502][root][INFO] - Step 30330880 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 30330880, mean_episode_return = None, mean_episode_step = 2047.4, total_loss = -105.06, pg_loss = -109.69, baseline_loss = 9.7553, entropy_loss = -5.1255, learner_queue_size = 32, train_seconds = 1.6898e+04, _tick = 6169, _time = 1.6546e+09)
[2022-06-07 15:05:30,506][root][INFO] - Step 30341120 @ 2046.4 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 30341120, mean_episode_return = 12.22, mean_episode_step = 2416.4, total_loss = 50.687, pg_loss = -28.961, baseline_loss = 84.811, entropy_loss = -5.1633, learner_queue_size = 32, train_seconds = 1.6904e+04, _tick = 6172, _time = 1.6546e+09)
[2022-06-07 15:05:35,510][root][INFO] - Step 30348800 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 30348800, mean_episode_return = 101.64, mean_episode_step = 2057.5, total_loss = 5.4857, pg_loss = -53.279, baseline_loss = 63.848, entropy_loss = -5.0834, learner_queue_size = 32, train_seconds = 1.6908e+04, _tick = 6174, _time = 1.6546e+09)
[2022-06-07 15:05:40,514][root][INFO] - Step 30359040 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 30359040, mean_episode_return = None, mean_episode_step = 2359.5, total_loss = 166.06, pg_loss = 54.538, baseline_loss = 116.53, entropy_loss = -5.0159, learner_queue_size = 32, train_seconds = 1.6914e+04, _tick = 6175, _time = 1.6546e+09)
[2022-06-07 15:05:45,520][root][INFO] - Step 30366720 @ 1534.2 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 30366720, mean_episode_return = 79.932, mean_episode_step = 2224.7, total_loss = 50.09, pg_loss = -25.587, baseline_loss = 80.616, entropy_loss = -4.9378, learner_queue_size = 32, train_seconds = 1.6918e+04, _tick = 6178, _time = 1.6546e+09)
[2022-06-07 15:05:50,526][root][INFO] - Step 30376960 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30376960, mean_episode_return = None, mean_episode_step = 2240.8, total_loss = 114.9, pg_loss = 67.827, baseline_loss = 52.057, entropy_loss = -4.9859, learner_queue_size = 32, train_seconds = 1.6924e+04, _tick = 6180, _time = 1.6546e+09)
[2022-06-07 15:05:55,530][root][INFO] - Step 30387200 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30387200, mean_episode_return = 9.755, mean_episode_step = 2264.3, total_loss = -142.01, pg_loss = -177.28, baseline_loss = 40.247, entropy_loss = -4.9765, learner_queue_size = 32, train_seconds = 1.6928e+04, _tick = 6184, _time = 1.6546e+09)
[2022-06-07 15:06:00,536][root][INFO] - Step 30394880 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 30394880, mean_episode_return = None, mean_episode_step = 1873.0, total_loss = 83.847, pg_loss = 30.411, baseline_loss = 58.447, entropy_loss = -5.0113, learner_queue_size = 32, train_seconds = 1.6934e+04, _tick = 6186, _time = 1.6546e+09)
[2022-06-07 15:06:05,540][root][INFO] - Step 30405120 @ 2046.2 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 30405120, mean_episode_return = None, mean_episode_step = 2156.1, total_loss = -27.364, pg_loss = -41.239, baseline_loss = 18.781, entropy_loss = -4.9064, learner_queue_size = 32, train_seconds = 1.6938e+04, _tick = 6188, _time = 1.6546e+09)
[2022-06-07 15:06:10,546][root][INFO] - Step 30412800 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30412800, mean_episode_return = 30.84, mean_episode_step = 1956.4, total_loss = 33.792, pg_loss = -31.435, baseline_loss = 70.241, entropy_loss = -5.0143, learner_queue_size = 32, train_seconds = 1.6944e+04, _tick = 6191, _time = 1.6546e+09)
[2022-06-07 15:06:15,550][root][INFO] - Step 30423040 @ 2046.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 30423040, mean_episode_return = 45.642, mean_episode_step = 2111.1, total_loss = -200.62, pg_loss = -248.45, baseline_loss = 52.899, entropy_loss = -5.0611, learner_queue_size = 32, train_seconds = 1.6948e+04, _tick = 6194, _time = 1.6546e+09)
[2022-06-07 15:06:20,554][root][INFO] - Step 30430720 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30430720, mean_episode_return = None, mean_episode_step = 1464.9, total_loss = 266.58, pg_loss = 183.42, baseline_loss = 88.158, entropy_loss = -4.9944, learner_queue_size = 32, train_seconds = 1.6954e+04, _tick = 6196, _time = 1.6546e+09)
[2022-06-07 15:06:25,558][root][INFO] - Step 30440960 @ 2046.3 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 30440960, mean_episode_return = 175.43, mean_episode_step = 2031.0, total_loss = 328.36, pg_loss = 176.77, baseline_loss = 156.66, entropy_loss = -5.0692, learner_queue_size = 32, train_seconds = 1.6959e+04, _tick = 6197, _time = 1.6546e+09)
[2022-06-07 15:06:30,562][root][INFO] - Step 30448640 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30448640, mean_episode_return = 17.6, mean_episode_step = 1772.4, total_loss = 182.44, pg_loss = 100.38, baseline_loss = 87.014, entropy_loss = -4.9541, learner_queue_size = 32, train_seconds = 1.6964e+04, _tick = 6200, _time = 1.6546e+09)
[2022-06-07 15:06:35,566][root][INFO] - Step 30458880 @ 2046.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 30458880, mean_episode_return = 31.006, mean_episode_step = 1296.1, total_loss = 59.169, pg_loss = -7.9534, baseline_loss = 72.129, entropy_loss = -5.0071, learner_queue_size = 32, train_seconds = 1.6969e+04, _tick = 6203, _time = 1.6546e+09)
[2022-06-07 15:06:40,572][root][INFO] - Step 30466560 @ 1534.2 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 30466560, mean_episode_return = 0.10957, mean_episode_step = 1964.9, total_loss = -76.574, pg_loss = -109.1, baseline_loss = 37.493, entropy_loss = -4.9641, learner_queue_size = 32, train_seconds = 1.6974e+04, _tick = 6205, _time = 1.6546e+09)
[2022-06-07 15:06:45,578][root][INFO] - Step 30476800 @ 2045.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 30476800, mean_episode_return = None, mean_episode_step = 2247.4, total_loss = -97.387, pg_loss = -102.47, baseline_loss = 9.9752, entropy_loss = -4.8879, learner_queue_size = 32, train_seconds = 1.6979e+04, _tick = 6206, _time = 1.6546e+09)
[2022-06-07 15:06:50,582][root][INFO] - Step 30487040 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 30487040, mean_episode_return = None, mean_episode_step = 1676.5, total_loss = -0.59666, pg_loss = -22.119, baseline_loss = 26.402, entropy_loss = -4.8798, learner_queue_size = 32, train_seconds = 1.6984e+04, _tick = 6208, _time = 1.6546e+09)
[2022-06-07 15:06:55,586][root][INFO] - Step 30494720 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30494720, mean_episode_return = None, mean_episode_step = 1978.8, total_loss = 131.05, pg_loss = 80.978, baseline_loss = 55.105, entropy_loss = -5.0358, learner_queue_size = 32, train_seconds = 1.6989e+04, _tick = 6209, _time = 1.6546e+09)
[2022-06-07 15:07:00,590][root][INFO] - Step 30504960 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 30504960, mean_episode_return = 56.762, mean_episode_step = 2091.3, total_loss = 91.156, pg_loss = 27.687, baseline_loss = 68.529, entropy_loss = -5.0612, learner_queue_size = 32, train_seconds = 1.6994e+04, _tick = 6213, _time = 1.6546e+09)
[2022-06-07 15:07:05,594][root][INFO] - Step 30512640 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30512640, mean_episode_return = 47.2, mean_episode_step = 2188.8, total_loss = -163.89, pg_loss = -170.51, baseline_loss = 11.655, entropy_loss = -5.0348, learner_queue_size = 32, train_seconds = 1.6999e+04, _tick = 6214, _time = 1.6546e+09)
[2022-06-07 15:07:10,598][root][INFO] - Step 30522880 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30522880, mean_episode_return = 6.0398, mean_episode_step = 2124.2, total_loss = 106.73, pg_loss = 43.562, baseline_loss = 68.167, entropy_loss = -4.9984, learner_queue_size = 32, train_seconds = 1.7004e+04, _tick = 6217, _time = 1.6546e+09)
[2022-06-07 15:07:15,602][root][INFO] - Step 30530560 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 30530560, mean_episode_return = None, mean_episode_step = 2040.2, total_loss = 38.816, pg_loss = -19.764, baseline_loss = 63.478, entropy_loss = -4.8978, learner_queue_size = 32, train_seconds = 1.7009e+04, _tick = 6219, _time = 1.6546e+09)
[2022-06-07 15:07:20,608][root][INFO] - Step 30540800 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 30540800, mean_episode_return = 65.088, mean_episode_step = 2125.8, total_loss = -218.2, pg_loss = -225.71, baseline_loss = 12.273, entropy_loss = -4.7553, learner_queue_size = 32, train_seconds = 1.7014e+04, _tick = 6223, _time = 1.6546e+09)
[2022-06-07 15:07:25,614][root][INFO] - Step 30548480 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30548480, mean_episode_return = 2.4297, mean_episode_step = 1421.0, total_loss = 352.63, pg_loss = 220.46, baseline_loss = 136.91, entropy_loss = -4.7429, learner_queue_size = 32, train_seconds = 1.7019e+04, _tick = 6226, _time = 1.6546e+09)
[2022-06-07 15:07:30,620][root][INFO] - Step 30558720 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30558720, mean_episode_return = 60.97, mean_episode_step = 2123.9, total_loss = -99.129, pg_loss = -136.68, baseline_loss = 42.268, entropy_loss = -4.7193, learner_queue_size = 32, train_seconds = 1.7024e+04, _tick = 6229, _time = 1.6546e+09)
[2022-06-07 15:07:35,626][root][INFO] - Step 30566400 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30566400, mean_episode_return = 54.341, mean_episode_step = 1641.7, total_loss = 178.42, pg_loss = 74.009, baseline_loss = 109.04, entropy_loss = -4.6292, learner_queue_size = 32, train_seconds = 1.7029e+04, _tick = 6231, _time = 1.6546e+09)
[2022-06-07 15:07:40,630][root][INFO] - Step 30576640 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30576640, mean_episode_return = -16.161, mean_episode_step = 1756.6, total_loss = -26.557, pg_loss = -76.689, baseline_loss = 54.666, entropy_loss = -4.5345, learner_queue_size = 32, train_seconds = 1.7034e+04, _tick = 6234, _time = 1.6546e+09)
[2022-06-07 15:07:45,634][root][INFO] - Step 30584320 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30584320, mean_episode_return = None, mean_episode_step = 1702.4, total_loss = -48.009, pg_loss = -57.844, baseline_loss = 14.414, entropy_loss = -4.5793, learner_queue_size = 32, train_seconds = 1.7039e+04, _tick = 6235, _time = 1.6546e+09)
[2022-06-07 15:07:50,638][root][INFO] - Step 30594560 @ 2046.3 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 30594560, mean_episode_return = None, mean_episode_step = 1710.6, total_loss = 78.947, pg_loss = 39.534, baseline_loss = 44.0, entropy_loss = -4.5866, learner_queue_size = 32, train_seconds = 1.7044e+04, _tick = 6238, _time = 1.6546e+09)
[2022-06-07 15:07:55,644][root][INFO] - Step 30602240 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30602240, mean_episode_return = -0.66036, mean_episode_step = 1780.6, total_loss = -87.109, pg_loss = -116.08, baseline_loss = 33.605, entropy_loss = -4.636, learner_queue_size = 32, train_seconds = 1.7049e+04, _tick = 6241, _time = 1.6546e+09)
[2022-06-07 15:08:00,650][root][INFO] - Step 30609920 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30609920, mean_episode_return = 72.663, mean_episode_step = 1633.0, total_loss = 191.78, pg_loss = 99.226, baseline_loss = 97.198, entropy_loss = -4.6425, learner_queue_size = 32, train_seconds = 1.7054e+04, _tick = 6243, _time = 1.6546e+09)
[2022-06-07 15:08:05,653][root][INFO] - Step 30620160 @ 2046.7 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 30620160, mean_episode_return = 34.781, mean_episode_step = 1546.4, total_loss = -39.788, pg_loss = -72.465, baseline_loss = 37.322, entropy_loss = -4.645, learner_queue_size = 32, train_seconds = 1.7059e+04, _tick = 6246, _time = 1.6546e+09)
[2022-06-07 15:08:10,658][root][INFO] - Step 30627840 @ 1534.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30627840, mean_episode_return = 9.65, mean_episode_step = 1630.4, total_loss = 253.03, pg_loss = 127.97, baseline_loss = 129.77, entropy_loss = -4.717, learner_queue_size = 32, train_seconds = 1.7064e+04, _tick = 6247, _time = 1.6546e+09)
[2022-06-07 15:08:15,662][root][INFO] - Step 30638080 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30638080, mean_episode_return = -1.6101, mean_episode_step = 1951.2, total_loss = -12.406, pg_loss = -67.265, baseline_loss = 59.637, entropy_loss = -4.7779, learner_queue_size = 32, train_seconds = 1.7069e+04, _tick = 6250, _time = 1.6546e+09)
[2022-06-07 15:08:20,668][root][INFO] - Step 30648320 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30648320, mean_episode_return = 83.54, mean_episode_step = 1995.1, total_loss = 49.058, pg_loss = -6.7968, baseline_loss = 60.661, entropy_loss = -4.8055, learner_queue_size = 32, train_seconds = 1.7074e+04, _tick = 6253, _time = 1.6546e+09)
[2022-06-07 15:08:25,670][root][INFO] - Step 30656000 @ 1535.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30656000, mean_episode_return = 86.23, mean_episode_step = 1406.2, total_loss = 38.863, pg_loss = -5.5675, baseline_loss = 49.295, entropy_loss = -4.8641, learner_queue_size = 32, train_seconds = 1.7079e+04, _tick = 6255, _time = 1.6546e+09)
[2022-06-07 15:08:30,676][root][INFO] - Step 30666240 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 30666240, mean_episode_return = 44.511, mean_episode_step = 1644.4, total_loss = -16.205, pg_loss = -56.629, baseline_loss = 45.203, entropy_loss = -4.7792, learner_queue_size = 32, train_seconds = 1.7084e+04, _tick = 6258, _time = 1.6546e+09)
[2022-06-07 15:08:35,682][root][INFO] - Step 30673920 @ 1534.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 30673920, mean_episode_return = 36.582, mean_episode_step = 2159.5, total_loss = -106.33, pg_loss = -115.49, baseline_loss = 13.99, entropy_loss = -4.8358, learner_queue_size = 32, train_seconds = 1.7089e+04, _tick = 6259, _time = 1.6546e+09)
[2022-06-07 15:08:40,688][root][INFO] - Step 30684160 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30684160, mean_episode_return = 30.841, mean_episode_step = 1441.9, total_loss = -53.214, pg_loss = -81.381, baseline_loss = 33.039, entropy_loss = -4.8718, learner_queue_size = 32, train_seconds = 1.7094e+04, _tick = 6262, _time = 1.6546e+09)
[2022-06-07 15:08:45,694][root][INFO] - Step 30691840 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30691840, mean_episode_return = 79.549, mean_episode_step = 1463.5, total_loss = 333.74, pg_loss = 217.96, baseline_loss = 120.75, entropy_loss = -4.9749, learner_queue_size = 32, train_seconds = 1.7099e+04, _tick = 6265, _time = 1.6546e+09)
[2022-06-07 15:08:50,698][root][INFO] - Step 30702080 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 30702080, mean_episode_return = None, mean_episode_step = 1886.9, total_loss = 227.82, pg_loss = 139.74, baseline_loss = 92.946, entropy_loss = -4.8642, learner_queue_size = 32, train_seconds = 1.7104e+04, _tick = 6266, _time = 1.6546e+09)
[2022-06-07 15:08:55,702][root][INFO] - Step 30709760 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30709760, mean_episode_return = None, mean_episode_step = 1766.1, total_loss = 565.89, pg_loss = 412.97, baseline_loss = 157.77, entropy_loss = -4.8546, learner_queue_size = 32, train_seconds = 1.7109e+04, _tick = 6267, _time = 1.6546e+09)
[2022-06-07 15:09:00,706][root][INFO] - Step 30720000 @ 2046.3 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 30720000, mean_episode_return = None, mean_episode_step = 1650.9, total_loss = -73.343, pg_loss = -82.963, baseline_loss = 14.491, entropy_loss = -4.8713, learner_queue_size = 32, train_seconds = 1.7114e+04, _tick = 6269, _time = 1.6546e+09)
[2022-06-07 15:09:05,710][root][INFO] - Step 30727680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30727680, mean_episode_return = None, mean_episode_step = 1774.6, total_loss = 58.366, pg_loss = 25.517, baseline_loss = 37.724, entropy_loss = -4.8747, learner_queue_size = 32, train_seconds = 1.7119e+04, _tick = 6270, _time = 1.6546e+09)
[2022-06-07 15:09:10,714][root][INFO] - Step 30737920 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30737920, mean_episode_return = 83.278, mean_episode_step = 1789.4, total_loss = 316.15, pg_loss = 197.32, baseline_loss = 123.64, entropy_loss = -4.807, learner_queue_size = 32, train_seconds = 1.7124e+04, _tick = 6271, _time = 1.6546e+09)
[2022-06-07 15:09:15,718][root][INFO] - Step 30745600 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 30745600, mean_episode_return = 53.951, mean_episode_step = 1522.6, total_loss = -110.52, pg_loss = -123.42, baseline_loss = 17.71, entropy_loss = -4.8133, learner_queue_size = 32, train_seconds = 1.7129e+04, _tick = 6273, _time = 1.6546e+09)
[2022-06-07 15:09:20,722][root][INFO] - Step 30755840 @ 2046.3 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 30755840, mean_episode_return = None, mean_episode_step = 1678.4, total_loss = 297.59, pg_loss = 199.48, baseline_loss = 102.87, entropy_loss = -4.7669, learner_queue_size = 32, train_seconds = 1.7134e+04, _tick = 6273, _time = 1.6546e+09)
[2022-06-07 15:09:25,726][root][INFO] - Step 30763520 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30763520, mean_episode_return = None, mean_episode_step = 1822.3, total_loss = -43.669, pg_loss = -73.719, baseline_loss = 34.768, entropy_loss = -4.7182, learner_queue_size = 32, train_seconds = 1.7139e+04, _tick = 6274, _time = 1.6546e+09)
[2022-06-07 15:09:30,730][root][INFO] - Step 30773760 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 30773760, mean_episode_return = 26.58, mean_episode_step = 1882.2, total_loss = -256.04, pg_loss = -263.62, baseline_loss = 12.356, entropy_loss = -4.7753, learner_queue_size = 32, train_seconds = 1.7144e+04, _tick = 6276, _time = 1.6546e+09)
[2022-06-07 15:09:35,734][root][INFO] - Step 30784000 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 30784000, mean_episode_return = None, mean_episode_step = 1937.9, total_loss = 141.68, pg_loss = 59.34, baseline_loss = 87.082, entropy_loss = -4.7411, learner_queue_size = 32, train_seconds = 1.7149e+04, _tick = 6278, _time = 1.6546e+09)
[2022-06-07 15:09:40,738][root][INFO] - Step 30791680 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30791680, mean_episode_return = 67.16, mean_episode_step = 1385.3, total_loss = -88.041, pg_loss = -128.7, baseline_loss = 45.385, entropy_loss = -4.7258, learner_queue_size = 32, train_seconds = 1.7154e+04, _tick = 6279, _time = 1.6546e+09)
[2022-06-07 15:09:45,744][root][INFO] - Step 30801920 @ 2045.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30801920, mean_episode_return = 31.337, mean_episode_step = 2048.4, total_loss = -178.76, pg_loss = -225.91, baseline_loss = 51.863, entropy_loss = -4.7151, learner_queue_size = 32, train_seconds = 1.7159e+04, _tick = 6283, _time = 1.6546e+09)
[2022-06-07 15:09:50,750][root][INFO] - Step 30809600 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30809600, mean_episode_return = None, mean_episode_step = 1994.1, total_loss = -102.06, pg_loss = -115.2, baseline_loss = 17.861, entropy_loss = -4.7197, learner_queue_size = 32, train_seconds = 1.7164e+04, _tick = 6284, _time = 1.6546e+09)
[2022-06-07 15:09:55,754][root][INFO] - Step 30819840 @ 2046.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30819840, mean_episode_return = None, mean_episode_step = 2477.2, total_loss = 310.81, pg_loss = 207.86, baseline_loss = 107.68, entropy_loss = -4.727, learner_queue_size = 32, train_seconds = 1.7169e+04, _tick = 6285, _time = 1.6546e+09)
[2022-06-07 15:10:00,760][root][INFO] - Step 30827520 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 30827520, mean_episode_return = None, mean_episode_step = 1765.0, total_loss = 44.802, pg_loss = 3.0529, baseline_loss = 46.465, entropy_loss = -4.716, learner_queue_size = 32, train_seconds = 1.7174e+04, _tick = 6287, _time = 1.6546e+09)
[2022-06-07 15:10:05,766][root][INFO] - Step 30837760 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 30837760, mean_episode_return = 92.888, mean_episode_step = 1784.6, total_loss = 327.93, pg_loss = 200.18, baseline_loss = 132.42, entropy_loss = -4.672, learner_queue_size = 32, train_seconds = 1.7179e+04, _tick = 6291, _time = 1.6546e+09)
[2022-06-07 15:10:10,772][root][INFO] - Step 30845440 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30845440, mean_episode_return = 11.963, mean_episode_step = 2299.5, total_loss = -152.15, pg_loss = -205.83, baseline_loss = 58.349, entropy_loss = -4.6716, learner_queue_size = 32, train_seconds = 1.7184e+04, _tick = 6293, _time = 1.6546e+09)
[2022-06-07 15:10:15,778][root][INFO] - Step 30855680 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 30855680, mean_episode_return = None, mean_episode_step = 1883.2, total_loss = 223.97, pg_loss = 164.04, baseline_loss = 64.613, entropy_loss = -4.6852, learner_queue_size = 32, train_seconds = 1.7189e+04, _tick = 6296, _time = 1.6546e+09)
[2022-06-07 15:10:20,784][root][INFO] - Step 30865920 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 30865920, mean_episode_return = 37.395, mean_episode_step = 2106.1, total_loss = 1252.2, pg_loss = 748.7, baseline_loss = 508.23, entropy_loss = -4.7174, learner_queue_size = 32, train_seconds = 1.7194e+04, _tick = 6299, _time = 1.6546e+09)
[2022-06-07 15:10:25,790][root][INFO] - Step 30873600 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30873600, mean_episode_return = 6.7396, mean_episode_step = 2196.6, total_loss = -142.06, pg_loss = -189.57, baseline_loss = 52.221, entropy_loss = -4.71, learner_queue_size = 32, train_seconds = 1.7199e+04, _tick = 6301, _time = 1.6546e+09)
[2022-06-07 15:10:30,794][root][INFO] - Step 30883840 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30883840, mean_episode_return = 65.331, mean_episode_step = 1694.7, total_loss = 186.94, pg_loss = 102.88, baseline_loss = 88.778, entropy_loss = -4.7151, learner_queue_size = 32, train_seconds = 1.7204e+04, _tick = 6303, _time = 1.6546e+09)
[2022-06-07 15:10:35,798][root][INFO] - Step 30891520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30891520, mean_episode_return = 179.05, mean_episode_step = 2312.3, total_loss = 130.96, pg_loss = 44.286, baseline_loss = 91.446, entropy_loss = -4.7762, learner_queue_size = 32, train_seconds = 1.7209e+04, _tick = 6305, _time = 1.6546e+09)
[2022-06-07 15:10:40,802][root][INFO] - Step 30901760 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30901760, mean_episode_return = 71.519, mean_episode_step = 1963.8, total_loss = -93.123, pg_loss = -128.03, baseline_loss = 39.766, entropy_loss = -4.8588, learner_queue_size = 32, train_seconds = 1.7214e+04, _tick = 6307, _time = 1.6546e+09)
[2022-06-07 15:10:45,806][root][INFO] - Step 30909440 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 30909440, mean_episode_return = 7.8998, mean_episode_step = 2268.3, total_loss = 101.81, pg_loss = 27.528, baseline_loss = 79.134, entropy_loss = -4.8484, learner_queue_size = 32, train_seconds = 1.7219e+04, _tick = 6310, _time = 1.6546e+09)
[2022-06-07 15:10:50,812][root][INFO] - Step 30919680 @ 2045.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30919680, mean_episode_return = None, mean_episode_step = 1500.1, total_loss = 233.15, pg_loss = 150.74, baseline_loss = 87.163, entropy_loss = -4.761, learner_queue_size = 32, train_seconds = 1.7224e+04, _tick = 6312, _time = 1.6546e+09)
[2022-06-07 15:10:55,818][root][INFO] - Step 30927360 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30927360, mean_episode_return = None, mean_episode_step = 1826.1, total_loss = 362.71, pg_loss = 265.94, baseline_loss = 101.6, entropy_loss = -4.835, learner_queue_size = 32, train_seconds = 1.7229e+04, _tick = 6314, _time = 1.6546e+09)
[2022-06-07 15:11:00,822][root][INFO] - Step 30937600 @ 2046.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30937600, mean_episode_return = None, mean_episode_step = 1832.8, total_loss = -134.66, pg_loss = -136.0, baseline_loss = 6.206, entropy_loss = -4.864, learner_queue_size = 32, train_seconds = 1.7234e+04, _tick = 6317, _time = 1.6546e+09)
[2022-06-07 15:11:05,828][root][INFO] - Step 30945280 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 30945280, mean_episode_return = None, mean_episode_step = 1773.0, total_loss = 113.71, pg_loss = 54.085, baseline_loss = 64.455, entropy_loss = -4.831, learner_queue_size = 32, train_seconds = 1.7239e+04, _tick = 6319, _time = 1.6546e+09)
[2022-06-07 15:11:10,834][root][INFO] - Step 30955520 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 30955520, mean_episode_return = None, mean_episode_step = 1277.0, total_loss = 355.2, pg_loss = 284.87, baseline_loss = 75.147, entropy_loss = -4.8173, learner_queue_size = 32, train_seconds = 1.7244e+04, _tick = 6320, _time = 1.6546e+09)
[2022-06-07 15:11:15,838][root][INFO] - Step 30963200 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30963200, mean_episode_return = -22.071, mean_episode_step = 1735.9, total_loss = 181.39, pg_loss = 99.891, baseline_loss = 86.267, entropy_loss = -4.7703, learner_queue_size = 32, train_seconds = 1.7249e+04, _tick = 6322, _time = 1.6546e+09)
[2022-06-07 15:11:20,842][root][INFO] - Step 30973440 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 30973440, mean_episode_return = 30.99, mean_episode_step = 1450.3, total_loss = 37.285, pg_loss = -52.6, baseline_loss = 94.668, entropy_loss = -4.7829, learner_queue_size = 32, train_seconds = 1.7254e+04, _tick = 6325, _time = 1.6546e+09)
[2022-06-07 15:11:25,843][root][INFO] - Step 30981120 @ 1535.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 30981120, mean_episode_return = 18.42, mean_episode_step = 1441.2, total_loss = 534.93, pg_loss = 339.48, baseline_loss = 200.24, entropy_loss = -4.7972, learner_queue_size = 32, train_seconds = 1.7259e+04, _tick = 6327, _time = 1.6546e+09)
[2022-06-07 15:11:30,846][root][INFO] - Step 30991360 @ 2047.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 30991360, mean_episode_return = None, mean_episode_step = 1632.0, total_loss = -85.351, pg_loss = -94.21, baseline_loss = 13.662, entropy_loss = -4.8029, learner_queue_size = 32, train_seconds = 1.7264e+04, _tick = 6328, _time = 1.6546e+09)
[2022-06-07 15:11:35,850][root][INFO] - Step 30999040 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 30999040, mean_episode_return = 65.385, mean_episode_step = 1630.9, total_loss = -84.861, pg_loss = -135.08, baseline_loss = 55.113, entropy_loss = -4.8968, learner_queue_size = 32, train_seconds = 1.7269e+04, _tick = 6329, _time = 1.6546e+09)
[2022-06-07 15:11:40,856][root][INFO] - Step 31009280 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 31009280, mean_episode_return = None, mean_episode_step = 1825.6, total_loss = 233.84, pg_loss = 172.42, baseline_loss = 66.284, entropy_loss = -4.8671, learner_queue_size = 32, train_seconds = 1.7274e+04, _tick = 6330, _time = 1.6546e+09)
[2022-06-07 15:11:45,862][root][INFO] - Step 31016960 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 31016960, mean_episode_return = 33.941, mean_episode_step = 1966.2, total_loss = -24.485, pg_loss = -83.966, baseline_loss = 64.412, entropy_loss = -4.9304, learner_queue_size = 32, train_seconds = 1.7279e+04, _tick = 6332, _time = 1.6546e+09)
[2022-06-07 15:11:50,866][root][INFO] - Step 31027200 @ 2046.3 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 31027200, mean_episode_return = None, mean_episode_step = 1896.1, total_loss = 70.402, pg_loss = 26.67, baseline_loss = 48.612, entropy_loss = -4.8803, learner_queue_size = 32, train_seconds = 1.7284e+04, _tick = 6333, _time = 1.6546e+09)
[2022-06-07 15:11:55,870][root][INFO] - Step 31034880 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 31034880, mean_episode_return = None, mean_episode_step = 1793.8, total_loss = 643.9, pg_loss = 463.15, baseline_loss = 185.65, entropy_loss = -4.8957, learner_queue_size = 32, train_seconds = 1.7289e+04, _tick = 6334, _time = 1.6546e+09)
[2022-06-07 15:12:00,874][root][INFO] - Step 31045120 @ 2046.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 31045120, mean_episode_return = 17.631, mean_episode_step = 1525.4, total_loss = -64.383, pg_loss = -93.484, baseline_loss = 33.989, entropy_loss = -4.8876, learner_queue_size = 32, train_seconds = 1.7294e+04, _tick = 6337, _time = 1.6546e+09)
[2022-06-07 15:12:05,878][root][INFO] - Step 31055360 @ 2046.3 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 31055360, mean_episode_return = 75.177, mean_episode_step = 1605.8, total_loss = -54.554, pg_loss = -136.36, baseline_loss = 86.754, entropy_loss = -4.9467, learner_queue_size = 32, train_seconds = 1.7299e+04, _tick = 6340, _time = 1.6546e+09)
[2022-06-07 15:12:10,882][root][INFO] - Step 31063040 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 31063040, mean_episode_return = 47.754, mean_episode_step = 2114.0, total_loss = 208.26, pg_loss = 148.74, baseline_loss = 64.441, entropy_loss = -4.9182, learner_queue_size = 32, train_seconds = 1.7304e+04, _tick = 6343, _time = 1.6546e+09)
[2022-06-07 15:12:15,886][root][INFO] - Step 31073280 @ 2046.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 31073280, mean_episode_return = 36.75, mean_episode_step = 1614.0, total_loss = 245.26, pg_loss = 132.7, baseline_loss = 117.49, entropy_loss = -4.9235, learner_queue_size = 32, train_seconds = 1.7309e+04, _tick = 6346, _time = 1.6546e+09)
[2022-06-07 15:12:20,890][root][INFO] - Step 31080960 @ 1534.7 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 31080960, mean_episode_return = 7.0498, mean_episode_step = 1750.8, total_loss = -209.61, pg_loss = -244.54, baseline_loss = 39.758, entropy_loss = -4.8227, learner_queue_size = 32, train_seconds = 1.7314e+04, _tick = 6349, _time = 1.6546e+09)
[2022-06-07 15:12:25,894][root][INFO] - Step 31091200 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31091200, mean_episode_return = 39.764, mean_episode_step = 1420.6, total_loss = -148.89, pg_loss = -176.23, baseline_loss = 32.131, entropy_loss = -4.7867, learner_queue_size = 32, train_seconds = 1.7319e+04, _tick = 6352, _time = 1.6546e+09)
[2022-06-07 15:12:30,898][root][INFO] - Step 31098880 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31098880, mean_episode_return = -17.96, mean_episode_step = 1523.6, total_loss = 167.23, pg_loss = 94.452, baseline_loss = 77.475, entropy_loss = -4.6959, learner_queue_size = 32, train_seconds = 1.7324e+04, _tick = 6355, _time = 1.6546e+09)
[2022-06-07 15:12:35,904][root][INFO] - Step 31106560 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 31106560, mean_episode_return = -3.9604, mean_episode_step = 1622.1, total_loss = -114.46, pg_loss = -128.0, baseline_loss = 18.202, entropy_loss = -4.6619, learner_queue_size = 32, train_seconds = 1.7329e+04, _tick = 6358, _time = 1.6546e+09)
[2022-06-07 15:12:40,910][root][INFO] - Step 31116800 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31116800, mean_episode_return = None, mean_episode_step = 1820.3, total_loss = -99.942, pg_loss = -124.26, baseline_loss = 29.003, entropy_loss = -4.6864, learner_queue_size = 32, train_seconds = 1.7334e+04, _tick = 6360, _time = 1.6546e+09)
[2022-06-07 15:12:45,914][root][INFO] - Step 31124480 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31124480, mean_episode_return = None, mean_episode_step = 1538.0, total_loss = 180.7, pg_loss = 140.69, baseline_loss = 44.728, entropy_loss = -4.7225, learner_queue_size = 32, train_seconds = 1.7339e+04, _tick = 6361, _time = 1.6546e+09)
[2022-06-07 15:12:50,918][root][INFO] - Step 31134720 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 31134720, mean_episode_return = None, mean_episode_step = 1805.8, total_loss = -79.042, pg_loss = -79.136, baseline_loss = 4.8644, entropy_loss = -4.7706, learner_queue_size = 32, train_seconds = 1.7344e+04, _tick = 6363, _time = 1.6546e+09)
[2022-06-07 15:12:55,922][root][INFO] - Step 31142400 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31142400, mean_episode_return = None, mean_episode_step = 1979.3, total_loss = -18.126, pg_loss = -37.637, baseline_loss = 24.251, entropy_loss = -4.7401, learner_queue_size = 32, train_seconds = 1.7349e+04, _tick = 6364, _time = 1.6546e+09)
[2022-06-07 15:13:00,926][root][INFO] - Step 31152640 @ 2046.3 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 31152640, mean_episode_return = 67.824, mean_episode_step = 1662.0, total_loss = -56.359, pg_loss = -122.58, baseline_loss = 70.967, entropy_loss = -4.7421, learner_queue_size = 32, train_seconds = 1.7354e+04, _tick = 6366, _time = 1.6546e+09)
[2022-06-07 15:13:05,928][root][INFO] - Step 31160320 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31160320, mean_episode_return = None, mean_episode_step = 2149.9, total_loss = 303.65, pg_loss = 186.21, baseline_loss = 122.15, entropy_loss = -4.7121, learner_queue_size = 32, train_seconds = 1.7359e+04, _tick = 6366, _time = 1.6546e+09)
[2022-06-07 15:13:10,934][root][INFO] - Step 31170560 @ 2045.6 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 31170560, mean_episode_return = None, mean_episode_step = 1768.4, total_loss = 252.04, pg_loss = 172.7, baseline_loss = 83.997, entropy_loss = -4.6552, learner_queue_size = 32, train_seconds = 1.7364e+04, _tick = 6367, _time = 1.6546e+09)
[2022-06-07 15:13:15,940][root][INFO] - Step 31178240 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31178240, mean_episode_return = None, mean_episode_step = 1717.4, total_loss = -31.83, pg_loss = -55.105, baseline_loss = 27.79, entropy_loss = -4.5149, learner_queue_size = 32, train_seconds = 1.7369e+04, _tick = 6368, _time = 1.6546e+09)
[2022-06-07 15:13:20,946][root][INFO] - Step 31188480 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 31188480, mean_episode_return = 18.54, mean_episode_step = 2134.8, total_loss = -60.703, pg_loss = -96.083, baseline_loss = 39.93, entropy_loss = -4.55, learner_queue_size = 32, train_seconds = 1.7374e+04, _tick = 6370, _time = 1.6546e+09)
[2022-06-07 15:13:25,952][root][INFO] - Step 31196160 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31196160, mean_episode_return = None, mean_episode_step = 1842.1, total_loss = 81.521, pg_loss = 32.059, baseline_loss = 54.098, entropy_loss = -4.6358, learner_queue_size = 32, train_seconds = 1.7379e+04, _tick = 6372, _time = 1.6546e+09)
[2022-06-07 15:13:30,958][root][INFO] - Step 31206400 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31206400, mean_episode_return = None, mean_episode_step = 1499.4, total_loss = 129.92, pg_loss = 79.612, baseline_loss = 54.842, entropy_loss = -4.534, learner_queue_size = 32, train_seconds = 1.7384e+04, _tick = 6374, _time = 1.6546e+09)
[2022-06-07 15:13:35,962][root][INFO] - Step 31214080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31214080, mean_episode_return = None, mean_episode_step = 2104.7, total_loss = -78.031, pg_loss = -104.57, baseline_loss = 31.063, entropy_loss = -4.527, learner_queue_size = 32, train_seconds = 1.7389e+04, _tick = 6376, _time = 1.6546e+09)
[2022-06-07 15:13:40,966][root][INFO] - Step 31224320 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31224320, mean_episode_return = 65.749, mean_episode_step = 2012.9, total_loss = -5.4006, pg_loss = -43.578, baseline_loss = 42.716, entropy_loss = -4.5391, learner_queue_size = 32, train_seconds = 1.7394e+04, _tick = 6379, _time = 1.6546e+09)
[2022-06-07 15:13:45,972][root][INFO] - Step 31232000 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31232000, mean_episode_return = None, mean_episode_step = 1527.7, total_loss = 335.88, pg_loss = 222.71, baseline_loss = 117.67, entropy_loss = -4.4972, learner_queue_size = 32, train_seconds = 1.7399e+04, _tick = 6381, _time = 1.6546e+09)
[2022-06-07 15:13:50,978][root][INFO] - Step 31242240 @ 2045.6 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 31242240, mean_episode_return = 136.45, mean_episode_step = 1916.8, total_loss = 160.24, pg_loss = 102.44, baseline_loss = 62.245, entropy_loss = -4.4534, learner_queue_size = 32, train_seconds = 1.7404e+04, _tick = 6384, _time = 1.6546e+09)
[2022-06-07 15:13:55,984][root][INFO] - Step 31249920 @ 1534.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 31249920, mean_episode_return = None, mean_episode_step = 1957.8, total_loss = 833.8, pg_loss = 481.56, baseline_loss = 356.64, entropy_loss = -4.3945, learner_queue_size = 32, train_seconds = 1.7409e+04, _tick = 6385, _time = 1.6546e+09)
[2022-06-07 15:14:00,990][root][INFO] - Step 31260160 @ 2045.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 31260160, mean_episode_return = 15.59, mean_episode_step = 2269.7, total_loss = -8.2647, pg_loss = -48.696, baseline_loss = 44.912, entropy_loss = -4.4803, learner_queue_size = 32, train_seconds = 1.7414e+04, _tick = 6387, _time = 1.6546e+09)
[2022-06-07 15:14:05,994][root][INFO] - Step 31270400 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31270400, mean_episode_return = None, mean_episode_step = 1660.1, total_loss = 220.17, pg_loss = 137.13, baseline_loss = 87.506, entropy_loss = -4.4627, learner_queue_size = 32, train_seconds = 1.7419e+04, _tick = 6387, _time = 1.6546e+09)
[2022-06-07 15:14:10,998][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 15:14:11,222][root][INFO] - Step 31278080 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31278080, mean_episode_return = -2.35, mean_episode_step = 2097.6, total_loss = -83.933, pg_loss = -103.05, baseline_loss = 23.571, entropy_loss = -4.4579, learner_queue_size = 32, train_seconds = 1.7424e+04, _tick = 6390, _time = 1.6546e+09)
[2022-06-07 15:14:16,226][root][INFO] - Step 31288320 @ 1958.7 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 31288320, mean_episode_return = 62.415, mean_episode_step = 2007.0, total_loss = -92.719, pg_loss = -148.05, baseline_loss = 59.806, entropy_loss = -4.4746, learner_queue_size = 32, train_seconds = 1.7429e+04, _tick = 6392, _time = 1.6546e+09)
[2022-06-07 15:14:21,230][root][INFO] - Step 31296000 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 31296000, mean_episode_return = None, mean_episode_step = 2003.8, total_loss = -85.954, pg_loss = -86.507, baseline_loss = 5.1102, entropy_loss = -4.5575, learner_queue_size = 32, train_seconds = 1.7434e+04, _tick = 6393, _time = 1.6546e+09)
[2022-06-07 15:14:26,234][root][INFO] - Step 31306240 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31306240, mean_episode_return = None, mean_episode_step = 1865.6, total_loss = 283.89, pg_loss = 169.98, baseline_loss = 118.48, entropy_loss = -4.5768, learner_queue_size = 32, train_seconds = 1.7439e+04, _tick = 6394, _time = 1.6546e+09)
[2022-06-07 15:14:31,238][root][INFO] - Step 31313920 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 31313920, mean_episode_return = 28.59, mean_episode_step = 1646.3, total_loss = -13.252, pg_loss = -47.153, baseline_loss = 38.477, entropy_loss = -4.5751, learner_queue_size = 32, train_seconds = 1.7444e+04, _tick = 6395, _time = 1.6546e+09)
[2022-06-07 15:14:36,242][root][INFO] - Step 31324160 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31324160, mean_episode_return = None, mean_episode_step = 2272.3, total_loss = 149.43, pg_loss = 93.176, baseline_loss = 60.96, entropy_loss = -4.7106, learner_queue_size = 32, train_seconds = 1.7449e+04, _tick = 6397, _time = 1.6546e+09)
[2022-06-07 15:14:41,244][root][INFO] - Step 31331840 @ 1535.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31331840, mean_episode_return = None, mean_episode_step = 2199.5, total_loss = 387.72, pg_loss = 209.46, baseline_loss = 182.86, entropy_loss = -4.6066, learner_queue_size = 32, train_seconds = 1.7454e+04, _tick = 6399, _time = 1.6546e+09)
[2022-06-07 15:14:46,246][root][INFO] - Step 31342080 @ 2047.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 31342080, mean_episode_return = None, mean_episode_step = 2070.8, total_loss = 181.79, pg_loss = 121.42, baseline_loss = 64.917, entropy_loss = -4.5464, learner_queue_size = 32, train_seconds = 1.7459e+04, _tick = 6401, _time = 1.6546e+09)
[2022-06-07 15:14:51,250][root][INFO] - Step 31349760 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31349760, mean_episode_return = None, mean_episode_step = 1799.7, total_loss = 70.006, pg_loss = 32.794, baseline_loss = 41.807, entropy_loss = -4.5945, learner_queue_size = 32, train_seconds = 1.7464e+04, _tick = 6403, _time = 1.6546e+09)
[2022-06-07 15:14:56,254][root][INFO] - Step 31360000 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 31360000, mean_episode_return = None, mean_episode_step = 2038.3, total_loss = 66.17, pg_loss = 35.274, baseline_loss = 35.459, entropy_loss = -4.5631, learner_queue_size = 32, train_seconds = 1.7469e+04, _tick = 6404, _time = 1.6546e+09)
[2022-06-07 15:15:01,260][root][INFO] - Step 31367680 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31367680, mean_episode_return = 55.431, mean_episode_step = 1849.7, total_loss = -131.73, pg_loss = -152.23, baseline_loss = 25.08, entropy_loss = -4.5754, learner_queue_size = 32, train_seconds = 1.7474e+04, _tick = 6406, _time = 1.6546e+09)
[2022-06-07 15:15:06,266][root][INFO] - Step 31377920 @ 2045.6 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 31377920, mean_episode_return = 32.7, mean_episode_step = 2356.7, total_loss = 152.67, pg_loss = 81.816, baseline_loss = 75.387, entropy_loss = -4.5302, learner_queue_size = 32, train_seconds = 1.7479e+04, _tick = 6410, _time = 1.6546e+09)
[2022-06-07 15:15:11,272][root][INFO] - Step 31388160 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31388160, mean_episode_return = None, mean_episode_step = 1657.3, total_loss = 27.88, pg_loss = -24.717, baseline_loss = 57.165, entropy_loss = -4.5687, learner_queue_size = 32, train_seconds = 1.7484e+04, _tick = 6412, _time = 1.6546e+09)
[2022-06-07 15:15:16,278][root][INFO] - Step 31395840 @ 1534.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 31395840, mean_episode_return = None, mean_episode_step = 2137.8, total_loss = -58.898, pg_loss = -91.281, baseline_loss = 37.001, entropy_loss = -4.6176, learner_queue_size = 32, train_seconds = 1.7489e+04, _tick = 6412, _time = 1.6546e+09)
[2022-06-07 15:15:21,282][root][INFO] - Step 31403520 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31403520, mean_episode_return = 58.101, mean_episode_step = 2100.2, total_loss = 0.021738, pg_loss = -42.252, baseline_loss = 46.955, entropy_loss = -4.6814, learner_queue_size = 32, train_seconds = 1.7494e+04, _tick = 6414, _time = 1.6546e+09)
[2022-06-07 15:15:26,286][root][INFO] - Step 31413760 @ 2046.3 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 31413760, mean_episode_return = 45.567, mean_episode_step = 2127.0, total_loss = 373.16, pg_loss = 214.59, baseline_loss = 163.19, entropy_loss = -4.6229, learner_queue_size = 32, train_seconds = 1.7499e+04, _tick = 6417, _time = 1.6546e+09)
[2022-06-07 15:15:31,290][root][INFO] - Step 31421440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31421440, mean_episode_return = None, mean_episode_step = 2425.9, total_loss = 13.981, pg_loss = -12.234, baseline_loss = 30.817, entropy_loss = -4.6017, learner_queue_size = 32, train_seconds = 1.7504e+04, _tick = 6419, _time = 1.6546e+09)
[2022-06-07 15:15:36,301][root][INFO] - Step 31431680 @ 2043.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 31431680, mean_episode_return = 44.735, mean_episode_step = 1712.1, total_loss = -231.3, pg_loss = -271.07, baseline_loss = 44.193, entropy_loss = -4.4316, learner_queue_size = 32, train_seconds = 1.7509e+04, _tick = 6423, _time = 1.6546e+09)
[2022-06-07 15:15:41,306][root][INFO] - Step 31439360 @ 1534.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 31439360, mean_episode_return = 54.754, mean_episode_step = 2131.9, total_loss = 196.59, pg_loss = 61.091, baseline_loss = 139.97, entropy_loss = -4.4687, learner_queue_size = 32, train_seconds = 1.7514e+04, _tick = 6426, _time = 1.6546e+09)
[2022-06-07 15:15:46,312][root][INFO] - Step 31449600 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 31449600, mean_episode_return = 82.709, mean_episode_step = 2029.6, total_loss = 363.16, pg_loss = 218.3, baseline_loss = 149.37, entropy_loss = -4.5106, learner_queue_size = 32, train_seconds = 1.7519e+04, _tick = 6429, _time = 1.6546e+09)
[2022-06-07 15:15:51,318][root][INFO] - Step 31459840 @ 2045.5 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 31459840, mean_episode_return = 35.732, mean_episode_step = 1668.6, total_loss = 348.27, pg_loss = 210.49, baseline_loss = 142.3, entropy_loss = -4.5143, learner_queue_size = 32, train_seconds = 1.7524e+04, _tick = 6432, _time = 1.6546e+09)
[2022-06-07 15:15:56,322][root][INFO] - Step 31467520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31467520, mean_episode_return = None, mean_episode_step = 1980.2, total_loss = 200.63, pg_loss = 133.47, baseline_loss = 71.693, entropy_loss = -4.5373, learner_queue_size = 32, train_seconds = 1.7529e+04, _tick = 6433, _time = 1.6546e+09)
[2022-06-07 15:16:01,326][root][INFO] - Step 31477760 @ 2046.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 31477760, mean_episode_return = None, mean_episode_step = 1949.8, total_loss = -136.93, pg_loss = -136.16, baseline_loss = 3.7823, entropy_loss = -4.5505, learner_queue_size = 32, train_seconds = 1.7534e+04, _tick = 6435, _time = 1.6546e+09)
[2022-06-07 15:16:06,332][root][INFO] - Step 31485440 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 31485440, mean_episode_return = None, mean_episode_step = 1627.9, total_loss = 28.747, pg_loss = -21.801, baseline_loss = 55.06, entropy_loss = -4.5118, learner_queue_size = 32, train_seconds = 1.7539e+04, _tick = 6436, _time = 1.6546e+09)
[2022-06-07 15:16:11,338][root][INFO] - Step 31495680 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 31495680, mean_episode_return = 12.219, mean_episode_step = 1735.9, total_loss = -77.543, pg_loss = -95.71, baseline_loss = 22.665, entropy_loss = -4.4985, learner_queue_size = 32, train_seconds = 1.7544e+04, _tick = 6440, _time = 1.6546e+09)
[2022-06-07 15:16:16,344][root][INFO] - Step 31503360 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31503360, mean_episode_return = 48.555, mean_episode_step = 1921.7, total_loss = 365.72, pg_loss = 200.96, baseline_loss = 169.27, entropy_loss = -4.5169, learner_queue_size = 32, train_seconds = 1.7549e+04, _tick = 6443, _time = 1.6546e+09)
[2022-06-07 15:16:21,350][root][INFO] - Step 31513600 @ 2045.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 31513600, mean_episode_return = None, mean_episode_step = 1564.7, total_loss = 337.95, pg_loss = 226.83, baseline_loss = 115.69, entropy_loss = -4.5651, learner_queue_size = 32, train_seconds = 1.7554e+04, _tick = 6445, _time = 1.6546e+09)
[2022-06-07 15:16:26,354][root][INFO] - Step 31521280 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31521280, mean_episode_return = None, mean_episode_step = 1680.2, total_loss = 12.575, pg_loss = -21.045, baseline_loss = 38.156, entropy_loss = -4.5359, learner_queue_size = 32, train_seconds = 1.7559e+04, _tick = 6447, _time = 1.6546e+09)
[2022-06-07 15:16:31,358][root][INFO] - Step 31531520 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31531520, mean_episode_return = None, mean_episode_step = 1570.8, total_loss = 202.04, pg_loss = 93.426, baseline_loss = 113.14, entropy_loss = -4.5256, learner_queue_size = 32, train_seconds = 1.7564e+04, _tick = 6448, _time = 1.6546e+09)
[2022-06-07 15:16:36,381][root][INFO] - Step 31539200 @ 1528.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31539200, mean_episode_return = None, mean_episode_step = 1537.4, total_loss = 599.79, pg_loss = 387.32, baseline_loss = 216.97, entropy_loss = -4.4964, learner_queue_size = 32, train_seconds = 1.7569e+04, _tick = 6450, _time = 1.6546e+09)
[2022-06-07 15:16:41,386][root][INFO] - Step 31549440 @ 2046.0 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 31549440, mean_episode_return = 111.74, mean_episode_step = 1883.2, total_loss = 426.06, pg_loss = 233.88, baseline_loss = 196.62, entropy_loss = -4.4502, learner_queue_size = 32, train_seconds = 1.7574e+04, _tick = 6452, _time = 1.6546e+09)
[2022-06-07 15:16:46,391][root][INFO] - Step 31557120 @ 1534.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31557120, mean_episode_return = 82.671, mean_episode_step = 1517.5, total_loss = 37.328, pg_loss = -10.965, baseline_loss = 52.621, entropy_loss = -4.3281, learner_queue_size = 32, train_seconds = 1.7579e+04, _tick = 6454, _time = 1.6546e+09)
[2022-06-07 15:16:51,394][root][INFO] - Step 31567360 @ 2046.9 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 31567360, mean_episode_return = None, mean_episode_step = 1799.5, total_loss = 401.85, pg_loss = 262.18, baseline_loss = 144.07, entropy_loss = -4.3988, learner_queue_size = 32, train_seconds = 1.7584e+04, _tick = 6456, _time = 1.6546e+09)
[2022-06-07 15:16:56,398][root][INFO] - Step 31577600 @ 2046.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 31577600, mean_episode_return = None, mean_episode_step = 2095.7, total_loss = 69.778, pg_loss = 28.02, baseline_loss = 46.128, entropy_loss = -4.3694, learner_queue_size = 32, train_seconds = 1.7589e+04, _tick = 6457, _time = 1.6546e+09)
[2022-06-07 15:17:01,402][root][INFO] - Step 31585280 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 31585280, mean_episode_return = None, mean_episode_step = 2280.4, total_loss = 28.217, pg_loss = 3.2432, baseline_loss = 29.354, entropy_loss = -4.3796, learner_queue_size = 32, train_seconds = 1.7594e+04, _tick = 6459, _time = 1.6546e+09)
[2022-06-07 15:17:06,406][root][INFO] - Step 31595520 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31595520, mean_episode_return = None, mean_episode_step = 1914.4, total_loss = 63.673, pg_loss = 21.931, baseline_loss = 46.223, entropy_loss = -4.481, learner_queue_size = 32, train_seconds = 1.7599e+04, _tick = 6459, _time = 1.6546e+09)
[2022-06-07 15:17:11,410][root][INFO] - Step 31603200 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 31603200, mean_episode_return = 42.705, mean_episode_step = 2081.8, total_loss = -92.423, pg_loss = -100.5, baseline_loss = 12.559, entropy_loss = -4.4787, learner_queue_size = 32, train_seconds = 1.7604e+04, _tick = 6460, _time = 1.6546e+09)
[2022-06-07 15:17:16,416][root][INFO] - Step 31613440 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 31613440, mean_episode_return = 72.599, mean_episode_step = 1839.3, total_loss = 20.335, pg_loss = -19.213, baseline_loss = 44.061, entropy_loss = -4.5127, learner_queue_size = 32, train_seconds = 1.7609e+04, _tick = 6461, _time = 1.6546e+09)
[2022-06-07 15:17:21,422][root][INFO] - Step 31621120 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31621120, mean_episode_return = 91.623, mean_episode_step = 2148.6, total_loss = 125.36, pg_loss = 55.375, baseline_loss = 74.397, entropy_loss = -4.408, learner_queue_size = 32, train_seconds = 1.7614e+04, _tick = 6463, _time = 1.6546e+09)
[2022-06-07 15:17:26,428][root][INFO] - Step 31631360 @ 2045.5 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 31631360, mean_episode_return = 31.48, mean_episode_step = 1785.1, total_loss = 291.27, pg_loss = 146.45, baseline_loss = 149.19, entropy_loss = -4.3751, learner_queue_size = 32, train_seconds = 1.7619e+04, _tick = 6465, _time = 1.6546e+09)
[2022-06-07 15:17:31,434][root][INFO] - Step 31639040 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31639040, mean_episode_return = 24.02, mean_episode_step = 1977.1, total_loss = 296.06, pg_loss = 144.14, baseline_loss = 156.26, entropy_loss = -4.3475, learner_queue_size = 32, train_seconds = 1.7624e+04, _tick = 6467, _time = 1.6546e+09)
[2022-06-07 15:17:36,438][root][INFO] - Step 31649280 @ 2046.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 31649280, mean_episode_return = None, mean_episode_step = 2634.3, total_loss = -5.6891, pg_loss = -22.443, baseline_loss = 21.111, entropy_loss = -4.3573, learner_queue_size = 32, train_seconds = 1.7629e+04, _tick = 6467, _time = 1.6546e+09)
[2022-06-07 15:17:41,444][root][INFO] - Step 31656960 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 31656960, mean_episode_return = 37.063, mean_episode_step = 1835.4, total_loss = 377.46, pg_loss = 219.11, baseline_loss = 162.75, entropy_loss = -4.4008, learner_queue_size = 32, train_seconds = 1.7634e+04, _tick = 6469, _time = 1.6546e+09)
[2022-06-07 15:17:46,450][root][INFO] - Step 31667200 @ 2045.6 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 31667200, mean_episode_return = 66.218, mean_episode_step = 2641.7, total_loss = 27.786, pg_loss = -22.209, baseline_loss = 54.399, entropy_loss = -4.4045, learner_queue_size = 32, train_seconds = 1.7639e+04, _tick = 6472, _time = 1.6546e+09)
[2022-06-07 15:17:51,454][root][INFO] - Step 31677440 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 31677440, mean_episode_return = 44.962, mean_episode_step = 1829.3, total_loss = 103.79, pg_loss = 27.378, baseline_loss = 80.789, entropy_loss = -4.3727, learner_queue_size = 32, train_seconds = 1.7644e+04, _tick = 6475, _time = 1.6546e+09)
[2022-06-07 15:17:56,460][root][INFO] - Step 31685120 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 31685120, mean_episode_return = None, mean_episode_step = 2160.4, total_loss = -98.696, pg_loss = -100.75, baseline_loss = 6.4225, entropy_loss = -4.368, learner_queue_size = 32, train_seconds = 1.765e+04, _tick = 6476, _time = 1.6546e+09)
[2022-06-07 15:18:01,461][root][INFO] - Step 31695360 @ 2047.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 31695360, mean_episode_return = None, mean_episode_step = 2247.6, total_loss = 119.4, pg_loss = 75.559, baseline_loss = 48.224, entropy_loss = -4.385, learner_queue_size = 32, train_seconds = 1.7654e+04, _tick = 6476, _time = 1.6546e+09)
[2022-06-07 15:18:06,466][root][INFO] - Step 31703040 @ 1534.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31703040, mean_episode_return = None, mean_episode_step = 1798.4, total_loss = 173.66, pg_loss = 84.739, baseline_loss = 93.334, entropy_loss = -4.4131, learner_queue_size = 32, train_seconds = 1.766e+04, _tick = 6477, _time = 1.6546e+09)
[2022-06-07 15:18:11,472][root][INFO] - Step 31713280 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 31713280, mean_episode_return = 28.241, mean_episode_step = 2055.0, total_loss = -124.56, pg_loss = -149.93, baseline_loss = 29.757, entropy_loss = -4.3885, learner_queue_size = 32, train_seconds = 1.7664e+04, _tick = 6478, _time = 1.6546e+09)
[2022-06-07 15:18:16,478][root][INFO] - Step 31720960 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 31720960, mean_episode_return = 25.73, mean_episode_step = 1917.7, total_loss = -100.49, pg_loss = -151.11, baseline_loss = 55.045, entropy_loss = -4.4231, learner_queue_size = 32, train_seconds = 1.767e+04, _tick = 6480, _time = 1.6546e+09)
[2022-06-07 15:18:21,482][root][INFO] - Step 31731200 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31731200, mean_episode_return = None, mean_episode_step = 2362.5, total_loss = -125.45, pg_loss = -125.76, baseline_loss = 4.7379, entropy_loss = -4.4225, learner_queue_size = 32, train_seconds = 1.7674e+04, _tick = 6481, _time = 1.6546e+09)
[2022-06-07 15:18:26,486][root][INFO] - Step 31738880 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31738880, mean_episode_return = 8.3898, mean_episode_step = 2581.0, total_loss = -11.774, pg_loss = -44.09, baseline_loss = 36.758, entropy_loss = -4.4421, learner_queue_size = 32, train_seconds = 1.768e+04, _tick = 6483, _time = 1.6546e+09)
[2022-06-07 15:18:31,490][root][INFO] - Step 31749120 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 31749120, mean_episode_return = None, mean_episode_step = 1996.7, total_loss = -150.18, pg_loss = -149.02, baseline_loss = 3.324, entropy_loss = -4.4912, learner_queue_size = 32, train_seconds = 1.7684e+04, _tick = 6484, _time = 1.6546e+09)
[2022-06-07 15:18:36,494][root][INFO] - Step 31756800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31756800, mean_episode_return = 62.614, mean_episode_step = 2192.2, total_loss = -42.993, pg_loss = -76.826, baseline_loss = 38.318, entropy_loss = -4.486, learner_queue_size = 32, train_seconds = 1.769e+04, _tick = 6487, _time = 1.6546e+09)
[2022-06-07 15:18:41,500][root][INFO] - Step 31767040 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 31767040, mean_episode_return = None, mean_episode_step = 2392.5, total_loss = 181.24, pg_loss = 126.06, baseline_loss = 59.759, entropy_loss = -4.5806, learner_queue_size = 32, train_seconds = 1.7694e+04, _tick = 6487, _time = 1.6546e+09)
[2022-06-07 15:18:46,506][root][INFO] - Step 31774720 @ 1534.2 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 31774720, mean_episode_return = None, mean_episode_step = 2079.8, total_loss = 403.08, pg_loss = 238.02, baseline_loss = 169.6, entropy_loss = -4.5434, learner_queue_size = 32, train_seconds = 1.77e+04, _tick = 6487, _time = 1.6546e+09)
[2022-06-07 15:18:51,510][root][INFO] - Step 31784960 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 31784960, mean_episode_return = None, mean_episode_step = 2710.8, total_loss = 4.9021, pg_loss = -27.955, baseline_loss = 37.418, entropy_loss = -4.5609, learner_queue_size = 32, train_seconds = 1.7704e+04, _tick = 6490, _time = 1.6546e+09)
[2022-06-07 15:18:56,516][root][INFO] - Step 31792640 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31792640, mean_episode_return = 86.647, mean_episode_step = 2270.9, total_loss = 205.36, pg_loss = 137.32, baseline_loss = 72.538, entropy_loss = -4.4993, learner_queue_size = 32, train_seconds = 1.771e+04, _tick = 6493, _time = 1.6546e+09)
[2022-06-07 15:19:01,522][root][INFO] - Step 31802880 @ 2045.6 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 31802880, mean_episode_return = None, mean_episode_step = 2456.2, total_loss = -183.57, pg_loss = -180.51, baseline_loss = 1.463, entropy_loss = -4.532, learner_queue_size = 32, train_seconds = 1.7714e+04, _tick = 6494, _time = 1.6546e+09)
[2022-06-07 15:19:06,528][root][INFO] - Step 31810560 @ 1534.1 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 31810560, mean_episode_return = 27.67, mean_episode_step = 1818.3, total_loss = 231.1, pg_loss = 148.77, baseline_loss = 86.881, entropy_loss = -4.5454, learner_queue_size = 32, train_seconds = 1.772e+04, _tick = 6497, _time = 1.6546e+09)
[2022-06-07 15:19:11,534][root][INFO] - Step 31820800 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 31820800, mean_episode_return = 51.784, mean_episode_step = 2561.2, total_loss = -83.128, pg_loss = -128.67, baseline_loss = 50.081, entropy_loss = -4.5424, learner_queue_size = 32, train_seconds = 1.7724e+04, _tick = 6499, _time = 1.6546e+09)
[2022-06-07 15:19:16,538][root][INFO] - Step 31828480 @ 1534.9 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 31828480, mean_episode_return = None, mean_episode_step = 2162.2, total_loss = -29.496, pg_loss = -38.278, baseline_loss = 13.28, entropy_loss = -4.4978, learner_queue_size = 32, train_seconds = 1.773e+04, _tick = 6501, _time = 1.6546e+09)
[2022-06-07 15:19:21,544][root][INFO] - Step 31838720 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 31838720, mean_episode_return = 49.94, mean_episode_step = 2427.0, total_loss = -54.247, pg_loss = -88.785, baseline_loss = 39.06, entropy_loss = -4.5217, learner_queue_size = 32, train_seconds = 1.7734e+04, _tick = 6505, _time = 1.6546e+09)
[2022-06-07 15:19:26,550][root][INFO] - Step 31846400 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 31846400, mean_episode_return = 30.47, mean_episode_step = 1801.3, total_loss = 240.99, pg_loss = 115.61, baseline_loss = 129.89, entropy_loss = -4.5075, learner_queue_size = 32, train_seconds = 1.774e+04, _tick = 6508, _time = 1.6546e+09)
[2022-06-07 15:19:31,554][root][INFO] - Step 31856640 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 31856640, mean_episode_return = 44.383, mean_episode_step = 1935.5, total_loss = -21.136, pg_loss = -53.606, baseline_loss = 36.893, entropy_loss = -4.4228, learner_queue_size = 32, train_seconds = 1.7744e+04, _tick = 6511, _time = 1.6546e+09)
[2022-06-07 15:19:36,558][root][INFO] - Step 31864320 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31864320, mean_episode_return = 67.539, mean_episode_step = 2134.6, total_loss = -24.967, pg_loss = -66.485, baseline_loss = 45.972, entropy_loss = -4.4536, learner_queue_size = 32, train_seconds = 1.775e+04, _tick = 6513, _time = 1.6546e+09)
[2022-06-07 15:19:41,562][root][INFO] - Step 31874560 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 31874560, mean_episode_return = 35.383, mean_episode_step = 1854.6, total_loss = 80.8, pg_loss = 41.757, baseline_loss = 43.522, entropy_loss = -4.4787, learner_queue_size = 32, train_seconds = 1.7755e+04, _tick = 6515, _time = 1.6546e+09)
[2022-06-07 15:19:46,566][root][INFO] - Step 31882240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31882240, mean_episode_return = 18.13, mean_episode_step = 1668.9, total_loss = 289.48, pg_loss = 170.92, baseline_loss = 122.96, entropy_loss = -4.4006, learner_queue_size = 32, train_seconds = 1.776e+04, _tick = 6517, _time = 1.6546e+09)
[2022-06-07 15:19:51,572][root][INFO] - Step 31892480 @ 2045.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 31892480, mean_episode_return = None, mean_episode_step = 2369.2, total_loss = -124.56, pg_loss = -124.57, baseline_loss = 4.3523, entropy_loss = -4.3488, learner_queue_size = 32, train_seconds = 1.7765e+04, _tick = 6518, _time = 1.6546e+09)
[2022-06-07 15:19:56,578][root][INFO] - Step 31902720 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31902720, mean_episode_return = None, mean_episode_step = 1774.5, total_loss = 82.895, pg_loss = 30.389, baseline_loss = 56.862, entropy_loss = -4.3555, learner_queue_size = 32, train_seconds = 1.777e+04, _tick = 6519, _time = 1.6546e+09)
[2022-06-07 15:20:01,582][root][INFO] - Step 31910400 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 31910400, mean_episode_return = 166.2, mean_episode_step = 1836.9, total_loss = -137.23, pg_loss = -140.35, baseline_loss = 7.4923, entropy_loss = -4.3653, learner_queue_size = 32, train_seconds = 1.7775e+04, _tick = 6521, _time = 1.6546e+09)
[2022-06-07 15:20:06,586][root][INFO] - Step 31920640 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 31920640, mean_episode_return = None, mean_episode_step = 2249.3, total_loss = -83.583, pg_loss = -93.32, baseline_loss = 14.206, entropy_loss = -4.4692, learner_queue_size = 32, train_seconds = 1.778e+04, _tick = 6522, _time = 1.6546e+09)
[2022-06-07 15:20:11,590][root][INFO] - Step 31928320 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31928320, mean_episode_return = None, mean_episode_step = 2022.5, total_loss = 109.89, pg_loss = 61.711, baseline_loss = 52.667, entropy_loss = -4.4835, learner_queue_size = 32, train_seconds = 1.7785e+04, _tick = 6522, _time = 1.6546e+09)
[2022-06-07 15:20:16,594][root][INFO] - Step 31938560 @ 2046.4 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 31938560, mean_episode_return = None, mean_episode_step = 2066.8, total_loss = -126.11, pg_loss = -124.46, baseline_loss = 2.9406, entropy_loss = -4.5929, learner_queue_size = 32, train_seconds = 1.779e+04, _tick = 6523, _time = 1.6546e+09)
[2022-06-07 15:20:21,600][root][INFO] - Step 31946240 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 31946240, mean_episode_return = None, mean_episode_step = 2156.5, total_loss = -99.292, pg_loss = -98.889, baseline_loss = 4.2725, entropy_loss = -4.6749, learner_queue_size = 32, train_seconds = 1.7795e+04, _tick = 6523, _time = 1.6546e+09)
[2022-06-07 15:20:26,606][root][INFO] - Step 31956480 @ 2045.6 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 31956480, mean_episode_return = None, mean_episode_step = 2605.9, total_loss = -165.13, pg_loss = -165.73, baseline_loss = 5.2614, entropy_loss = -4.6599, learner_queue_size = 32, train_seconds = 1.78e+04, _tick = 6524, _time = 1.6546e+09)
[2022-06-07 15:20:31,612][root][INFO] - Step 31964160 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 31964160, mean_episode_return = None, mean_episode_step = 2290.2, total_loss = 171.65, pg_loss = 105.01, baseline_loss = 71.276, entropy_loss = -4.6424, learner_queue_size = 32, train_seconds = 1.7805e+04, _tick = 6525, _time = 1.6546e+09)
[2022-06-07 15:20:36,618][root][INFO] - Step 31974400 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 31974400, mean_episode_return = None, mean_episode_step = 3084.4, total_loss = -106.56, pg_loss = -106.46, baseline_loss = 4.5181, entropy_loss = -4.6135, learner_queue_size = 32, train_seconds = 1.781e+04, _tick = 6526, _time = 1.6546e+09)
[2022-06-07 15:20:41,624][root][INFO] - Step 31982080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 31982080, mean_episode_return = 74.641, mean_episode_step = 2597.0, total_loss = -171.51, pg_loss = -173.25, baseline_loss = 6.4133, entropy_loss = -4.6783, learner_queue_size = 32, train_seconds = 1.7815e+04, _tick = 6528, _time = 1.6546e+09)
[2022-06-07 15:20:46,630][root][INFO] - Step 31992320 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 31992320, mean_episode_return = None, mean_episode_step = 2470.9, total_loss = 20.412, pg_loss = -17.419, baseline_loss = 42.466, entropy_loss = -4.6358, learner_queue_size = 32, train_seconds = 1.782e+04, _tick = 6528, _time = 1.6546e+09)
[2022-06-07 15:20:51,636][root][INFO] - Step 32002560 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 32002560, mean_episode_return = 61.796, mean_episode_step = 1935.3, total_loss = 587.96, pg_loss = 411.94, baseline_loss = 180.5, entropy_loss = -4.4856, learner_queue_size = 32, train_seconds = 1.7825e+04, _tick = 6531, _time = 1.6546e+09)
[2022-06-07 15:20:56,642][root][INFO] - Step 32010240 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 32010240, mean_episode_return = 46.622, mean_episode_step = 2725.0, total_loss = -122.7, pg_loss = -144.6, baseline_loss = 26.365, entropy_loss = -4.4714, learner_queue_size = 32, train_seconds = 1.783e+04, _tick = 6533, _time = 1.6546e+09)
[2022-06-07 15:21:01,648][root][INFO] - Step 32020480 @ 2045.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 32020480, mean_episode_return = None, mean_episode_step = 2318.4, total_loss = 66.424, pg_loss = 45.351, baseline_loss = 25.462, entropy_loss = -4.3893, learner_queue_size = 32, train_seconds = 1.7835e+04, _tick = 6534, _time = 1.6546e+09)
[2022-06-07 15:21:06,650][root][INFO] - Step 32028160 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32028160, mean_episode_return = None, mean_episode_step = 2300.3, total_loss = -98.941, pg_loss = -100.11, baseline_loss = 5.6818, entropy_loss = -4.513, learner_queue_size = 32, train_seconds = 1.784e+04, _tick = 6534, _time = 1.6546e+09)
[2022-06-07 15:21:11,656][root][INFO] - Step 32038400 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32038400, mean_episode_return = 40.022, mean_episode_step = 2818.8, total_loss = 481.89, pg_loss = 287.28, baseline_loss = 199.18, entropy_loss = -4.5732, learner_queue_size = 32, train_seconds = 1.7845e+04, _tick = 6535, _time = 1.6546e+09)
[2022-06-07 15:21:16,662][root][INFO] - Step 32046080 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32046080, mean_episode_return = 18.83, mean_episode_step = 2071.7, total_loss = -25.198, pg_loss = -71.065, baseline_loss = 50.462, entropy_loss = -4.5951, learner_queue_size = 32, train_seconds = 1.785e+04, _tick = 6537, _time = 1.6546e+09)
[2022-06-07 15:21:21,667][root][INFO] - Step 32056320 @ 2045.7 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32056320, mean_episode_return = None, mean_episode_step = 2524.5, total_loss = -108.87, pg_loss = -117.71, baseline_loss = 13.412, entropy_loss = -4.5774, learner_queue_size = 32, train_seconds = 1.7855e+04, _tick = 6538, _time = 1.6546e+09)
[2022-06-07 15:21:26,670][root][INFO] - Step 32064000 @ 1535.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32064000, mean_episode_return = None, mean_episode_step = 2096.8, total_loss = 26.028, pg_loss = 3.1127, baseline_loss = 27.607, entropy_loss = -4.6921, learner_queue_size = 32, train_seconds = 1.786e+04, _tick = 6538, _time = 1.6546e+09)
[2022-06-07 15:21:31,674][root][INFO] - Step 32074240 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32074240, mean_episode_return = None, mean_episode_step = 2038.4, total_loss = 323.23, pg_loss = 236.59, baseline_loss = 91.45, entropy_loss = -4.8167, learner_queue_size = 32, train_seconds = 1.7865e+04, _tick = 6540, _time = 1.6546e+09)
[2022-06-07 15:21:36,678][root][INFO] - Step 32081920 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 32081920, mean_episode_return = -1.8903, mean_episode_step = 2257.3, total_loss = 530.03, pg_loss = 361.0, baseline_loss = 173.85, entropy_loss = -4.8225, learner_queue_size = 32, train_seconds = 1.787e+04, _tick = 6541, _time = 1.6546e+09)
[2022-06-07 15:21:41,682][root][INFO] - Step 32092160 @ 2046.3 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 32092160, mean_episode_return = None, mean_episode_step = 2028.3, total_loss = 262.24, pg_loss = 208.52, baseline_loss = 58.518, entropy_loss = -4.8006, learner_queue_size = 32, train_seconds = 1.7875e+04, _tick = 6544, _time = 1.6546e+09)
[2022-06-07 15:21:46,686][root][INFO] - Step 32099840 @ 1534.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 32099840, mean_episode_return = 84.509, mean_episode_step = 2188.3, total_loss = -118.88, pg_loss = -119.65, baseline_loss = 5.5995, entropy_loss = -4.8238, learner_queue_size = 32, train_seconds = 1.788e+04, _tick = 6547, _time = 1.6546e+09)
[2022-06-07 15:21:51,690][root][INFO] - Step 32110080 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32110080, mean_episode_return = 16.93, mean_episode_step = 2569.4, total_loss = 350.52, pg_loss = 228.11, baseline_loss = 127.27, entropy_loss = -4.8611, learner_queue_size = 32, train_seconds = 1.7885e+04, _tick = 6550, _time = 1.6546e+09)
[2022-06-07 15:21:56,694][root][INFO] - Step 32117760 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32117760, mean_episode_return = 96.243, mean_episode_step = 2506.1, total_loss = 813.08, pg_loss = 571.39, baseline_loss = 246.51, entropy_loss = -4.8191, learner_queue_size = 32, train_seconds = 1.789e+04, _tick = 6552, _time = 1.6546e+09)
[2022-06-07 15:22:01,698][root][INFO] - Step 32128000 @ 2046.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 32128000, mean_episode_return = 21.31, mean_episode_step = 2549.0, total_loss = -28.369, pg_loss = -41.977, baseline_loss = 18.417, entropy_loss = -4.8097, learner_queue_size = 32, train_seconds = 1.7895e+04, _tick = 6555, _time = 1.6546e+09)
[2022-06-07 15:22:06,704][root][INFO] - Step 32135680 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 32135680, mean_episode_return = None, mean_episode_step = 2101.1, total_loss = 471.55, pg_loss = 332.23, baseline_loss = 144.15, entropy_loss = -4.8269, learner_queue_size = 32, train_seconds = 1.79e+04, _tick = 6556, _time = 1.6546e+09)
[2022-06-07 15:22:11,710][root][INFO] - Step 32145920 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32145920, mean_episode_return = 85.184, mean_episode_step = 2366.0, total_loss = 67.099, pg_loss = 35.503, baseline_loss = 36.426, entropy_loss = -4.8304, learner_queue_size = 32, train_seconds = 1.7905e+04, _tick = 6559, _time = 1.6546e+09)
[2022-06-07 15:22:16,716][root][INFO] - Step 32156160 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 32156160, mean_episode_return = 5.8797, mean_episode_step = 1941.4, total_loss = 7.5526, pg_loss = -31.375, baseline_loss = 43.661, entropy_loss = -4.733, learner_queue_size = 32, train_seconds = 1.791e+04, _tick = 6562, _time = 1.6546e+09)
[2022-06-07 15:22:21,722][root][INFO] - Step 32163840 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32163840, mean_episode_return = 79.917, mean_episode_step = 1935.4, total_loss = 21.633, pg_loss = -63.011, baseline_loss = 89.389, entropy_loss = -4.7446, learner_queue_size = 32, train_seconds = 1.7915e+04, _tick = 6563, _time = 1.6546e+09)
[2022-06-07 15:22:26,728][root][INFO] - Step 32174080 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32174080, mean_episode_return = 96.288, mean_episode_step = 2187.3, total_loss = 598.09, pg_loss = 429.22, baseline_loss = 173.53, entropy_loss = -4.6613, learner_queue_size = 32, train_seconds = 1.792e+04, _tick = 6566, _time = 1.6546e+09)
[2022-06-07 15:22:31,734][root][INFO] - Step 32181760 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32181760, mean_episode_return = None, mean_episode_step = 1717.4, total_loss = 59.576, pg_loss = 37.494, baseline_loss = 26.735, entropy_loss = -4.6538, learner_queue_size = 32, train_seconds = 1.7925e+04, _tick = 6568, _time = 1.6546e+09)
[2022-06-07 15:22:36,738][root][INFO] - Step 32192000 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 32192000, mean_episode_return = None, mean_episode_step = 1845.9, total_loss = -85.453, pg_loss = -85.773, baseline_loss = 5.1217, entropy_loss = -4.8011, learner_queue_size = 32, train_seconds = 1.793e+04, _tick = 6570, _time = 1.6546e+09)
[2022-06-07 15:22:41,744][root][INFO] - Step 32199680 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 32199680, mean_episode_return = None, mean_episode_step = 1955.3, total_loss = 328.06, pg_loss = 214.09, baseline_loss = 118.79, entropy_loss = -4.8196, learner_queue_size = 32, train_seconds = 1.7935e+04, _tick = 6571, _time = 1.6546e+09)
[2022-06-07 15:22:46,750][root][INFO] - Step 32209920 @ 2045.7 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 32209920, mean_episode_return = 57.91, mean_episode_step = 2077.3, total_loss = 52.431, pg_loss = 15.801, baseline_loss = 41.376, entropy_loss = -4.746, learner_queue_size = 32, train_seconds = 1.794e+04, _tick = 6573, _time = 1.6546e+09)
[2022-06-07 15:22:51,754][root][INFO] - Step 32217600 @ 1534.7 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 32217600, mean_episode_return = -31.731, mean_episode_step = 2638.7, total_loss = 126.74, pg_loss = 36.388, baseline_loss = 95.043, entropy_loss = -4.6904, learner_queue_size = 32, train_seconds = 1.7945e+04, _tick = 6576, _time = 1.6546e+09)
[2022-06-07 15:22:56,757][root][INFO] - Step 32225280 @ 1534.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32225280, mean_episode_return = None, mean_episode_step = 2055.4, total_loss = 129.91, pg_loss = 85.519, baseline_loss = 49.054, entropy_loss = -4.6632, learner_queue_size = 32, train_seconds = 1.795e+04, _tick = 6576, _time = 1.6546e+09)
[2022-06-07 15:23:01,762][root][INFO] - Step 32235520 @ 2046.2 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 32235520, mean_episode_return = -10.931, mean_episode_step = 2097.8, total_loss = 430.19, pg_loss = 324.22, baseline_loss = 110.64, entropy_loss = -4.674, learner_queue_size = 32, train_seconds = 1.7955e+04, _tick = 6579, _time = 1.6546e+09)
[2022-06-07 15:23:06,766][root][INFO] - Step 32243200 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32243200, mean_episode_return = None, mean_episode_step = 2225.9, total_loss = 78.622, pg_loss = 31.134, baseline_loss = 52.181, entropy_loss = -4.6927, learner_queue_size = 32, train_seconds = 1.796e+04, _tick = 6580, _time = 1.6546e+09)
[2022-06-07 15:23:11,771][root][INFO] - Step 32253440 @ 2045.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32253440, mean_episode_return = 85.974, mean_episode_step = 2735.7, total_loss = -308.2, pg_loss = -322.13, baseline_loss = 18.694, entropy_loss = -4.7662, learner_queue_size = 32, train_seconds = 1.7965e+04, _tick = 6582, _time = 1.6546e+09)
[2022-06-07 15:23:16,774][root][INFO] - Step 32263680 @ 2047.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 32263680, mean_episode_return = 6.6099, mean_episode_step = 1908.6, total_loss = -11.8, pg_loss = -61.777, baseline_loss = 54.864, entropy_loss = -4.8875, learner_queue_size = 32, train_seconds = 1.797e+04, _tick = 6585, _time = 1.6546e+09)
[2022-06-07 15:23:21,778][root][INFO] - Step 32271360 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32271360, mean_episode_return = None, mean_episode_step = 1624.5, total_loss = 418.68, pg_loss = 316.48, baseline_loss = 107.09, entropy_loss = -4.8953, learner_queue_size = 32, train_seconds = 1.7975e+04, _tick = 6587, _time = 1.6546e+09)
[2022-06-07 15:23:26,782][root][INFO] - Step 32279040 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32279040, mean_episode_return = 53.447, mean_episode_step = 1731.3, total_loss = 38.11, pg_loss = -38.093, baseline_loss = 80.993, entropy_loss = -4.79, learner_queue_size = 32, train_seconds = 1.798e+04, _tick = 6589, _time = 1.6546e+09)
[2022-06-07 15:23:31,786][root][INFO] - Step 32289280 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32289280, mean_episode_return = 22.332, mean_episode_step = 1650.2, total_loss = 81.547, pg_loss = 32.585, baseline_loss = 53.707, entropy_loss = -4.7447, learner_queue_size = 32, train_seconds = 1.7985e+04, _tick = 6593, _time = 1.6546e+09)
[2022-06-07 15:23:36,792][root][INFO] - Step 32299520 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 32299520, mean_episode_return = None, mean_episode_step = 2422.9, total_loss = 28.387, pg_loss = 2.1494, baseline_loss = 30.99, entropy_loss = -4.7529, learner_queue_size = 32, train_seconds = 1.799e+04, _tick = 6594, _time = 1.6546e+09)
[2022-06-07 15:23:41,795][root][INFO] - Step 32307200 @ 1535.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32307200, mean_episode_return = None, mean_episode_step = 1739.3, total_loss = 210.03, pg_loss = 117.41, baseline_loss = 97.468, entropy_loss = -4.8522, learner_queue_size = 32, train_seconds = 1.7995e+04, _tick = 6595, _time = 1.6546e+09)
[2022-06-07 15:23:46,801][root][INFO] - Step 32317440 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 32317440, mean_episode_return = 68.496, mean_episode_step = 1880.7, total_loss = 94.308, pg_loss = 41.658, baseline_loss = 57.424, entropy_loss = -4.7743, learner_queue_size = 32, train_seconds = 1.8e+04, _tick = 6596, _time = 1.6546e+09)
[2022-06-07 15:23:51,806][root][INFO] - Step 32325120 @ 1534.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32325120, mean_episode_return = None, mean_episode_step = 2120.7, total_loss = -153.18, pg_loss = -161.3, baseline_loss = 12.899, entropy_loss = -4.7755, learner_queue_size = 32, train_seconds = 1.8005e+04, _tick = 6597, _time = 1.6546e+09)
[2022-06-07 15:23:56,810][root][INFO] - Step 32335360 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32335360, mean_episode_return = None, mean_episode_step = 2063.7, total_loss = 35.629, pg_loss = -0.043923, baseline_loss = 40.468, entropy_loss = -4.7952, learner_queue_size = 32, train_seconds = 1.801e+04, _tick = 6599, _time = 1.6546e+09)
[2022-06-07 15:24:01,816][root][INFO] - Step 32343040 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32343040, mean_episode_return = 71.906, mean_episode_step = 2168.7, total_loss = 147.58, pg_loss = 17.713, baseline_loss = 134.78, entropy_loss = -4.9127, learner_queue_size = 32, train_seconds = 1.8015e+04, _tick = 6601, _time = 1.6546e+09)
[2022-06-07 15:24:06,822][root][INFO] - Step 32353280 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32353280, mean_episode_return = None, mean_episode_step = 1551.3, total_loss = 51.596, pg_loss = -2.9314, baseline_loss = 59.344, entropy_loss = -4.8165, learner_queue_size = 32, train_seconds = 1.802e+04, _tick = 6602, _time = 1.6546e+09)
[2022-06-07 15:24:11,826][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 15:24:12,000][root][INFO] - Step 32360960 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32360960, mean_episode_return = 16.43, mean_episode_step = 1983.3, total_loss = 132.42, pg_loss = 52.389, baseline_loss = 84.818, entropy_loss = -4.7904, learner_queue_size = 32, train_seconds = 1.8025e+04, _tick = 6604, _time = 1.6546e+09)
[2022-06-07 15:24:17,010][root][INFO] - Step 32371200 @ 1975.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32371200, mean_episode_return = None, mean_episode_step = 1548.2, total_loss = -56.354, pg_loss = -74.463, baseline_loss = 23.01, entropy_loss = -4.9016, learner_queue_size = 32, train_seconds = 1.803e+04, _tick = 6604, _time = 1.6546e+09)
[2022-06-07 15:24:22,014][root][INFO] - Step 32378880 @ 1534.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32378880, mean_episode_return = None, mean_episode_step = 2452.3, total_loss = 75.926, pg_loss = 15.872, baseline_loss = 65.001, entropy_loss = -4.9468, learner_queue_size = 32, train_seconds = 1.8035e+04, _tick = 6604, _time = 1.6546e+09)
[2022-06-07 15:24:27,018][root][INFO] - Step 32389120 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 32389120, mean_episode_return = 15.0, mean_episode_step = 2169.0, total_loss = 40.306, pg_loss = -23.396, baseline_loss = 68.6, entropy_loss = -4.8979, learner_queue_size = 32, train_seconds = 1.804e+04, _tick = 6606, _time = 1.6546e+09)
[2022-06-07 15:24:32,022][root][INFO] - Step 32396800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32396800, mean_episode_return = None, mean_episode_step = 2395.5, total_loss = -51.47, pg_loss = -68.848, baseline_loss = 22.2, entropy_loss = -4.8222, learner_queue_size = 32, train_seconds = 1.8045e+04, _tick = 6608, _time = 1.6546e+09)
[2022-06-07 15:24:37,026][root][INFO] - Step 32407040 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32407040, mean_episode_return = -6.7903, mean_episode_step = 2123.9, total_loss = -93.166, pg_loss = -118.84, baseline_loss = 30.595, entropy_loss = -4.9195, learner_queue_size = 32, train_seconds = 1.805e+04, _tick = 6609, _time = 1.6546e+09)
[2022-06-07 15:24:42,031][root][INFO] - Step 32417280 @ 2046.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 32417280, mean_episode_return = None, mean_episode_step = 2091.8, total_loss = -113.64, pg_loss = -116.05, baseline_loss = 7.2797, entropy_loss = -4.8728, learner_queue_size = 32, train_seconds = 1.8055e+04, _tick = 6610, _time = 1.6546e+09)
[2022-06-07 15:24:47,034][root][INFO] - Step 32424960 @ 1535.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 32424960, mean_episode_return = None, mean_episode_step = 2174.7, total_loss = 230.93, pg_loss = 138.66, baseline_loss = 97.192, entropy_loss = -4.9169, learner_queue_size = 32, train_seconds = 1.806e+04, _tick = 6610, _time = 1.6546e+09)
[2022-06-07 15:24:52,040][root][INFO] - Step 32435200 @ 2045.6 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 32435200, mean_episode_return = 33.902, mean_episode_step = 1833.4, total_loss = 234.97, pg_loss = 147.25, baseline_loss = 92.657, entropy_loss = -4.9374, learner_queue_size = 32, train_seconds = 1.8065e+04, _tick = 6614, _time = 1.6546e+09)
[2022-06-07 15:24:57,042][root][INFO] - Step 32442880 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32442880, mean_episode_return = 16.171, mean_episode_step = 2180.4, total_loss = 173.05, pg_loss = 93.042, baseline_loss = 84.982, entropy_loss = -4.9712, learner_queue_size = 32, train_seconds = 1.807e+04, _tick = 6615, _time = 1.6546e+09)
[2022-06-07 15:25:02,047][root][INFO] - Step 32453120 @ 2046.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 32453120, mean_episode_return = -10.371, mean_episode_step = 2300.3, total_loss = 83.686, pg_loss = -22.52, baseline_loss = 111.15, entropy_loss = -4.9449, learner_queue_size = 32, train_seconds = 1.8075e+04, _tick = 6616, _time = 1.6546e+09)
[2022-06-07 15:25:07,050][root][INFO] - Step 32460800 @ 1535.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32460800, mean_episode_return = 87.299, mean_episode_step = 1869.1, total_loss = 168.36, pg_loss = 82.679, baseline_loss = 90.556, entropy_loss = -4.8734, learner_queue_size = 32, train_seconds = 1.808e+04, _tick = 6618, _time = 1.6546e+09)
[2022-06-07 15:25:12,054][root][INFO] - Step 32471040 @ 2046.4 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 32471040, mean_episode_return = 62.796, mean_episode_step = 2395.2, total_loss = 287.08, pg_loss = 201.89, baseline_loss = 90.102, entropy_loss = -4.9114, learner_queue_size = 32, train_seconds = 1.8085e+04, _tick = 6621, _time = 1.6546e+09)
[2022-06-07 15:25:17,060][root][INFO] - Step 32478720 @ 1534.0 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 32478720, mean_episode_return = None, mean_episode_step = 2326.0, total_loss = -96.913, pg_loss = -101.63, baseline_loss = 9.5644, entropy_loss = -4.8452, learner_queue_size = 32, train_seconds = 1.809e+04, _tick = 6623, _time = 1.6546e+09)
[2022-06-07 15:25:22,066][root][INFO] - Step 32488960 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 32488960, mean_episode_return = -3.6506, mean_episode_step = 2376.2, total_loss = 233.78, pg_loss = 130.91, baseline_loss = 107.78, entropy_loss = -4.9077, learner_queue_size = 32, train_seconds = 1.8095e+04, _tick = 6624, _time = 1.6546e+09)
[2022-06-07 15:25:27,070][root][INFO] - Step 32496640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32496640, mean_episode_return = 12.24, mean_episode_step = 2025.8, total_loss = 197.38, pg_loss = 69.684, baseline_loss = 132.65, entropy_loss = -4.9525, learner_queue_size = 32, train_seconds = 1.81e+04, _tick = 6626, _time = 1.6546e+09)
[2022-06-07 15:25:32,074][root][INFO] - Step 32506880 @ 2046.3 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32506880, mean_episode_return = -14.05, mean_episode_step = 1970.3, total_loss = 100.56, pg_loss = 29.948, baseline_loss = 75.573, entropy_loss = -4.9648, learner_queue_size = 32, train_seconds = 1.8105e+04, _tick = 6629, _time = 1.6546e+09)
[2022-06-07 15:25:37,078][root][INFO] - Step 32514560 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 32514560, mean_episode_return = None, mean_episode_step = 2161.5, total_loss = -36.704, pg_loss = -55.761, baseline_loss = 23.992, entropy_loss = -4.9346, learner_queue_size = 32, train_seconds = 1.811e+04, _tick = 6629, _time = 1.6546e+09)
[2022-06-07 15:25:42,082][root][INFO] - Step 32524800 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 32524800, mean_episode_return = None, mean_episode_step = 1972.5, total_loss = 35.992, pg_loss = 15.978, baseline_loss = 24.817, entropy_loss = -4.8025, learner_queue_size = 32, train_seconds = 1.8115e+04, _tick = 6630, _time = 1.6546e+09)
[2022-06-07 15:25:47,088][root][INFO] - Step 32532480 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32532480, mean_episode_return = None, mean_episode_step = 2390.1, total_loss = 331.75, pg_loss = 236.65, baseline_loss = 99.97, entropy_loss = -4.8675, learner_queue_size = 32, train_seconds = 1.812e+04, _tick = 6631, _time = 1.6546e+09)
[2022-06-07 15:25:52,090][root][INFO] - Step 32542720 @ 2047.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32542720, mean_episode_return = 31.332, mean_episode_step = 1897.6, total_loss = 418.48, pg_loss = 254.54, baseline_loss = 168.92, entropy_loss = -4.9753, learner_queue_size = 32, train_seconds = 1.8125e+04, _tick = 6634, _time = 1.6546e+09)
[2022-06-07 15:25:57,094][root][INFO] - Step 32550400 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32550400, mean_episode_return = 83.129, mean_episode_step = 1948.1, total_loss = 103.39, pg_loss = 41.253, baseline_loss = 67.19, entropy_loss = -5.0553, learner_queue_size = 32, train_seconds = 1.813e+04, _tick = 6635, _time = 1.6546e+09)
[2022-06-07 15:26:02,098][root][INFO] - Step 32560640 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32560640, mean_episode_return = 35.214, mean_episode_step = 2008.4, total_loss = -70.191, pg_loss = -136.12, baseline_loss = 70.875, entropy_loss = -4.9435, learner_queue_size = 32, train_seconds = 1.8135e+04, _tick = 6637, _time = 1.6546e+09)
[2022-06-07 15:26:07,102][root][INFO] - Step 32568320 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 32568320, mean_episode_return = None, mean_episode_step = 2551.3, total_loss = -45.524, pg_loss = -63.358, baseline_loss = 22.786, entropy_loss = -4.9523, learner_queue_size = 32, train_seconds = 1.814e+04, _tick = 6638, _time = 1.6546e+09)
[2022-06-07 15:26:12,106][root][INFO] - Step 32578560 @ 2046.3 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 32578560, mean_episode_return = None, mean_episode_step = 2329.8, total_loss = 68.47, pg_loss = 27.565, baseline_loss = 45.84, entropy_loss = -4.9341, learner_queue_size = 32, train_seconds = 1.8145e+04, _tick = 6640, _time = 1.6546e+09)
[2022-06-07 15:26:17,111][root][INFO] - Step 32586240 @ 1534.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32586240, mean_episode_return = None, mean_episode_step = 2229.3, total_loss = -109.17, pg_loss = -109.23, baseline_loss = 5.0676, entropy_loss = -5.0145, learner_queue_size = 32, train_seconds = 1.815e+04, _tick = 6642, _time = 1.6546e+09)
[2022-06-07 15:26:22,117][root][INFO] - Step 32596480 @ 2045.5 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 32596480, mean_episode_return = 31.19, mean_episode_step = 1900.3, total_loss = 96.233, pg_loss = 8.0204, baseline_loss = 93.371, entropy_loss = -5.1589, learner_queue_size = 32, train_seconds = 1.8155e+04, _tick = 6646, _time = 1.6546e+09)
[2022-06-07 15:26:27,122][root][INFO] - Step 32604160 @ 1534.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 32604160, mean_episode_return = 82.141, mean_episode_step = 1999.1, total_loss = -185.28, pg_loss = -183.78, baseline_loss = 3.5783, entropy_loss = -5.0732, learner_queue_size = 32, train_seconds = 1.816e+04, _tick = 6648, _time = 1.6546e+09)
[2022-06-07 15:26:32,126][root][INFO] - Step 32614400 @ 2046.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 32614400, mean_episode_return = 37.701, mean_episode_step = 2065.5, total_loss = -20.712, pg_loss = -58.267, baseline_loss = 42.591, entropy_loss = -5.037, learner_queue_size = 32, train_seconds = 1.8165e+04, _tick = 6652, _time = 1.6546e+09)
[2022-06-07 15:26:37,132][root][INFO] - Step 32622080 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32622080, mean_episode_return = None, mean_episode_step = 2230.7, total_loss = -130.08, pg_loss = -126.47, baseline_loss = 1.3774, entropy_loss = -4.9855, learner_queue_size = 32, train_seconds = 1.817e+04, _tick = 6654, _time = 1.6546e+09)
[2022-06-07 15:26:42,138][root][INFO] - Step 32632320 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32632320, mean_episode_return = 62.349, mean_episode_step = 1920.8, total_loss = 88.748, pg_loss = 43.127, baseline_loss = 50.576, entropy_loss = -4.9545, learner_queue_size = 32, train_seconds = 1.8175e+04, _tick = 6657, _time = 1.6546e+09)
[2022-06-07 15:26:47,142][root][INFO] - Step 32642560 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 32642560, mean_episode_return = None, mean_episode_step = 2074.8, total_loss = 320.73, pg_loss = 213.42, baseline_loss = 112.23, entropy_loss = -4.9193, learner_queue_size = 32, train_seconds = 1.818e+04, _tick = 6659, _time = 1.6546e+09)
[2022-06-07 15:26:52,148][root][INFO] - Step 32650240 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32650240, mean_episode_return = None, mean_episode_step = 2627.4, total_loss = -55.337, pg_loss = -72.278, baseline_loss = 21.843, entropy_loss = -4.9016, learner_queue_size = 32, train_seconds = 1.8185e+04, _tick = 6659, _time = 1.6546e+09)
[2022-06-07 15:26:57,154][root][INFO] - Step 32660480 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 32660480, mean_episode_return = 75.306, mean_episode_step = 2201.8, total_loss = -105.9, pg_loss = -122.86, baseline_loss = 21.959, entropy_loss = -5.0038, learner_queue_size = 32, train_seconds = 1.819e+04, _tick = 6662, _time = 1.6546e+09)
[2022-06-07 15:27:02,158][root][INFO] - Step 32668160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32668160, mean_episode_return = 31.036, mean_episode_step = 2180.4, total_loss = -95.024, pg_loss = -93.804, baseline_loss = 3.7581, entropy_loss = -4.9776, learner_queue_size = 32, train_seconds = 1.8195e+04, _tick = 6664, _time = 1.6546e+09)
[2022-06-07 15:27:07,162][root][INFO] - Step 32678400 @ 2046.4 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 32678400, mean_episode_return = None, mean_episode_step = 2004.9, total_loss = 227.66, pg_loss = 160.3, baseline_loss = 72.372, entropy_loss = -5.0187, learner_queue_size = 32, train_seconds = 1.82e+04, _tick = 6666, _time = 1.6546e+09)
[2022-06-07 15:27:12,166][root][INFO] - Step 32686080 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 32686080, mean_episode_return = 153.71, mean_episode_step = 2032.3, total_loss = 271.49, pg_loss = 169.59, baseline_loss = 106.84, entropy_loss = -4.9423, learner_queue_size = 32, train_seconds = 1.8205e+04, _tick = 6669, _time = 1.6546e+09)
[2022-06-07 15:27:17,170][root][INFO] - Step 32696320 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 32696320, mean_episode_return = None, mean_episode_step = 2562.3, total_loss = -13.569, pg_loss = -23.766, baseline_loss = 15.137, entropy_loss = -4.9409, learner_queue_size = 32, train_seconds = 1.821e+04, _tick = 6672, _time = 1.6546e+09)
[2022-06-07 15:27:22,174][root][INFO] - Step 32704000 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 32704000, mean_episode_return = 77.429, mean_episode_step = 1682.2, total_loss = 328.7, pg_loss = 188.09, baseline_loss = 145.5, entropy_loss = -4.8902, learner_queue_size = 32, train_seconds = 1.8215e+04, _tick = 6674, _time = 1.6546e+09)
[2022-06-07 15:27:27,178][root][INFO] - Step 32714240 @ 2046.3 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 32714240, mean_episode_return = 59.563, mean_episode_step = 1960.2, total_loss = 115.55, pg_loss = 47.564, baseline_loss = 72.84, entropy_loss = -4.8537, learner_queue_size = 32, train_seconds = 1.822e+04, _tick = 6678, _time = 1.6546e+09)
[2022-06-07 15:27:32,184][root][INFO] - Step 32721920 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32721920, mean_episode_return = 82.483, mean_episode_step = 1917.9, total_loss = 683.07, pg_loss = 401.08, baseline_loss = 286.9, entropy_loss = -4.91, learner_queue_size = 32, train_seconds = 1.8225e+04, _tick = 6681, _time = 1.6546e+09)
[2022-06-07 15:27:37,190][root][INFO] - Step 32732160 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 32732160, mean_episode_return = None, mean_episode_step = 2293.9, total_loss = -73.903, pg_loss = -74.371, baseline_loss = 5.3948, entropy_loss = -4.9269, learner_queue_size = 32, train_seconds = 1.823e+04, _tick = 6681, _time = 1.6546e+09)
[2022-06-07 15:27:42,195][root][INFO] - Step 32739840 @ 1534.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 32739840, mean_episode_return = 93.718, mean_episode_step = 1979.7, total_loss = 413.25, pg_loss = 250.53, baseline_loss = 167.68, entropy_loss = -4.9599, learner_queue_size = 32, train_seconds = 1.8235e+04, _tick = 6682, _time = 1.6546e+09)
[2022-06-07 15:27:47,201][root][INFO] - Step 32750080 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 32750080, mean_episode_return = 44.65, mean_episode_step = 1877.5, total_loss = 75.898, pg_loss = 27.0, baseline_loss = 53.883, entropy_loss = -4.9847, learner_queue_size = 32, train_seconds = 1.824e+04, _tick = 6685, _time = 1.6546e+09)
[2022-06-07 15:27:52,206][root][INFO] - Step 32757760 @ 1534.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32757760, mean_episode_return = None, mean_episode_step = 2003.3, total_loss = -68.309, pg_loss = -83.746, baseline_loss = 20.444, entropy_loss = -5.0067, learner_queue_size = 32, train_seconds = 1.8245e+04, _tick = 6687, _time = 1.6546e+09)
[2022-06-07 15:27:57,212][root][INFO] - Step 32768000 @ 2045.5 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 32768000, mean_episode_return = None, mean_episode_step = 1918.6, total_loss = 147.66, pg_loss = 81.921, baseline_loss = 70.764, entropy_loss = -5.0272, learner_queue_size = 32, train_seconds = 1.825e+04, _tick = 6689, _time = 1.6546e+09)
[2022-06-07 15:28:02,218][root][INFO] - Step 32775680 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32775680, mean_episode_return = -4.8005, mean_episode_step = 2224.7, total_loss = 258.53, pg_loss = 187.7, baseline_loss = 75.818, entropy_loss = -4.9906, learner_queue_size = 32, train_seconds = 1.8255e+04, _tick = 6692, _time = 1.6546e+09)
[2022-06-07 15:28:07,222][root][INFO] - Step 32785920 @ 2046.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 32785920, mean_episode_return = 33.39, mean_episode_step = 1894.3, total_loss = -173.0, pg_loss = -174.19, baseline_loss = 6.2776, entropy_loss = -5.0932, learner_queue_size = 32, train_seconds = 1.826e+04, _tick = 6694, _time = 1.6546e+09)
[2022-06-07 15:28:12,226][root][INFO] - Step 32793600 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32793600, mean_episode_return = None, mean_episode_step = 2116.4, total_loss = 61.234, pg_loss = 1.957, baseline_loss = 64.46, entropy_loss = -5.1829, learner_queue_size = 32, train_seconds = 1.8265e+04, _tick = 6695, _time = 1.6546e+09)
[2022-06-07 15:28:17,230][root][INFO] - Step 32803840 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32803840, mean_episode_return = None, mean_episode_step = 2067.5, total_loss = 146.97, pg_loss = 93.148, baseline_loss = 58.944, entropy_loss = -5.1203, learner_queue_size = 32, train_seconds = 1.827e+04, _tick = 6696, _time = 1.6546e+09)
[2022-06-07 15:28:22,234][root][INFO] - Step 32811520 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32811520, mean_episode_return = 99.599, mean_episode_step = 2192.2, total_loss = -210.29, pg_loss = -218.51, baseline_loss = 13.181, entropy_loss = -4.9688, learner_queue_size = 32, train_seconds = 1.8275e+04, _tick = 6698, _time = 1.6546e+09)
[2022-06-07 15:28:27,240][root][INFO] - Step 32821760 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 32821760, mean_episode_return = 42.034, mean_episode_step = 1920.5, total_loss = 55.25, pg_loss = -4.1658, baseline_loss = 64.436, entropy_loss = -5.0198, learner_queue_size = 32, train_seconds = 1.828e+04, _tick = 6699, _time = 1.6546e+09)
[2022-06-07 15:28:32,246][root][INFO] - Step 32829440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32829440, mean_episode_return = None, mean_episode_step = 2113.8, total_loss = 63.323, pg_loss = 30.438, baseline_loss = 37.845, entropy_loss = -4.9604, learner_queue_size = 32, train_seconds = 1.8285e+04, _tick = 6700, _time = 1.6546e+09)
[2022-06-07 15:28:37,250][root][INFO] - Step 32839680 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 32839680, mean_episode_return = None, mean_episode_step = 2053.9, total_loss = 241.36, pg_loss = 147.58, baseline_loss = 98.877, entropy_loss = -5.0959, learner_queue_size = 32, train_seconds = 1.829e+04, _tick = 6702, _time = 1.6546e+09)
[2022-06-07 15:28:42,254][root][INFO] - Step 32847360 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32847360, mean_episode_return = None, mean_episode_step = 2321.1, total_loss = -18.091, pg_loss = -35.241, baseline_loss = 22.256, entropy_loss = -5.1054, learner_queue_size = 32, train_seconds = 1.8295e+04, _tick = 6703, _time = 1.6546e+09)
[2022-06-07 15:28:47,258][root][INFO] - Step 32857600 @ 2046.3 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 32857600, mean_episode_return = 6.9897, mean_episode_step = 1965.0, total_loss = 42.661, pg_loss = -5.7859, baseline_loss = 53.55, entropy_loss = -5.1039, learner_queue_size = 32, train_seconds = 1.83e+04, _tick = 6705, _time = 1.6546e+09)
[2022-06-07 15:28:52,264][root][INFO] - Step 32865280 @ 1534.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32865280, mean_episode_return = None, mean_episode_step = 2056.8, total_loss = -57.575, pg_loss = -74.031, baseline_loss = 21.587, entropy_loss = -5.1308, learner_queue_size = 32, train_seconds = 1.8305e+04, _tick = 6706, _time = 1.6546e+09)
[2022-06-07 15:28:57,269][root][INFO] - Step 32875520 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32875520, mean_episode_return = None, mean_episode_step = 2022.3, total_loss = 204.67, pg_loss = 143.1, baseline_loss = 66.78, entropy_loss = -5.208, learner_queue_size = 32, train_seconds = 1.831e+04, _tick = 6706, _time = 1.6546e+09)
[2022-06-07 15:29:02,275][root][INFO] - Step 32883200 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32883200, mean_episode_return = 64.67, mean_episode_step = 1993.5, total_loss = 51.819, pg_loss = 13.112, baseline_loss = 43.929, entropy_loss = -5.222, learner_queue_size = 32, train_seconds = 1.8315e+04, _tick = 6709, _time = 1.6546e+09)
[2022-06-07 15:29:07,294][root][INFO] - Step 32893440 @ 2040.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32893440, mean_episode_return = 43.692, mean_episode_step = 2451.9, total_loss = -4.8311, pg_loss = -54.221, baseline_loss = 54.498, entropy_loss = -5.1083, learner_queue_size = 32, train_seconds = 1.832e+04, _tick = 6711, _time = 1.6546e+09)
[2022-06-07 15:29:12,300][root][INFO] - Step 32901120 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 32901120, mean_episode_return = None, mean_episode_step = 2092.8, total_loss = 196.65, pg_loss = 147.49, baseline_loss = 54.336, entropy_loss = -5.1822, learner_queue_size = 32, train_seconds = 1.8325e+04, _tick = 6712, _time = 1.6546e+09)
[2022-06-07 15:29:17,306][root][INFO] - Step 32911360 @ 2045.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32911360, mean_episode_return = None, mean_episode_step = 1839.0, total_loss = 598.34, pg_loss = 478.58, baseline_loss = 124.96, entropy_loss = -5.1931, learner_queue_size = 32, train_seconds = 1.833e+04, _tick = 6713, _time = 1.6546e+09)
[2022-06-07 15:29:22,310][root][INFO] - Step 32919040 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32919040, mean_episode_return = None, mean_episode_step = 1673.9, total_loss = 336.66, pg_loss = 254.69, baseline_loss = 87.165, entropy_loss = -5.1936, learner_queue_size = 32, train_seconds = 1.8335e+04, _tick = 6714, _time = 1.6546e+09)
[2022-06-07 15:29:27,314][root][INFO] - Step 32929280 @ 2046.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 32929280, mean_episode_return = None, mean_episode_step = 2402.2, total_loss = 195.53, pg_loss = 152.22, baseline_loss = 48.508, entropy_loss = -5.197, learner_queue_size = 32, train_seconds = 1.834e+04, _tick = 6715, _time = 1.6546e+09)
[2022-06-07 15:29:32,320][root][INFO] - Step 32936960 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 32936960, mean_episode_return = 56.622, mean_episode_step = 1781.4, total_loss = -51.404, pg_loss = -112.32, baseline_loss = 66.149, entropy_loss = -5.2343, learner_queue_size = 32, train_seconds = 1.8345e+04, _tick = 6718, _time = 1.6546e+09)
[2022-06-07 15:29:37,322][root][INFO] - Step 32947200 @ 2047.2 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 32947200, mean_episode_return = None, mean_episode_step = 1981.4, total_loss = 353.27, pg_loss = 238.16, baseline_loss = 120.31, entropy_loss = -5.2064, learner_queue_size = 32, train_seconds = 1.835e+04, _tick = 6720, _time = 1.6546e+09)
[2022-06-07 15:29:42,326][root][INFO] - Step 32954880 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 32954880, mean_episode_return = None, mean_episode_step = 2361.1, total_loss = 243.08, pg_loss = 151.26, baseline_loss = 97.04, entropy_loss = -5.2222, learner_queue_size = 32, train_seconds = 1.8355e+04, _tick = 6721, _time = 1.6546e+09)
[2022-06-07 15:29:47,332][root][INFO] - Step 32965120 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 32965120, mean_episode_return = None, mean_episode_step = 1840.5, total_loss = 17.297, pg_loss = 3.5975, baseline_loss = 18.91, entropy_loss = -5.2101, learner_queue_size = 32, train_seconds = 1.836e+04, _tick = 6723, _time = 1.6546e+09)
[2022-06-07 15:29:52,338][root][INFO] - Step 32972800 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 32972800, mean_episode_return = 39.207, mean_episode_step = 2392.6, total_loss = -81.264, pg_loss = -124.29, baseline_loss = 48.308, entropy_loss = -5.284, learner_queue_size = 32, train_seconds = 1.8365e+04, _tick = 6724, _time = 1.6546e+09)
[2022-06-07 15:29:57,344][root][INFO] - Step 32983040 @ 2045.7 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 32983040, mean_episode_return = 33.579, mean_episode_step = 1863.0, total_loss = -143.83, pg_loss = -161.93, baseline_loss = 23.341, entropy_loss = -5.2421, learner_queue_size = 32, train_seconds = 1.837e+04, _tick = 6726, _time = 1.6546e+09)
[2022-06-07 15:30:02,350][root][INFO] - Step 32990720 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 32990720, mean_episode_return = 20.971, mean_episode_step = 2053.7, total_loss = 33.462, pg_loss = -21.549, baseline_loss = 60.286, entropy_loss = -5.2755, learner_queue_size = 32, train_seconds = 1.8375e+04, _tick = 6728, _time = 1.6546e+09)
[2022-06-07 15:30:07,354][root][INFO] - Step 33000960 @ 2046.2 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 33000960, mean_episode_return = 18.77, mean_episode_step = 2142.0, total_loss = -125.64, pg_loss = -131.6, baseline_loss = 11.239, entropy_loss = -5.2829, learner_queue_size = 32, train_seconds = 1.838e+04, _tick = 6730, _time = 1.6546e+09)
[2022-06-07 15:30:12,358][root][INFO] - Step 33008640 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33008640, mean_episode_return = None, mean_episode_step = 2005.4, total_loss = 311.86, pg_loss = 222.17, baseline_loss = 94.973, entropy_loss = -5.2859, learner_queue_size = 32, train_seconds = 1.8385e+04, _tick = 6732, _time = 1.6546e+09)
[2022-06-07 15:30:17,362][root][INFO] - Step 33018880 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 33018880, mean_episode_return = 24.435, mean_episode_step = 2531.6, total_loss = -241.03, pg_loss = -286.03, baseline_loss = 50.29, entropy_loss = -5.2864, learner_queue_size = 32, train_seconds = 1.839e+04, _tick = 6734, _time = 1.6546e+09)
[2022-06-07 15:30:22,366][root][INFO] - Step 33026560 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 33026560, mean_episode_return = 47.78, mean_episode_step = 2060.1, total_loss = -177.25, pg_loss = -203.24, baseline_loss = 31.34, entropy_loss = -5.3493, learner_queue_size = 32, train_seconds = 1.8395e+04, _tick = 6736, _time = 1.6546e+09)
[2022-06-07 15:30:27,371][root][INFO] - Step 33036800 @ 2045.9 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 33036800, mean_episode_return = 35.214, mean_episode_step = 2067.4, total_loss = -66.31, pg_loss = -133.03, baseline_loss = 72.128, entropy_loss = -5.4108, learner_queue_size = 32, train_seconds = 1.84e+04, _tick = 6740, _time = 1.6546e+09)
[2022-06-07 15:30:32,374][root][INFO] - Step 33044480 @ 1535.1 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 33044480, mean_episode_return = None, mean_episode_step = 2144.0, total_loss = 86.226, pg_loss = 35.849, baseline_loss = 55.731, entropy_loss = -5.3539, learner_queue_size = 32, train_seconds = 1.8405e+04, _tick = 6742, _time = 1.6546e+09)
[2022-06-07 15:30:37,378][root][INFO] - Step 33054720 @ 2046.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 33054720, mean_episode_return = -33.221, mean_episode_step = 1611.5, total_loss = 53.508, pg_loss = 3.3692, baseline_loss = 55.423, entropy_loss = -5.2847, learner_queue_size = 32, train_seconds = 1.841e+04, _tick = 6745, _time = 1.6546e+09)
[2022-06-07 15:30:42,382][root][INFO] - Step 33062400 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 33062400, mean_episode_return = 57.083, mean_episode_step = 2031.5, total_loss = 100.31, pg_loss = 15.205, baseline_loss = 90.435, entropy_loss = -5.3277, learner_queue_size = 32, train_seconds = 1.8415e+04, _tick = 6747, _time = 1.6546e+09)
[2022-06-07 15:30:47,388][root][INFO] - Step 33072640 @ 2045.5 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 33072640, mean_episode_return = None, mean_episode_step = 1592.6, total_loss = 220.22, pg_loss = 162.3, baseline_loss = 63.219, entropy_loss = -5.3041, learner_queue_size = 32, train_seconds = 1.842e+04, _tick = 6748, _time = 1.6546e+09)
[2022-06-07 15:30:52,394][root][INFO] - Step 33080320 @ 1534.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 33080320, mean_episode_return = 77.209, mean_episode_step = 2220.2, total_loss = 21.43, pg_loss = -37.213, baseline_loss = 63.867, entropy_loss = -5.2241, learner_queue_size = 32, train_seconds = 1.8425e+04, _tick = 6751, _time = 1.6546e+09)
[2022-06-07 15:30:57,400][root][INFO] - Step 33090560 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 33090560, mean_episode_return = 116.6, mean_episode_step = 2069.5, total_loss = -77.721, pg_loss = -109.05, baseline_loss = 36.541, entropy_loss = -5.2144, learner_queue_size = 32, train_seconds = 1.843e+04, _tick = 6754, _time = 1.6546e+09)
[2022-06-07 15:31:02,402][root][INFO] - Step 33098240 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33098240, mean_episode_return = -69.176, mean_episode_step = 2186.4, total_loss = 34.278, pg_loss = -9.8014, baseline_loss = 49.351, entropy_loss = -5.2719, learner_queue_size = 32, train_seconds = 1.8435e+04, _tick = 6755, _time = 1.6546e+09)
[2022-06-07 15:31:07,406][root][INFO] - Step 33108480 @ 2046.3 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 33108480, mean_episode_return = None, mean_episode_step = 1878.5, total_loss = -58.146, pg_loss = -81.106, baseline_loss = 28.213, entropy_loss = -5.2532, learner_queue_size = 32, train_seconds = 1.844e+04, _tick = 6757, _time = 1.6546e+09)
[2022-06-07 15:31:12,410][root][INFO] - Step 33116160 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33116160, mean_episode_return = 99.939, mean_episode_step = 1690.9, total_loss = 93.512, pg_loss = 34.289, baseline_loss = 64.387, entropy_loss = -5.1632, learner_queue_size = 32, train_seconds = 1.8445e+04, _tick = 6759, _time = 1.6546e+09)
[2022-06-07 15:31:17,414][root][INFO] - Step 33126400 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33126400, mean_episode_return = None, mean_episode_step = 2672.6, total_loss = 87.846, pg_loss = 30.626, baseline_loss = 62.261, entropy_loss = -5.0409, learner_queue_size = 32, train_seconds = 1.845e+04, _tick = 6760, _time = 1.6546e+09)
[2022-06-07 15:31:22,422][root][INFO] - Step 33136640 @ 2044.6 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 33136640, mean_episode_return = None, mean_episode_step = 1870.4, total_loss = 316.92, pg_loss = 189.21, baseline_loss = 132.68, entropy_loss = -4.9799, learner_queue_size = 32, train_seconds = 1.8455e+04, _tick = 6761, _time = 1.6546e+09)
[2022-06-07 15:31:27,426][root][INFO] - Step 33144320 @ 1534.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33144320, mean_episode_return = 27.69, mean_episode_step = 1810.4, total_loss = -148.41, pg_loss = -170.33, baseline_loss = 26.809, entropy_loss = -4.8914, learner_queue_size = 32, train_seconds = 1.846e+04, _tick = 6762, _time = 1.6546e+09)
[2022-06-07 15:31:32,432][root][INFO] - Step 33154560 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33154560, mean_episode_return = None, mean_episode_step = 2222.5, total_loss = 508.55, pg_loss = 317.23, baseline_loss = 196.26, entropy_loss = -4.9386, learner_queue_size = 32, train_seconds = 1.8465e+04, _tick = 6764, _time = 1.6546e+09)
[2022-06-07 15:31:37,438][root][INFO] - Step 33162240 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 33162240, mean_episode_return = 56.15, mean_episode_step = 1992.2, total_loss = 8.1768, pg_loss = -55.759, baseline_loss = 68.849, entropy_loss = -4.9139, learner_queue_size = 32, train_seconds = 1.847e+04, _tick = 6767, _time = 1.6546e+09)
[2022-06-07 15:31:42,444][root][INFO] - Step 33172480 @ 2045.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 33172480, mean_episode_return = None, mean_episode_step = 2067.5, total_loss = 53.993, pg_loss = -16.551, baseline_loss = 75.54, entropy_loss = -4.996, learner_queue_size = 32, train_seconds = 1.8475e+04, _tick = 6769, _time = 1.6546e+09)
[2022-06-07 15:31:47,450][root][INFO] - Step 33180160 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 33180160, mean_episode_return = 70.119, mean_episode_step = 2070.8, total_loss = 168.61, pg_loss = 45.206, baseline_loss = 128.27, entropy_loss = -4.8692, learner_queue_size = 32, train_seconds = 1.848e+04, _tick = 6771, _time = 1.6546e+09)
[2022-06-07 15:31:52,454][root][INFO] - Step 33190400 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 33190400, mean_episode_return = 38.967, mean_episode_step = 1665.6, total_loss = -115.17, pg_loss = -154.18, baseline_loss = 43.885, entropy_loss = -4.8772, learner_queue_size = 32, train_seconds = 1.8485e+04, _tick = 6773, _time = 1.6546e+09)
[2022-06-07 15:31:57,459][root][INFO] - Step 33198080 @ 1534.6 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 33198080, mean_episode_return = 56.081, mean_episode_step = 1587.3, total_loss = 211.59, pg_loss = 96.25, baseline_loss = 120.23, entropy_loss = -4.8896, learner_queue_size = 32, train_seconds = 1.849e+04, _tick = 6776, _time = 1.6546e+09)
[2022-06-07 15:32:02,465][root][INFO] - Step 33208320 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33208320, mean_episode_return = 45.301, mean_episode_step = 1932.8, total_loss = 210.54, pg_loss = 129.05, baseline_loss = 86.391, entropy_loss = -4.9043, learner_queue_size = 32, train_seconds = 1.8496e+04, _tick = 6779, _time = 1.6546e+09)
[2022-06-07 15:32:07,470][root][INFO] - Step 33216000 @ 1534.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33216000, mean_episode_return = None, mean_episode_step = 1749.4, total_loss = 158.11, pg_loss = 54.951, baseline_loss = 108.04, entropy_loss = -4.8742, learner_queue_size = 32, train_seconds = 1.85e+04, _tick = 6781, _time = 1.6546e+09)
[2022-06-07 15:32:12,482][root][INFO] - Step 33226240 @ 2043.2 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 33226240, mean_episode_return = None, mean_episode_step = 1749.4, total_loss = 312.61, pg_loss = 195.77, baseline_loss = 121.72, entropy_loss = -4.8751, learner_queue_size = 32, train_seconds = 1.8506e+04, _tick = 6782, _time = 1.6546e+09)
[2022-06-07 15:32:17,486][root][INFO] - Step 33236480 @ 2046.3 SPS. Inference batcher size: 40. Learner queue size: 32. Other stats: (step = 33236480, mean_episode_return = None, mean_episode_step = 2000.8, total_loss = -156.22, pg_loss = -167.33, baseline_loss = 15.958, entropy_loss = -4.8489, learner_queue_size = 32, train_seconds = 1.851e+04, _tick = 6782, _time = 1.6546e+09)
[2022-06-07 15:32:22,490][root][INFO] - Step 33244160 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 33244160, mean_episode_return = None, mean_episode_step = 1793.2, total_loss = -113.61, pg_loss = -122.55, baseline_loss = 13.734, entropy_loss = -4.7932, learner_queue_size = 32, train_seconds = 1.8516e+04, _tick = 6783, _time = 1.6546e+09)
[2022-06-07 15:32:27,496][root][INFO] - Step 33254400 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 33254400, mean_episode_return = None, mean_episode_step = 1981.7, total_loss = -25.879, pg_loss = -50.274, baseline_loss = 29.215, entropy_loss = -4.8204, learner_queue_size = 32, train_seconds = 1.852e+04, _tick = 6783, _time = 1.6546e+09)
[2022-06-07 15:32:32,502][root][INFO] - Step 33262080 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 33262080, mean_episode_return = None, mean_episode_step = 2477.6, total_loss = 140.15, pg_loss = 66.141, baseline_loss = 78.766, entropy_loss = -4.7551, learner_queue_size = 32, train_seconds = 1.8526e+04, _tick = 6785, _time = 1.6546e+09)
[2022-06-07 15:32:37,506][root][INFO] - Step 33272320 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 33272320, mean_episode_return = 70.519, mean_episode_step = 2131.7, total_loss = -264.9, pg_loss = -287.84, baseline_loss = 27.711, entropy_loss = -4.7702, learner_queue_size = 32, train_seconds = 1.853e+04, _tick = 6788, _time = 1.6546e+09)
[2022-06-07 15:32:42,510][root][INFO] - Step 33280000 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 33280000, mean_episode_return = 9.2046, mean_episode_step = 2124.2, total_loss = -144.74, pg_loss = -178.85, baseline_loss = 38.876, entropy_loss = -4.7699, learner_queue_size = 32, train_seconds = 1.8536e+04, _tick = 6790, _time = 1.6546e+09)
[2022-06-07 15:32:47,516][root][INFO] - Step 33290240 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 33290240, mean_episode_return = 69.61, mean_episode_step = 1661.5, total_loss = 451.04, pg_loss = 299.05, baseline_loss = 156.77, entropy_loss = -4.7741, learner_queue_size = 32, train_seconds = 1.854e+04, _tick = 6793, _time = 1.6546e+09)
[2022-06-07 15:32:52,523][root][INFO] - Step 33297920 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33297920, mean_episode_return = 106.51, mean_episode_step = 1945.4, total_loss = -86.862, pg_loss = -117.84, baseline_loss = 35.683, entropy_loss = -4.7091, learner_queue_size = 32, train_seconds = 1.8546e+04, _tick = 6796, _time = 1.6546e+09)
[2022-06-07 15:32:57,529][root][INFO] - Step 33308160 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33308160, mean_episode_return = None, mean_episode_step = 2127.2, total_loss = 258.47, pg_loss = 175.17, baseline_loss = 88.086, entropy_loss = -4.7805, learner_queue_size = 32, train_seconds = 1.855e+04, _tick = 6797, _time = 1.6546e+09)
[2022-06-07 15:33:02,534][root][INFO] - Step 33315840 @ 1534.3 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 33315840, mean_episode_return = 75.23, mean_episode_step = 1760.1, total_loss = -88.653, pg_loss = -100.26, baseline_loss = 16.426, entropy_loss = -4.8145, learner_queue_size = 32, train_seconds = 1.8556e+04, _tick = 6799, _time = 1.6546e+09)
[2022-06-07 15:33:07,538][root][INFO] - Step 33326080 @ 2046.4 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 33326080, mean_episode_return = 90.548, mean_episode_step = 2046.4, total_loss = 74.068, pg_loss = -4.4804, baseline_loss = 83.255, entropy_loss = -4.7068, learner_queue_size = 32, train_seconds = 1.856e+04, _tick = 6803, _time = 1.6546e+09)
[2022-06-07 15:33:12,544][root][INFO] - Step 33333760 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33333760, mean_episode_return = 84.672, mean_episode_step = 2697.3, total_loss = -172.17, pg_loss = -199.31, baseline_loss = 31.848, entropy_loss = -4.7134, learner_queue_size = 32, train_seconds = 1.8566e+04, _tick = 6806, _time = 1.6546e+09)
[2022-06-07 15:33:17,546][root][INFO] - Step 33344000 @ 2047.1 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 33344000, mean_episode_return = -2.6, mean_episode_step = 1775.2, total_loss = -120.74, pg_loss = -137.5, baseline_loss = 21.506, entropy_loss = -4.7476, learner_queue_size = 32, train_seconds = 1.857e+04, _tick = 6808, _time = 1.6546e+09)
[2022-06-07 15:33:22,550][root][INFO] - Step 33351680 @ 1534.7 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 33351680, mean_episode_return = 2.355, mean_episode_step = 1917.2, total_loss = -117.71, pg_loss = -145.84, baseline_loss = 32.92, entropy_loss = -4.7864, learner_queue_size = 32, train_seconds = 1.8576e+04, _tick = 6810, _time = 1.6546e+09)
[2022-06-07 15:33:27,554][root][INFO] - Step 33361920 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 33361920, mean_episode_return = None, mean_episode_step = 2287.9, total_loss = -75.01, pg_loss = -98.855, baseline_loss = 28.661, entropy_loss = -4.8159, learner_queue_size = 32, train_seconds = 1.858e+04, _tick = 6813, _time = 1.6546e+09)
[2022-06-07 15:33:32,558][root][INFO] - Step 33369600 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 33369600, mean_episode_return = None, mean_episode_step = 1717.7, total_loss = 75.14, pg_loss = 33.135, baseline_loss = 46.814, entropy_loss = -4.8085, learner_queue_size = 32, train_seconds = 1.8586e+04, _tick = 6813, _time = 1.6546e+09)
[2022-06-07 15:33:37,562][root][INFO] - Step 33379840 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 33379840, mean_episode_return = 59.179, mean_episode_step = 2098.8, total_loss = -17.567, pg_loss = -66.572, baseline_loss = 53.877, entropy_loss = -4.8719, learner_queue_size = 32, train_seconds = 1.8591e+04, _tick = 6815, _time = 1.6546e+09)
[2022-06-07 15:33:42,568][root][INFO] - Step 33387520 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 33387520, mean_episode_return = 68.751, mean_episode_step = 2321.1, total_loss = -49.931, pg_loss = -70.8, baseline_loss = 25.698, entropy_loss = -4.8284, learner_queue_size = 32, train_seconds = 1.8596e+04, _tick = 6818, _time = 1.6546e+09)
[2022-06-07 15:33:47,574][root][INFO] - Step 33397760 @ 2045.6 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 33397760, mean_episode_return = None, mean_episode_step = 1819.0, total_loss = 22.416, pg_loss = 1.8983, baseline_loss = 25.389, entropy_loss = -4.8717, learner_queue_size = 32, train_seconds = 1.8601e+04, _tick = 6818, _time = 1.6546e+09)
[2022-06-07 15:33:52,578][root][INFO] - Step 33408000 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 33408000, mean_episode_return = None, mean_episode_step = 2042.2, total_loss = -60.95, pg_loss = -68.275, baseline_loss = 12.296, entropy_loss = -4.9715, learner_queue_size = 32, train_seconds = 1.8606e+04, _tick = 6819, _time = 1.6546e+09)
[2022-06-07 15:33:57,582][root][INFO] - Step 33415680 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33415680, mean_episode_return = 64.356, mean_episode_step = 1778.4, total_loss = -129.04, pg_loss = -135.61, baseline_loss = 11.655, entropy_loss = -5.0879, learner_queue_size = 32, train_seconds = 1.8611e+04, _tick = 6822, _time = 1.6546e+09)
[2022-06-07 15:34:02,588][root][INFO] - Step 33423360 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 33423360, mean_episode_return = 84.788, mean_episode_step = 1848.6, total_loss = 97.136, pg_loss = 46.462, baseline_loss = 55.732, entropy_loss = -5.0574, learner_queue_size = 32, train_seconds = 1.8616e+04, _tick = 6823, _time = 1.6546e+09)
[2022-06-07 15:34:07,590][root][INFO] - Step 33433600 @ 2047.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 33433600, mean_episode_return = 31.916, mean_episode_step = 1955.0, total_loss = -22.865, pg_loss = -68.684, baseline_loss = 50.929, entropy_loss = -5.1101, learner_queue_size = 32, train_seconds = 1.8621e+04, _tick = 6826, _time = 1.6546e+09)
[2022-06-07 15:34:12,594][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 15:34:12,892][root][INFO] - Step 33441280 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 33443840, mean_episode_return = None, mean_episode_step = 1996.7, total_loss = -90.241, pg_loss = -97.329, baseline_loss = 12.15, entropy_loss = -5.0616, learner_queue_size = 32, train_seconds = 1.8626e+04, _tick = 6828, _time = 1.6546e+09)
[2022-06-07 15:34:17,898][root][INFO] - Step 33451520 @ 1930.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33451520, mean_episode_return = 61.543, mean_episode_step = 1876.6, total_loss = -170.6, pg_loss = -177.77, baseline_loss = 12.285, entropy_loss = -5.1188, learner_queue_size = 32, train_seconds = 1.8631e+04, _tick = 6831, _time = 1.6546e+09)
[2022-06-07 15:34:22,904][root][INFO] - Step 33459200 @ 1534.1 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 33459200, mean_episode_return = None, mean_episode_step = 1940.3, total_loss = -38.732, pg_loss = -40.626, baseline_loss = 7.0389, entropy_loss = -5.1444, learner_queue_size = 32, train_seconds = 1.8636e+04, _tick = 6833, _time = 1.6546e+09)
[2022-06-07 15:34:27,906][root][INFO] - Step 33469440 @ 2047.1 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 33469440, mean_episode_return = -1.3404, mean_episode_step = 1804.0, total_loss = 246.75, pg_loss = 164.71, baseline_loss = 87.223, entropy_loss = -5.1829, learner_queue_size = 32, train_seconds = 1.8641e+04, _tick = 6836, _time = 1.6546e+09)
[2022-06-07 15:34:32,910][root][INFO] - Step 33477120 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33477120, mean_episode_return = 1.8947, mean_episode_step = 1788.2, total_loss = 50.448, pg_loss = 15.207, baseline_loss = 40.379, entropy_loss = -5.1383, learner_queue_size = 32, train_seconds = 1.8646e+04, _tick = 6839, _time = 1.6546e+09)
[2022-06-07 15:34:37,914][root][INFO] - Step 33487360 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 33487360, mean_episode_return = 33.148, mean_episode_step = 1841.1, total_loss = 200.09, pg_loss = 129.01, baseline_loss = 76.083, entropy_loss = -4.9964, learner_queue_size = 32, train_seconds = 1.8651e+04, _tick = 6843, _time = 1.6546e+09)
[2022-06-07 15:34:42,918][root][INFO] - Step 33495040 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 33495040, mean_episode_return = 45.404, mean_episode_step = 1537.1, total_loss = 100.76, pg_loss = -6.4612, baseline_loss = 112.18, entropy_loss = -4.9583, learner_queue_size = 32, train_seconds = 1.8656e+04, _tick = 6846, _time = 1.6546e+09)
[2022-06-07 15:34:47,922][root][INFO] - Step 33505280 @ 2046.3 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 33505280, mean_episode_return = None, mean_episode_step = 1995.3, total_loss = 856.05, pg_loss = 625.96, baseline_loss = 235.04, entropy_loss = -4.9497, learner_queue_size = 32, train_seconds = 1.8661e+04, _tick = 6848, _time = 1.6546e+09)
[2022-06-07 15:34:52,928][root][INFO] - Step 33512960 @ 1534.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 33512960, mean_episode_return = 9.0498, mean_episode_step = 1634.3, total_loss = 97.064, pg_loss = 10.925, baseline_loss = 91.108, entropy_loss = -4.9691, learner_queue_size = 32, train_seconds = 1.8666e+04, _tick = 6850, _time = 1.6546e+09)
[2022-06-07 15:34:57,934][root][INFO] - Step 33523200 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33523200, mean_episode_return = 24.98, mean_episode_step = 1272.1, total_loss = 776.52, pg_loss = 536.62, baseline_loss = 244.77, entropy_loss = -4.873, learner_queue_size = 32, train_seconds = 1.8671e+04, _tick = 6853, _time = 1.6546e+09)
[2022-06-07 15:35:02,938][root][INFO] - Step 33530880 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33530880, mean_episode_return = 45.061, mean_episode_step = 1422.4, total_loss = 163.0, pg_loss = 92.133, baseline_loss = 75.816, entropy_loss = -4.9532, learner_queue_size = 32, train_seconds = 1.8676e+04, _tick = 6854, _time = 1.6546e+09)
[2022-06-07 15:35:07,942][root][INFO] - Step 33541120 @ 2046.2 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 33541120, mean_episode_return = -3.0306, mean_episode_step = 1518.1, total_loss = -86.15, pg_loss = -108.26, baseline_loss = 27.035, entropy_loss = -4.926, learner_queue_size = 32, train_seconds = 1.8681e+04, _tick = 6856, _time = 1.6546e+09)
[2022-06-07 15:35:12,948][root][INFO] - Step 33548800 @ 1534.1 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 33548800, mean_episode_return = 172.42, mean_episode_step = 1487.6, total_loss = 466.28, pg_loss = 296.45, baseline_loss = 174.75, entropy_loss = -4.9176, learner_queue_size = 32, train_seconds = 1.8686e+04, _tick = 6858, _time = 1.6546e+09)
[2022-06-07 15:35:17,954][root][INFO] - Step 33559040 @ 2045.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33559040, mean_episode_return = None, mean_episode_step = 1803.3, total_loss = 155.1, pg_loss = 101.82, baseline_loss = 58.193, entropy_loss = -4.9099, learner_queue_size = 32, train_seconds = 1.8691e+04, _tick = 6860, _time = 1.6546e+09)
[2022-06-07 15:35:22,958][root][INFO] - Step 33569280 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33569280, mean_episode_return = 11.6, mean_episode_step = 1561.9, total_loss = -56.5, pg_loss = -71.42, baseline_loss = 19.748, entropy_loss = -4.829, learner_queue_size = 32, train_seconds = 1.8696e+04, _tick = 6862, _time = 1.6546e+09)
[2022-06-07 15:35:27,962][root][INFO] - Step 33576960 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33576960, mean_episode_return = 70.953, mean_episode_step = 1908.3, total_loss = -175.72, pg_loss = -175.76, baseline_loss = 4.836, entropy_loss = -4.7979, learner_queue_size = 32, train_seconds = 1.8701e+04, _tick = 6864, _time = 1.6546e+09)
[2022-06-07 15:35:32,966][root][INFO] - Step 33584640 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33584640, mean_episode_return = None, mean_episode_step = 1969.0, total_loss = 368.49, pg_loss = 276.62, baseline_loss = 96.661, entropy_loss = -4.783, learner_queue_size = 32, train_seconds = 1.8706e+04, _tick = 6866, _time = 1.6546e+09)
[2022-06-07 15:35:37,970][root][INFO] - Step 33594880 @ 2046.3 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 33594880, mean_episode_return = 22.182, mean_episode_step = 1476.7, total_loss = -21.043, pg_loss = -68.793, baseline_loss = 52.533, entropy_loss = -4.7831, learner_queue_size = 32, train_seconds = 1.8711e+04, _tick = 6870, _time = 1.6546e+09)
[2022-06-07 15:35:42,974][root][INFO] - Step 33602560 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 33602560, mean_episode_return = 137.21, mean_episode_step = 1764.1, total_loss = -42.278, pg_loss = -72.153, baseline_loss = 34.644, entropy_loss = -4.7686, learner_queue_size = 32, train_seconds = 1.8716e+04, _tick = 6871, _time = 1.6546e+09)
[2022-06-07 15:35:47,978][root][INFO] - Step 33612800 @ 2046.3 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 33612800, mean_episode_return = 77.336, mean_episode_step = 1612.5, total_loss = 5.9956, pg_loss = -63.766, baseline_loss = 74.508, entropy_loss = -4.7459, learner_queue_size = 32, train_seconds = 1.8721e+04, _tick = 6875, _time = 1.6546e+09)
[2022-06-07 15:35:52,984][root][INFO] - Step 33620480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33620480, mean_episode_return = None, mean_episode_step = 1703.9, total_loss = 496.63, pg_loss = 357.09, baseline_loss = 144.33, entropy_loss = -4.7953, learner_queue_size = 32, train_seconds = 1.8726e+04, _tick = 6877, _time = 1.6546e+09)
[2022-06-07 15:35:57,990][root][INFO] - Step 33630720 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 33630720, mean_episode_return = None, mean_episode_step = 2064.0, total_loss = 376.45, pg_loss = 228.07, baseline_loss = 153.12, entropy_loss = -4.7542, learner_queue_size = 32, train_seconds = 1.8731e+04, _tick = 6877, _time = 1.6546e+09)
[2022-06-07 15:36:02,995][root][INFO] - Step 33638400 @ 1534.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33638400, mean_episode_return = 113.22, mean_episode_step = 2038.9, total_loss = 626.99, pg_loss = 431.84, baseline_loss = 199.95, entropy_loss = -4.8048, learner_queue_size = 32, train_seconds = 1.8736e+04, _tick = 6878, _time = 1.6546e+09)
[2022-06-07 15:36:08,001][root][INFO] - Step 33648640 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 33648640, mean_episode_return = 92.759, mean_episode_step = 2399.7, total_loss = 173.75, pg_loss = 93.96, baseline_loss = 84.719, entropy_loss = -4.9277, learner_queue_size = 32, train_seconds = 1.8741e+04, _tick = 6880, _time = 1.6546e+09)
[2022-06-07 15:36:13,007][root][INFO] - Step 33656320 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 33656320, mean_episode_return = 9.4596, mean_episode_step = 1817.1, total_loss = -273.01, pg_loss = -289.88, baseline_loss = 21.801, entropy_loss = -4.9374, learner_queue_size = 32, train_seconds = 1.8746e+04, _tick = 6882, _time = 1.6546e+09)
[2022-06-07 15:36:18,013][root][INFO] - Step 33666560 @ 2045.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 33666560, mean_episode_return = 67.251, mean_episode_step = 2314.5, total_loss = 14.25, pg_loss = -26.399, baseline_loss = 45.544, entropy_loss = -4.8949, learner_queue_size = 32, train_seconds = 1.8751e+04, _tick = 6886, _time = 1.6546e+09)
[2022-06-07 15:36:23,019][root][INFO] - Step 33674240 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33674240, mean_episode_return = 29.65, mean_episode_step = 1864.1, total_loss = -259.58, pg_loss = -284.97, baseline_loss = 30.373, entropy_loss = -4.9885, learner_queue_size = 32, train_seconds = 1.8756e+04, _tick = 6889, _time = 1.6546e+09)
[2022-06-07 15:36:28,022][root][INFO] - Step 33684480 @ 2046.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33684480, mean_episode_return = None, mean_episode_step = 2213.5, total_loss = -10.629, pg_loss = -53.597, baseline_loss = 47.967, entropy_loss = -4.9995, learner_queue_size = 32, train_seconds = 1.8761e+04, _tick = 6889, _time = 1.6546e+09)
[2022-06-07 15:36:33,026][root][INFO] - Step 33692160 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 33692160, mean_episode_return = 9.5364, mean_episode_step = 1865.2, total_loss = 69.405, pg_loss = 15.664, baseline_loss = 58.819, entropy_loss = -5.0784, learner_queue_size = 32, train_seconds = 1.8766e+04, _tick = 6890, _time = 1.6546e+09)
[2022-06-07 15:36:38,030][root][INFO] - Step 33702400 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33702400, mean_episode_return = None, mean_episode_step = 1739.1, total_loss = 701.97, pg_loss = 536.41, baseline_loss = 170.63, entropy_loss = -5.0753, learner_queue_size = 32, train_seconds = 1.8771e+04, _tick = 6893, _time = 1.6546e+09)
[2022-06-07 15:36:43,034][root][INFO] - Step 33710080 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 33710080, mean_episode_return = None, mean_episode_step = 2202.8, total_loss = 288.05, pg_loss = 224.0, baseline_loss = 69.147, entropy_loss = -5.0905, learner_queue_size = 32, train_seconds = 1.8776e+04, _tick = 6895, _time = 1.6546e+09)
[2022-06-07 15:36:48,038][root][INFO] - Step 33720320 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 33720320, mean_episode_return = 39.8, mean_episode_step = 1984.0, total_loss = 472.09, pg_loss = 222.5, baseline_loss = 254.66, entropy_loss = -5.0658, learner_queue_size = 32, train_seconds = 1.8781e+04, _tick = 6898, _time = 1.6546e+09)
[2022-06-07 15:36:53,044][root][INFO] - Step 33728000 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 33728000, mean_episode_return = 24.83, mean_episode_step = 1854.4, total_loss = -198.48, pg_loss = -197.75, baseline_loss = 4.2966, entropy_loss = -5.0283, learner_queue_size = 32, train_seconds = 1.8786e+04, _tick = 6900, _time = 1.6546e+09)
[2022-06-07 15:36:58,050][root][INFO] - Step 33738240 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 33738240, mean_episode_return = None, mean_episode_step = 1895.8, total_loss = 192.77, pg_loss = 102.51, baseline_loss = 95.285, entropy_loss = -5.0237, learner_queue_size = 32, train_seconds = 1.8791e+04, _tick = 6903, _time = 1.6546e+09)
[2022-06-07 15:37:03,054][root][INFO] - Step 33745920 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33745920, mean_episode_return = None, mean_episode_step = 1615.2, total_loss = -200.11, pg_loss = -197.85, baseline_loss = 2.7596, entropy_loss = -5.0184, learner_queue_size = 32, train_seconds = 1.8796e+04, _tick = 6905, _time = 1.6546e+09)
[2022-06-07 15:37:08,058][root][INFO] - Step 33756160 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 33756160, mean_episode_return = None, mean_episode_step = 1953.1, total_loss = -55.289, pg_loss = -72.346, baseline_loss = 22.139, entropy_loss = -5.0817, learner_queue_size = 32, train_seconds = 1.8801e+04, _tick = 6908, _time = 1.6546e+09)
[2022-06-07 15:37:13,062][root][INFO] - Step 33763840 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33763840, mean_episode_return = 80.33, mean_episode_step = 1951.5, total_loss = -32.034, pg_loss = -71.472, baseline_loss = 44.529, entropy_loss = -5.0904, learner_queue_size = 32, train_seconds = 1.8806e+04, _tick = 6910, _time = 1.6546e+09)
[2022-06-07 15:37:18,066][root][INFO] - Step 33774080 @ 2046.3 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 33774080, mean_episode_return = None, mean_episode_step = 1975.7, total_loss = 43.122, pg_loss = 22.137, baseline_loss = 26.089, entropy_loss = -5.1036, learner_queue_size = 32, train_seconds = 1.8811e+04, _tick = 6911, _time = 1.6546e+09)
[2022-06-07 15:37:23,070][root][INFO] - Step 33784320 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 33784320, mean_episode_return = 50.281, mean_episode_step = 2196.1, total_loss = 149.15, pg_loss = 100.66, baseline_loss = 53.582, entropy_loss = -5.0833, learner_queue_size = 32, train_seconds = 1.8816e+04, _tick = 6915, _time = 1.6546e+09)
[2022-06-07 15:37:28,074][root][INFO] - Step 33792000 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 33792000, mean_episode_return = None, mean_episode_step = 1784.2, total_loss = 37.441, pg_loss = 27.365, baseline_loss = 15.185, entropy_loss = -5.1093, learner_queue_size = 32, train_seconds = 1.8821e+04, _tick = 6917, _time = 1.6546e+09)
[2022-06-07 15:37:33,078][root][INFO] - Step 33802240 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 33802240, mean_episode_return = None, mean_episode_step = 2053.6, total_loss = 374.77, pg_loss = 269.37, baseline_loss = 110.43, entropy_loss = -5.0295, learner_queue_size = 32, train_seconds = 1.8826e+04, _tick = 6918, _time = 1.6546e+09)
[2022-06-07 15:37:38,082][root][INFO] - Step 33809920 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 33809920, mean_episode_return = None, mean_episode_step = 1999.2, total_loss = 132.48, pg_loss = 72.841, baseline_loss = 64.713, entropy_loss = -5.0749, learner_queue_size = 32, train_seconds = 1.8831e+04, _tick = 6918, _time = 1.6546e+09)
[2022-06-07 15:37:43,086][root][INFO] - Step 33820160 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 33820160, mean_episode_return = None, mean_episode_step = 2148.4, total_loss = -147.63, pg_loss = -147.66, baseline_loss = 5.0242, entropy_loss = -4.9914, learner_queue_size = 32, train_seconds = 1.8836e+04, _tick = 6920, _time = 1.6546e+09)
[2022-06-07 15:37:48,090][root][INFO] - Step 33827840 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 33827840, mean_episode_return = None, mean_episode_step = 1412.8, total_loss = 313.87, pg_loss = 226.32, baseline_loss = 92.524, entropy_loss = -4.9781, learner_queue_size = 32, train_seconds = 1.8841e+04, _tick = 6921, _time = 1.6546e+09)
[2022-06-07 15:37:53,094][root][INFO] - Step 33838080 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 33838080, mean_episode_return = 7.2346, mean_episode_step = 2098.7, total_loss = 71.356, pg_loss = -9.5325, baseline_loss = 85.882, entropy_loss = -4.9934, learner_queue_size = 32, train_seconds = 1.8846e+04, _tick = 6923, _time = 1.6546e+09)
[2022-06-07 15:37:58,098][root][INFO] - Step 33845760 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33845760, mean_episode_return = None, mean_episode_step = 1710.3, total_loss = -110.3, pg_loss = -113.68, baseline_loss = 8.2841, entropy_loss = -4.9049, learner_queue_size = 32, train_seconds = 1.8851e+04, _tick = 6925, _time = 1.6546e+09)
[2022-06-07 15:38:03,104][root][INFO] - Step 33856000 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 33856000, mean_episode_return = 32.237, mean_episode_step = 1758.4, total_loss = 342.11, pg_loss = 198.25, baseline_loss = 148.74, entropy_loss = -4.8807, learner_queue_size = 32, train_seconds = 1.8856e+04, _tick = 6927, _time = 1.6546e+09)
[2022-06-07 15:38:08,110][root][INFO] - Step 33863680 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33863680, mean_episode_return = 78.309, mean_episode_step = 1872.7, total_loss = 218.04, pg_loss = 99.429, baseline_loss = 123.46, entropy_loss = -4.8541, learner_queue_size = 32, train_seconds = 1.8861e+04, _tick = 6929, _time = 1.6546e+09)
[2022-06-07 15:38:13,111][root][INFO] - Step 33873920 @ 2047.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 33873920, mean_episode_return = None, mean_episode_step = 2493.7, total_loss = 83.303, pg_loss = 15.104, baseline_loss = 73.037, entropy_loss = -4.8381, learner_queue_size = 32, train_seconds = 1.8866e+04, _tick = 6930, _time = 1.6546e+09)
[2022-06-07 15:38:18,114][root][INFO] - Step 33881600 @ 1535.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 33881600, mean_episode_return = None, mean_episode_step = 1956.6, total_loss = -77.232, pg_loss = -89.303, baseline_loss = 16.965, entropy_loss = -4.8936, learner_queue_size = 32, train_seconds = 1.8871e+04, _tick = 6931, _time = 1.6546e+09)
[2022-06-07 15:38:23,120][root][INFO] - Step 33891840 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 33891840, mean_episode_return = None, mean_episode_step = 1915.9, total_loss = 31.396, pg_loss = -21.736, baseline_loss = 58.054, entropy_loss = -4.9223, learner_queue_size = 32, train_seconds = 1.8876e+04, _tick = 6934, _time = 1.6546e+09)
[2022-06-07 15:38:28,126][root][INFO] - Step 33899520 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 33899520, mean_episode_return = 25.73, mean_episode_step = 1824.9, total_loss = -121.54, pg_loss = -157.22, baseline_loss = 40.652, entropy_loss = -4.9726, learner_queue_size = 32, train_seconds = 1.8881e+04, _tick = 6935, _time = 1.6546e+09)
[2022-06-07 15:38:33,130][root][INFO] - Step 33909760 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 33909760, mean_episode_return = None, mean_episode_step = 2230.7, total_loss = 391.44, pg_loss = 258.58, baseline_loss = 137.84, entropy_loss = -4.9797, learner_queue_size = 32, train_seconds = 1.8886e+04, _tick = 6937, _time = 1.6546e+09)
[2022-06-07 15:38:38,134][root][INFO] - Step 33917440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33917440, mean_episode_return = None, mean_episode_step = 1752.0, total_loss = 91.402, pg_loss = 56.933, baseline_loss = 39.434, entropy_loss = -4.9649, learner_queue_size = 32, train_seconds = 1.8891e+04, _tick = 6937, _time = 1.6546e+09)
[2022-06-07 15:38:43,140][root][INFO] - Step 33927680 @ 2045.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 33927680, mean_episode_return = -22.931, mean_episode_step = 2507.3, total_loss = -97.884, pg_loss = -126.71, baseline_loss = 33.766, entropy_loss = -4.9419, learner_queue_size = 32, train_seconds = 1.8896e+04, _tick = 6941, _time = 1.6546e+09)
[2022-06-07 15:38:48,146][root][INFO] - Step 33935360 @ 1534.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 33935360, mean_episode_return = None, mean_episode_step = 2213.2, total_loss = -14.008, pg_loss = -37.863, baseline_loss = 28.809, entropy_loss = -4.9538, learner_queue_size = 32, train_seconds = 1.8901e+04, _tick = 6941, _time = 1.6546e+09)
[2022-06-07 15:38:53,150][root][INFO] - Step 33945600 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33945600, mean_episode_return = 31.611, mean_episode_step = 1850.3, total_loss = -64.378, pg_loss = -86.321, baseline_loss = 26.923, entropy_loss = -4.98, learner_queue_size = 32, train_seconds = 1.8906e+04, _tick = 6944, _time = 1.6546e+09)
[2022-06-07 15:38:58,154][root][INFO] - Step 33953280 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 33953280, mean_episode_return = None, mean_episode_step = 1842.6, total_loss = 170.53, pg_loss = 106.52, baseline_loss = 69.064, entropy_loss = -5.0552, learner_queue_size = 32, train_seconds = 1.8911e+04, _tick = 6945, _time = 1.6546e+09)
[2022-06-07 15:39:03,158][root][INFO] - Step 33963520 @ 2046.3 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 33963520, mean_episode_return = None, mean_episode_step = 1997.2, total_loss = -128.13, pg_loss = -126.05, baseline_loss = 2.9327, entropy_loss = -5.0163, learner_queue_size = 32, train_seconds = 1.8916e+04, _tick = 6946, _time = 1.6546e+09)
[2022-06-07 15:39:08,162][root][INFO] - Step 33971200 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 33971200, mean_episode_return = None, mean_episode_step = 2123.6, total_loss = -55.785, pg_loss = -75.233, baseline_loss = 24.501, entropy_loss = -5.0531, learner_queue_size = 32, train_seconds = 1.8921e+04, _tick = 6948, _time = 1.6546e+09)
[2022-06-07 15:39:13,166][root][INFO] - Step 33981440 @ 2046.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 33981440, mean_episode_return = 12.809, mean_episode_step = 1896.9, total_loss = 472.4, pg_loss = 279.56, baseline_loss = 197.91, entropy_loss = -5.0718, learner_queue_size = 32, train_seconds = 1.8926e+04, _tick = 6950, _time = 1.6546e+09)
[2022-06-07 15:39:18,170][root][INFO] - Step 33989120 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 33989120, mean_episode_return = 62.346, mean_episode_step = 1736.1, total_loss = -46.493, pg_loss = -62.512, baseline_loss = 21.014, entropy_loss = -4.9955, learner_queue_size = 32, train_seconds = 1.8931e+04, _tick = 6953, _time = 1.6546e+09)
[2022-06-07 15:39:23,174][root][INFO] - Step 33999360 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 33999360, mean_episode_return = None, mean_episode_step = 1893.7, total_loss = 214.07, pg_loss = 163.34, baseline_loss = 55.681, entropy_loss = -4.9502, learner_queue_size = 32, train_seconds = 1.8936e+04, _tick = 6955, _time = 1.6546e+09)
[2022-06-07 15:39:28,178][root][INFO] - Step 34007040 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 34007040, mean_episode_return = -4.2504, mean_episode_step = 2158.4, total_loss = 154.72, pg_loss = 103.73, baseline_loss = 55.929, entropy_loss = -4.9397, learner_queue_size = 32, train_seconds = 1.8941e+04, _tick = 6958, _time = 1.6546e+09)
[2022-06-07 15:39:33,182][root][INFO] - Step 34017280 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 34017280, mean_episode_return = 51.881, mean_episode_step = 2031.1, total_loss = 710.12, pg_loss = 512.85, baseline_loss = 202.32, entropy_loss = -5.0472, learner_queue_size = 32, train_seconds = 1.8946e+04, _tick = 6961, _time = 1.6546e+09)
[2022-06-07 15:39:38,188][root][INFO] - Step 34024960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34024960, mean_episode_return = 40.351, mean_episode_step = 2061.3, total_loss = 28.331, pg_loss = -30.994, baseline_loss = 64.293, entropy_loss = -4.9682, learner_queue_size = 32, train_seconds = 1.8951e+04, _tick = 6964, _time = 1.6546e+09)
[2022-06-07 15:39:43,194][root][INFO] - Step 34035200 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34035200, mean_episode_return = 59.528, mean_episode_step = 2440.6, total_loss = 156.87, pg_loss = 14.281, baseline_loss = 147.61, entropy_loss = -5.0176, learner_queue_size = 32, train_seconds = 1.8956e+04, _tick = 6967, _time = 1.6546e+09)
[2022-06-07 15:39:48,198][root][INFO] - Step 34042880 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34042880, mean_episode_return = 40.27, mean_episode_step = 1552.5, total_loss = 187.52, pg_loss = 113.25, baseline_loss = 79.224, entropy_loss = -4.9547, learner_queue_size = 32, train_seconds = 1.8961e+04, _tick = 6969, _time = 1.6546e+09)
[2022-06-07 15:39:53,202][root][INFO] - Step 34053120 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 34053120, mean_episode_return = None, mean_episode_step = 2248.4, total_loss = -89.071, pg_loss = -109.88, baseline_loss = 25.776, entropy_loss = -4.9677, learner_queue_size = 32, train_seconds = 1.8966e+04, _tick = 6970, _time = 1.6546e+09)
[2022-06-07 15:39:58,206][root][INFO] - Step 34060800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34060800, mean_episode_return = 5.9997, mean_episode_step = 1786.9, total_loss = 94.303, pg_loss = 39.377, baseline_loss = 59.918, entropy_loss = -4.9915, learner_queue_size = 32, train_seconds = 1.8971e+04, _tick = 6972, _time = 1.6546e+09)
[2022-06-07 15:40:03,212][root][INFO] - Step 34071040 @ 2045.5 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 34071040, mean_episode_return = 110.97, mean_episode_step = 2039.1, total_loss = 100.65, pg_loss = -24.689, baseline_loss = 130.33, entropy_loss = -4.9927, learner_queue_size = 32, train_seconds = 1.8976e+04, _tick = 6975, _time = 1.6546e+09)
[2022-06-07 15:40:08,218][root][INFO] - Step 34078720 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 34078720, mean_episode_return = 23.0, mean_episode_step = 2438.3, total_loss = 225.9, pg_loss = 133.14, baseline_loss = 97.688, entropy_loss = -4.9299, learner_queue_size = 32, train_seconds = 1.8981e+04, _tick = 6977, _time = 1.6546e+09)
[2022-06-07 15:40:13,222][root][INFO] - Step 34088960 @ 2046.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 34088960, mean_episode_return = 23.53, mean_episode_step = 1673.7, total_loss = 80.107, pg_loss = 13.152, baseline_loss = 71.921, entropy_loss = -4.9658, learner_queue_size = 32, train_seconds = 1.8986e+04, _tick = 6981, _time = 1.6546e+09)
[2022-06-07 15:40:18,226][root][INFO] - Step 34096640 @ 1534.9 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 34096640, mean_episode_return = 78.548, mean_episode_step = 2050.4, total_loss = 110.58, pg_loss = 34.044, baseline_loss = 81.437, entropy_loss = -4.9053, learner_queue_size = 32, train_seconds = 1.8991e+04, _tick = 6983, _time = 1.6546e+09)
[2022-06-07 15:40:23,232][root][INFO] - Step 34106880 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34106880, mean_episode_return = 63.54, mean_episode_step = 1954.9, total_loss = 165.95, pg_loss = 89.038, baseline_loss = 81.75, entropy_loss = -4.8419, learner_queue_size = 32, train_seconds = 1.8996e+04, _tick = 6986, _time = 1.6546e+09)
[2022-06-07 15:40:28,238][root][INFO] - Step 34117120 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 34117120, mean_episode_return = 10.26, mean_episode_step = 2073.4, total_loss = 20.283, pg_loss = -34.45, baseline_loss = 59.417, entropy_loss = -4.6843, learner_queue_size = 32, train_seconds = 1.9001e+04, _tick = 6987, _time = 1.6546e+09)
[2022-06-07 15:40:33,242][root][INFO] - Step 34124800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34124800, mean_episode_return = 61.64, mean_episode_step = 1536.9, total_loss = 56.009, pg_loss = -42.657, baseline_loss = 103.4, entropy_loss = -4.7359, learner_queue_size = 32, train_seconds = 1.9006e+04, _tick = 6988, _time = 1.6546e+09)
[2022-06-07 15:40:38,246][root][INFO] - Step 34135040 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 34135040, mean_episode_return = None, mean_episode_step = 2051.4, total_loss = -13.14, pg_loss = -33.125, baseline_loss = 24.472, entropy_loss = -4.4869, learner_queue_size = 32, train_seconds = 1.9011e+04, _tick = 6989, _time = 1.6546e+09)
[2022-06-07 15:40:43,250][root][INFO] - Step 34142720 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 34142720, mean_episode_return = None, mean_episode_step = 2084.3, total_loss = -40.711, pg_loss = -44.832, baseline_loss = 8.6778, entropy_loss = -4.5566, learner_queue_size = 32, train_seconds = 1.9016e+04, _tick = 6990, _time = 1.6546e+09)
[2022-06-07 15:40:48,256][root][INFO] - Step 34152960 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34152960, mean_episode_return = None, mean_episode_step = 2258.8, total_loss = 287.92, pg_loss = 177.1, baseline_loss = 115.38, entropy_loss = -4.5678, learner_queue_size = 32, train_seconds = 1.9021e+04, _tick = 6992, _time = 1.6546e+09)
[2022-06-07 15:40:53,262][root][INFO] - Step 34160640 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34160640, mean_episode_return = 13.359, mean_episode_step = 2006.8, total_loss = -53.134, pg_loss = -97.553, baseline_loss = 48.83, entropy_loss = -4.4112, learner_queue_size = 32, train_seconds = 1.9026e+04, _tick = 6994, _time = 1.6546e+09)
[2022-06-07 15:40:58,268][root][INFO] - Step 34170880 @ 2045.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 34170880, mean_episode_return = None, mean_episode_step = 2164.5, total_loss = 131.93, pg_loss = 68.772, baseline_loss = 67.479, entropy_loss = -4.3223, learner_queue_size = 32, train_seconds = 1.9031e+04, _tick = 6996, _time = 1.6546e+09)
[2022-06-07 15:41:03,274][root][INFO] - Step 34178560 @ 1534.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 34178560, mean_episode_return = 58.63, mean_episode_step = 2130.2, total_loss = 13.114, pg_loss = -26.142, baseline_loss = 43.607, entropy_loss = -4.3508, learner_queue_size = 32, train_seconds = 1.9036e+04, _tick = 6998, _time = 1.6546e+09)
[2022-06-07 15:41:08,278][root][INFO] - Step 34188800 @ 2046.3 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 34188800, mean_episode_return = None, mean_episode_step = 2078.5, total_loss = 109.02, pg_loss = 18.985, baseline_loss = 94.546, entropy_loss = -4.5116, learner_queue_size = 32, train_seconds = 1.9041e+04, _tick = 7000, _time = 1.6546e+09)
[2022-06-07 15:41:13,282][root][INFO] - Step 34196480 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34196480, mean_episode_return = None, mean_episode_step = 1647.9, total_loss = -2.8891, pg_loss = -28.733, baseline_loss = 30.564, entropy_loss = -4.7198, learner_queue_size = 32, train_seconds = 1.9046e+04, _tick = 7000, _time = 1.6546e+09)
[2022-06-07 15:41:18,288][root][INFO] - Step 34206720 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 34206720, mean_episode_return = 99.896, mean_episode_step = 2309.1, total_loss = -129.3, pg_loss = -135.54, baseline_loss = 10.934, entropy_loss = -4.6895, learner_queue_size = 32, train_seconds = 1.9051e+04, _tick = 7003, _time = 1.6546e+09)
[2022-06-07 15:41:23,294][root][INFO] - Step 34214400 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34214400, mean_episode_return = 24.15, mean_episode_step = 2419.7, total_loss = 1226.4, pg_loss = 292.87, baseline_loss = 938.33, entropy_loss = -4.7847, learner_queue_size = 32, train_seconds = 1.9056e+04, _tick = 7006, _time = 1.6546e+09)
[2022-06-07 15:41:28,298][root][INFO] - Step 34224640 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34224640, mean_episode_return = 48.624, mean_episode_step = 1920.0, total_loss = -90.448, pg_loss = -130.16, baseline_loss = 44.445, entropy_loss = -4.733, learner_queue_size = 32, train_seconds = 1.9061e+04, _tick = 7010, _time = 1.6546e+09)
[2022-06-07 15:41:33,302][root][INFO] - Step 34232320 @ 1534.9 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 34232320, mean_episode_return = -37.7, mean_episode_step = 1906.5, total_loss = 230.31, pg_loss = 156.27, baseline_loss = 78.755, entropy_loss = -4.7186, learner_queue_size = 32, train_seconds = 1.9066e+04, _tick = 7012, _time = 1.6546e+09)
[2022-06-07 15:41:38,308][root][INFO] - Step 34242560 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 34242560, mean_episode_return = 12.87, mean_episode_step = 1947.9, total_loss = 330.94, pg_loss = 201.42, baseline_loss = 134.29, entropy_loss = -4.7635, learner_queue_size = 32, train_seconds = 1.9071e+04, _tick = 7015, _time = 1.6546e+09)
[2022-06-07 15:41:43,314][root][INFO] - Step 34250240 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 34250240, mean_episode_return = 56.41, mean_episode_step = 1971.2, total_loss = 113.46, pg_loss = 39.791, baseline_loss = 78.433, entropy_loss = -4.7661, learner_queue_size = 32, train_seconds = 1.9076e+04, _tick = 7018, _time = 1.6546e+09)
[2022-06-07 15:41:48,318][root][INFO] - Step 34260480 @ 2046.3 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 34260480, mean_episode_return = None, mean_episode_step = 2270.1, total_loss = 225.12, pg_loss = 140.48, baseline_loss = 89.448, entropy_loss = -4.8099, learner_queue_size = 32, train_seconds = 1.9081e+04, _tick = 7019, _time = 1.6546e+09)
[2022-06-07 15:41:53,322][root][INFO] - Step 34268160 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 34268160, mean_episode_return = 10.84, mean_episode_step = 1672.5, total_loss = 243.39, pg_loss = 135.09, baseline_loss = 113.19, entropy_loss = -4.8943, learner_queue_size = 32, train_seconds = 1.9086e+04, _tick = 7022, _time = 1.6546e+09)
[2022-06-07 15:41:58,326][root][INFO] - Step 34278400 @ 2046.3 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 34278400, mean_episode_return = None, mean_episode_step = 2315.4, total_loss = 203.5, pg_loss = 107.78, baseline_loss = 100.66, entropy_loss = -4.9389, learner_queue_size = 32, train_seconds = 1.9091e+04, _tick = 7025, _time = 1.6546e+09)
[2022-06-07 15:42:03,330][root][INFO] - Step 34286080 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 34286080, mean_episode_return = None, mean_episode_step = 1519.5, total_loss = 19.95, pg_loss = -43.841, baseline_loss = 68.745, entropy_loss = -4.9548, learner_queue_size = 32, train_seconds = 1.9096e+04, _tick = 7027, _time = 1.6546e+09)
[2022-06-07 15:42:08,336][root][INFO] - Step 34296320 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 34296320, mean_episode_return = 40.654, mean_episode_step = 1730.7, total_loss = 51.576, pg_loss = 12.574, baseline_loss = 43.789, entropy_loss = -4.787, learner_queue_size = 32, train_seconds = 1.9101e+04, _tick = 7029, _time = 1.6546e+09)
[2022-06-07 15:42:13,342][root][INFO] - Step 34304000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34304000, mean_episode_return = None, mean_episode_step = 2340.3, total_loss = -38.919, pg_loss = -68.543, baseline_loss = 34.379, entropy_loss = -4.7552, learner_queue_size = 32, train_seconds = 1.9106e+04, _tick = 7030, _time = 1.6546e+09)
[2022-06-07 15:42:18,348][root][INFO] - Step 34314240 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 34314240, mean_episode_return = None, mean_episode_step = 2108.7, total_loss = 860.75, pg_loss = 463.28, baseline_loss = 402.29, entropy_loss = -4.8198, learner_queue_size = 32, train_seconds = 1.9111e+04, _tick = 7032, _time = 1.6546e+09)
[2022-06-07 15:42:23,354][root][INFO] - Step 34321920 @ 1534.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34321920, mean_episode_return = None, mean_episode_step = 2271.3, total_loss = 65.927, pg_loss = 18.871, baseline_loss = 51.917, entropy_loss = -4.8609, learner_queue_size = 32, train_seconds = 1.9116e+04, _tick = 7033, _time = 1.6546e+09)
[2022-06-07 15:42:28,358][root][INFO] - Step 34332160 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 34332160, mean_episode_return = 74.168, mean_episode_step = 1720.8, total_loss = -110.3, pg_loss = -147.11, baseline_loss = 41.711, entropy_loss = -4.8974, learner_queue_size = 32, train_seconds = 1.9121e+04, _tick = 7035, _time = 1.6546e+09)
[2022-06-07 15:42:33,364][root][INFO] - Step 34339840 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 34339840, mean_episode_return = None, mean_episode_step = 1920.6, total_loss = 133.47, pg_loss = 63.032, baseline_loss = 75.392, entropy_loss = -4.9503, learner_queue_size = 32, train_seconds = 1.9126e+04, _tick = 7036, _time = 1.6546e+09)
[2022-06-07 15:42:38,370][root][INFO] - Step 34350080 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34350080, mean_episode_return = 78.539, mean_episode_step = 1836.5, total_loss = -41.49, pg_loss = -92.785, baseline_loss = 56.209, entropy_loss = -4.9137, learner_queue_size = 32, train_seconds = 1.9131e+04, _tick = 7039, _time = 1.6546e+09)
[2022-06-07 15:42:43,374][root][INFO] - Step 34357760 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34357760, mean_episode_return = None, mean_episode_step = 2329.9, total_loss = -30.48, pg_loss = -59.872, baseline_loss = 34.369, entropy_loss = -4.9763, learner_queue_size = 32, train_seconds = 1.9136e+04, _tick = 7041, _time = 1.6546e+09)
[2022-06-07 15:42:48,380][root][INFO] - Step 34368000 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 34368000, mean_episode_return = None, mean_episode_step = 2005.3, total_loss = 46.164, pg_loss = 23.421, baseline_loss = 27.735, entropy_loss = -4.9916, learner_queue_size = 32, train_seconds = 1.9141e+04, _tick = 7043, _time = 1.6546e+09)
[2022-06-07 15:42:53,386][root][INFO] - Step 34375680 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34375680, mean_episode_return = None, mean_episode_step = 2118.8, total_loss = 139.85, pg_loss = 88.819, baseline_loss = 56.013, entropy_loss = -4.9834, learner_queue_size = 32, train_seconds = 1.9146e+04, _tick = 7045, _time = 1.6546e+09)
[2022-06-07 15:42:58,390][root][INFO] - Step 34385920 @ 2046.4 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 34385920, mean_episode_return = None, mean_episode_step = 2014.4, total_loss = 49.443, pg_loss = 22.996, baseline_loss = 31.375, entropy_loss = -4.9277, learner_queue_size = 32, train_seconds = 1.9151e+04, _tick = 7047, _time = 1.6546e+09)
[2022-06-07 15:43:03,394][root][INFO] - Step 34393600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34393600, mean_episode_return = 14.391, mean_episode_step = 2196.2, total_loss = 263.72, pg_loss = 154.78, baseline_loss = 113.87, entropy_loss = -4.9317, learner_queue_size = 32, train_seconds = 1.9156e+04, _tick = 7050, _time = 1.6546e+09)
[2022-06-07 15:43:08,399][root][INFO] - Step 34403840 @ 2046.0 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 34403840, mean_episode_return = 72.579, mean_episode_step = 1898.1, total_loss = -145.12, pg_loss = -178.61, baseline_loss = 38.43, entropy_loss = -4.9375, learner_queue_size = 32, train_seconds = 1.9161e+04, _tick = 7054, _time = 1.6546e+09)
[2022-06-07 15:43:13,419][root][INFO] - Step 34411520 @ 1529.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 34411520, mean_episode_return = 25.651, mean_episode_step = 2323.7, total_loss = -121.39, pg_loss = -142.67, baseline_loss = 26.204, entropy_loss = -4.9179, learner_queue_size = 32, train_seconds = 1.9166e+04, _tick = 7057, _time = 1.6546e+09)
[2022-06-07 15:43:18,422][root][INFO] - Step 34419200 @ 1535.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 34419200, mean_episode_return = None, mean_episode_step = 2455.3, total_loss = 387.1, pg_loss = 254.18, baseline_loss = 137.81, entropy_loss = -4.8851, learner_queue_size = 32, train_seconds = 1.9171e+04, _tick = 7059, _time = 1.6546e+09)
[2022-06-07 15:43:23,428][root][INFO] - Step 34429440 @ 2045.6 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 34429440, mean_episode_return = None, mean_episode_step = 2404.8, total_loss = -101.37, pg_loss = -118.09, baseline_loss = 21.582, entropy_loss = -4.8641, learner_queue_size = 32, train_seconds = 1.9176e+04, _tick = 7061, _time = 1.6546e+09)
[2022-06-07 15:43:28,433][root][INFO] - Step 34437120 @ 1534.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 34437120, mean_episode_return = 120.75, mean_episode_step = 1754.2, total_loss = -42.958, pg_loss = -95.804, baseline_loss = 57.749, entropy_loss = -4.9017, learner_queue_size = 32, train_seconds = 1.9181e+04, _tick = 7062, _time = 1.6546e+09)
[2022-06-07 15:43:33,438][root][INFO] - Step 34447360 @ 2045.9 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 34447360, mean_episode_return = None, mean_episode_step = 2427.0, total_loss = -23.816, pg_loss = -48.74, baseline_loss = 29.875, entropy_loss = -4.9501, learner_queue_size = 32, train_seconds = 1.9186e+04, _tick = 7062, _time = 1.6546e+09)
[2022-06-07 15:43:38,443][root][INFO] - Step 34455040 @ 1534.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34455040, mean_episode_return = 97.426, mean_episode_step = 1583.1, total_loss = -53.541, pg_loss = -105.7, baseline_loss = 57.146, entropy_loss = -4.9826, learner_queue_size = 32, train_seconds = 1.9191e+04, _tick = 7063, _time = 1.6546e+09)
[2022-06-07 15:43:43,452][root][INFO] - Step 34465280 @ 2044.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 34465280, mean_episode_return = 24.974, mean_episode_step = 2796.7, total_loss = 129.0, pg_loss = 31.361, baseline_loss = 102.58, entropy_loss = -4.9436, learner_queue_size = 32, train_seconds = 1.9196e+04, _tick = 7066, _time = 1.6546e+09)
[2022-06-07 15:43:48,458][root][INFO] - Step 34472960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34472960, mean_episode_return = 70.434, mean_episode_step = 2379.8, total_loss = -236.95, pg_loss = -254.05, baseline_loss = 22.072, entropy_loss = -4.9723, learner_queue_size = 32, train_seconds = 1.9202e+04, _tick = 7067, _time = 1.6546e+09)
[2022-06-07 15:43:53,462][root][INFO] - Step 34483200 @ 2046.3 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 34483200, mean_episode_return = 39.363, mean_episode_step = 1645.2, total_loss = 176.02, pg_loss = 62.958, baseline_loss = 118.03, entropy_loss = -4.9709, learner_queue_size = 32, train_seconds = 1.9206e+04, _tick = 7070, _time = 1.6546e+09)
[2022-06-07 15:43:58,468][root][INFO] - Step 34493440 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34493440, mean_episode_return = None, mean_episode_step = 1957.9, total_loss = 245.55, pg_loss = 164.3, baseline_loss = 86.158, entropy_loss = -4.911, learner_queue_size = 32, train_seconds = 1.9212e+04, _tick = 7071, _time = 1.6546e+09)
[2022-06-07 15:44:03,474][root][INFO] - Step 34501120 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 34501120, mean_episode_return = 143.45, mean_episode_step = 2078.6, total_loss = -63.58, pg_loss = -99.699, baseline_loss = 41.025, entropy_loss = -4.9065, learner_queue_size = 32, train_seconds = 1.9216e+04, _tick = 7074, _time = 1.6546e+09)
[2022-06-07 15:44:08,480][root][INFO] - Step 34511360 @ 2045.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 34511360, mean_episode_return = 95.799, mean_episode_step = 1794.7, total_loss = -119.4, pg_loss = -162.56, baseline_loss = 48.137, entropy_loss = -4.9794, learner_queue_size = 32, train_seconds = 1.9222e+04, _tick = 7076, _time = 1.6546e+09)
[2022-06-07 15:44:13,482][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 15:44:13,676][root][INFO] - Step 34519040 @ 1535.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34519040, mean_episode_return = 42.86, mean_episode_step = 2018.7, total_loss = 5.087, pg_loss = -42.943, baseline_loss = 53.068, entropy_loss = -5.0375, learner_queue_size = 32, train_seconds = 1.9226e+04, _tick = 7079, _time = 1.6546e+09)
[2022-06-07 15:44:18,678][root][INFO] - Step 34529280 @ 1970.7 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 34529280, mean_episode_return = 91.456, mean_episode_step = 1709.4, total_loss = 177.09, pg_loss = 49.412, baseline_loss = 132.65, entropy_loss = -4.9705, learner_queue_size = 32, train_seconds = 1.9232e+04, _tick = 7080, _time = 1.6546e+09)
[2022-06-07 15:44:23,682][root][INFO] - Step 34536960 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34536960, mean_episode_return = None, mean_episode_step = 2208.1, total_loss = -42.154, pg_loss = -65.703, baseline_loss = 28.465, entropy_loss = -4.9157, learner_queue_size = 32, train_seconds = 1.9237e+04, _tick = 7081, _time = 1.6546e+09)
[2022-06-07 15:44:28,686][root][INFO] - Step 34547200 @ 2046.4 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 34547200, mean_episode_return = 17.535, mean_episode_step = 1881.7, total_loss = -305.27, pg_loss = -328.93, baseline_loss = 28.654, entropy_loss = -4.9933, learner_queue_size = 32, train_seconds = 1.9242e+04, _tick = 7083, _time = 1.6546e+09)
[2022-06-07 15:44:33,692][root][INFO] - Step 34554880 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34554880, mean_episode_return = None, mean_episode_step = 2013.6, total_loss = -116.29, pg_loss = -116.38, baseline_loss = 5.1318, entropy_loss = -5.0388, learner_queue_size = 32, train_seconds = 1.9247e+04, _tick = 7085, _time = 1.6546e+09)
[2022-06-07 15:44:38,698][root][INFO] - Step 34565120 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34565120, mean_episode_return = 76.223, mean_episode_step = 1689.0, total_loss = 196.3, pg_loss = 110.93, baseline_loss = 90.348, entropy_loss = -4.9829, learner_queue_size = 32, train_seconds = 1.9252e+04, _tick = 7088, _time = 1.6546e+09)
[2022-06-07 15:44:43,704][root][INFO] - Step 34572800 @ 1534.1 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 34572800, mean_episode_return = 148.14, mean_episode_step = 2131.8, total_loss = -58.694, pg_loss = -88.198, baseline_loss = 34.526, entropy_loss = -5.0227, learner_queue_size = 32, train_seconds = 1.9257e+04, _tick = 7089, _time = 1.6546e+09)
[2022-06-07 15:44:48,710][root][INFO] - Step 34580480 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34580480, mean_episode_return = None, mean_episode_step = 2085.1, total_loss = 46.137, pg_loss = 16.529, baseline_loss = 34.76, entropy_loss = -5.1522, learner_queue_size = 32, train_seconds = 1.9262e+04, _tick = 7089, _time = 1.6546e+09)
[2022-06-07 15:44:53,714][root][INFO] - Step 34590720 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 34590720, mean_episode_return = 47.763, mean_episode_step = 1571.1, total_loss = 221.03, pg_loss = 141.18, baseline_loss = 84.995, entropy_loss = -5.1396, learner_queue_size = 32, train_seconds = 1.9267e+04, _tick = 7092, _time = 1.6546e+09)
[2022-06-07 15:44:58,718][root][INFO] - Step 34598400 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 34598400, mean_episode_return = 31.265, mean_episode_step = 1740.0, total_loss = 388.08, pg_loss = 220.04, baseline_loss = 173.14, entropy_loss = -5.103, learner_queue_size = 32, train_seconds = 1.9272e+04, _tick = 7093, _time = 1.6546e+09)
[2022-06-07 15:45:03,722][root][INFO] - Step 34608640 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 34608640, mean_episode_return = None, mean_episode_step = 2061.9, total_loss = -210.08, pg_loss = -207.12, baseline_loss = 2.2079, entropy_loss = -5.1638, learner_queue_size = 32, train_seconds = 1.9277e+04, _tick = 7094, _time = 1.6546e+09)
[2022-06-07 15:45:08,728][root][INFO] - Step 34616320 @ 1534.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34616320, mean_episode_return = 20.47, mean_episode_step = 1693.1, total_loss = 197.32, pg_loss = 126.99, baseline_loss = 75.526, entropy_loss = -5.1954, learner_queue_size = 32, train_seconds = 1.9282e+04, _tick = 7095, _time = 1.6546e+09)
[2022-06-07 15:45:13,734][root][INFO] - Step 34626560 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34626560, mean_episode_return = None, mean_episode_step = 2063.8, total_loss = 124.87, pg_loss = 75.211, baseline_loss = 54.861, entropy_loss = -5.2046, learner_queue_size = 32, train_seconds = 1.9287e+04, _tick = 7096, _time = 1.6546e+09)
[2022-06-07 15:45:18,740][root][INFO] - Step 34634240 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34634240, mean_episode_return = 153.86, mean_episode_step = 1991.4, total_loss = 212.13, pg_loss = 90.008, baseline_loss = 127.36, entropy_loss = -5.237, learner_queue_size = 32, train_seconds = 1.9292e+04, _tick = 7097, _time = 1.6546e+09)
[2022-06-07 15:45:23,746][root][INFO] - Step 34644480 @ 2045.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34644480, mean_episode_return = 45.673, mean_episode_step = 2405.7, total_loss = 7.0671, pg_loss = -114.32, baseline_loss = 126.6, entropy_loss = -5.2085, learner_queue_size = 32, train_seconds = 1.9297e+04, _tick = 7100, _time = 1.6546e+09)
[2022-06-07 15:45:28,750][root][INFO] - Step 34652160 @ 1534.7 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 34652160, mean_episode_return = 95.321, mean_episode_step = 1944.9, total_loss = -301.84, pg_loss = -334.55, baseline_loss = 37.94, entropy_loss = -5.227, learner_queue_size = 32, train_seconds = 1.9302e+04, _tick = 7102, _time = 1.6546e+09)
[2022-06-07 15:45:33,754][root][INFO] - Step 34662400 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 34662400, mean_episode_return = 27.316, mean_episode_step = 1843.5, total_loss = 117.94, pg_loss = 39.381, baseline_loss = 83.771, entropy_loss = -5.2113, learner_queue_size = 32, train_seconds = 1.9307e+04, _tick = 7104, _time = 1.6546e+09)
[2022-06-07 15:45:38,760][root][INFO] - Step 34670080 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 34670080, mean_episode_return = 60.611, mean_episode_step = 2498.8, total_loss = -216.3, pg_loss = -219.74, baseline_loss = 8.632, entropy_loss = -5.1983, learner_queue_size = 32, train_seconds = 1.9312e+04, _tick = 7106, _time = 1.6546e+09)
[2022-06-07 15:45:43,766][root][INFO] - Step 34680320 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 34680320, mean_episode_return = None, mean_episode_step = 1698.0, total_loss = 411.56, pg_loss = 297.99, baseline_loss = 118.74, entropy_loss = -5.1731, learner_queue_size = 32, train_seconds = 1.9317e+04, _tick = 7108, _time = 1.6546e+09)
[2022-06-07 15:45:48,772][root][INFO] - Step 34688000 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34688000, mean_episode_return = 69.918, mean_episode_step = 2325.8, total_loss = 105.07, pg_loss = 38.692, baseline_loss = 71.589, entropy_loss = -5.2147, learner_queue_size = 32, train_seconds = 1.9322e+04, _tick = 7111, _time = 1.6546e+09)
[2022-06-07 15:45:53,778][root][INFO] - Step 34698240 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34698240, mean_episode_return = None, mean_episode_step = 2071.8, total_loss = 480.38, pg_loss = 316.89, baseline_loss = 168.66, entropy_loss = -5.1683, learner_queue_size = 32, train_seconds = 1.9327e+04, _tick = 7113, _time = 1.6546e+09)
[2022-06-07 15:45:58,782][root][INFO] - Step 34705920 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34705920, mean_episode_return = 7.4596, mean_episode_step = 1993.4, total_loss = -33.507, pg_loss = -75.296, baseline_loss = 46.935, entropy_loss = -5.1457, learner_queue_size = 32, train_seconds = 1.9332e+04, _tick = 7116, _time = 1.6546e+09)
[2022-06-07 15:46:03,786][root][INFO] - Step 34716160 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 34716160, mean_episode_return = 84.827, mean_episode_step = 2226.7, total_loss = -109.16, pg_loss = -177.61, baseline_loss = 73.595, entropy_loss = -5.1418, learner_queue_size = 32, train_seconds = 1.9337e+04, _tick = 7119, _time = 1.6546e+09)
[2022-06-07 15:46:08,791][root][INFO] - Step 34723840 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34723840, mean_episode_return = 45.161, mean_episode_step = 1758.7, total_loss = -7.8308, pg_loss = -62.964, baseline_loss = 60.266, entropy_loss = -5.133, learner_queue_size = 32, train_seconds = 1.9342e+04, _tick = 7122, _time = 1.6546e+09)
[2022-06-07 15:46:13,794][root][INFO] - Step 34734080 @ 2046.9 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 34734080, mean_episode_return = None, mean_episode_step = 2136.1, total_loss = -95.378, pg_loss = -109.09, baseline_loss = 18.884, entropy_loss = -5.1722, learner_queue_size = 32, train_seconds = 1.9347e+04, _tick = 7124, _time = 1.6546e+09)
[2022-06-07 15:46:18,798][root][INFO] - Step 34744320 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 34744320, mean_episode_return = 31.571, mean_episode_step = 1655.1, total_loss = -36.354, pg_loss = -72.858, baseline_loss = 41.655, entropy_loss = -5.1511, learner_queue_size = 32, train_seconds = 1.9352e+04, _tick = 7127, _time = 1.6546e+09)
[2022-06-07 15:46:23,800][root][INFO] - Step 34752000 @ 1535.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 34752000, mean_episode_return = 37.069, mean_episode_step = 1929.7, total_loss = -146.75, pg_loss = -181.19, baseline_loss = 39.619, entropy_loss = -5.1799, learner_queue_size = 32, train_seconds = 1.9357e+04, _tick = 7128, _time = 1.6546e+09)
[2022-06-07 15:46:28,802][root][INFO] - Step 34762240 @ 2047.0 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 34762240, mean_episode_return = None, mean_episode_step = 2351.7, total_loss = -17.393, pg_loss = -46.709, baseline_loss = 34.541, entropy_loss = -5.2258, learner_queue_size = 32, train_seconds = 1.9362e+04, _tick = 7131, _time = 1.6546e+09)
[2022-06-07 15:46:33,806][root][INFO] - Step 34769920 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 34769920, mean_episode_return = 96.254, mean_episode_step = 2350.6, total_loss = 484.21, pg_loss = 324.52, baseline_loss = 164.95, entropy_loss = -5.2705, learner_queue_size = 32, train_seconds = 1.9367e+04, _tick = 7133, _time = 1.6546e+09)
[2022-06-07 15:46:38,810][root][INFO] - Step 34780160 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 34780160, mean_episode_return = None, mean_episode_step = 1793.3, total_loss = 107.85, pg_loss = 49.202, baseline_loss = 63.917, entropy_loss = -5.2657, learner_queue_size = 32, train_seconds = 1.9372e+04, _tick = 7135, _time = 1.6546e+09)
[2022-06-07 15:46:43,812][root][INFO] - Step 34787840 @ 1535.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34787840, mean_episode_return = None, mean_episode_step = 1885.3, total_loss = 294.12, pg_loss = 203.15, baseline_loss = 96.225, entropy_loss = -5.2555, learner_queue_size = 32, train_seconds = 1.9377e+04, _tick = 7137, _time = 1.6546e+09)
[2022-06-07 15:46:48,814][root][INFO] - Step 34798080 @ 2047.0 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 34798080, mean_episode_return = 65.892, mean_episode_step = 2013.2, total_loss = 228.66, pg_loss = 32.348, baseline_loss = 201.6, entropy_loss = -5.2929, learner_queue_size = 32, train_seconds = 1.9382e+04, _tick = 7139, _time = 1.6546e+09)
[2022-06-07 15:46:53,818][root][INFO] - Step 34805760 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 34805760, mean_episode_return = 55.703, mean_episode_step = 1667.1, total_loss = -194.37, pg_loss = -218.96, baseline_loss = 29.829, entropy_loss = -5.244, learner_queue_size = 32, train_seconds = 1.9387e+04, _tick = 7141, _time = 1.6546e+09)
[2022-06-07 15:46:58,822][root][INFO] - Step 34816000 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 34816000, mean_episode_return = 48.377, mean_episode_step = 2093.3, total_loss = 253.8, pg_loss = 156.02, baseline_loss = 103.04, entropy_loss = -5.2684, learner_queue_size = 32, train_seconds = 1.9392e+04, _tick = 7143, _time = 1.6546e+09)
[2022-06-07 15:47:03,828][root][INFO] - Step 34823680 @ 1534.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 34823680, mean_episode_return = None, mean_episode_step = 1884.6, total_loss = -75.74, pg_loss = -77.772, baseline_loss = 7.2742, entropy_loss = -5.2422, learner_queue_size = 32, train_seconds = 1.9397e+04, _tick = 7145, _time = 1.6546e+09)
[2022-06-07 15:47:08,834][root][INFO] - Step 34833920 @ 2045.7 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 34833920, mean_episode_return = 19.3, mean_episode_step = 2245.1, total_loss = -172.07, pg_loss = -179.41, baseline_loss = 12.54, entropy_loss = -5.194, learner_queue_size = 32, train_seconds = 1.9402e+04, _tick = 7147, _time = 1.6546e+09)
[2022-06-07 15:47:13,838][root][INFO] - Step 34841600 @ 1534.8 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 34841600, mean_episode_return = 15.68, mean_episode_step = 1802.5, total_loss = 75.052, pg_loss = -12.582, baseline_loss = 92.801, entropy_loss = -5.1675, learner_queue_size = 32, train_seconds = 1.9407e+04, _tick = 7150, _time = 1.6546e+09)
[2022-06-07 15:47:18,842][root][INFO] - Step 34851840 @ 2046.4 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 34851840, mean_episode_return = None, mean_episode_step = 2220.0, total_loss = 278.06, pg_loss = 179.67, baseline_loss = 103.6, entropy_loss = -5.2135, learner_queue_size = 32, train_seconds = 1.9412e+04, _tick = 7152, _time = 1.6546e+09)
[2022-06-07 15:47:23,848][root][INFO] - Step 34859520 @ 1534.2 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 34859520, mean_episode_return = None, mean_episode_step = 2248.5, total_loss = 29.941, pg_loss = 4.7202, baseline_loss = 30.454, entropy_loss = -5.2331, learner_queue_size = 32, train_seconds = 1.9417e+04, _tick = 7153, _time = 1.6546e+09)
[2022-06-07 15:47:28,850][root][INFO] - Step 34869760 @ 2047.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 34869760, mean_episode_return = None, mean_episode_step = 2101.2, total_loss = -19.741, pg_loss = -41.207, baseline_loss = 26.73, entropy_loss = -5.2638, learner_queue_size = 32, train_seconds = 1.9422e+04, _tick = 7154, _time = 1.6546e+09)
[2022-06-07 15:47:33,854][root][INFO] - Step 34877440 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 34877440, mean_episode_return = None, mean_episode_step = 2429.3, total_loss = -91.26, pg_loss = -103.17, baseline_loss = 17.156, entropy_loss = -5.2429, learner_queue_size = 32, train_seconds = 1.9427e+04, _tick = 7155, _time = 1.6546e+09)
[2022-06-07 15:47:38,858][root][INFO] - Step 34887680 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34887680, mean_episode_return = None, mean_episode_step = 1790.1, total_loss = 22.689, pg_loss = -4.6259, baseline_loss = 32.536, entropy_loss = -5.221, learner_queue_size = 32, train_seconds = 1.9432e+04, _tick = 7158, _time = 1.6546e+09)
[2022-06-07 15:47:43,862][root][INFO] - Step 34895360 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 34895360, mean_episode_return = 100.25, mean_episode_step = 1578.2, total_loss = 322.75, pg_loss = 194.46, baseline_loss = 133.49, entropy_loss = -5.2021, learner_queue_size = 32, train_seconds = 1.9437e+04, _tick = 7161, _time = 1.6546e+09)
[2022-06-07 15:47:48,866][root][INFO] - Step 34905600 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 34905600, mean_episode_return = 39.35, mean_episode_step = 1222.8, total_loss = -54.024, pg_loss = -70.06, baseline_loss = 21.256, entropy_loss = -5.2198, learner_queue_size = 32, train_seconds = 1.9442e+04, _tick = 7165, _time = 1.6546e+09)
[2022-06-07 15:47:53,870][root][INFO] - Step 34913280 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34913280, mean_episode_return = None, mean_episode_step = 1702.5, total_loss = -123.62, pg_loss = -128.7, baseline_loss = 10.261, entropy_loss = -5.1836, learner_queue_size = 32, train_seconds = 1.9447e+04, _tick = 7165, _time = 1.6546e+09)
[2022-06-07 15:47:58,874][root][INFO] - Step 34923520 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 34923520, mean_episode_return = -7.9304, mean_episode_step = 2329.3, total_loss = -202.53, pg_loss = -212.35, baseline_loss = 15.025, entropy_loss = -5.2055, learner_queue_size = 32, train_seconds = 1.9452e+04, _tick = 7168, _time = 1.6546e+09)
[2022-06-07 15:48:03,878][root][INFO] - Step 34931200 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 34931200, mean_episode_return = 46.259, mean_episode_step = 2073.0, total_loss = -147.25, pg_loss = -187.66, baseline_loss = 45.613, entropy_loss = -5.2036, learner_queue_size = 32, train_seconds = 1.9457e+04, _tick = 7171, _time = 1.6546e+09)
[2022-06-07 15:48:08,882][root][INFO] - Step 34941440 @ 2046.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 34941440, mean_episode_return = 94.726, mean_episode_step = 1883.4, total_loss = 169.05, pg_loss = 103.41, baseline_loss = 70.914, entropy_loss = -5.2708, learner_queue_size = 32, train_seconds = 1.9462e+04, _tick = 7173, _time = 1.6546e+09)
[2022-06-07 15:48:13,886][root][INFO] - Step 34949120 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 34949120, mean_episode_return = 156.08, mean_episode_step = 2014.0, total_loss = 117.23, pg_loss = 57.636, baseline_loss = 64.88, entropy_loss = -5.2823, learner_queue_size = 32, train_seconds = 1.9467e+04, _tick = 7176, _time = 1.6546e+09)
[2022-06-07 15:48:18,892][root][INFO] - Step 34959360 @ 2045.5 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 34959360, mean_episode_return = 6.4797, mean_episode_step = 1811.2, total_loss = 168.43, pg_loss = 95.025, baseline_loss = 78.711, entropy_loss = -5.3016, learner_queue_size = 32, train_seconds = 1.9472e+04, _tick = 7179, _time = 1.6546e+09)
[2022-06-07 15:48:23,898][root][INFO] - Step 34967040 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 34967040, mean_episode_return = 41.461, mean_episode_step = 2139.2, total_loss = -38.704, pg_loss = -71.656, baseline_loss = 38.232, entropy_loss = -5.2801, learner_queue_size = 32, train_seconds = 1.9477e+04, _tick = 7181, _time = 1.6546e+09)
[2022-06-07 15:48:28,902][root][INFO] - Step 34977280 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 34977280, mean_episode_return = -16.041, mean_episode_step = 2297.7, total_loss = -99.381, pg_loss = -107.1, baseline_loss = 13.053, entropy_loss = -5.3294, learner_queue_size = 32, train_seconds = 1.9482e+04, _tick = 7183, _time = 1.6546e+09)
[2022-06-07 15:48:33,908][root][INFO] - Step 34987520 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 34987520, mean_episode_return = 151.96, mean_episode_step = 1498.8, total_loss = 22.132, pg_loss = -24.874, baseline_loss = 52.322, entropy_loss = -5.3157, learner_queue_size = 32, train_seconds = 1.9487e+04, _tick = 7187, _time = 1.6546e+09)
[2022-06-07 15:48:38,914][root][INFO] - Step 34995200 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 34995200, mean_episode_return = 9.2995, mean_episode_step = 2127.5, total_loss = 455.18, pg_loss = 226.47, baseline_loss = 234.03, entropy_loss = -5.3137, learner_queue_size = 32, train_seconds = 1.9492e+04, _tick = 7189, _time = 1.6546e+09)
[2022-06-07 15:48:43,920][root][INFO] - Step 35005440 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 35005440, mean_episode_return = None, mean_episode_step = 1758.6, total_loss = 415.86, pg_loss = 289.98, baseline_loss = 131.25, entropy_loss = -5.3634, learner_queue_size = 32, train_seconds = 1.9497e+04, _tick = 7190, _time = 1.6546e+09)
[2022-06-07 15:48:48,922][root][INFO] - Step 35013120 @ 1535.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 35013120, mean_episode_return = None, mean_episode_step = 1801.4, total_loss = 83.553, pg_loss = 41.806, baseline_loss = 47.081, entropy_loss = -5.3335, learner_queue_size = 32, train_seconds = 1.9502e+04, _tick = 7191, _time = 1.6546e+09)
[2022-06-07 15:48:53,926][root][INFO] - Step 35020800 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35020800, mean_episode_return = 110.71, mean_episode_step = 2278.2, total_loss = 95.673, pg_loss = 37.459, baseline_loss = 63.526, entropy_loss = -5.3128, learner_queue_size = 32, train_seconds = 1.9507e+04, _tick = 7192, _time = 1.6546e+09)
[2022-06-07 15:48:58,930][root][INFO] - Step 35031040 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 35031040, mean_episode_return = 16.15, mean_episode_step = 2517.1, total_loss = -6.2533, pg_loss = -48.121, baseline_loss = 47.105, entropy_loss = -5.2373, learner_queue_size = 32, train_seconds = 1.9512e+04, _tick = 7195, _time = 1.6546e+09)
[2022-06-07 15:49:03,934][root][INFO] - Step 35038720 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35038720, mean_episode_return = 15.419, mean_episode_step = 1701.5, total_loss = 18.269, pg_loss = -25.298, baseline_loss = 48.76, entropy_loss = -5.1928, learner_queue_size = 32, train_seconds = 1.9517e+04, _tick = 7196, _time = 1.6546e+09)
[2022-06-07 15:49:08,938][root][INFO] - Step 35048960 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35048960, mean_episode_return = None, mean_episode_step = 1797.1, total_loss = -94.089, pg_loss = -112.11, baseline_loss = 23.168, entropy_loss = -5.152, learner_queue_size = 32, train_seconds = 1.9522e+04, _tick = 7197, _time = 1.6546e+09)
[2022-06-07 15:49:13,942][root][INFO] - Step 35056640 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 35056640, mean_episode_return = 113.63, mean_episode_step = 2281.5, total_loss = 250.69, pg_loss = 157.57, baseline_loss = 98.283, entropy_loss = -5.1541, learner_queue_size = 32, train_seconds = 1.9527e+04, _tick = 7198, _time = 1.6546e+09)
[2022-06-07 15:49:18,946][root][INFO] - Step 35066880 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 35066880, mean_episode_return = None, mean_episode_step = 2228.9, total_loss = 71.451, pg_loss = 42.851, baseline_loss = 33.633, entropy_loss = -5.0328, learner_queue_size = 32, train_seconds = 1.9532e+04, _tick = 7200, _time = 1.6546e+09)
[2022-06-07 15:49:23,952][root][INFO] - Step 35074560 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35074560, mean_episode_return = 97.826, mean_episode_step = 2079.6, total_loss = 139.87, pg_loss = 37.428, baseline_loss = 107.58, entropy_loss = -5.1413, learner_queue_size = 32, train_seconds = 1.9537e+04, _tick = 7203, _time = 1.6546e+09)
[2022-06-07 15:49:28,958][root][INFO] - Step 35084800 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35084800, mean_episode_return = 21.89, mean_episode_step = 2273.9, total_loss = -7.0265, pg_loss = -45.35, baseline_loss = 43.46, entropy_loss = -5.1365, learner_queue_size = 32, train_seconds = 1.9542e+04, _tick = 7207, _time = 1.6546e+09)
[2022-06-07 15:49:33,962][root][INFO] - Step 35095040 @ 2046.4 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 35095040, mean_episode_return = None, mean_episode_step = 1627.2, total_loss = -9.7637, pg_loss = -47.736, baseline_loss = 43.039, entropy_loss = -5.0666, learner_queue_size = 32, train_seconds = 1.9547e+04, _tick = 7210, _time = 1.6546e+09)
[2022-06-07 15:49:38,966][root][INFO] - Step 35102720 @ 1534.7 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 35102720, mean_episode_return = 37.821, mean_episode_step = 2051.7, total_loss = 211.79, pg_loss = 138.95, baseline_loss = 77.959, entropy_loss = -5.1266, learner_queue_size = 32, train_seconds = 1.9552e+04, _tick = 7213, _time = 1.6546e+09)
[2022-06-07 15:49:43,970][root][INFO] - Step 35112960 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 35112960, mean_episode_return = None, mean_episode_step = 1532.1, total_loss = -187.45, pg_loss = -186.33, baseline_loss = 3.9814, entropy_loss = -5.0968, learner_queue_size = 32, train_seconds = 1.9557e+04, _tick = 7215, _time = 1.6546e+09)
[2022-06-07 15:49:48,974][root][INFO] - Step 35120640 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35120640, mean_episode_return = 30.49, mean_episode_step = 2221.9, total_loss = 178.36, pg_loss = 75.764, baseline_loss = 107.66, entropy_loss = -5.0645, learner_queue_size = 32, train_seconds = 1.9562e+04, _tick = 7217, _time = 1.6546e+09)
[2022-06-07 15:49:53,978][root][INFO] - Step 35130880 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 35130880, mean_episode_return = 69.239, mean_episode_step = 1834.6, total_loss = -60.075, pg_loss = -114.68, baseline_loss = 59.645, entropy_loss = -5.0391, learner_queue_size = 32, train_seconds = 1.9567e+04, _tick = 7220, _time = 1.6546e+09)
[2022-06-07 15:49:58,984][root][INFO] - Step 35138560 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35138560, mean_episode_return = 54.692, mean_episode_step = 2017.2, total_loss = 159.87, pg_loss = 23.444, baseline_loss = 141.49, entropy_loss = -5.0617, learner_queue_size = 32, train_seconds = 1.9572e+04, _tick = 7222, _time = 1.6546e+09)
[2022-06-07 15:50:03,990][root][INFO] - Step 35148800 @ 2045.6 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 35148800, mean_episode_return = None, mean_episode_step = 2115.0, total_loss = -207.66, pg_loss = -205.23, baseline_loss = 2.6613, entropy_loss = -5.0889, learner_queue_size = 32, train_seconds = 1.9577e+04, _tick = 7224, _time = 1.6546e+09)
[2022-06-07 15:50:08,996][root][INFO] - Step 35156480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35156480, mean_episode_return = 39.723, mean_episode_step = 1498.0, total_loss = -84.346, pg_loss = -116.5, baseline_loss = 37.269, entropy_loss = -5.1129, learner_queue_size = 32, train_seconds = 1.9582e+04, _tick = 7227, _time = 1.6546e+09)
[2022-06-07 15:50:14,002][root][INFO] - Step 35166720 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35166720, mean_episode_return = None, mean_episode_step = 2224.5, total_loss = -124.89, pg_loss = -135.29, baseline_loss = 15.513, entropy_loss = -5.1092, learner_queue_size = 32, train_seconds = 1.9587e+04, _tick = 7227, _time = 1.6546e+09)
[2022-06-07 15:50:19,006][root][INFO] - Step 35174400 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35174400, mean_episode_return = 11.613, mean_episode_step = 1919.7, total_loss = -62.743, pg_loss = -129.24, baseline_loss = 71.592, entropy_loss = -5.0989, learner_queue_size = 32, train_seconds = 1.9592e+04, _tick = 7229, _time = 1.6546e+09)
[2022-06-07 15:50:24,010][root][INFO] - Step 35184640 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 35184640, mean_episode_return = 24.0, mean_episode_step = 1801.5, total_loss = -30.881, pg_loss = -57.95, baseline_loss = 32.191, entropy_loss = -5.121, learner_queue_size = 32, train_seconds = 1.9597e+04, _tick = 7233, _time = 1.6546e+09)
[2022-06-07 15:50:29,022][root][INFO] - Step 35192320 @ 1532.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 35192320, mean_episode_return = None, mean_episode_step = 1688.8, total_loss = 79.922, pg_loss = 39.159, baseline_loss = 45.865, entropy_loss = -5.102, learner_queue_size = 32, train_seconds = 1.9602e+04, _tick = 7234, _time = 1.6546e+09)
[2022-06-07 15:50:34,026][root][INFO] - Step 35202560 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 35202560, mean_episode_return = 122.06, mean_episode_step = 1572.7, total_loss = -83.808, pg_loss = -92.758, baseline_loss = 14.141, entropy_loss = -5.192, learner_queue_size = 32, train_seconds = 1.9607e+04, _tick = 7236, _time = 1.6546e+09)
[2022-06-07 15:50:39,032][root][INFO] - Step 35210240 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 35210240, mean_episode_return = None, mean_episode_step = 2839.2, total_loss = -185.51, pg_loss = -182.66, baseline_loss = 2.3528, entropy_loss = -5.2029, learner_queue_size = 32, train_seconds = 1.9612e+04, _tick = 7236, _time = 1.6546e+09)
[2022-06-07 15:50:44,038][root][INFO] - Step 35220480 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35220480, mean_episode_return = None, mean_episode_step = 1716.0, total_loss = 14.21, pg_loss = -11.047, baseline_loss = 30.507, entropy_loss = -5.2494, learner_queue_size = 32, train_seconds = 1.9617e+04, _tick = 7238, _time = 1.6546e+09)
[2022-06-07 15:50:49,044][root][INFO] - Step 35228160 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 35228160, mean_episode_return = -39.51, mean_episode_step = 1670.4, total_loss = 151.89, pg_loss = 67.404, baseline_loss = 89.698, entropy_loss = -5.2107, learner_queue_size = 32, train_seconds = 1.9622e+04, _tick = 7241, _time = 1.6546e+09)
[2022-06-07 15:50:54,050][root][INFO] - Step 35238400 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 35238400, mean_episode_return = None, mean_episode_step = 1518.1, total_loss = -111.26, pg_loss = -125.7, baseline_loss = 19.636, entropy_loss = -5.1932, learner_queue_size = 32, train_seconds = 1.9627e+04, _tick = 7243, _time = 1.6546e+09)
[2022-06-07 15:50:59,054][root][INFO] - Step 35246080 @ 1534.9 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35246080, mean_episode_return = 11.09, mean_episode_step = 2462.0, total_loss = 41.897, pg_loss = 4.6882, baseline_loss = 42.345, entropy_loss = -5.1356, learner_queue_size = 32, train_seconds = 1.9632e+04, _tick = 7244, _time = 1.6546e+09)
[2022-06-07 15:51:04,058][root][INFO] - Step 35256320 @ 2046.3 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 35256320, mean_episode_return = 11.13, mean_episode_step = 2127.2, total_loss = -179.07, pg_loss = -185.77, baseline_loss = 11.885, entropy_loss = -5.1823, learner_queue_size = 32, train_seconds = 1.9637e+04, _tick = 7246, _time = 1.6546e+09)
[2022-06-07 15:51:09,064][root][INFO] - Step 35264000 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35264000, mean_episode_return = 58.131, mean_episode_step = 1533.3, total_loss = -196.5, pg_loss = -207.03, baseline_loss = 15.704, entropy_loss = -5.1769, learner_queue_size = 32, train_seconds = 1.9642e+04, _tick = 7248, _time = 1.6546e+09)
[2022-06-07 15:51:14,070][root][INFO] - Step 35274240 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35274240, mean_episode_return = None, mean_episode_step = 1706.0, total_loss = 223.77, pg_loss = 125.3, baseline_loss = 103.62, entropy_loss = -5.1543, learner_queue_size = 32, train_seconds = 1.9647e+04, _tick = 7248, _time = 1.6546e+09)
[2022-06-07 15:51:19,074][root][INFO] - Step 35281920 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35281920, mean_episode_return = 56.861, mean_episode_step = 1893.4, total_loss = -44.538, pg_loss = -96.587, baseline_loss = 57.191, entropy_loss = -5.1419, learner_queue_size = 32, train_seconds = 1.9652e+04, _tick = 7250, _time = 1.6546e+09)
[2022-06-07 15:51:24,078][root][INFO] - Step 35292160 @ 2046.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 35292160, mean_episode_return = 30.9, mean_episode_step = 1518.7, total_loss = -155.18, pg_loss = -170.51, baseline_loss = 20.486, entropy_loss = -5.1527, learner_queue_size = 32, train_seconds = 1.9657e+04, _tick = 7253, _time = 1.6546e+09)
[2022-06-07 15:51:29,082][root][INFO] - Step 35299840 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 35299840, mean_episode_return = 97.806, mean_episode_step = 2095.7, total_loss = -119.18, pg_loss = -139.99, baseline_loss = 25.948, entropy_loss = -5.1408, learner_queue_size = 32, train_seconds = 1.9662e+04, _tick = 7255, _time = 1.6546e+09)
[2022-06-07 15:51:34,088][root][INFO] - Step 35310080 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 35310080, mean_episode_return = -3.9804, mean_episode_step = 2105.4, total_loss = 192.05, pg_loss = 117.14, baseline_loss = 80.051, entropy_loss = -5.1428, learner_queue_size = 32, train_seconds = 1.9667e+04, _tick = 7259, _time = 1.6546e+09)
[2022-06-07 15:51:39,094][root][INFO] - Step 35317760 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35317760, mean_episode_return = 35.043, mean_episode_step = 2074.3, total_loss = -62.101, pg_loss = -100.09, baseline_loss = 43.099, entropy_loss = -5.1054, learner_queue_size = 32, train_seconds = 1.9672e+04, _tick = 7260, _time = 1.6546e+09)
[2022-06-07 15:51:44,100][root][INFO] - Step 35328000 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 35328000, mean_episode_return = None, mean_episode_step = 2242.8, total_loss = -125.63, pg_loss = -133.81, baseline_loss = 13.351, entropy_loss = -5.1717, learner_queue_size = 32, train_seconds = 1.9677e+04, _tick = 7262, _time = 1.6546e+09)
[2022-06-07 15:51:49,106][root][INFO] - Step 35335680 @ 1534.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 35335680, mean_episode_return = 88.737, mean_episode_step = 1815.3, total_loss = 166.13, pg_loss = 80.361, baseline_loss = 90.964, entropy_loss = -5.1959, learner_queue_size = 32, train_seconds = 1.9682e+04, _tick = 7264, _time = 1.6546e+09)
[2022-06-07 15:51:54,110][root][INFO] - Step 35345920 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 35345920, mean_episode_return = 36.836, mean_episode_step = 2019.9, total_loss = 76.515, pg_loss = 21.825, baseline_loss = 59.933, entropy_loss = -5.2428, learner_queue_size = 32, train_seconds = 1.9687e+04, _tick = 7267, _time = 1.6546e+09)
[2022-06-07 15:51:59,117][root][INFO] - Step 35356160 @ 2045.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 35356160, mean_episode_return = None, mean_episode_step = 1998.2, total_loss = 240.92, pg_loss = 155.47, baseline_loss = 90.702, entropy_loss = -5.2525, learner_queue_size = 32, train_seconds = 1.9692e+04, _tick = 7270, _time = 1.6546e+09)
[2022-06-07 15:52:04,122][root][INFO] - Step 35363840 @ 1534.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 35363840, mean_episode_return = 35.4, mean_episode_step = 1935.6, total_loss = -147.0, pg_loss = -158.83, baseline_loss = 17.123, entropy_loss = -5.2861, learner_queue_size = 32, train_seconds = 1.9697e+04, _tick = 7273, _time = 1.6546e+09)
[2022-06-07 15:52:09,126][root][INFO] - Step 35374080 @ 2046.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 35374080, mean_episode_return = 45.559, mean_episode_step = 1901.3, total_loss = 143.96, pg_loss = 46.202, baseline_loss = 103.03, entropy_loss = -5.2659, learner_queue_size = 32, train_seconds = 1.9702e+04, _tick = 7276, _time = 1.6546e+09)
[2022-06-07 15:52:14,130][root][INFO] - Step 35381760 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35381760, mean_episode_return = 42.132, mean_episode_step = 1759.0, total_loss = 322.2, pg_loss = 210.29, baseline_loss = 117.15, entropy_loss = -5.2336, learner_queue_size = 32, train_seconds = 1.9707e+04, _tick = 7278, _time = 1.6546e+09)
[2022-06-07 15:52:19,134][root][INFO] - Step 35392000 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35392000, mean_episode_return = None, mean_episode_step = 2001.3, total_loss = 132.06, pg_loss = 71.59, baseline_loss = 65.719, entropy_loss = -5.2496, learner_queue_size = 32, train_seconds = 1.9712e+04, _tick = 7279, _time = 1.6546e+09)
[2022-06-07 15:52:24,140][root][INFO] - Step 35399680 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35399680, mean_episode_return = None, mean_episode_step = 2055.6, total_loss = 288.95, pg_loss = 191.34, baseline_loss = 102.82, entropy_loss = -5.2196, learner_queue_size = 32, train_seconds = 1.9717e+04, _tick = 7279, _time = 1.6546e+09)
[2022-06-07 15:52:29,142][root][INFO] - Step 35409920 @ 2047.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 35409920, mean_episode_return = 8.4291, mean_episode_step = 2187.3, total_loss = -145.51, pg_loss = -145.11, baseline_loss = 4.857, entropy_loss = -5.259, learner_queue_size = 32, train_seconds = 1.9722e+04, _tick = 7281, _time = 1.6546e+09)
[2022-06-07 15:52:34,146][root][INFO] - Step 35417600 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 35417600, mean_episode_return = None, mean_episode_step = 2353.9, total_loss = 423.64, pg_loss = 313.18, baseline_loss = 115.75, entropy_loss = -5.2919, learner_queue_size = 32, train_seconds = 1.9727e+04, _tick = 7283, _time = 1.6546e+09)
[2022-06-07 15:52:39,152][root][INFO] - Step 35427840 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 35427840, mean_episode_return = None, mean_episode_step = 1730.8, total_loss = 576.27, pg_loss = 395.62, baseline_loss = 185.94, entropy_loss = -5.2933, learner_queue_size = 32, train_seconds = 1.9732e+04, _tick = 7285, _time = 1.6546e+09)
[2022-06-07 15:52:44,158][root][INFO] - Step 35435520 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 35435520, mean_episode_return = 156.37, mean_episode_step = 1937.9, total_loss = -34.361, pg_loss = -54.577, baseline_loss = 25.51, entropy_loss = -5.2942, learner_queue_size = 32, train_seconds = 1.9737e+04, _tick = 7287, _time = 1.6546e+09)
[2022-06-07 15:52:49,164][root][INFO] - Step 35445760 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 35445760, mean_episode_return = None, mean_episode_step = 2452.8, total_loss = -33.662, pg_loss = -57.704, baseline_loss = 29.353, entropy_loss = -5.3104, learner_queue_size = 32, train_seconds = 1.9742e+04, _tick = 7288, _time = 1.6546e+09)
[2022-06-07 15:52:54,170][root][INFO] - Step 35453440 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35453440, mean_episode_return = 35.62, mean_episode_step = 2121.8, total_loss = -18.102, pg_loss = -57.627, baseline_loss = 44.746, entropy_loss = -5.2201, learner_queue_size = 32, train_seconds = 1.9747e+04, _tick = 7289, _time = 1.6546e+09)
[2022-06-07 15:52:59,176][root][INFO] - Step 35463680 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35463680, mean_episode_return = None, mean_episode_step = 2027.8, total_loss = 295.68, pg_loss = 219.09, baseline_loss = 81.819, entropy_loss = -5.2377, learner_queue_size = 32, train_seconds = 1.9752e+04, _tick = 7291, _time = 1.6546e+09)
[2022-06-07 15:53:04,182][root][INFO] - Step 35471360 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 35471360, mean_episode_return = None, mean_episode_step = 2609.7, total_loss = 216.63, pg_loss = 129.23, baseline_loss = 92.585, entropy_loss = -5.1879, learner_queue_size = 32, train_seconds = 1.9757e+04, _tick = 7292, _time = 1.6546e+09)
[2022-06-07 15:53:09,186][root][INFO] - Step 35479040 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 35479040, mean_episode_return = None, mean_episode_step = 2295.2, total_loss = 167.2, pg_loss = 109.65, baseline_loss = 62.718, entropy_loss = -5.168, learner_queue_size = 32, train_seconds = 1.9762e+04, _tick = 7293, _time = 1.6546e+09)
[2022-06-07 15:53:14,190][root][INFO] - Step 35489280 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 35489280, mean_episode_return = 20.26, mean_episode_step = 1754.3, total_loss = 67.075, pg_loss = 12.817, baseline_loss = 59.342, entropy_loss = -5.0831, learner_queue_size = 32, train_seconds = 1.9767e+04, _tick = 7296, _time = 1.6546e+09)
[2022-06-07 15:53:19,194][root][INFO] - Step 35499520 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 35499520, mean_episode_return = None, mean_episode_step = 2010.1, total_loss = 200.57, pg_loss = 104.61, baseline_loss = 100.95, entropy_loss = -4.9887, learner_queue_size = 32, train_seconds = 1.9772e+04, _tick = 7297, _time = 1.6546e+09)
[2022-06-07 15:53:24,200][root][INFO] - Step 35507200 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35507200, mean_episode_return = None, mean_episode_step = 1755.9, total_loss = 69.247, pg_loss = 9.9146, baseline_loss = 64.278, entropy_loss = -4.9458, learner_queue_size = 32, train_seconds = 1.9777e+04, _tick = 7298, _time = 1.6546e+09)
[2022-06-07 15:53:29,206][root][INFO] - Step 35517440 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35517440, mean_episode_return = None, mean_episode_step = 2421.2, total_loss = -170.97, pg_loss = -167.66, baseline_loss = 1.6361, entropy_loss = -4.9365, learner_queue_size = 32, train_seconds = 1.9782e+04, _tick = 7300, _time = 1.6546e+09)
[2022-06-07 15:53:34,207][root][INFO] - Step 35525120 @ 1535.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35525120, mean_episode_return = None, mean_episode_step = 2144.2, total_loss = 160.12, pg_loss = 101.33, baseline_loss = 63.696, entropy_loss = -4.908, learner_queue_size = 32, train_seconds = 1.9787e+04, _tick = 7301, _time = 1.6546e+09)
[2022-06-07 15:53:39,213][root][INFO] - Step 35532800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35532800, mean_episode_return = -1.5607, mean_episode_step = 1799.5, total_loss = 35.432, pg_loss = -4.4215, baseline_loss = 44.797, entropy_loss = -4.943, learner_queue_size = 32, train_seconds = 1.9792e+04, _tick = 7303, _time = 1.6546e+09)
[2022-06-07 15:53:44,218][root][INFO] - Step 35543040 @ 2046.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35543040, mean_episode_return = 67.233, mean_episode_step = 2329.7, total_loss = 155.72, pg_loss = 72.839, baseline_loss = 87.816, entropy_loss = -4.9404, learner_queue_size = 32, train_seconds = 1.9797e+04, _tick = 7306, _time = 1.6546e+09)
[2022-06-07 15:53:49,224][root][INFO] - Step 35550720 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35550720, mean_episode_return = None, mean_episode_step = 1723.1, total_loss = 25.785, pg_loss = -0.55455, baseline_loss = 31.227, entropy_loss = -4.888, learner_queue_size = 32, train_seconds = 1.9802e+04, _tick = 7307, _time = 1.6546e+09)
[2022-06-07 15:53:54,230][root][INFO] - Step 35560960 @ 2045.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 35560960, mean_episode_return = 16.701, mean_episode_step = 2478.7, total_loss = 103.12, pg_loss = 52.222, baseline_loss = 55.794, entropy_loss = -4.8937, learner_queue_size = 32, train_seconds = 1.9807e+04, _tick = 7309, _time = 1.6546e+09)
[2022-06-07 15:53:59,236][root][INFO] - Step 35571200 @ 2045.5 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 35571200, mean_episode_return = 5.9397, mean_episode_step = 1954.3, total_loss = 92.569, pg_loss = 26.053, baseline_loss = 71.398, entropy_loss = -4.8812, learner_queue_size = 32, train_seconds = 1.9812e+04, _tick = 7312, _time = 1.6546e+09)
[2022-06-07 15:54:04,242][root][INFO] - Step 35578880 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 35578880, mean_episode_return = None, mean_episode_step = 2164.3, total_loss = 54.258, pg_loss = 12.24, baseline_loss = 46.823, entropy_loss = -4.8043, learner_queue_size = 32, train_seconds = 1.9817e+04, _tick = 7313, _time = 1.6546e+09)
[2022-06-07 15:54:09,248][root][INFO] - Step 35589120 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 35589120, mean_episode_return = 49.711, mean_episode_step = 1870.3, total_loss = -74.953, pg_loss = -93.268, baseline_loss = 23.167, entropy_loss = -4.8519, learner_queue_size = 32, train_seconds = 1.9822e+04, _tick = 7315, _time = 1.6546e+09)
[2022-06-07 15:54:14,254][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 15:54:14,443][root][INFO] - Step 35596800 @ 1534.2 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 35596800, mean_episode_return = 4.7494, mean_episode_step = 1582.3, total_loss = 327.53, pg_loss = 217.68, baseline_loss = 114.72, entropy_loss = -4.8732, learner_queue_size = 32, train_seconds = 1.9827e+04, _tick = 7317, _time = 1.6546e+09)
[2022-06-07 15:54:19,446][root][INFO] - Step 35607040 @ 1972.3 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 35607040, mean_episode_return = 2.1795, mean_episode_step = 1748.8, total_loss = -221.55, pg_loss = -231.19, baseline_loss = 14.524, entropy_loss = -4.8778, learner_queue_size = 32, train_seconds = 1.9832e+04, _tick = 7320, _time = 1.6546e+09)
[2022-06-07 15:54:24,450][root][INFO] - Step 35614720 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35614720, mean_episode_return = None, mean_episode_step = 1946.8, total_loss = -68.523, pg_loss = -89.454, baseline_loss = 25.845, entropy_loss = -4.9135, learner_queue_size = 32, train_seconds = 1.9837e+04, _tick = 7321, _time = 1.6546e+09)
[2022-06-07 15:54:29,454][root][INFO] - Step 35624960 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 35624960, mean_episode_return = 38.022, mean_episode_step = 2107.1, total_loss = 115.8, pg_loss = 47.502, baseline_loss = 73.261, entropy_loss = -4.9601, learner_queue_size = 32, train_seconds = 1.9842e+04, _tick = 7322, _time = 1.6546e+09)
[2022-06-07 15:54:34,458][root][INFO] - Step 35632640 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35632640, mean_episode_return = 83.377, mean_episode_step = 2009.9, total_loss = 157.74, pg_loss = 89.229, baseline_loss = 73.432, entropy_loss = -4.9184, learner_queue_size = 32, train_seconds = 1.9848e+04, _tick = 7325, _time = 1.6546e+09)
[2022-06-07 15:54:39,462][root][INFO] - Step 35642880 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 35642880, mean_episode_return = 30.33, mean_episode_step = 2025.4, total_loss = -59.413, pg_loss = -101.22, baseline_loss = 46.719, entropy_loss = -4.9145, learner_queue_size = 32, train_seconds = 1.9852e+04, _tick = 7328, _time = 1.6546e+09)
[2022-06-07 15:54:44,466][root][INFO] - Step 35650560 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35650560, mean_episode_return = 78.631, mean_episode_step = 2697.2, total_loss = 68.424, pg_loss = -2.3999, baseline_loss = 75.68, entropy_loss = -4.8557, learner_queue_size = 32, train_seconds = 1.9858e+04, _tick = 7330, _time = 1.6546e+09)
[2022-06-07 15:54:49,470][root][INFO] - Step 35660800 @ 2046.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 35660800, mean_episode_return = 33.31, mean_episode_step = 2043.4, total_loss = 220.59, pg_loss = 107.94, baseline_loss = 117.55, entropy_loss = -4.8972, learner_queue_size = 32, train_seconds = 1.9862e+04, _tick = 7333, _time = 1.6546e+09)
[2022-06-07 15:54:54,474][root][INFO] - Step 35668480 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 35668480, mean_episode_return = None, mean_episode_step = 2210.7, total_loss = 262.78, pg_loss = 187.89, baseline_loss = 79.826, entropy_loss = -4.9322, learner_queue_size = 32, train_seconds = 1.9868e+04, _tick = 7334, _time = 1.6546e+09)
[2022-06-07 15:54:59,478][root][INFO] - Step 35678720 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 35678720, mean_episode_return = None, mean_episode_step = 1996.6, total_loss = 79.007, pg_loss = 33.041, baseline_loss = 50.85, entropy_loss = -4.8836, learner_queue_size = 32, train_seconds = 1.9872e+04, _tick = 7336, _time = 1.6546e+09)
[2022-06-07 15:55:04,484][root][INFO] - Step 35686400 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35686400, mean_episode_return = 45.754, mean_episode_step = 1989.5, total_loss = -124.26, pg_loss = -157.22, baseline_loss = 37.72, entropy_loss = -4.7652, learner_queue_size = 32, train_seconds = 1.9878e+04, _tick = 7338, _time = 1.6546e+09)
[2022-06-07 15:55:09,490][root][INFO] - Step 35696640 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35696640, mean_episode_return = 34.341, mean_episode_step = 2258.9, total_loss = -212.82, pg_loss = -212.65, baseline_loss = 4.5528, entropy_loss = -4.7223, learner_queue_size = 32, train_seconds = 1.9882e+04, _tick = 7340, _time = 1.6546e+09)
[2022-06-07 15:55:14,494][root][INFO] - Step 35706880 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 35706880, mean_episode_return = None, mean_episode_step = 2459.5, total_loss = 29.98, pg_loss = 0.78848, baseline_loss = 33.89, entropy_loss = -4.6984, learner_queue_size = 32, train_seconds = 1.9888e+04, _tick = 7340, _time = 1.6546e+09)
[2022-06-07 15:55:19,498][root][INFO] - Step 35714560 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 35714560, mean_episode_return = None, mean_episode_step = 2084.3, total_loss = 110.06, pg_loss = 55.531, baseline_loss = 59.262, entropy_loss = -4.7378, learner_queue_size = 32, train_seconds = 1.9892e+04, _tick = 7342, _time = 1.6546e+09)
[2022-06-07 15:55:24,504][root][INFO] - Step 35724800 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 35724800, mean_episode_return = None, mean_episode_step = 1921.8, total_loss = 87.99, pg_loss = 58.688, baseline_loss = 34.059, entropy_loss = -4.7574, learner_queue_size = 32, train_seconds = 1.9898e+04, _tick = 7344, _time = 1.6546e+09)
[2022-06-07 15:55:29,510][root][INFO] - Step 35732480 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 35732480, mean_episode_return = None, mean_episode_step = 2136.0, total_loss = 115.34, pg_loss = 40.842, baseline_loss = 79.244, entropy_loss = -4.7452, learner_queue_size = 32, train_seconds = 1.9902e+04, _tick = 7344, _time = 1.6546e+09)
[2022-06-07 15:55:34,516][root][INFO] - Step 35742720 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35742720, mean_episode_return = 62.679, mean_episode_step = 1984.7, total_loss = 9.6507, pg_loss = -38.27, baseline_loss = 52.717, entropy_loss = -4.7966, learner_queue_size = 32, train_seconds = 1.9908e+04, _tick = 7347, _time = 1.6546e+09)
[2022-06-07 15:55:39,522][root][INFO] - Step 35750400 @ 1534.1 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 35750400, mean_episode_return = None, mean_episode_step = 1640.8, total_loss = 1017.6, pg_loss = 548.59, baseline_loss = 473.73, entropy_loss = -4.7133, learner_queue_size = 32, train_seconds = 1.9912e+04, _tick = 7348, _time = 1.6546e+09)
[2022-06-07 15:55:44,528][root][INFO] - Step 35758080 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 35758080, mean_episode_return = 27.217, mean_episode_step = 2504.9, total_loss = -235.07, pg_loss = -276.07, baseline_loss = 45.635, entropy_loss = -4.6336, learner_queue_size = 32, train_seconds = 1.9918e+04, _tick = 7349, _time = 1.6546e+09)
[2022-06-07 15:55:49,535][root][INFO] - Step 35768320 @ 2045.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 35768320, mean_episode_return = 4.2696, mean_episode_step = 2351.2, total_loss = -2.5592, pg_loss = -60.678, baseline_loss = 62.779, entropy_loss = -4.6601, learner_queue_size = 32, train_seconds = 1.9922e+04, _tick = 7353, _time = 1.6546e+09)
[2022-06-07 15:55:54,541][root][INFO] - Step 35776000 @ 1534.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 35776000, mean_episode_return = 29.282, mean_episode_step = 1645.8, total_loss = 19.225, pg_loss = -42.463, baseline_loss = 66.366, entropy_loss = -4.6779, learner_queue_size = 32, train_seconds = 1.9928e+04, _tick = 7355, _time = 1.6546e+09)
[2022-06-07 15:55:59,547][root][INFO] - Step 35786240 @ 2045.6 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 35786240, mean_episode_return = None, mean_episode_step = 2265.4, total_loss = -156.81, pg_loss = -154.84, baseline_loss = 2.7565, entropy_loss = -4.728, learner_queue_size = 32, train_seconds = 1.9932e+04, _tick = 7356, _time = 1.6546e+09)
[2022-06-07 15:56:04,550][root][INFO] - Step 35793920 @ 1535.0 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 35793920, mean_episode_return = None, mean_episode_step = 2232.4, total_loss = 500.4, pg_loss = 370.75, baseline_loss = 134.41, entropy_loss = -4.7611, learner_queue_size = 32, train_seconds = 1.9938e+04, _tick = 7357, _time = 1.6546e+09)
[2022-06-07 15:56:09,556][root][INFO] - Step 35804160 @ 2045.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 35804160, mean_episode_return = None, mean_episode_step = 2489.7, total_loss = 166.35, pg_loss = 122.7, baseline_loss = 48.357, entropy_loss = -4.6973, learner_queue_size = 32, train_seconds = 1.9943e+04, _tick = 7360, _time = 1.6546e+09)
[2022-06-07 15:56:14,562][root][INFO] - Step 35811840 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35811840, mean_episode_return = 129.2, mean_episode_step = 2218.8, total_loss = 166.29, pg_loss = 44.715, baseline_loss = 126.3, entropy_loss = -4.7266, learner_queue_size = 32, train_seconds = 1.9948e+04, _tick = 7363, _time = 1.6546e+09)
[2022-06-07 15:56:19,568][root][INFO] - Step 35822080 @ 2045.6 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35822080, mean_episode_return = 43.537, mean_episode_step = 1769.5, total_loss = 76.336, pg_loss = -0.45272, baseline_loss = 81.428, entropy_loss = -4.6394, learner_queue_size = 32, train_seconds = 1.9953e+04, _tick = 7366, _time = 1.6546e+09)
[2022-06-07 15:56:24,570][root][INFO] - Step 35829760 @ 1535.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 35829760, mean_episode_return = 119.66, mean_episode_step = 2253.4, total_loss = -237.84, pg_loss = -251.52, baseline_loss = 18.297, entropy_loss = -4.6153, learner_queue_size = 32, train_seconds = 1.9958e+04, _tick = 7368, _time = 1.6546e+09)
[2022-06-07 15:56:29,576][root][INFO] - Step 35837440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35837440, mean_episode_return = 55.471, mean_episode_step = 2027.6, total_loss = 371.07, pg_loss = 208.56, baseline_loss = 167.14, entropy_loss = -4.6389, learner_queue_size = 32, train_seconds = 1.9963e+04, _tick = 7371, _time = 1.6546e+09)
[2022-06-07 15:56:34,582][root][INFO] - Step 35847680 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35847680, mean_episode_return = 28.405, mean_episode_step = 1849.4, total_loss = 331.04, pg_loss = 178.8, baseline_loss = 156.89, entropy_loss = -4.6503, learner_queue_size = 32, train_seconds = 1.9968e+04, _tick = 7373, _time = 1.6546e+09)
[2022-06-07 15:56:39,588][root][INFO] - Step 35855360 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 35855360, mean_episode_return = None, mean_episode_step = 1387.7, total_loss = 659.6, pg_loss = 473.22, baseline_loss = 191.0, entropy_loss = -4.6151, learner_queue_size = 32, train_seconds = 1.9973e+04, _tick = 7374, _time = 1.6546e+09)
[2022-06-07 15:56:44,594][root][INFO] - Step 35865600 @ 2045.6 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 35865600, mean_episode_return = None, mean_episode_step = 2107.1, total_loss = -169.81, pg_loss = -168.34, baseline_loss = 3.1063, entropy_loss = -4.5769, learner_queue_size = 32, train_seconds = 1.9978e+04, _tick = 7375, _time = 1.6546e+09)
[2022-06-07 15:56:49,600][root][INFO] - Step 35873280 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 35873280, mean_episode_return = 75.86, mean_episode_step = 1750.7, total_loss = 224.09, pg_loss = 112.58, baseline_loss = 116.13, entropy_loss = -4.6209, learner_queue_size = 32, train_seconds = 1.9983e+04, _tick = 7376, _time = 1.6546e+09)
[2022-06-07 15:56:54,602][root][INFO] - Step 35883520 @ 2047.1 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 35883520, mean_episode_return = 69.886, mean_episode_step = 2059.1, total_loss = 333.44, pg_loss = 177.74, baseline_loss = 160.4, entropy_loss = -4.6971, learner_queue_size = 32, train_seconds = 1.9988e+04, _tick = 7380, _time = 1.6546e+09)
[2022-06-07 15:56:59,608][root][INFO] - Step 35891200 @ 1534.2 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 35891200, mean_episode_return = 59.231, mean_episode_step = 1822.6, total_loss = 70.259, pg_loss = 0.028337, baseline_loss = 74.964, entropy_loss = -4.7332, learner_queue_size = 32, train_seconds = 1.9993e+04, _tick = 7382, _time = 1.6546e+09)
[2022-06-07 15:57:04,610][root][INFO] - Step 35901440 @ 2047.2 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 35901440, mean_episode_return = 30.3, mean_episode_step = 1971.5, total_loss = 143.96, pg_loss = 58.935, baseline_loss = 89.82, entropy_loss = -4.7932, learner_queue_size = 32, train_seconds = 1.9998e+04, _tick = 7384, _time = 1.6546e+09)
[2022-06-07 15:57:09,616][root][INFO] - Step 35909120 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35909120, mean_episode_return = 71.668, mean_episode_step = 2567.9, total_loss = 241.05, pg_loss = 146.13, baseline_loss = 99.661, entropy_loss = -4.747, learner_queue_size = 32, train_seconds = 2.0003e+04, _tick = 7387, _time = 1.6546e+09)
[2022-06-07 15:57:14,618][root][INFO] - Step 35919360 @ 2047.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35919360, mean_episode_return = 64.19, mean_episode_step = 1805.9, total_loss = -162.39, pg_loss = -171.12, baseline_loss = 13.565, entropy_loss = -4.8328, learner_queue_size = 32, train_seconds = 2.0008e+04, _tick = 7391, _time = 1.6546e+09)
[2022-06-07 15:57:19,622][root][INFO] - Step 35927040 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 35927040, mean_episode_return = None, mean_episode_step = 2062.7, total_loss = 383.48, pg_loss = 250.36, baseline_loss = 137.99, entropy_loss = -4.8698, learner_queue_size = 32, train_seconds = 2.0013e+04, _tick = 7393, _time = 1.6546e+09)
[2022-06-07 15:57:24,626][root][INFO] - Step 35937280 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 35937280, mean_episode_return = -0.015957, mean_episode_step = 2468.7, total_loss = 362.63, pg_loss = 243.68, baseline_loss = 123.78, entropy_loss = -4.8263, learner_queue_size = 32, train_seconds = 2.0018e+04, _tick = 7396, _time = 1.6546e+09)
[2022-06-07 15:57:29,630][root][INFO] - Step 35947520 @ 2046.3 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 35947520, mean_episode_return = None, mean_episode_step = 1711.3, total_loss = 213.75, pg_loss = 126.32, baseline_loss = 92.278, entropy_loss = -4.8538, learner_queue_size = 32, train_seconds = 2.0023e+04, _tick = 7397, _time = 1.6546e+09)
[2022-06-07 15:57:34,634][root][INFO] - Step 35955200 @ 1534.8 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 35955200, mean_episode_return = 64.709, mean_episode_step = 1762.9, total_loss = -199.15, pg_loss = -208.78, baseline_loss = 14.423, entropy_loss = -4.7938, learner_queue_size = 32, train_seconds = 2.0028e+04, _tick = 7400, _time = 1.6546e+09)
[2022-06-07 15:57:39,640][root][INFO] - Step 35962880 @ 1534.2 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 35962880, mean_episode_return = None, mean_episode_step = 1543.8, total_loss = 164.23, pg_loss = 109.48, baseline_loss = 59.507, entropy_loss = -4.7534, learner_queue_size = 32, train_seconds = 2.0033e+04, _tick = 7402, _time = 1.6546e+09)
[2022-06-07 15:57:44,646][root][INFO] - Step 35973120 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 35973120, mean_episode_return = 21.54, mean_episode_step = 2375.4, total_loss = -57.828, pg_loss = -98.98, baseline_loss = 45.844, entropy_loss = -4.6914, learner_queue_size = 32, train_seconds = 2.0038e+04, _tick = 7406, _time = 1.6546e+09)
[2022-06-07 15:57:49,652][root][INFO] - Step 35980800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 35980800, mean_episode_return = 18.45, mean_episode_step = 1810.9, total_loss = 500.25, pg_loss = 220.25, baseline_loss = 284.76, entropy_loss = -4.7486, learner_queue_size = 32, train_seconds = 2.0043e+04, _tick = 7408, _time = 1.6546e+09)
[2022-06-07 15:57:54,658][root][INFO] - Step 35991040 @ 2045.7 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 35991040, mean_episode_return = 49.99, mean_episode_step = 1948.5, total_loss = 141.23, pg_loss = 41.873, baseline_loss = 104.11, entropy_loss = -4.7477, learner_queue_size = 32, train_seconds = 2.0048e+04, _tick = 7411, _time = 1.6546e+09)
[2022-06-07 15:57:59,664][root][INFO] - Step 35998720 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 35998720, mean_episode_return = 37.661, mean_episode_step = 1946.3, total_loss = -157.28, pg_loss = -172.03, baseline_loss = 19.465, entropy_loss = -4.7084, learner_queue_size = 32, train_seconds = 2.0053e+04, _tick = 7413, _time = 1.6546e+09)
[2022-06-07 15:58:04,670][root][INFO] - Step 36008960 @ 2045.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 36008960, mean_episode_return = 39.833, mean_episode_step = 1512.7, total_loss = -144.19, pg_loss = -157.33, baseline_loss = 17.871, entropy_loss = -4.7305, learner_queue_size = 32, train_seconds = 2.0058e+04, _tick = 7417, _time = 1.6546e+09)
[2022-06-07 15:58:09,676][root][INFO] - Step 36016640 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36016640, mean_episode_return = None, mean_episode_step = 1387.4, total_loss = -200.77, pg_loss = -198.69, baseline_loss = 2.6508, entropy_loss = -4.7294, learner_queue_size = 32, train_seconds = 2.0063e+04, _tick = 7418, _time = 1.6546e+09)
[2022-06-07 15:58:14,678][root][INFO] - Step 36026880 @ 2047.1 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 36026880, mean_episode_return = -60.536, mean_episode_step = 1798.4, total_loss = 70.348, pg_loss = -5.2461, baseline_loss = 80.358, entropy_loss = -4.7635, learner_queue_size = 32, train_seconds = 2.0068e+04, _tick = 7421, _time = 1.6546e+09)
[2022-06-07 15:58:19,684][root][INFO] - Step 36034560 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 36034560, mean_episode_return = None, mean_episode_step = 2139.5, total_loss = -0.44498, pg_loss = -20.569, baseline_loss = 24.811, entropy_loss = -4.6875, learner_queue_size = 32, train_seconds = 2.0073e+04, _tick = 7422, _time = 1.6546e+09)
[2022-06-07 15:58:24,690][root][INFO] - Step 36044800 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36044800, mean_episode_return = None, mean_episode_step = 1338.5, total_loss = 186.52, pg_loss = 126.46, baseline_loss = 64.903, entropy_loss = -4.8428, learner_queue_size = 32, train_seconds = 2.0078e+04, _tick = 7423, _time = 1.6546e+09)
[2022-06-07 15:58:29,696][root][INFO] - Step 36055040 @ 2045.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 36055040, mean_episode_return = 37.032, mean_episode_step = 1792.8, total_loss = -119.82, pg_loss = -138.94, baseline_loss = 23.914, entropy_loss = -4.7921, learner_queue_size = 32, train_seconds = 2.0083e+04, _tick = 7425, _time = 1.6546e+09)
[2022-06-07 15:58:34,702][root][INFO] - Step 36062720 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36062720, mean_episode_return = None, mean_episode_step = 2252.0, total_loss = 122.54, pg_loss = 59.705, baseline_loss = 67.629, entropy_loss = -4.7901, learner_queue_size = 32, train_seconds = 2.0088e+04, _tick = 7426, _time = 1.6546e+09)
[2022-06-07 15:58:39,706][root][INFO] - Step 36072960 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 36072960, mean_episode_return = None, mean_episode_step = 1869.1, total_loss = 178.02, pg_loss = 125.72, baseline_loss = 57.08, entropy_loss = -4.7831, learner_queue_size = 32, train_seconds = 2.0093e+04, _tick = 7428, _time = 1.6546e+09)
[2022-06-07 15:58:44,712][root][INFO] - Step 36080640 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36080640, mean_episode_return = None, mean_episode_step = 1560.5, total_loss = 67.37, pg_loss = 25.407, baseline_loss = 46.795, entropy_loss = -4.8318, learner_queue_size = 32, train_seconds = 2.0098e+04, _tick = 7429, _time = 1.6546e+09)
[2022-06-07 15:58:49,718][root][INFO] - Step 36090880 @ 2045.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 36090880, mean_episode_return = 45.24, mean_episode_step = 2364.8, total_loss = 118.4, pg_loss = 49.912, baseline_loss = 73.376, entropy_loss = -4.8904, learner_queue_size = 32, train_seconds = 2.0103e+04, _tick = 7433, _time = 1.6546e+09)
[2022-06-07 15:58:54,724][root][INFO] - Step 36098560 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 36098560, mean_episode_return = 13.54, mean_episode_step = 1659.4, total_loss = -144.21, pg_loss = -157.9, baseline_loss = 18.704, entropy_loss = -5.013, learner_queue_size = 32, train_seconds = 2.0108e+04, _tick = 7435, _time = 1.6546e+09)
[2022-06-07 15:58:59,730][root][INFO] - Step 36108800 @ 2045.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 36108800, mean_episode_return = 97.327, mean_episode_step = 2012.4, total_loss = -65.561, pg_loss = -135.78, baseline_loss = 75.238, entropy_loss = -5.0166, learner_queue_size = 32, train_seconds = 2.0113e+04, _tick = 7437, _time = 1.6546e+09)
[2022-06-07 15:59:04,736][root][INFO] - Step 36116480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36116480, mean_episode_return = None, mean_episode_step = 1938.3, total_loss = -145.85, pg_loss = -143.51, baseline_loss = 2.7253, entropy_loss = -5.0658, learner_queue_size = 32, train_seconds = 2.0118e+04, _tick = 7437, _time = 1.6546e+09)
[2022-06-07 15:59:09,742][root][INFO] - Step 36126720 @ 2045.6 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 36126720, mean_episode_return = None, mean_episode_step = 1897.3, total_loss = 44.981, pg_loss = 11.896, baseline_loss = 38.12, entropy_loss = -5.0352, learner_queue_size = 32, train_seconds = 2.0123e+04, _tick = 7438, _time = 1.6546e+09)
[2022-06-07 15:59:14,746][root][INFO] - Step 36136960 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 36136960, mean_episode_return = 28.15, mean_episode_step = 1794.3, total_loss = 53.727, pg_loss = 0.60682, baseline_loss = 58.135, entropy_loss = -5.014, learner_queue_size = 32, train_seconds = 2.0128e+04, _tick = 7441, _time = 1.6546e+09)
[2022-06-07 15:59:19,750][root][INFO] - Step 36144640 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 36144640, mean_episode_return = 133.66, mean_episode_step = 2003.1, total_loss = -94.908, pg_loss = -136.68, baseline_loss = 46.738, entropy_loss = -4.9664, learner_queue_size = 32, train_seconds = 2.0133e+04, _tick = 7444, _time = 1.6546e+09)
[2022-06-07 15:59:24,756][root][INFO] - Step 36152320 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36152320, mean_episode_return = 11.079, mean_episode_step = 1936.9, total_loss = 16.215, pg_loss = -17.996, baseline_loss = 39.134, entropy_loss = -4.9227, learner_queue_size = 32, train_seconds = 2.0138e+04, _tick = 7445, _time = 1.6546e+09)
[2022-06-07 15:59:29,762][root][INFO] - Step 36162560 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 36162560, mean_episode_return = None, mean_episode_step = 1754.7, total_loss = 239.17, pg_loss = 171.23, baseline_loss = 72.843, entropy_loss = -4.9035, learner_queue_size = 32, train_seconds = 2.0143e+04, _tick = 7447, _time = 1.6546e+09)
[2022-06-07 15:59:34,768][root][INFO] - Step 36170240 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36170240, mean_episode_return = 16.709, mean_episode_step = 2033.9, total_loss = 5.0714, pg_loss = -34.906, baseline_loss = 44.903, entropy_loss = -4.9256, learner_queue_size = 32, train_seconds = 2.0148e+04, _tick = 7449, _time = 1.6546e+09)
[2022-06-07 15:59:39,774][root][INFO] - Step 36180480 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 36180480, mean_episode_return = None, mean_episode_step = 2427.8, total_loss = 104.0, pg_loss = 71.374, baseline_loss = 37.61, entropy_loss = -4.9875, learner_queue_size = 32, train_seconds = 2.0153e+04, _tick = 7450, _time = 1.6546e+09)
[2022-06-07 15:59:44,778][root][INFO] - Step 36188160 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 36188160, mean_episode_return = None, mean_episode_step = 1803.3, total_loss = -154.52, pg_loss = -152.35, baseline_loss = 2.7823, entropy_loss = -4.9505, learner_queue_size = 32, train_seconds = 2.0158e+04, _tick = 7451, _time = 1.6546e+09)
[2022-06-07 15:59:49,782][root][INFO] - Step 36198400 @ 2046.3 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 36198400, mean_episode_return = None, mean_episode_step = 2045.9, total_loss = 50.103, pg_loss = 14.402, baseline_loss = 40.708, entropy_loss = -5.007, learner_queue_size = 32, train_seconds = 2.0163e+04, _tick = 7453, _time = 1.6546e+09)
[2022-06-07 15:59:54,786][root][INFO] - Step 36208640 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36208640, mean_episode_return = None, mean_episode_step = 1869.2, total_loss = 141.82, pg_loss = 100.43, baseline_loss = 46.385, entropy_loss = -4.9923, learner_queue_size = 32, train_seconds = 2.0168e+04, _tick = 7453, _time = 1.6546e+09)
[2022-06-07 15:59:59,790][root][INFO] - Step 36216320 @ 1534.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 36216320, mean_episode_return = 83.315, mean_episode_step = 2039.0, total_loss = 470.81, pg_loss = 331.59, baseline_loss = 144.21, entropy_loss = -4.978, learner_queue_size = 32, train_seconds = 2.0173e+04, _tick = 7455, _time = 1.6546e+09)
[2022-06-07 16:00:04,794][root][INFO] - Step 36226560 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36226560, mean_episode_return = -31.171, mean_episode_step = 1820.9, total_loss = -22.348, pg_loss = -52.383, baseline_loss = 35.015, entropy_loss = -4.9804, learner_queue_size = 32, train_seconds = 2.0178e+04, _tick = 7457, _time = 1.6546e+09)
[2022-06-07 16:00:09,800][root][INFO] - Step 36234240 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36234240, mean_episode_return = 53.55, mean_episode_step = 1739.1, total_loss = 69.722, pg_loss = 15.896, baseline_loss = 58.795, entropy_loss = -4.9687, learner_queue_size = 32, train_seconds = 2.0183e+04, _tick = 7458, _time = 1.6546e+09)
[2022-06-07 16:00:14,806][root][INFO] - Step 36244480 @ 2045.6 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 36244480, mean_episode_return = None, mean_episode_step = 2062.6, total_loss = 45.971, pg_loss = -7.1223, baseline_loss = 58.014, entropy_loss = -4.9208, learner_queue_size = 32, train_seconds = 2.0188e+04, _tick = 7460, _time = 1.6546e+09)
[2022-06-07 16:00:19,818][root][INFO] - Step 36252160 @ 1532.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36252160, mean_episode_return = None, mean_episode_step = 2178.0, total_loss = 302.58, pg_loss = 210.87, baseline_loss = 96.669, entropy_loss = -4.9587, learner_queue_size = 32, train_seconds = 2.0193e+04, _tick = 7460, _time = 1.6546e+09)
[2022-06-07 16:00:24,822][root][INFO] - Step 36262400 @ 2046.4 SPS. Inference batcher size: 47. Learner queue size: 32. Other stats: (step = 36262400, mean_episode_return = None, mean_episode_step = 2009.2, total_loss = 80.549, pg_loss = 45.334, baseline_loss = 40.177, entropy_loss = -4.9615, learner_queue_size = 32, train_seconds = 2.0198e+04, _tick = 7462, _time = 1.6546e+09)
[2022-06-07 16:00:29,826][root][INFO] - Step 36270080 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36270080, mean_episode_return = None, mean_episode_step = 2014.6, total_loss = 103.07, pg_loss = 60.002, baseline_loss = 48.101, entropy_loss = -5.038, learner_queue_size = 32, train_seconds = 2.0203e+04, _tick = 7463, _time = 1.6546e+09)
[2022-06-07 16:00:34,830][root][INFO] - Step 36280320 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36280320, mean_episode_return = 11.89, mean_episode_step = 1825.7, total_loss = 388.81, pg_loss = 271.67, baseline_loss = 122.2, entropy_loss = -5.0651, learner_queue_size = 32, train_seconds = 2.0208e+04, _tick = 7465, _time = 1.6546e+09)
[2022-06-07 16:00:39,834][root][INFO] - Step 36288000 @ 1534.7 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 36288000, mean_episode_return = 80.311, mean_episode_step = 2597.0, total_loss = -56.272, pg_loss = -108.31, baseline_loss = 57.052, entropy_loss = -5.0093, learner_queue_size = 32, train_seconds = 2.0213e+04, _tick = 7467, _time = 1.6546e+09)
[2022-06-07 16:00:44,838][root][INFO] - Step 36298240 @ 2046.4 SPS. Inference batcher size: 50. Learner queue size: 32. Other stats: (step = 36298240, mean_episode_return = None, mean_episode_step = 1781.0, total_loss = 134.81, pg_loss = 84.319, baseline_loss = 55.426, entropy_loss = -4.9308, learner_queue_size = 32, train_seconds = 2.0218e+04, _tick = 7468, _time = 1.6546e+09)
[2022-06-07 16:00:49,844][root][INFO] - Step 36305920 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36305920, mean_episode_return = 46.817, mean_episode_step = 2253.7, total_loss = 11.07, pg_loss = -50.023, baseline_loss = 66.096, entropy_loss = -5.0028, learner_queue_size = 32, train_seconds = 2.0223e+04, _tick = 7469, _time = 1.6546e+09)
[2022-06-07 16:00:54,850][root][INFO] - Step 36316160 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36316160, mean_episode_return = 40.17, mean_episode_step = 2050.1, total_loss = 291.36, pg_loss = 195.19, baseline_loss = 101.11, entropy_loss = -4.9431, learner_queue_size = 32, train_seconds = 2.0228e+04, _tick = 7473, _time = 1.6546e+09)
[2022-06-07 16:00:59,856][root][INFO] - Step 36323840 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36323840, mean_episode_return = None, mean_episode_step = 2367.6, total_loss = 297.52, pg_loss = 205.73, baseline_loss = 96.725, entropy_loss = -4.9334, learner_queue_size = 32, train_seconds = 2.0233e+04, _tick = 7474, _time = 1.6546e+09)
[2022-06-07 16:01:04,862][root][INFO] - Step 36334080 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 36334080, mean_episode_return = 68.54, mean_episode_step = 2103.1, total_loss = 59.814, pg_loss = 20.19, baseline_loss = 44.561, entropy_loss = -4.9365, learner_queue_size = 32, train_seconds = 2.0238e+04, _tick = 7477, _time = 1.6546e+09)
[2022-06-07 16:01:09,866][root][INFO] - Step 36344320 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36344320, mean_episode_return = None, mean_episode_step = 2199.7, total_loss = 208.6, pg_loss = 141.19, baseline_loss = 72.376, entropy_loss = -4.9581, learner_queue_size = 32, train_seconds = 2.0243e+04, _tick = 7478, _time = 1.6546e+09)
[2022-06-07 16:01:14,870][root][INFO] - Step 36352000 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36352000, mean_episode_return = None, mean_episode_step = 1781.0, total_loss = 240.09, pg_loss = 184.54, baseline_loss = 60.562, entropy_loss = -5.0164, learner_queue_size = 32, train_seconds = 2.0248e+04, _tick = 7479, _time = 1.6546e+09)
[2022-06-07 16:01:19,876][root][INFO] - Step 36362240 @ 2045.5 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 36362240, mean_episode_return = None, mean_episode_step = 1738.7, total_loss = -91.029, pg_loss = -91.366, baseline_loss = 5.4497, entropy_loss = -5.1133, learner_queue_size = 32, train_seconds = 2.0253e+04, _tick = 7481, _time = 1.6546e+09)
[2022-06-07 16:01:24,882][root][INFO] - Step 36369920 @ 1534.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 36369920, mean_episode_return = 9.7994, mean_episode_step = 1761.0, total_loss = 534.6, pg_loss = 384.04, baseline_loss = 155.72, entropy_loss = -5.1526, learner_queue_size = 32, train_seconds = 2.0258e+04, _tick = 7484, _time = 1.6546e+09)
[2022-06-07 16:01:29,886][root][INFO] - Step 36380160 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 36380160, mean_episode_return = None, mean_episode_step = 2453.6, total_loss = -112.41, pg_loss = -120.31, baseline_loss = 13.037, entropy_loss = -5.1356, learner_queue_size = 32, train_seconds = 2.0263e+04, _tick = 7484, _time = 1.6546e+09)
[2022-06-07 16:01:34,890][root][INFO] - Step 36387840 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 36387840, mean_episode_return = None, mean_episode_step = 2070.2, total_loss = 48.492, pg_loss = 16.622, baseline_loss = 36.992, entropy_loss = -5.1215, learner_queue_size = 32, train_seconds = 2.0268e+04, _tick = 7484, _time = 1.6546e+09)
[2022-06-07 16:01:39,894][root][INFO] - Step 36398080 @ 2046.3 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 36398080, mean_episode_return = 47.24, mean_episode_step = 2309.4, total_loss = 651.34, pg_loss = 439.48, baseline_loss = 217.07, entropy_loss = -5.2185, learner_queue_size = 32, train_seconds = 2.0273e+04, _tick = 7486, _time = 1.6546e+09)
[2022-06-07 16:01:44,900][root][INFO] - Step 36405760 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36405760, mean_episode_return = 37.79, mean_episode_step = 2312.2, total_loss = 519.96, pg_loss = 351.63, baseline_loss = 173.53, entropy_loss = -5.2026, learner_queue_size = 32, train_seconds = 2.0278e+04, _tick = 7487, _time = 1.6546e+09)
[2022-06-07 16:01:49,906][root][INFO] - Step 36416000 @ 2045.6 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 36416000, mean_episode_return = 43.271, mean_episode_step = 2476.7, total_loss = 338.08, pg_loss = 158.38, baseline_loss = 184.96, entropy_loss = -5.259, learner_queue_size = 32, train_seconds = 2.0283e+04, _tick = 7489, _time = 1.6546e+09)
[2022-06-07 16:01:54,910][root][INFO] - Step 36423680 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 36423680, mean_episode_return = 2.4125, mean_episode_step = 2801.9, total_loss = -255.46, pg_loss = -287.65, baseline_loss = 37.492, entropy_loss = -5.2968, learner_queue_size = 32, train_seconds = 2.0288e+04, _tick = 7491, _time = 1.6546e+09)
[2022-06-07 16:01:59,917][root][INFO] - Step 36431360 @ 1534.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 36431360, mean_episode_return = -31.911, mean_episode_step = 2026.7, total_loss = 48.226, pg_loss = -3.3811, baseline_loss = 56.943, entropy_loss = -5.3355, learner_queue_size = 32, train_seconds = 2.0293e+04, _tick = 7494, _time = 1.6546e+09)
[2022-06-07 16:02:04,923][root][INFO] - Step 36441600 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36441600, mean_episode_return = 23.556, mean_episode_step = 2382.7, total_loss = -178.51, pg_loss = -185.68, baseline_loss = 12.495, entropy_loss = -5.325, learner_queue_size = 32, train_seconds = 2.0298e+04, _tick = 7498, _time = 1.6546e+09)
[2022-06-07 16:02:09,925][root][INFO] - Step 36451840 @ 2047.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 36451840, mean_episode_return = 34.301, mean_episode_step = 1835.4, total_loss = 157.87, pg_loss = 104.49, baseline_loss = 58.718, entropy_loss = -5.3305, learner_queue_size = 32, train_seconds = 2.0303e+04, _tick = 7501, _time = 1.6546e+09)
[2022-06-07 16:02:14,930][root][INFO] - Step 36459520 @ 1534.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 36459520, mean_episode_return = None, mean_episode_step = 2228.5, total_loss = -172.89, pg_loss = -170.6, baseline_loss = 3.0368, entropy_loss = -5.3325, learner_queue_size = 32, train_seconds = 2.0308e+04, _tick = 7502, _time = 1.6546e+09)
[2022-06-07 16:02:19,932][root][INFO] - Step 36467200 @ 1535.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36467200, mean_episode_return = None, mean_episode_step = 1762.6, total_loss = 245.24, pg_loss = 169.67, baseline_loss = 80.901, entropy_loss = -5.3326, learner_queue_size = 32, train_seconds = 2.0313e+04, _tick = 7504, _time = 1.6546e+09)
[2022-06-07 16:02:24,938][root][INFO] - Step 36477440 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36477440, mean_episode_return = 87.039, mean_episode_step = 2014.1, total_loss = 114.32, pg_loss = 34.557, baseline_loss = 85.115, entropy_loss = -5.3514, learner_queue_size = 32, train_seconds = 2.0318e+04, _tick = 7508, _time = 1.6546e+09)
[2022-06-07 16:02:29,944][root][INFO] - Step 36487680 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36487680, mean_episode_return = 79.893, mean_episode_step = 2211.7, total_loss = 467.21, pg_loss = 285.55, baseline_loss = 187.03, entropy_loss = -5.3726, learner_queue_size = 32, train_seconds = 2.0323e+04, _tick = 7510, _time = 1.6546e+09)
[2022-06-07 16:02:34,946][root][INFO] - Step 36495360 @ 1535.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36495360, mean_episode_return = 29.36, mean_episode_step = 2118.3, total_loss = 367.14, pg_loss = 205.88, baseline_loss = 166.61, entropy_loss = -5.3448, learner_queue_size = 32, train_seconds = 2.0328e+04, _tick = 7513, _time = 1.6546e+09)
[2022-06-07 16:02:39,952][root][INFO] - Step 36505600 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36505600, mean_episode_return = -1.9003, mean_episode_step = 2145.4, total_loss = -186.44, pg_loss = -209.99, baseline_loss = 28.918, entropy_loss = -5.3744, learner_queue_size = 32, train_seconds = 2.0333e+04, _tick = 7514, _time = 1.6546e+09)
[2022-06-07 16:02:44,958][root][INFO] - Step 36513280 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36513280, mean_episode_return = None, mean_episode_step = 2179.0, total_loss = -24.041, pg_loss = -45.599, baseline_loss = 26.95, entropy_loss = -5.3918, learner_queue_size = 32, train_seconds = 2.0338e+04, _tick = 7516, _time = 1.6546e+09)
[2022-06-07 16:02:49,964][root][INFO] - Step 36520960 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36520960, mean_episode_return = 70.468, mean_episode_step = 2099.9, total_loss = -124.4, pg_loss = -156.31, baseline_loss = 37.291, entropy_loss = -5.3879, learner_queue_size = 32, train_seconds = 2.0343e+04, _tick = 7519, _time = 1.6546e+09)
[2022-06-07 16:02:54,970][root][INFO] - Step 36531200 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 36531200, mean_episode_return = 56.262, mean_episode_step = 1761.4, total_loss = -24.476, pg_loss = -53.908, baseline_loss = 34.869, entropy_loss = -5.4371, learner_queue_size = 32, train_seconds = 2.0348e+04, _tick = 7522, _time = 1.6546e+09)
[2022-06-07 16:02:59,974][root][INFO] - Step 36538880 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36538880, mean_episode_return = 96.989, mean_episode_step = 2230.6, total_loss = -281.79, pg_loss = -293.46, baseline_loss = 17.074, entropy_loss = -5.4051, learner_queue_size = 32, train_seconds = 2.0353e+04, _tick = 7525, _time = 1.6546e+09)
[2022-06-07 16:03:04,978][root][INFO] - Step 36549120 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36549120, mean_episode_return = 6.9897, mean_episode_step = 1668.3, total_loss = 304.41, pg_loss = 208.78, baseline_loss = 101.05, entropy_loss = -5.4153, learner_queue_size = 32, train_seconds = 2.0358e+04, _tick = 7528, _time = 1.6546e+09)
[2022-06-07 16:03:09,982][root][INFO] - Step 36559360 @ 2046.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 36559360, mean_episode_return = None, mean_episode_step = 2027.3, total_loss = 64.025, pg_loss = 21.221, baseline_loss = 48.215, entropy_loss = -5.4106, learner_queue_size = 32, train_seconds = 2.0363e+04, _tick = 7531, _time = 1.6546e+09)
[2022-06-07 16:03:14,986][root][INFO] - Step 36567040 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 36567040, mean_episode_return = 16.23, mean_episode_step = 2195.9, total_loss = 356.77, pg_loss = 200.8, baseline_loss = 161.36, entropy_loss = -5.4026, learner_queue_size = 32, train_seconds = 2.0368e+04, _tick = 7534, _time = 1.6546e+09)
[2022-06-07 16:03:19,990][root][INFO] - Step 36577280 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 36577280, mean_episode_return = -9.5807, mean_episode_step = 2009.0, total_loss = 387.13, pg_loss = 191.43, baseline_loss = 201.11, entropy_loss = -5.4083, learner_queue_size = 32, train_seconds = 2.0373e+04, _tick = 7537, _time = 1.6546e+09)
[2022-06-07 16:03:24,996][root][INFO] - Step 36584960 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36584960, mean_episode_return = 15.9, mean_episode_step = 2374.1, total_loss = 50.857, pg_loss = -20.041, baseline_loss = 76.327, entropy_loss = -5.4291, learner_queue_size = 32, train_seconds = 2.0378e+04, _tick = 7538, _time = 1.6546e+09)
[2022-06-07 16:03:29,998][root][INFO] - Step 36595200 @ 2047.2 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 36595200, mean_episode_return = 66.688, mean_episode_step = 1912.6, total_loss = 116.72, pg_loss = 21.296, baseline_loss = 100.8, entropy_loss = -5.3754, learner_queue_size = 32, train_seconds = 2.0383e+04, _tick = 7540, _time = 1.6546e+09)
[2022-06-07 16:03:35,002][root][INFO] - Step 36602880 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36602880, mean_episode_return = 4.13, mean_episode_step = 2097.5, total_loss = -195.23, pg_loss = -199.6, baseline_loss = 9.6782, entropy_loss = -5.3069, learner_queue_size = 32, train_seconds = 2.0388e+04, _tick = 7543, _time = 1.6546e+09)
[2022-06-07 16:03:40,006][root][INFO] - Step 36613120 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36613120, mean_episode_return = -6.0201, mean_episode_step = 1476.7, total_loss = -31.288, pg_loss = -52.192, baseline_loss = 26.238, entropy_loss = -5.3351, learner_queue_size = 32, train_seconds = 2.0393e+04, _tick = 7547, _time = 1.6546e+09)
[2022-06-07 16:03:45,010][root][INFO] - Step 36620800 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 36620800, mean_episode_return = 42.891, mean_episode_step = 1758.0, total_loss = 10.751, pg_loss = -29.834, baseline_loss = 45.856, entropy_loss = -5.2716, learner_queue_size = 32, train_seconds = 2.0398e+04, _tick = 7549, _time = 1.6546e+09)
[2022-06-07 16:03:50,016][root][INFO] - Step 36631040 @ 2045.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 36631040, mean_episode_return = 72.312, mean_episode_step = 1694.6, total_loss = -33.189, pg_loss = -78.957, baseline_loss = 51.082, entropy_loss = -5.314, learner_queue_size = 32, train_seconds = 2.0403e+04, _tick = 7551, _time = 1.6546e+09)
[2022-06-07 16:03:55,022][root][INFO] - Step 36638720 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36638720, mean_episode_return = None, mean_episode_step = 2048.8, total_loss = 130.46, pg_loss = 82.652, baseline_loss = 53.111, entropy_loss = -5.3019, learner_queue_size = 32, train_seconds = 2.0408e+04, _tick = 7551, _time = 1.6546e+09)
[2022-06-07 16:04:00,027][root][INFO] - Step 36648960 @ 2046.2 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 36648960, mean_episode_return = None, mean_episode_step = 1986.9, total_loss = 22.248, pg_loss = -17.147, baseline_loss = 44.726, entropy_loss = -5.3308, learner_queue_size = 32, train_seconds = 2.0413e+04, _tick = 7554, _time = 1.6546e+09)
[2022-06-07 16:04:05,030][root][INFO] - Step 36656640 @ 1535.0 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36656640, mean_episode_return = None, mean_episode_step = 2275.4, total_loss = 110.93, pg_loss = 57.776, baseline_loss = 58.509, entropy_loss = -5.3509, learner_queue_size = 32, train_seconds = 2.0418e+04, _tick = 7555, _time = 1.6546e+09)
[2022-06-07 16:04:10,036][root][INFO] - Step 36666880 @ 2045.5 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 36666880, mean_episode_return = 130.56, mean_episode_step = 1662.3, total_loss = 9.0756, pg_loss = -21.695, baseline_loss = 36.094, entropy_loss = -5.3227, learner_queue_size = 32, train_seconds = 2.0423e+04, _tick = 7558, _time = 1.6546e+09)
[2022-06-07 16:04:15,043][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 16:04:15,277][root][INFO] - Step 36674560 @ 1534.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36674560, mean_episode_return = None, mean_episode_step = 2465.2, total_loss = 59.277, pg_loss = 22.804, baseline_loss = 41.767, entropy_loss = -5.2948, learner_queue_size = 32, train_seconds = 2.0428e+04, _tick = 7558, _time = 1.6546e+09)
[2022-06-07 16:04:20,283][root][INFO] - Step 36684800 @ 1954.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36684800, mean_episode_return = None, mean_episode_step = 1866.7, total_loss = 4.3314, pg_loss = -33.871, baseline_loss = 43.564, entropy_loss = -5.3611, learner_queue_size = 32, train_seconds = 2.0433e+04, _tick = 7561, _time = 1.6546e+09)
[2022-06-07 16:04:25,288][root][INFO] - Step 36692480 @ 1534.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 36692480, mean_episode_return = 4.3397, mean_episode_step = 1854.7, total_loss = 152.56, pg_loss = 107.2, baseline_loss = 50.705, entropy_loss = -5.3408, learner_queue_size = 32, train_seconds = 2.0438e+04, _tick = 7564, _time = 1.6546e+09)
[2022-06-07 16:04:30,294][root][INFO] - Step 36702720 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36702720, mean_episode_return = None, mean_episode_step = 1689.2, total_loss = -65.868, pg_loss = -80.863, baseline_loss = 20.308, entropy_loss = -5.3123, learner_queue_size = 32, train_seconds = 2.0443e+04, _tick = 7565, _time = 1.6546e+09)
[2022-06-07 16:04:35,298][root][INFO] - Step 36712960 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 36712960, mean_episode_return = None, mean_episode_step = 1766.8, total_loss = -170.21, pg_loss = -166.24, baseline_loss = 1.2907, entropy_loss = -5.2562, learner_queue_size = 32, train_seconds = 2.0448e+04, _tick = 7567, _time = 1.6546e+09)
[2022-06-07 16:04:40,302][root][INFO] - Step 36720640 @ 1534.7 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 36720640, mean_episode_return = None, mean_episode_step = 2330.9, total_loss = -75.58, pg_loss = -100.07, baseline_loss = 29.73, entropy_loss = -5.2432, learner_queue_size = 32, train_seconds = 2.0453e+04, _tick = 7568, _time = 1.6546e+09)
[2022-06-07 16:04:45,306][root][INFO] - Step 36730880 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 36730880, mean_episode_return = None, mean_episode_step = 1731.2, total_loss = 326.4, pg_loss = 231.99, baseline_loss = 99.657, entropy_loss = -5.2455, learner_queue_size = 32, train_seconds = 2.0458e+04, _tick = 7569, _time = 1.6546e+09)
[2022-06-07 16:04:50,310][root][INFO] - Step 36738560 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 36738560, mean_episode_return = 63.149, mean_episode_step = 2163.4, total_loss = -104.12, pg_loss = -121.12, baseline_loss = 22.198, entropy_loss = -5.1992, learner_queue_size = 32, train_seconds = 2.0463e+04, _tick = 7571, _time = 1.6546e+09)
[2022-06-07 16:04:55,314][root][INFO] - Step 36748800 @ 2046.4 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 36748800, mean_episode_return = 21.521, mean_episode_step = 1716.7, total_loss = 151.78, pg_loss = 55.168, baseline_loss = 101.81, entropy_loss = -5.19, learner_queue_size = 32, train_seconds = 2.0468e+04, _tick = 7573, _time = 1.6546e+09)
[2022-06-07 16:05:00,318][root][INFO] - Step 36756480 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 36756480, mean_episode_return = None, mean_episode_step = 1941.4, total_loss = 52.554, pg_loss = 21.118, baseline_loss = 36.623, entropy_loss = -5.1869, learner_queue_size = 32, train_seconds = 2.0473e+04, _tick = 7573, _time = 1.6546e+09)
[2022-06-07 16:05:05,322][root][INFO] - Step 36766720 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 36766720, mean_episode_return = None, mean_episode_step = 1686.6, total_loss = 35.863, pg_loss = 6.2715, baseline_loss = 34.804, entropy_loss = -5.2127, learner_queue_size = 32, train_seconds = 2.0478e+04, _tick = 7574, _time = 1.6546e+09)
[2022-06-07 16:05:10,326][root][INFO] - Step 36774400 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36774400, mean_episode_return = None, mean_episode_step = 2413.9, total_loss = 93.767, pg_loss = 60.483, baseline_loss = 38.545, entropy_loss = -5.2616, learner_queue_size = 32, train_seconds = 2.0483e+04, _tick = 7575, _time = 1.6546e+09)
[2022-06-07 16:05:15,330][root][INFO] - Step 36784640 @ 2046.4 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 36784640, mean_episode_return = 102.82, mean_episode_step = 1787.8, total_loss = 101.09, pg_loss = 20.889, baseline_loss = 85.457, entropy_loss = -5.2543, learner_queue_size = 32, train_seconds = 2.0488e+04, _tick = 7576, _time = 1.6546e+09)
[2022-06-07 16:05:20,336][root][INFO] - Step 36792320 @ 1534.2 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 36792320, mean_episode_return = 1.5796, mean_episode_step = 1854.0, total_loss = 87.22, pg_loss = 42.076, baseline_loss = 50.424, entropy_loss = -5.2799, learner_queue_size = 32, train_seconds = 2.0493e+04, _tick = 7579, _time = 1.6546e+09)
[2022-06-07 16:05:25,338][root][INFO] - Step 36802560 @ 2047.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 36802560, mean_episode_return = 45.852, mean_episode_step = 2197.2, total_loss = 36.01, pg_loss = -11.547, baseline_loss = 52.899, entropy_loss = -5.3422, learner_queue_size = 32, train_seconds = 2.0498e+04, _tick = 7582, _time = 1.6546e+09)
[2022-06-07 16:05:30,342][root][INFO] - Step 36810240 @ 1534.8 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 36810240, mean_episode_return = 57.081, mean_episode_step = 2021.2, total_loss = -160.87, pg_loss = -167.14, baseline_loss = 11.6, entropy_loss = -5.3313, learner_queue_size = 32, train_seconds = 2.0503e+04, _tick = 7584, _time = 1.6546e+09)
[2022-06-07 16:05:35,346][root][INFO] - Step 36820480 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 36820480, mean_episode_return = -12.97, mean_episode_step = 2126.0, total_loss = 870.36, pg_loss = 648.57, baseline_loss = 227.16, entropy_loss = -5.3746, learner_queue_size = 32, train_seconds = 2.0508e+04, _tick = 7586, _time = 1.6546e+09)
[2022-06-07 16:05:40,350][root][INFO] - Step 36830720 @ 2046.2 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 36830720, mean_episode_return = 32.48, mean_episode_step = 2054.6, total_loss = 188.14, pg_loss = 106.72, baseline_loss = 86.828, entropy_loss = -5.4076, learner_queue_size = 32, train_seconds = 2.0513e+04, _tick = 7589, _time = 1.6546e+09)
[2022-06-07 16:05:45,354][root][INFO] - Step 36838400 @ 1534.9 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36838400, mean_episode_return = None, mean_episode_step = 2619.2, total_loss = -161.8, pg_loss = -158.53, baseline_loss = 2.1295, entropy_loss = -5.4049, learner_queue_size = 32, train_seconds = 2.0518e+04, _tick = 7590, _time = 1.6546e+09)
[2022-06-07 16:05:50,358][root][INFO] - Step 36848640 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36848640, mean_episode_return = 36.83, mean_episode_step = 1912.3, total_loss = 46.552, pg_loss = 8.0089, baseline_loss = 43.996, entropy_loss = -5.4535, learner_queue_size = 32, train_seconds = 2.0523e+04, _tick = 7593, _time = 1.6546e+09)
[2022-06-07 16:05:55,364][root][INFO] - Step 36856320 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36856320, mean_episode_return = 15.22, mean_episode_step = 2151.0, total_loss = -33.994, pg_loss = -56.573, baseline_loss = 28.02, entropy_loss = -5.4412, learner_queue_size = 32, train_seconds = 2.0528e+04, _tick = 7596, _time = 1.6546e+09)
[2022-06-07 16:06:00,370][root][INFO] - Step 36866560 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36866560, mean_episode_return = 1.5897, mean_episode_step = 1526.1, total_loss = 386.45, pg_loss = 233.08, baseline_loss = 158.8, entropy_loss = -5.43, learner_queue_size = 32, train_seconds = 2.0533e+04, _tick = 7600, _time = 1.6546e+09)
[2022-06-07 16:06:05,374][root][INFO] - Step 36874240 @ 1534.8 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 36874240, mean_episode_return = 127.92, mean_episode_step = 2139.7, total_loss = -2.987, pg_loss = -25.504, baseline_loss = 27.918, entropy_loss = -5.4008, learner_queue_size = 32, train_seconds = 2.0538e+04, _tick = 7602, _time = 1.6546e+09)
[2022-06-07 16:06:10,378][root][INFO] - Step 36884480 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 36884480, mean_episode_return = None, mean_episode_step = 1712.5, total_loss = 448.81, pg_loss = 330.84, baseline_loss = 123.38, entropy_loss = -5.4111, learner_queue_size = 32, train_seconds = 2.0543e+04, _tick = 7603, _time = 1.6546e+09)
[2022-06-07 16:06:15,382][root][INFO] - Step 36892160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 36892160, mean_episode_return = 2.4119, mean_episode_step = 2303.7, total_loss = -54.04, pg_loss = -95.956, baseline_loss = 47.321, entropy_loss = -5.4049, learner_queue_size = 32, train_seconds = 2.0548e+04, _tick = 7606, _time = 1.6546e+09)
[2022-06-07 16:06:20,388][root][INFO] - Step 36902400 @ 2045.5 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 36902400, mean_episode_return = None, mean_episode_step = 1944.7, total_loss = 284.82, pg_loss = 195.86, baseline_loss = 94.399, entropy_loss = -5.4352, learner_queue_size = 32, train_seconds = 2.0553e+04, _tick = 7608, _time = 1.6546e+09)
[2022-06-07 16:06:25,394][root][INFO] - Step 36910080 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 36910080, mean_episode_return = 62.526, mean_episode_step = 2242.6, total_loss = -153.91, pg_loss = -194.46, baseline_loss = 45.967, entropy_loss = -5.417, learner_queue_size = 32, train_seconds = 2.0558e+04, _tick = 7611, _time = 1.6546e+09)
[2022-06-07 16:06:30,398][root][INFO] - Step 36920320 @ 2046.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 36920320, mean_episode_return = 73.39, mean_episode_step = 1402.1, total_loss = -161.41, pg_loss = -162.91, baseline_loss = 6.9176, entropy_loss = -5.4192, learner_queue_size = 32, train_seconds = 2.0563e+04, _tick = 7613, _time = 1.6546e+09)
[2022-06-07 16:06:35,402][root][INFO] - Step 36928000 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 36928000, mean_episode_return = 95.75, mean_episode_step = 1995.8, total_loss = 594.38, pg_loss = 457.41, baseline_loss = 142.37, entropy_loss = -5.4044, learner_queue_size = 32, train_seconds = 2.0568e+04, _tick = 7615, _time = 1.6546e+09)
[2022-06-07 16:06:40,406][root][INFO] - Step 36938240 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 36938240, mean_episode_return = 54.862, mean_episode_step = 2081.0, total_loss = 139.0, pg_loss = 93.151, baseline_loss = 51.274, entropy_loss = -5.4268, learner_queue_size = 32, train_seconds = 2.0573e+04, _tick = 7619, _time = 1.6546e+09)
[2022-06-07 16:06:45,410][root][INFO] - Step 36945920 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 36945920, mean_episode_return = 92.268, mean_episode_step = 1814.1, total_loss = 207.51, pg_loss = 108.97, baseline_loss = 103.91, entropy_loss = -5.3721, learner_queue_size = 32, train_seconds = 2.0578e+04, _tick = 7622, _time = 1.6546e+09)
[2022-06-07 16:06:50,414][root][INFO] - Step 36956160 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 36956160, mean_episode_return = 62.299, mean_episode_step = 1589.6, total_loss = 124.57, pg_loss = 78.783, baseline_loss = 51.135, entropy_loss = -5.3526, learner_queue_size = 32, train_seconds = 2.0583e+04, _tick = 7624, _time = 1.6546e+09)
[2022-06-07 16:06:55,420][root][INFO] - Step 36963840 @ 1534.2 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 36963840, mean_episode_return = None, mean_episode_step = 2234.7, total_loss = 283.26, pg_loss = 208.8, baseline_loss = 79.833, entropy_loss = -5.3678, learner_queue_size = 32, train_seconds = 2.0588e+04, _tick = 7625, _time = 1.6546e+09)
[2022-06-07 16:07:00,424][root][INFO] - Step 36974080 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 36974080, mean_episode_return = None, mean_episode_step = 1924.5, total_loss = 46.893, pg_loss = 9.2555, baseline_loss = 42.995, entropy_loss = -5.357, learner_queue_size = 32, train_seconds = 2.0593e+04, _tick = 7627, _time = 1.6546e+09)
[2022-06-07 16:07:05,430][root][INFO] - Step 36981760 @ 1534.2 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 36981760, mean_episode_return = None, mean_episode_step = 2013.2, total_loss = -20.968, pg_loss = -54.212, baseline_loss = 38.593, entropy_loss = -5.3485, learner_queue_size = 32, train_seconds = 2.0598e+04, _tick = 7628, _time = 1.6546e+09)
[2022-06-07 16:07:10,436][root][INFO] - Step 36992000 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 36992000, mean_episode_return = 71.188, mean_episode_step = 2005.9, total_loss = -140.85, pg_loss = -146.87, baseline_loss = 11.379, entropy_loss = -5.3551, learner_queue_size = 32, train_seconds = 2.0603e+04, _tick = 7630, _time = 1.6546e+09)
[2022-06-07 16:07:15,442][root][INFO] - Step 36999680 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 36999680, mean_episode_return = None, mean_episode_step = 1450.8, total_loss = -27.567, pg_loss = -48.638, baseline_loss = 26.449, entropy_loss = -5.3782, learner_queue_size = 32, train_seconds = 2.0608e+04, _tick = 7631, _time = 1.6546e+09)
[2022-06-07 16:07:20,446][root][INFO] - Step 37009920 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 37009920, mean_episode_return = 59.346, mean_episode_step = 2302.4, total_loss = 86.28, pg_loss = 29.7, baseline_loss = 61.946, entropy_loss = -5.3652, learner_queue_size = 32, train_seconds = 2.0613e+04, _tick = 7632, _time = 1.6546e+09)
[2022-06-07 16:07:25,452][root][INFO] - Step 37017600 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37017600, mean_episode_return = 43.322, mean_episode_step = 2449.8, total_loss = -111.14, pg_loss = -126.89, baseline_loss = 21.058, entropy_loss = -5.3141, learner_queue_size = 32, train_seconds = 2.0618e+04, _tick = 7634, _time = 1.6546e+09)
[2022-06-07 16:07:30,458][root][INFO] - Step 37027840 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 37027840, mean_episode_return = -55.147, mean_episode_step = 1887.9, total_loss = 138.58, pg_loss = 66.329, baseline_loss = 77.618, entropy_loss = -5.3721, learner_queue_size = 32, train_seconds = 2.0624e+04, _tick = 7637, _time = 1.6546e+09)
[2022-06-07 16:07:35,462][root][INFO] - Step 37035520 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 37035520, mean_episode_return = 50.923, mean_episode_step = 1809.0, total_loss = 144.39, pg_loss = 84.733, baseline_loss = 65.05, entropy_loss = -5.3911, learner_queue_size = 32, train_seconds = 2.0628e+04, _tick = 7639, _time = 1.6546e+09)
[2022-06-07 16:07:40,466][root][INFO] - Step 37045760 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37045760, mean_episode_return = 19.12, mean_episode_step = 2157.9, total_loss = -42.141, pg_loss = -56.686, baseline_loss = 19.96, entropy_loss = -5.4149, learner_queue_size = 32, train_seconds = 2.0634e+04, _tick = 7642, _time = 1.6546e+09)
[2022-06-07 16:07:45,470][root][INFO] - Step 37053440 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 37053440, mean_episode_return = 18.534, mean_episode_step = 1815.7, total_loss = -149.0, pg_loss = -195.27, baseline_loss = 51.698, entropy_loss = -5.4322, learner_queue_size = 32, train_seconds = 2.0638e+04, _tick = 7643, _time = 1.6546e+09)
[2022-06-07 16:07:50,476][root][INFO] - Step 37063680 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 37063680, mean_episode_return = 11.829, mean_episode_step = 2311.7, total_loss = -176.71, pg_loss = -175.91, baseline_loss = 4.6285, entropy_loss = -5.4274, learner_queue_size = 32, train_seconds = 2.0644e+04, _tick = 7646, _time = 1.6546e+09)
[2022-06-07 16:07:55,482][root][INFO] - Step 37071360 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 37071360, mean_episode_return = 91.085, mean_episode_step = 1781.0, total_loss = 185.43, pg_loss = 117.88, baseline_loss = 72.98, entropy_loss = -5.4281, learner_queue_size = 32, train_seconds = 2.0648e+04, _tick = 7648, _time = 1.6546e+09)
[2022-06-07 16:08:00,486][root][INFO] - Step 37081600 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 37081600, mean_episode_return = 58.629, mean_episode_step = 1908.3, total_loss = 20.435, pg_loss = -25.722, baseline_loss = 51.594, entropy_loss = -5.4367, learner_queue_size = 32, train_seconds = 2.0654e+04, _tick = 7650, _time = 1.6546e+09)
[2022-06-07 16:08:05,490][root][INFO] - Step 37091840 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 37091840, mean_episode_return = -0.86061, mean_episode_step = 2749.3, total_loss = 165.48, pg_loss = 88.474, baseline_loss = 82.441, entropy_loss = -5.431, learner_queue_size = 32, train_seconds = 2.0658e+04, _tick = 7654, _time = 1.6546e+09)
[2022-06-07 16:08:10,496][root][INFO] - Step 37099520 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37099520, mean_episode_return = 11.5, mean_episode_step = 1996.2, total_loss = 37.381, pg_loss = -20.393, baseline_loss = 63.207, entropy_loss = -5.4327, learner_queue_size = 32, train_seconds = 2.0664e+04, _tick = 7655, _time = 1.6546e+09)
[2022-06-07 16:08:15,502][root][INFO] - Step 37107200 @ 1534.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 37107200, mean_episode_return = None, mean_episode_step = 2409.7, total_loss = -17.948, pg_loss = -48.38, baseline_loss = 35.942, entropy_loss = -5.5105, learner_queue_size = 32, train_seconds = 2.0668e+04, _tick = 7656, _time = 1.6546e+09)
[2022-06-07 16:08:20,506][root][INFO] - Step 37117440 @ 2046.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 37117440, mean_episode_return = None, mean_episode_step = 2009.6, total_loss = -21.341, pg_loss = -36.349, baseline_loss = 20.533, entropy_loss = -5.5243, learner_queue_size = 32, train_seconds = 2.0674e+04, _tick = 7657, _time = 1.6546e+09)
[2022-06-07 16:08:25,510][root][INFO] - Step 37127680 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37127680, mean_episode_return = None, mean_episode_step = 2127.4, total_loss = 205.62, pg_loss = 147.11, baseline_loss = 64.027, entropy_loss = -5.5196, learner_queue_size = 32, train_seconds = 2.0678e+04, _tick = 7659, _time = 1.6546e+09)
[2022-06-07 16:08:30,514][root][INFO] - Step 37135360 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37135360, mean_episode_return = None, mean_episode_step = 1771.0, total_loss = 569.56, pg_loss = 446.15, baseline_loss = 128.9, entropy_loss = -5.4952, learner_queue_size = 32, train_seconds = 2.0684e+04, _tick = 7661, _time = 1.6546e+09)
[2022-06-07 16:08:35,518][root][INFO] - Step 37145600 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 37145600, mean_episode_return = None, mean_episode_step = 2403.6, total_loss = 223.02, pg_loss = 155.65, baseline_loss = 72.861, entropy_loss = -5.4926, learner_queue_size = 32, train_seconds = 2.0688e+04, _tick = 7664, _time = 1.6546e+09)
[2022-06-07 16:08:40,522][root][INFO] - Step 37153280 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37153280, mean_episode_return = 23.0, mean_episode_step = 1503.9, total_loss = -61.995, pg_loss = -94.773, baseline_loss = 38.254, entropy_loss = -5.4759, learner_queue_size = 32, train_seconds = 2.0694e+04, _tick = 7665, _time = 1.6546e+09)
[2022-06-07 16:08:45,528][root][INFO] - Step 37163520 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 37163520, mean_episode_return = 63.009, mean_episode_step = 1651.4, total_loss = -38.169, pg_loss = -57.624, baseline_loss = 24.971, entropy_loss = -5.5153, learner_queue_size = 32, train_seconds = 2.0698e+04, _tick = 7667, _time = 1.6546e+09)
[2022-06-07 16:08:50,530][root][INFO] - Step 37171200 @ 1535.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37171200, mean_episode_return = None, mean_episode_step = 2110.8, total_loss = 70.893, pg_loss = 21.234, baseline_loss = 55.188, entropy_loss = -5.5287, learner_queue_size = 32, train_seconds = 2.0704e+04, _tick = 7668, _time = 1.6546e+09)
[2022-06-07 16:08:55,536][root][INFO] - Step 37178880 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 37178880, mean_episode_return = None, mean_episode_step = 1831.7, total_loss = -118.01, pg_loss = -124.41, baseline_loss = 11.911, entropy_loss = -5.5093, learner_queue_size = 32, train_seconds = 2.0708e+04, _tick = 7670, _time = 1.6546e+09)
[2022-06-07 16:09:00,542][root][INFO] - Step 37189120 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 37189120, mean_episode_return = 82.43, mean_episode_step = 2229.2, total_loss = -52.127, pg_loss = -78.439, baseline_loss = 31.815, entropy_loss = -5.5027, learner_queue_size = 32, train_seconds = 2.0714e+04, _tick = 7673, _time = 1.6546e+09)
[2022-06-07 16:09:05,548][root][INFO] - Step 37196800 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 37196800, mean_episode_return = 68.871, mean_episode_step = 2030.9, total_loss = 210.93, pg_loss = 117.96, baseline_loss = 98.455, entropy_loss = -5.4867, learner_queue_size = 32, train_seconds = 2.0718e+04, _tick = 7676, _time = 1.6546e+09)
[2022-06-07 16:09:10,554][root][INFO] - Step 37207040 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37207040, mean_episode_return = 86.361, mean_episode_step = 2184.1, total_loss = -249.53, pg_loss = -262.48, baseline_loss = 18.434, entropy_loss = -5.484, learner_queue_size = 32, train_seconds = 2.0724e+04, _tick = 7679, _time = 1.6546e+09)
[2022-06-07 16:09:15,560][root][INFO] - Step 37214720 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 37214720, mean_episode_return = 236.71, mean_episode_step = 2017.0, total_loss = 45.969, pg_loss = 6.6945, baseline_loss = 44.794, entropy_loss = -5.5198, learner_queue_size = 32, train_seconds = 2.0729e+04, _tick = 7682, _time = 1.6546e+09)
[2022-06-07 16:09:20,566][root][INFO] - Step 37224960 @ 2045.6 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 37224960, mean_episode_return = None, mean_episode_step = 1763.2, total_loss = 141.34, pg_loss = 90.986, baseline_loss = 55.87, entropy_loss = -5.5192, learner_queue_size = 32, train_seconds = 2.0734e+04, _tick = 7685, _time = 1.6546e+09)
[2022-06-07 16:09:25,572][root][INFO] - Step 37232640 @ 1534.2 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 37232640, mean_episode_return = 11.64, mean_episode_step = 1712.8, total_loss = 87.772, pg_loss = 29.282, baseline_loss = 63.96, entropy_loss = -5.4696, learner_queue_size = 32, train_seconds = 2.0739e+04, _tick = 7687, _time = 1.6546e+09)
[2022-06-07 16:09:30,588][root][INFO] - Step 37242880 @ 2041.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 37242880, mean_episode_return = 38.816, mean_episode_step = 1845.1, total_loss = 26.328, pg_loss = -13.518, baseline_loss = 45.29, entropy_loss = -5.4448, learner_queue_size = 32, train_seconds = 2.0744e+04, _tick = 7690, _time = 1.6546e+09)
[2022-06-07 16:09:35,594][root][INFO] - Step 37253120 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 37253120, mean_episode_return = 39.913, mean_episode_step = 1525.2, total_loss = -77.084, pg_loss = -109.07, baseline_loss = 37.407, entropy_loss = -5.4195, learner_queue_size = 32, train_seconds = 2.0749e+04, _tick = 7693, _time = 1.6546e+09)
[2022-06-07 16:09:40,600][root][INFO] - Step 37260800 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 37260800, mean_episode_return = None, mean_episode_step = 1981.3, total_loss = 284.58, pg_loss = 193.27, baseline_loss = 96.721, entropy_loss = -5.4159, learner_queue_size = 32, train_seconds = 2.0754e+04, _tick = 7694, _time = 1.6546e+09)
[2022-06-07 16:09:45,606][root][INFO] - Step 37271040 @ 2045.7 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 37271040, mean_episode_return = 62.743, mean_episode_step = 1811.8, total_loss = 221.1, pg_loss = 156.02, baseline_loss = 70.45, entropy_loss = -5.3772, learner_queue_size = 32, train_seconds = 2.0759e+04, _tick = 7697, _time = 1.6546e+09)
[2022-06-07 16:09:50,610][root][INFO] - Step 37278720 @ 1534.7 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 37278720, mean_episode_return = 76.165, mean_episode_step = 1612.8, total_loss = 389.01, pg_loss = 265.63, baseline_loss = 128.7, entropy_loss = -5.3182, learner_queue_size = 32, train_seconds = 2.0764e+04, _tick = 7698, _time = 1.6546e+09)
[2022-06-07 16:09:55,614][root][INFO] - Step 37288960 @ 2046.4 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 37288960, mean_episode_return = -4.8803, mean_episode_step = 1585.4, total_loss = 287.93, pg_loss = 205.21, baseline_loss = 88.082, entropy_loss = -5.3595, learner_queue_size = 32, train_seconds = 2.0769e+04, _tick = 7700, _time = 1.6546e+09)
[2022-06-07 16:10:00,618][root][INFO] - Step 37296640 @ 1534.7 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 37296640, mean_episode_return = None, mean_episode_step = 2357.1, total_loss = 236.39, pg_loss = 165.79, baseline_loss = 75.954, entropy_loss = -5.3602, learner_queue_size = 32, train_seconds = 2.0774e+04, _tick = 7701, _time = 1.6546e+09)
[2022-06-07 16:10:05,622][root][INFO] - Step 37306880 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 37306880, mean_episode_return = 95.972, mean_episode_step = 1638.7, total_loss = 335.47, pg_loss = 218.77, baseline_loss = 122.01, entropy_loss = -5.3082, learner_queue_size = 32, train_seconds = 2.0779e+04, _tick = 7704, _time = 1.6546e+09)
[2022-06-07 16:10:10,628][root][INFO] - Step 37314560 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 37314560, mean_episode_return = None, mean_episode_step = 1633.7, total_loss = 219.98, pg_loss = 137.88, baseline_loss = 87.416, entropy_loss = -5.3119, learner_queue_size = 32, train_seconds = 2.0784e+04, _tick = 7706, _time = 1.6546e+09)
[2022-06-07 16:10:15,634][root][INFO] - Step 37324800 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 37324800, mean_episode_return = 58.606, mean_episode_step = 1673.3, total_loss = -100.03, pg_loss = -157.9, baseline_loss = 63.177, entropy_loss = -5.3049, learner_queue_size = 32, train_seconds = 2.0789e+04, _tick = 7708, _time = 1.6546e+09)
[2022-06-07 16:10:20,638][root][INFO] - Step 37332480 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 37332480, mean_episode_return = None, mean_episode_step = 1619.2, total_loss = -184.69, pg_loss = -182.19, baseline_loss = 2.7999, entropy_loss = -5.304, learner_queue_size = 32, train_seconds = 2.0794e+04, _tick = 7708, _time = 1.6546e+09)
[2022-06-07 16:10:25,640][root][INFO] - Step 37342720 @ 2047.0 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 37342720, mean_episode_return = 87.872, mean_episode_step = 1934.3, total_loss = 147.17, pg_loss = 59.531, baseline_loss = 92.938, entropy_loss = -5.297, learner_queue_size = 32, train_seconds = 2.0799e+04, _tick = 7710, _time = 1.6546e+09)
[2022-06-07 16:10:30,646][root][INFO] - Step 37350400 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37350400, mean_episode_return = None, mean_episode_step = 1536.8, total_loss = -102.77, pg_loss = -111.4, baseline_loss = 13.895, entropy_loss = -5.2675, learner_queue_size = 32, train_seconds = 2.0804e+04, _tick = 7711, _time = 1.6546e+09)
[2022-06-07 16:10:35,650][root][INFO] - Step 37360640 @ 2046.5 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 37360640, mean_episode_return = None, mean_episode_step = 1469.4, total_loss = -145.09, pg_loss = -144.69, baseline_loss = 4.8947, entropy_loss = -5.2951, learner_queue_size = 32, train_seconds = 2.0809e+04, _tick = 7711, _time = 1.6546e+09)
[2022-06-07 16:10:40,654][root][INFO] - Step 37368320 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 37368320, mean_episode_return = 82.475, mean_episode_step = 2127.6, total_loss = 312.19, pg_loss = 182.94, baseline_loss = 134.56, entropy_loss = -5.3197, learner_queue_size = 32, train_seconds = 2.0814e+04, _tick = 7714, _time = 1.6546e+09)
[2022-06-07 16:10:45,658][root][INFO] - Step 37378560 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 37378560, mean_episode_return = 29.181, mean_episode_step = 1581.7, total_loss = -38.317, pg_loss = -69.881, baseline_loss = 36.906, entropy_loss = -5.342, learner_queue_size = 32, train_seconds = 2.0819e+04, _tick = 7716, _time = 1.6546e+09)
[2022-06-07 16:10:50,662][root][INFO] - Step 37386240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37386240, mean_episode_return = 197.41, mean_episode_step = 1861.1, total_loss = 102.44, pg_loss = 43.911, baseline_loss = 63.89, entropy_loss = -5.3602, learner_queue_size = 32, train_seconds = 2.0824e+04, _tick = 7719, _time = 1.6546e+09)
[2022-06-07 16:10:55,666][root][INFO] - Step 37396480 @ 2046.4 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 37396480, mean_episode_return = 12.339, mean_episode_step = 2156.6, total_loss = 211.86, pg_loss = 114.14, baseline_loss = 103.14, entropy_loss = -5.4219, learner_queue_size = 32, train_seconds = 2.0829e+04, _tick = 7722, _time = 1.6546e+09)
[2022-06-07 16:11:00,670][root][INFO] - Step 37404160 @ 1534.7 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 37404160, mean_episode_return = None, mean_episode_step = 1791.0, total_loss = 12.164, pg_loss = -12.315, baseline_loss = 29.923, entropy_loss = -5.4439, learner_queue_size = 32, train_seconds = 2.0834e+04, _tick = 7723, _time = 1.6546e+09)
[2022-06-07 16:11:05,681][root][INFO] - Step 37411840 @ 1532.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37411840, mean_episode_return = 57.801, mean_episode_step = 1785.8, total_loss = 161.62, pg_loss = 96.307, baseline_loss = 70.725, entropy_loss = -5.4119, learner_queue_size = 32, train_seconds = 2.0839e+04, _tick = 7726, _time = 1.6546e+09)
[2022-06-07 16:11:10,686][root][INFO] - Step 37422080 @ 2045.9 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 37422080, mean_episode_return = None, mean_episode_step = 1458.9, total_loss = 469.73, pg_loss = 372.27, baseline_loss = 102.84, entropy_loss = -5.3774, learner_queue_size = 32, train_seconds = 2.0844e+04, _tick = 7728, _time = 1.6546e+09)
[2022-06-07 16:11:15,692][root][INFO] - Step 37429760 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37429760, mean_episode_return = None, mean_episode_step = 2365.0, total_loss = 130.58, pg_loss = 68.765, baseline_loss = 67.183, entropy_loss = -5.3719, learner_queue_size = 32, train_seconds = 2.0849e+04, _tick = 7729, _time = 1.6546e+09)
[2022-06-07 16:11:20,698][root][INFO] - Step 37440000 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37440000, mean_episode_return = None, mean_episode_step = 1683.3, total_loss = 302.85, pg_loss = 145.96, baseline_loss = 162.25, entropy_loss = -5.3719, learner_queue_size = 32, train_seconds = 2.0854e+04, _tick = 7732, _time = 1.6546e+09)
[2022-06-07 16:11:25,704][root][INFO] - Step 37450240 @ 2045.5 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 37450240, mean_episode_return = 51.863, mean_episode_step = 1867.1, total_loss = 221.09, pg_loss = 133.07, baseline_loss = 93.386, entropy_loss = -5.3716, learner_queue_size = 32, train_seconds = 2.0859e+04, _tick = 7734, _time = 1.6546e+09)
[2022-06-07 16:11:30,710][root][INFO] - Step 37457920 @ 1534.2 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 37457920, mean_episode_return = None, mean_episode_step = 1601.9, total_loss = 636.06, pg_loss = 461.47, baseline_loss = 179.95, entropy_loss = -5.3622, learner_queue_size = 32, train_seconds = 2.0864e+04, _tick = 7735, _time = 1.6546e+09)
[2022-06-07 16:11:35,716][root][INFO] - Step 37465600 @ 1534.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 37465600, mean_episode_return = None, mean_episode_step = 1857.4, total_loss = 123.03, pg_loss = 74.273, baseline_loss = 54.1, entropy_loss = -5.3389, learner_queue_size = 32, train_seconds = 2.0869e+04, _tick = 7737, _time = 1.6546e+09)
[2022-06-07 16:11:40,722][root][INFO] - Step 37475840 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37475840, mean_episode_return = 14.8, mean_episode_step = 1886.9, total_loss = 259.97, pg_loss = 133.34, baseline_loss = 132.0, entropy_loss = -5.3725, learner_queue_size = 32, train_seconds = 2.0874e+04, _tick = 7739, _time = 1.6546e+09)
[2022-06-07 16:11:45,726][root][INFO] - Step 37483520 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37483520, mean_episode_return = 50.781, mean_episode_step = 1670.5, total_loss = -64.908, pg_loss = -93.547, baseline_loss = 34.021, entropy_loss = -5.3821, learner_queue_size = 32, train_seconds = 2.0879e+04, _tick = 7740, _time = 1.6546e+09)
[2022-06-07 16:11:50,732][root][INFO] - Step 37493760 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 37493760, mean_episode_return = None, mean_episode_step = 2018.6, total_loss = 240.64, pg_loss = 165.56, baseline_loss = 80.456, entropy_loss = -5.3767, learner_queue_size = 32, train_seconds = 2.0884e+04, _tick = 7741, _time = 1.6546e+09)
[2022-06-07 16:11:55,738][root][INFO] - Step 37501440 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37501440, mean_episode_return = None, mean_episode_step = 1578.0, total_loss = -186.29, pg_loss = -182.75, baseline_loss = 1.8037, entropy_loss = -5.3395, learner_queue_size = 32, train_seconds = 2.0889e+04, _tick = 7741, _time = 1.6546e+09)
[2022-06-07 16:12:00,742][root][INFO] - Step 37511680 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 37511680, mean_episode_return = 66.149, mean_episode_step = 2308.2, total_loss = 328.08, pg_loss = 212.53, baseline_loss = 120.89, entropy_loss = -5.3445, learner_queue_size = 32, train_seconds = 2.0894e+04, _tick = 7745, _time = 1.6546e+09)
[2022-06-07 16:12:05,746][root][INFO] - Step 37521920 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 37521920, mean_episode_return = 38.661, mean_episode_step = 1926.7, total_loss = -228.16, pg_loss = -234.17, baseline_loss = 11.35, entropy_loss = -5.3435, learner_queue_size = 32, train_seconds = 2.0899e+04, _tick = 7747, _time = 1.6546e+09)
[2022-06-07 16:12:10,750][root][INFO] - Step 37529600 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 37529600, mean_episode_return = 58.818, mean_episode_step = 2070.5, total_loss = 322.69, pg_loss = 165.04, baseline_loss = 163.0, entropy_loss = -5.3497, learner_queue_size = 32, train_seconds = 2.0904e+04, _tick = 7749, _time = 1.6546e+09)
[2022-06-07 16:12:15,754][root][INFO] - Step 37539840 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37539840, mean_episode_return = 4.4994, mean_episode_step = 1787.3, total_loss = -24.517, pg_loss = -71.423, baseline_loss = 52.228, entropy_loss = -5.3213, learner_queue_size = 32, train_seconds = 2.0909e+04, _tick = 7753, _time = 1.6546e+09)
[2022-06-07 16:12:20,760][root][INFO] - Step 37547520 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37547520, mean_episode_return = None, mean_episode_step = 1865.7, total_loss = 210.07, pg_loss = 143.79, baseline_loss = 71.489, entropy_loss = -5.2064, learner_queue_size = 32, train_seconds = 2.0914e+04, _tick = 7755, _time = 1.6546e+09)
[2022-06-07 16:12:25,766][root][INFO] - Step 37557760 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 37557760, mean_episode_return = 113.97, mean_episode_step = 1795.3, total_loss = 143.74, pg_loss = 73.262, baseline_loss = 75.676, entropy_loss = -5.1955, learner_queue_size = 32, train_seconds = 2.0919e+04, _tick = 7757, _time = 1.6546e+09)
[2022-06-07 16:12:30,772][root][INFO] - Step 37565440 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 37565440, mean_episode_return = 56.07, mean_episode_step = 1786.2, total_loss = 461.88, pg_loss = 316.15, baseline_loss = 150.95, entropy_loss = -5.2122, learner_queue_size = 32, train_seconds = 2.0924e+04, _tick = 7759, _time = 1.6546e+09)
[2022-06-07 16:12:35,778][root][INFO] - Step 37575680 @ 2045.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 37575680, mean_episode_return = 14.629, mean_episode_step = 2430.2, total_loss = -137.55, pg_loss = -157.37, baseline_loss = 25.027, entropy_loss = -5.2118, learner_queue_size = 32, train_seconds = 2.0929e+04, _tick = 7762, _time = 1.6546e+09)
[2022-06-07 16:12:40,782][root][INFO] - Step 37583360 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37583360, mean_episode_return = None, mean_episode_step = 1997.6, total_loss = -181.7, pg_loss = -179.91, baseline_loss = 3.4598, entropy_loss = -5.2488, learner_queue_size = 32, train_seconds = 2.0934e+04, _tick = 7763, _time = 1.6546e+09)
[2022-06-07 16:12:45,788][root][INFO] - Step 37593600 @ 2045.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 37593600, mean_episode_return = 103.72, mean_episode_step = 2034.3, total_loss = 249.18, pg_loss = 101.92, baseline_loss = 152.51, entropy_loss = -5.2426, learner_queue_size = 32, train_seconds = 2.0939e+04, _tick = 7766, _time = 1.6546e+09)
[2022-06-07 16:12:50,794][root][INFO] - Step 37601280 @ 1534.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 37601280, mean_episode_return = None, mean_episode_step = 2151.1, total_loss = -57.674, pg_loss = -77.763, baseline_loss = 25.212, entropy_loss = -5.1235, learner_queue_size = 32, train_seconds = 2.0944e+04, _tick = 7767, _time = 1.6546e+09)
[2022-06-07 16:12:55,798][root][INFO] - Step 37611520 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 37611520, mean_episode_return = 99.208, mean_episode_step = 1695.6, total_loss = 306.33, pg_loss = 172.4, baseline_loss = 139.08, entropy_loss = -5.1504, learner_queue_size = 32, train_seconds = 2.0949e+04, _tick = 7770, _time = 1.6546e+09)
[2022-06-07 16:13:00,802][root][INFO] - Step 37619200 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 37619200, mean_episode_return = 19.59, mean_episode_step = 1765.0, total_loss = 663.98, pg_loss = 410.77, baseline_loss = 258.37, entropy_loss = -5.154, learner_queue_size = 32, train_seconds = 2.0954e+04, _tick = 7772, _time = 1.6546e+09)
[2022-06-07 16:13:05,806][root][INFO] - Step 37629440 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37629440, mean_episode_return = None, mean_episode_step = 1690.8, total_loss = 191.55, pg_loss = 129.81, baseline_loss = 66.868, entropy_loss = -5.1264, learner_queue_size = 32, train_seconds = 2.0959e+04, _tick = 7774, _time = 1.6546e+09)
[2022-06-07 16:13:10,810][root][INFO] - Step 37637120 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37637120, mean_episode_return = None, mean_episode_step = 2310.2, total_loss = 75.687, pg_loss = 35.492, baseline_loss = 45.34, entropy_loss = -5.1453, learner_queue_size = 32, train_seconds = 2.0964e+04, _tick = 7775, _time = 1.6546e+09)
[2022-06-07 16:13:15,814][root][INFO] - Step 37647360 @ 2046.3 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 37647360, mean_episode_return = None, mean_episode_step = 1607.4, total_loss = -81.452, pg_loss = -96.542, baseline_loss = 20.307, entropy_loss = -5.216, learner_queue_size = 32, train_seconds = 2.0969e+04, _tick = 7776, _time = 1.6546e+09)
[2022-06-07 16:13:20,818][root][INFO] - Step 37655040 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 37655040, mean_episode_return = 31.012, mean_episode_step = 2073.6, total_loss = 225.16, pg_loss = 145.5, baseline_loss = 84.847, entropy_loss = -5.1856, learner_queue_size = 32, train_seconds = 2.0974e+04, _tick = 7778, _time = 1.6546e+09)
[2022-06-07 16:13:25,822][root][INFO] - Step 37665280 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 37665280, mean_episode_return = None, mean_episode_step = 2530.4, total_loss = 261.74, pg_loss = 172.98, baseline_loss = 94.037, entropy_loss = -5.2773, learner_queue_size = 32, train_seconds = 2.0979e+04, _tick = 7781, _time = 1.6546e+09)
[2022-06-07 16:13:30,826][root][INFO] - Step 37672960 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 37672960, mean_episode_return = 118.72, mean_episode_step = 1512.6, total_loss = 622.49, pg_loss = 447.73, baseline_loss = 180.0, entropy_loss = -5.2414, learner_queue_size = 32, train_seconds = 2.0984e+04, _tick = 7784, _time = 1.6546e+09)
[2022-06-07 16:13:35,830][root][INFO] - Step 37683200 @ 2046.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 37683200, mean_episode_return = None, mean_episode_step = 2004.0, total_loss = 138.64, pg_loss = 88.096, baseline_loss = 55.817, entropy_loss = -5.2717, learner_queue_size = 32, train_seconds = 2.0989e+04, _tick = 7786, _time = 1.6546e+09)
[2022-06-07 16:13:40,834][root][INFO] - Step 37690880 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 37690880, mean_episode_return = None, mean_episode_step = 2529.0, total_loss = 353.47, pg_loss = 183.52, baseline_loss = 175.21, entropy_loss = -5.2588, learner_queue_size = 32, train_seconds = 2.0994e+04, _tick = 7787, _time = 1.6546e+09)
[2022-06-07 16:13:45,838][root][INFO] - Step 37701120 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 37701120, mean_episode_return = None, mean_episode_step = 2287.9, total_loss = -50.962, pg_loss = -70.216, baseline_loss = 24.491, entropy_loss = -5.2374, learner_queue_size = 32, train_seconds = 2.0999e+04, _tick = 7790, _time = 1.6546e+09)
[2022-06-07 16:13:50,842][root][INFO] - Step 37708800 @ 1534.7 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 37708800, mean_episode_return = None, mean_episode_step = 2582.1, total_loss = 427.66, pg_loss = 274.92, baseline_loss = 157.96, entropy_loss = -5.2236, learner_queue_size = 32, train_seconds = 2.1004e+04, _tick = 7790, _time = 1.6546e+09)
[2022-06-07 16:13:55,846][root][INFO] - Step 37719040 @ 2046.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 37719040, mean_episode_return = 57.478, mean_episode_step = 1550.1, total_loss = 279.03, pg_loss = 155.86, baseline_loss = 128.31, entropy_loss = -5.1366, learner_queue_size = 32, train_seconds = 2.1009e+04, _tick = 7792, _time = 1.6546e+09)
[2022-06-07 16:14:00,850][root][INFO] - Step 37726720 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 37726720, mean_episode_return = None, mean_episode_step = 2220.9, total_loss = -27.678, pg_loss = -64.376, baseline_loss = 41.816, entropy_loss = -5.1179, learner_queue_size = 32, train_seconds = 2.1014e+04, _tick = 7793, _time = 1.6546e+09)
[2022-06-07 16:14:05,854][root][INFO] - Step 37736960 @ 2046.4 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 37736960, mean_episode_return = 25.888, mean_episode_step = 2033.8, total_loss = -25.854, pg_loss = -107.41, baseline_loss = 86.666, entropy_loss = -5.1137, learner_queue_size = 32, train_seconds = 2.1019e+04, _tick = 7796, _time = 1.6546e+09)
[2022-06-07 16:14:10,856][root][INFO] - Step 37744640 @ 1535.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 37744640, mean_episode_return = 41.752, mean_episode_step = 2352.0, total_loss = -85.147, pg_loss = -113.54, baseline_loss = 33.489, entropy_loss = -5.0911, learner_queue_size = 32, train_seconds = 2.1024e+04, _tick = 7799, _time = 1.6546e+09)
[2022-06-07 16:14:15,862][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 16:14:16,065][root][INFO] - Step 37754880 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 37754880, mean_episode_return = None, mean_episode_step = 2016.9, total_loss = 146.44, pg_loss = 88.263, baseline_loss = 63.252, entropy_loss = -5.0765, learner_queue_size = 32, train_seconds = 2.1029e+04, _tick = 7802, _time = 1.6546e+09)
[2022-06-07 16:14:21,070][root][INFO] - Step 37762560 @ 1474.6 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 37762560, mean_episode_return = 28.371, mean_episode_step = 1751.4, total_loss = 206.49, pg_loss = 107.04, baseline_loss = 104.56, entropy_loss = -5.1026, learner_queue_size = 32, train_seconds = 2.1034e+04, _tick = 7804, _time = 1.6546e+09)
[2022-06-07 16:14:26,074][root][INFO] - Step 37772800 @ 2046.4 SPS. Inference batcher size: 51. Learner queue size: 32. Other stats: (step = 37772800, mean_episode_return = None, mean_episode_step = 2600.4, total_loss = -230.46, pg_loss = -227.02, baseline_loss = 1.6571, entropy_loss = -5.1041, learner_queue_size = 32, train_seconds = 2.1039e+04, _tick = 7806, _time = 1.6546e+09)
[2022-06-07 16:14:31,080][root][INFO] - Step 37780480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37780480, mean_episode_return = 4.5497, mean_episode_step = 1393.9, total_loss = 195.31, pg_loss = 113.51, baseline_loss = 86.895, entropy_loss = -5.096, learner_queue_size = 32, train_seconds = 2.1044e+04, _tick = 7808, _time = 1.6546e+09)
[2022-06-07 16:14:36,086][root][INFO] - Step 37790720 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37790720, mean_episode_return = 52.05, mean_episode_step = 1864.1, total_loss = -235.01, pg_loss = -254.09, baseline_loss = 24.163, entropy_loss = -5.0831, learner_queue_size = 32, train_seconds = 2.1049e+04, _tick = 7810, _time = 1.6546e+09)
[2022-06-07 16:14:41,092][root][INFO] - Step 37798400 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 37798400, mean_episode_return = 76.148, mean_episode_step = 1440.5, total_loss = -151.2, pg_loss = -166.66, baseline_loss = 20.601, entropy_loss = -5.1432, learner_queue_size = 32, train_seconds = 2.1054e+04, _tick = 7811, _time = 1.6546e+09)
[2022-06-07 16:14:46,098][root][INFO] - Step 37808640 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 37808640, mean_episode_return = None, mean_episode_step = 2196.2, total_loss = 165.51, pg_loss = 105.66, baseline_loss = 65.047, entropy_loss = -5.2062, learner_queue_size = 32, train_seconds = 2.1059e+04, _tick = 7811, _time = 1.6546e+09)
[2022-06-07 16:14:51,102][root][INFO] - Step 37816320 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 37816320, mean_episode_return = 13.226, mean_episode_step = 2106.7, total_loss = 61.126, pg_loss = -9.3555, baseline_loss = 75.643, entropy_loss = -5.1618, learner_queue_size = 32, train_seconds = 2.1064e+04, _tick = 7812, _time = 1.6546e+09)
[2022-06-07 16:14:56,106][root][INFO] - Step 37826560 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37826560, mean_episode_return = None, mean_episode_step = 2357.9, total_loss = -169.17, pg_loss = -167.28, baseline_loss = 3.2632, entropy_loss = -5.1563, learner_queue_size = 32, train_seconds = 2.1069e+04, _tick = 7814, _time = 1.6546e+09)
[2022-06-07 16:15:01,110][root][INFO] - Step 37834240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37834240, mean_episode_return = None, mean_episode_step = 2119.3, total_loss = 290.67, pg_loss = 187.26, baseline_loss = 108.59, entropy_loss = -5.1718, learner_queue_size = 32, train_seconds = 2.1074e+04, _tick = 7815, _time = 1.6546e+09)
[2022-06-07 16:15:06,116][root][INFO] - Step 37844480 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 37844480, mean_episode_return = 12.21, mean_episode_step = 2201.9, total_loss = -98.444, pg_loss = -110.19, baseline_loss = 16.963, entropy_loss = -5.2129, learner_queue_size = 32, train_seconds = 2.1079e+04, _tick = 7819, _time = 1.6546e+09)
[2022-06-07 16:15:11,122][root][INFO] - Step 37852160 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 37852160, mean_episode_return = 56.182, mean_episode_step = 2383.4, total_loss = -196.4, pg_loss = -197.28, baseline_loss = 6.0881, entropy_loss = -5.2103, learner_queue_size = 32, train_seconds = 2.1084e+04, _tick = 7822, _time = 1.6546e+09)
[2022-06-07 16:15:16,126][root][INFO] - Step 37862400 @ 2046.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 37862400, mean_episode_return = 27.9, mean_episode_step = 1849.3, total_loss = 21.684, pg_loss = -8.8959, baseline_loss = 35.814, entropy_loss = -5.2349, learner_queue_size = 32, train_seconds = 2.1089e+04, _tick = 7824, _time = 1.6546e+09)
[2022-06-07 16:15:21,132][root][INFO] - Step 37870080 @ 1534.1 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 37870080, mean_episode_return = 28.683, mean_episode_step = 2269.1, total_loss = 150.62, pg_loss = 46.598, baseline_loss = 109.33, entropy_loss = -5.3136, learner_queue_size = 32, train_seconds = 2.1094e+04, _tick = 7827, _time = 1.6546e+09)
[2022-06-07 16:15:26,134][root][INFO] - Step 37880320 @ 2047.2 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 37880320, mean_episode_return = None, mean_episode_step = 1828.5, total_loss = -164.8, pg_loss = -165.61, baseline_loss = 6.1076, entropy_loss = -5.3015, learner_queue_size = 32, train_seconds = 2.1099e+04, _tick = 7830, _time = 1.6546e+09)
[2022-06-07 16:15:31,140][root][INFO] - Step 37890560 @ 2045.4 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 37890560, mean_episode_return = 140.23, mean_episode_step = 2723.8, total_loss = -91.632, pg_loss = -116.43, baseline_loss = 30.058, entropy_loss = -5.2643, learner_queue_size = 32, train_seconds = 2.1104e+04, _tick = 7833, _time = 1.6546e+09)
[2022-06-07 16:15:36,146][root][INFO] - Step 37898240 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 37898240, mean_episode_return = None, mean_episode_step = 2095.1, total_loss = 60.822, pg_loss = 26.922, baseline_loss = 39.159, entropy_loss = -5.2594, learner_queue_size = 32, train_seconds = 2.1109e+04, _tick = 7835, _time = 1.6546e+09)
[2022-06-07 16:15:41,152][root][INFO] - Step 37908480 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37908480, mean_episode_return = 135.37, mean_episode_step = 1941.9, total_loss = -53.323, pg_loss = -114.16, baseline_loss = 66.135, entropy_loss = -5.3027, learner_queue_size = 32, train_seconds = 2.1114e+04, _tick = 7837, _time = 1.6546e+09)
[2022-06-07 16:15:46,158][root][INFO] - Step 37916160 @ 1534.3 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 37916160, mean_episode_return = 72.591, mean_episode_step = 2362.1, total_loss = -11.961, pg_loss = -45.316, baseline_loss = 38.643, entropy_loss = -5.2879, learner_queue_size = 32, train_seconds = 2.1119e+04, _tick = 7840, _time = 1.6546e+09)
[2022-06-07 16:15:51,164][root][INFO] - Step 37923840 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 37923840, mean_episode_return = 62.911, mean_episode_step = 1936.4, total_loss = 120.48, pg_loss = 59.943, baseline_loss = 65.798, entropy_loss = -5.2572, learner_queue_size = 32, train_seconds = 2.1124e+04, _tick = 7842, _time = 1.6546e+09)
[2022-06-07 16:15:56,170][root][INFO] - Step 37934080 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 37934080, mean_episode_return = 73.019, mean_episode_step = 1939.6, total_loss = 51.581, pg_loss = 5.4176, baseline_loss = 51.317, entropy_loss = -5.1534, learner_queue_size = 32, train_seconds = 2.1129e+04, _tick = 7846, _time = 1.6546e+09)
[2022-06-07 16:16:01,174][root][INFO] - Step 37944320 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 37944320, mean_episode_return = None, mean_episode_step = 2125.2, total_loss = 224.0, pg_loss = 157.29, baseline_loss = 71.932, entropy_loss = -5.2263, learner_queue_size = 32, train_seconds = 2.1134e+04, _tick = 7848, _time = 1.6546e+09)
[2022-06-07 16:16:06,178][root][INFO] - Step 37952000 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 37952000, mean_episode_return = None, mean_episode_step = 1585.8, total_loss = 98.164, pg_loss = 51.855, baseline_loss = 51.543, entropy_loss = -5.2334, learner_queue_size = 32, train_seconds = 2.1139e+04, _tick = 7848, _time = 1.6546e+09)
[2022-06-07 16:16:11,182][root][INFO] - Step 37962240 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 37962240, mean_episode_return = 42.071, mean_episode_step = 2172.6, total_loss = 25.94, pg_loss = -17.916, baseline_loss = 49.06, entropy_loss = -5.2042, learner_queue_size = 32, train_seconds = 2.1144e+04, _tick = 7852, _time = 1.6546e+09)
[2022-06-07 16:16:16,194][root][INFO] - Step 37969920 @ 1532.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 37969920, mean_episode_return = None, mean_episode_step = 2195.5, total_loss = 240.86, pg_loss = 149.82, baseline_loss = 96.272, entropy_loss = -5.2361, learner_queue_size = 32, train_seconds = 2.1149e+04, _tick = 7853, _time = 1.6546e+09)
[2022-06-07 16:16:21,198][root][INFO] - Step 37980160 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 37980160, mean_episode_return = 61.624, mean_episode_step = 1990.1, total_loss = -150.0, pg_loss = -168.52, baseline_loss = 23.761, entropy_loss = -5.2392, learner_queue_size = 32, train_seconds = 2.1154e+04, _tick = 7856, _time = 1.6546e+09)
[2022-06-07 16:16:26,204][root][INFO] - Step 37987840 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 37987840, mean_episode_return = None, mean_episode_step = 1901.0, total_loss = 814.89, pg_loss = 585.21, baseline_loss = 234.86, entropy_loss = -5.1824, learner_queue_size = 32, train_seconds = 2.1159e+04, _tick = 7857, _time = 1.6546e+09)
[2022-06-07 16:16:31,210][root][INFO] - Step 37998080 @ 2045.6 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 37998080, mean_episode_return = None, mean_episode_step = 2190.4, total_loss = 181.66, pg_loss = 126.26, baseline_loss = 60.588, entropy_loss = -5.1956, learner_queue_size = 32, train_seconds = 2.1164e+04, _tick = 7857, _time = 1.6546e+09)
[2022-06-07 16:16:36,214][root][INFO] - Step 38005760 @ 1534.7 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 38005760, mean_episode_return = None, mean_episode_step = 2126.3, total_loss = 157.06, pg_loss = 113.38, baseline_loss = 48.875, entropy_loss = -5.2023, learner_queue_size = 32, train_seconds = 2.1169e+04, _tick = 7858, _time = 1.6546e+09)
[2022-06-07 16:16:41,218][root][INFO] - Step 38016000 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 38016000, mean_episode_return = None, mean_episode_step = 1480.4, total_loss = 130.56, pg_loss = 83.8, baseline_loss = 51.939, entropy_loss = -5.1778, learner_queue_size = 32, train_seconds = 2.1174e+04, _tick = 7859, _time = 1.6546e+09)
[2022-06-07 16:16:46,228][root][INFO] - Step 38023680 @ 1533.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 38023680, mean_episode_return = None, mean_episode_step = 2400.2, total_loss = -84.574, pg_loss = -102.56, baseline_loss = 23.183, entropy_loss = -5.1932, learner_queue_size = 32, train_seconds = 2.1179e+04, _tick = 7860, _time = 1.6546e+09)
[2022-06-07 16:16:51,234][root][INFO] - Step 38033920 @ 2045.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 38033920, mean_episode_return = None, mean_episode_step = 2177.9, total_loss = 9.448, pg_loss = -14.671, baseline_loss = 29.276, entropy_loss = -5.1568, learner_queue_size = 32, train_seconds = 2.1184e+04, _tick = 7860, _time = 1.6546e+09)
[2022-06-07 16:16:56,238][root][INFO] - Step 38041600 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 38041600, mean_episode_return = 13.675, mean_episode_step = 2146.0, total_loss = 229.24, pg_loss = 107.5, baseline_loss = 126.91, entropy_loss = -5.1722, learner_queue_size = 32, train_seconds = 2.1189e+04, _tick = 7862, _time = 1.6546e+09)
[2022-06-07 16:17:01,244][root][INFO] - Step 38051840 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 38051840, mean_episode_return = 55.253, mean_episode_step = 2218.6, total_loss = -18.475, pg_loss = -45.446, baseline_loss = 32.104, entropy_loss = -5.1326, learner_queue_size = 32, train_seconds = 2.1194e+04, _tick = 7863, _time = 1.6546e+09)
[2022-06-07 16:17:06,250][root][INFO] - Step 38062080 @ 2045.5 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 38062080, mean_episode_return = None, mean_episode_step = 1686.6, total_loss = 185.5, pg_loss = 121.86, baseline_loss = 68.74, entropy_loss = -5.1002, learner_queue_size = 32, train_seconds = 2.1199e+04, _tick = 7863, _time = 1.6546e+09)
[2022-06-07 16:17:11,256][root][INFO] - Step 38069760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38069760, mean_episode_return = None, mean_episode_step = 1893.1, total_loss = -166.7, pg_loss = -164.08, baseline_loss = 2.4998, entropy_loss = -5.1145, learner_queue_size = 32, train_seconds = 2.1204e+04, _tick = 7864, _time = 1.6546e+09)
[2022-06-07 16:17:16,262][root][INFO] - Step 38080000 @ 2045.6 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 38080000, mean_episode_return = 95.771, mean_episode_step = 2304.7, total_loss = -184.13, pg_loss = -190.73, baseline_loss = 11.721, entropy_loss = -5.1242, learner_queue_size = 32, train_seconds = 2.1209e+04, _tick = 7866, _time = 1.6546e+09)
[2022-06-07 16:17:21,266][root][INFO] - Step 38087680 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 38087680, mean_episode_return = None, mean_episode_step = 2004.5, total_loss = 209.38, pg_loss = 140.59, baseline_loss = 73.947, entropy_loss = -5.1635, learner_queue_size = 32, train_seconds = 2.1214e+04, _tick = 7867, _time = 1.6546e+09)
[2022-06-07 16:17:26,270][root][INFO] - Step 38097920 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38097920, mean_episode_return = None, mean_episode_step = 2851.4, total_loss = 204.61, pg_loss = 119.64, baseline_loss = 90.154, entropy_loss = -5.1826, learner_queue_size = 32, train_seconds = 2.1219e+04, _tick = 7868, _time = 1.6546e+09)
[2022-06-07 16:17:31,274][root][INFO] - Step 38105600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38105600, mean_episode_return = None, mean_episode_step = 2253.3, total_loss = 222.09, pg_loss = 160.0, baseline_loss = 67.298, entropy_loss = -5.2062, learner_queue_size = 32, train_seconds = 2.1224e+04, _tick = 7869, _time = 1.6546e+09)
[2022-06-07 16:17:36,280][root][INFO] - Step 38113280 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38113280, mean_episode_return = 12.92, mean_episode_step = 2036.2, total_loss = 177.4, pg_loss = 108.21, baseline_loss = 74.446, entropy_loss = -5.2486, learner_queue_size = 32, train_seconds = 2.1229e+04, _tick = 7871, _time = 1.6546e+09)
[2022-06-07 16:17:41,286][root][INFO] - Step 38123520 @ 2045.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 38123520, mean_episode_return = 44.194, mean_episode_step = 1955.3, total_loss = -111.5, pg_loss = -151.95, baseline_loss = 45.736, entropy_loss = -5.2894, learner_queue_size = 32, train_seconds = 2.1234e+04, _tick = 7874, _time = 1.6546e+09)
[2022-06-07 16:17:46,290][root][INFO] - Step 38133760 @ 2046.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 38133760, mean_episode_return = 46.706, mean_episode_step = 2097.0, total_loss = -78.976, pg_loss = -105.25, baseline_loss = 31.581, entropy_loss = -5.3078, learner_queue_size = 32, train_seconds = 2.1239e+04, _tick = 7877, _time = 1.6546e+09)
[2022-06-07 16:17:51,294][root][INFO] - Step 38141440 @ 1534.8 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 38141440, mean_episode_return = 32.081, mean_episode_step = 2165.0, total_loss = -127.48, pg_loss = -148.77, baseline_loss = 26.561, entropy_loss = -5.2703, learner_queue_size = 32, train_seconds = 2.1244e+04, _tick = 7880, _time = 1.6546e+09)
[2022-06-07 16:17:56,297][root][INFO] - Step 38149120 @ 1534.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38149120, mean_episode_return = 63.13, mean_episode_step = 2086.7, total_loss = -186.55, pg_loss = -207.14, baseline_loss = 25.891, entropy_loss = -5.3016, learner_queue_size = 32, train_seconds = 2.1249e+04, _tick = 7881, _time = 1.6546e+09)
[2022-06-07 16:18:01,303][root][INFO] - Step 38159360 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 38159360, mean_episode_return = None, mean_episode_step = 2074.4, total_loss = -5.2323, pg_loss = -30.432, baseline_loss = 30.473, entropy_loss = -5.2737, learner_queue_size = 32, train_seconds = 2.1254e+04, _tick = 7883, _time = 1.6546e+09)
[2022-06-07 16:18:06,306][root][INFO] - Step 38167040 @ 1535.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38167040, mean_episode_return = 63.859, mean_episode_step = 2268.3, total_loss = 119.89, pg_loss = 53.677, baseline_loss = 71.52, entropy_loss = -5.3106, learner_queue_size = 32, train_seconds = 2.1259e+04, _tick = 7884, _time = 1.6546e+09)
[2022-06-07 16:18:11,312][root][INFO] - Step 38177280 @ 2045.6 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 38177280, mean_episode_return = 62.782, mean_episode_step = 2085.6, total_loss = 683.7, pg_loss = 430.92, baseline_loss = 258.05, entropy_loss = -5.2743, learner_queue_size = 32, train_seconds = 2.1264e+04, _tick = 7887, _time = 1.6546e+09)
[2022-06-07 16:18:16,318][root][INFO] - Step 38184960 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38184960, mean_episode_return = 76.268, mean_episode_step = 1923.8, total_loss = 232.22, pg_loss = 115.94, baseline_loss = 121.55, entropy_loss = -5.2657, learner_queue_size = 32, train_seconds = 2.1269e+04, _tick = 7890, _time = 1.6546e+09)
[2022-06-07 16:18:21,324][root][INFO] - Step 38195200 @ 2045.5 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 38195200, mean_episode_return = None, mean_episode_step = 2041.8, total_loss = 380.78, pg_loss = 269.43, baseline_loss = 116.59, entropy_loss = -5.2456, learner_queue_size = 32, train_seconds = 2.1274e+04, _tick = 7893, _time = 1.6546e+09)
[2022-06-07 16:18:26,330][root][INFO] - Step 38202880 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38202880, mean_episode_return = None, mean_episode_step = 2065.9, total_loss = -76.429, pg_loss = -93.886, baseline_loss = 22.739, entropy_loss = -5.2828, learner_queue_size = 32, train_seconds = 2.1279e+04, _tick = 7895, _time = 1.6546e+09)
[2022-06-07 16:18:31,334][root][INFO] - Step 38213120 @ 2046.5 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 38213120, mean_episode_return = None, mean_episode_step = 1843.9, total_loss = 194.31, pg_loss = 123.98, baseline_loss = 75.59, entropy_loss = -5.262, learner_queue_size = 32, train_seconds = 2.1284e+04, _tick = 7898, _time = 1.6546e+09)
[2022-06-07 16:18:36,340][root][INFO] - Step 38220800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38220800, mean_episode_return = 41.474, mean_episode_step = 1905.1, total_loss = -149.39, pg_loss = -158.55, baseline_loss = 14.458, entropy_loss = -5.2932, learner_queue_size = 32, train_seconds = 2.1289e+04, _tick = 7901, _time = 1.6546e+09)
[2022-06-07 16:18:41,346][root][INFO] - Step 38231040 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38231040, mean_episode_return = None, mean_episode_step = 2283.7, total_loss = -125.29, pg_loss = -126.08, baseline_loss = 6.1364, entropy_loss = -5.3419, learner_queue_size = 32, train_seconds = 2.1294e+04, _tick = 7902, _time = 1.6546e+09)
[2022-06-07 16:18:46,350][root][INFO] - Step 38238720 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38238720, mean_episode_return = None, mean_episode_step = 2016.0, total_loss = 134.52, pg_loss = 68.788, baseline_loss = 71.053, entropy_loss = -5.3229, learner_queue_size = 32, train_seconds = 2.1299e+04, _tick = 7903, _time = 1.6546e+09)
[2022-06-07 16:18:51,354][root][INFO] - Step 38248960 @ 2046.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 38248960, mean_episode_return = 38.291, mean_episode_step = 2362.1, total_loss = -255.49, pg_loss = -262.43, baseline_loss = 12.323, entropy_loss = -5.3925, learner_queue_size = 32, train_seconds = 2.1304e+04, _tick = 7905, _time = 1.6546e+09)
[2022-06-07 16:18:56,362][root][INFO] - Step 38256640 @ 1533.6 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 38256640, mean_episode_return = 80.962, mean_episode_step = 1967.9, total_loss = -156.92, pg_loss = -169.83, baseline_loss = 18.307, entropy_loss = -5.3971, learner_queue_size = 32, train_seconds = 2.1309e+04, _tick = 7907, _time = 1.6546e+09)
[2022-06-07 16:19:01,368][root][INFO] - Step 38264320 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 38264320, mean_episode_return = 70.149, mean_episode_step = 2165.9, total_loss = 136.84, pg_loss = 63.364, baseline_loss = 78.881, entropy_loss = -5.4012, learner_queue_size = 32, train_seconds = 2.1314e+04, _tick = 7909, _time = 1.6546e+09)
[2022-06-07 16:19:06,370][root][INFO] - Step 38274560 @ 2047.1 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 38274560, mean_episode_return = 17.755, mean_episode_step = 1946.4, total_loss = 291.28, pg_loss = 211.54, baseline_loss = 85.112, entropy_loss = -5.3758, learner_queue_size = 32, train_seconds = 2.1319e+04, _tick = 7911, _time = 1.6546e+09)
[2022-06-07 16:19:11,376][root][INFO] - Step 38284800 @ 2045.6 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 38284800, mean_episode_return = 23.75, mean_episode_step = 2694.1, total_loss = -179.37, pg_loss = -183.32, baseline_loss = 9.2683, entropy_loss = -5.3227, learner_queue_size = 32, train_seconds = 2.1324e+04, _tick = 7913, _time = 1.6546e+09)
[2022-06-07 16:19:16,378][root][INFO] - Step 38292480 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38292480, mean_episode_return = None, mean_episode_step = 2136.1, total_loss = -88.086, pg_loss = -95.43, baseline_loss = 12.679, entropy_loss = -5.3351, learner_queue_size = 32, train_seconds = 2.1329e+04, _tick = 7914, _time = 1.6546e+09)
[2022-06-07 16:19:21,382][root][INFO] - Step 38300160 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 38300160, mean_episode_return = None, mean_episode_step = 2428.8, total_loss = 6.9081, pg_loss = -15.046, baseline_loss = 27.339, entropy_loss = -5.3851, learner_queue_size = 32, train_seconds = 2.1334e+04, _tick = 7915, _time = 1.6546e+09)
[2022-06-07 16:19:26,386][root][INFO] - Step 38310400 @ 2046.3 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 38310400, mean_episode_return = 44.591, mean_episode_step = 1922.8, total_loss = -195.52, pg_loss = -219.45, baseline_loss = 29.317, entropy_loss = -5.3874, learner_queue_size = 32, train_seconds = 2.1339e+04, _tick = 7918, _time = 1.6546e+09)
[2022-06-07 16:19:31,392][root][INFO] - Step 38318080 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 38318080, mean_episode_return = -4.3406, mean_episode_step = 2365.5, total_loss = 207.41, pg_loss = 136.7, baseline_loss = 76.115, entropy_loss = -5.4048, learner_queue_size = 32, train_seconds = 2.1344e+04, _tick = 7919, _time = 1.6546e+09)
[2022-06-07 16:19:36,398][root][INFO] - Step 38328320 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 38328320, mean_episode_return = 66.668, mean_episode_step = 1966.0, total_loss = 259.87, pg_loss = 90.772, baseline_loss = 174.51, entropy_loss = -5.4126, learner_queue_size = 32, train_seconds = 2.1349e+04, _tick = 7922, _time = 1.6546e+09)
[2022-06-07 16:19:41,402][root][INFO] - Step 38336000 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 38336000, mean_episode_return = None, mean_episode_step = 2288.0, total_loss = 172.18, pg_loss = 102.53, baseline_loss = 75.042, entropy_loss = -5.3921, learner_queue_size = 32, train_seconds = 2.1354e+04, _tick = 7923, _time = 1.6546e+09)
[2022-06-07 16:19:46,406][root][INFO] - Step 38346240 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38346240, mean_episode_return = 109.14, mean_episode_step = 1484.4, total_loss = 906.65, pg_loss = 623.26, baseline_loss = 288.77, entropy_loss = -5.3739, learner_queue_size = 32, train_seconds = 2.1359e+04, _tick = 7926, _time = 1.6546e+09)
[2022-06-07 16:19:51,412][root][INFO] - Step 38353920 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38353920, mean_episode_return = 36.567, mean_episode_step = 2100.3, total_loss = 96.215, pg_loss = 26.757, baseline_loss = 74.835, entropy_loss = -5.376, learner_queue_size = 32, train_seconds = 2.1364e+04, _tick = 7929, _time = 1.6546e+09)
[2022-06-07 16:19:56,435][root][INFO] - Step 38364160 @ 2038.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 38364160, mean_episode_return = 24.66, mean_episode_step = 1414.6, total_loss = 342.14, pg_loss = 220.25, baseline_loss = 127.27, entropy_loss = -5.3785, learner_queue_size = 32, train_seconds = 2.1369e+04, _tick = 7931, _time = 1.6546e+09)
[2022-06-07 16:20:01,438][root][INFO] - Step 38374400 @ 2046.9 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 38374400, mean_episode_return = None, mean_episode_step = 1595.1, total_loss = 174.94, pg_loss = 130.27, baseline_loss = 49.995, entropy_loss = -5.33, learner_queue_size = 32, train_seconds = 2.1374e+04, _tick = 7934, _time = 1.6546e+09)
[2022-06-07 16:20:06,444][root][INFO] - Step 38382080 @ 1534.2 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 38382080, mean_episode_return = 111.4, mean_episode_step = 1946.0, total_loss = 106.83, pg_loss = 53.924, baseline_loss = 58.258, entropy_loss = -5.3504, learner_queue_size = 32, train_seconds = 2.1379e+04, _tick = 7936, _time = 1.6546e+09)
[2022-06-07 16:20:11,450][root][INFO] - Step 38389760 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38389760, mean_episode_return = None, mean_episode_step = 2047.1, total_loss = 282.58, pg_loss = 205.13, baseline_loss = 82.757, entropy_loss = -5.3117, learner_queue_size = 32, train_seconds = 2.1384e+04, _tick = 7938, _time = 1.6546e+09)
[2022-06-07 16:20:16,454][root][INFO] - Step 38400000 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38400000, mean_episode_return = 6.2396, mean_episode_step = 2098.3, total_loss = -156.46, pg_loss = -160.75, baseline_loss = 9.5847, entropy_loss = -5.2872, learner_queue_size = 32, train_seconds = 2.1389e+04, _tick = 7940, _time = 1.6546e+09)
[2022-06-07 16:20:21,458][root][INFO] - Step 38410240 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 38410240, mean_episode_return = None, mean_episode_step = 1781.4, total_loss = 20.69, pg_loss = -11.174, baseline_loss = 37.114, entropy_loss = -5.2502, learner_queue_size = 32, train_seconds = 2.1394e+04, _tick = 7943, _time = 1.6546e+09)
[2022-06-07 16:20:26,462][root][INFO] - Step 38417920 @ 1534.8 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 38417920, mean_episode_return = 128.55, mean_episode_step = 2297.7, total_loss = 284.13, pg_loss = 203.9, baseline_loss = 85.446, entropy_loss = -5.2233, learner_queue_size = 32, train_seconds = 2.14e+04, _tick = 7945, _time = 1.6546e+09)
[2022-06-07 16:20:31,466][root][INFO] - Step 38428160 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 38428160, mean_episode_return = 252.88, mean_episode_step = 1628.8, total_loss = -135.24, pg_loss = -149.36, baseline_loss = 19.357, entropy_loss = -5.2354, learner_queue_size = 32, train_seconds = 2.1404e+04, _tick = 7948, _time = 1.6546e+09)
[2022-06-07 16:20:36,470][root][INFO] - Step 38435840 @ 1534.8 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 38435840, mean_episode_return = None, mean_episode_step = 1687.8, total_loss = -53.648, pg_loss = -65.09, baseline_loss = 16.652, entropy_loss = -5.2107, learner_queue_size = 32, train_seconds = 2.141e+04, _tick = 7948, _time = 1.6546e+09)
[2022-06-07 16:20:41,474][root][INFO] - Step 38443520 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 38443520, mean_episode_return = 50.945, mean_episode_step = 1843.4, total_loss = -223.58, pg_loss = -223.39, baseline_loss = 5.0609, entropy_loss = -5.2557, learner_queue_size = 32, train_seconds = 2.1414e+04, _tick = 7950, _time = 1.6546e+09)
[2022-06-07 16:20:46,480][root][INFO] - Step 38453760 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 38453760, mean_episode_return = 35.912, mean_episode_step = 1999.1, total_loss = -78.638, pg_loss = -106.42, baseline_loss = 33.077, entropy_loss = -5.2993, learner_queue_size = 32, train_seconds = 2.142e+04, _tick = 7952, _time = 1.6546e+09)
[2022-06-07 16:20:51,486][root][INFO] - Step 38461440 @ 1534.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 38461440, mean_episode_return = None, mean_episode_step = 2062.0, total_loss = 94.37, pg_loss = 37.751, baseline_loss = 61.852, entropy_loss = -5.2323, learner_queue_size = 32, train_seconds = 2.1424e+04, _tick = 7953, _time = 1.6546e+09)
[2022-06-07 16:20:56,488][root][INFO] - Step 38471680 @ 2047.2 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 38471680, mean_episode_return = 11.59, mean_episode_step = 1990.5, total_loss = -177.57, pg_loss = -205.0, baseline_loss = 32.615, entropy_loss = -5.1846, learner_queue_size = 32, train_seconds = 2.143e+04, _tick = 7956, _time = 1.6546e+09)
[2022-06-07 16:21:01,494][root][INFO] - Step 38479360 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38479360, mean_episode_return = 102.92, mean_episode_step = 1905.1, total_loss = 76.821, pg_loss = -14.51, baseline_loss = 96.559, entropy_loss = -5.2277, learner_queue_size = 32, train_seconds = 2.1434e+04, _tick = 7957, _time = 1.6546e+09)
[2022-06-07 16:21:06,498][root][INFO] - Step 38489600 @ 2046.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 38489600, mean_episode_return = 133.1, mean_episode_step = 2421.3, total_loss = -204.78, pg_loss = -215.84, baseline_loss = 16.25, entropy_loss = -5.1943, learner_queue_size = 32, train_seconds = 2.144e+04, _tick = 7958, _time = 1.6546e+09)
[2022-06-07 16:21:11,502][root][INFO] - Step 38497280 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 38497280, mean_episode_return = None, mean_episode_step = 1523.9, total_loss = 445.98, pg_loss = 292.16, baseline_loss = 159.01, entropy_loss = -5.1885, learner_queue_size = 32, train_seconds = 2.1444e+04, _tick = 7958, _time = 1.6546e+09)
[2022-06-07 16:21:16,506][root][INFO] - Step 38507520 @ 2046.4 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 38507520, mean_episode_return = 106.43, mean_episode_step = 2606.8, total_loss = -7.206, pg_loss = -54.081, baseline_loss = 52.011, entropy_loss = -5.1353, learner_queue_size = 32, train_seconds = 2.145e+04, _tick = 7960, _time = 1.6546e+09)
[2022-06-07 16:21:21,510][root][INFO] - Step 38515200 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 38515200, mean_episode_return = 54.468, mean_episode_step = 1523.4, total_loss = -30.261, pg_loss = -89.108, baseline_loss = 63.937, entropy_loss = -5.0896, learner_queue_size = 32, train_seconds = 2.1454e+04, _tick = 7963, _time = 1.6546e+09)
[2022-06-07 16:21:26,514][root][INFO] - Step 38525440 @ 2046.3 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 38525440, mean_episode_return = -7.5106, mean_episode_step = 1846.5, total_loss = -51.791, pg_loss = -130.17, baseline_loss = 83.45, entropy_loss = -5.0683, learner_queue_size = 32, train_seconds = 2.146e+04, _tick = 7965, _time = 1.6546e+09)
[2022-06-07 16:21:31,518][root][INFO] - Step 38533120 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 38533120, mean_episode_return = None, mean_episode_step = 1798.8, total_loss = 496.06, pg_loss = 304.95, baseline_loss = 196.18, entropy_loss = -5.0649, learner_queue_size = 32, train_seconds = 2.1464e+04, _tick = 7966, _time = 1.6546e+09)
[2022-06-07 16:21:36,522][root][INFO] - Step 38543360 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38543360, mean_episode_return = 92.999, mean_episode_step = 2734.4, total_loss = -213.83, pg_loss = -219.8, baseline_loss = 10.969, entropy_loss = -5.0064, learner_queue_size = 32, train_seconds = 2.147e+04, _tick = 7968, _time = 1.6546e+09)
[2022-06-07 16:21:41,528][root][INFO] - Step 38551040 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 38551040, mean_episode_return = None, mean_episode_step = 1745.0, total_loss = 255.03, pg_loss = 162.35, baseline_loss = 97.703, entropy_loss = -5.0235, learner_queue_size = 32, train_seconds = 2.1474e+04, _tick = 7970, _time = 1.6546e+09)
[2022-06-07 16:21:46,534][root][INFO] - Step 38561280 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38561280, mean_episode_return = 60.63, mean_episode_step = 2351.2, total_loss = -113.08, pg_loss = -160.09, baseline_loss = 51.995, entropy_loss = -4.9883, learner_queue_size = 32, train_seconds = 2.148e+04, _tick = 7972, _time = 1.6546e+09)
[2022-06-07 16:21:51,540][root][INFO] - Step 38571520 @ 2045.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38571520, mean_episode_return = None, mean_episode_step = 1716.6, total_loss = 16.4, pg_loss = -19.957, baseline_loss = 41.323, entropy_loss = -4.9656, learner_queue_size = 32, train_seconds = 2.1484e+04, _tick = 7973, _time = 1.6546e+09)
[2022-06-07 16:21:56,546][root][INFO] - Step 38579200 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38579200, mean_episode_return = None, mean_episode_step = 2121.1, total_loss = 341.48, pg_loss = 236.97, baseline_loss = 109.47, entropy_loss = -4.9594, learner_queue_size = 32, train_seconds = 2.149e+04, _tick = 7974, _time = 1.6546e+09)
[2022-06-07 16:22:01,550][root][INFO] - Step 38589440 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 38589440, mean_episode_return = None, mean_episode_step = 2231.4, total_loss = -66.082, pg_loss = -68.377, baseline_loss = 7.2283, entropy_loss = -4.9331, learner_queue_size = 32, train_seconds = 2.1494e+04, _tick = 7976, _time = 1.6546e+09)
[2022-06-07 16:22:06,554][root][INFO] - Step 38597120 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38597120, mean_episode_return = 91.76, mean_episode_step = 2475.5, total_loss = -83.697, pg_loss = -154.01, baseline_loss = 75.24, entropy_loss = -4.9264, learner_queue_size = 32, train_seconds = 2.15e+04, _tick = 7978, _time = 1.6546e+09)
[2022-06-07 16:22:11,560][root][INFO] - Step 38604800 @ 1534.1 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 38604800, mean_episode_return = 136.79, mean_episode_step = 1369.1, total_loss = 324.56, pg_loss = 171.08, baseline_loss = 158.41, entropy_loss = -4.9217, learner_queue_size = 32, train_seconds = 2.1505e+04, _tick = 7980, _time = 1.6546e+09)
[2022-06-07 16:22:16,566][root][INFO] - Step 38615040 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 38615040, mean_episode_return = 7.4597, mean_episode_step = 1837.6, total_loss = -147.33, pg_loss = -163.42, baseline_loss = 20.935, entropy_loss = -4.8454, learner_queue_size = 32, train_seconds = 2.151e+04, _tick = 7983, _time = 1.6546e+09)
[2022-06-07 16:22:21,570][root][INFO] - Step 38625280 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38625280, mean_episode_return = None, mean_episode_step = 2039.9, total_loss = 141.36, pg_loss = 76.444, baseline_loss = 69.817, entropy_loss = -4.8974, learner_queue_size = 32, train_seconds = 2.1515e+04, _tick = 7986, _time = 1.6546e+09)
[2022-06-07 16:22:26,574][root][INFO] - Step 38632960 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38632960, mean_episode_return = None, mean_episode_step = 1500.4, total_loss = 157.3, pg_loss = 81.92, baseline_loss = 80.274, entropy_loss = -4.8912, learner_queue_size = 32, train_seconds = 2.152e+04, _tick = 7988, _time = 1.6546e+09)
[2022-06-07 16:22:31,580][root][INFO] - Step 38643200 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38643200, mean_episode_return = None, mean_episode_step = 1820.5, total_loss = -168.41, pg_loss = -166.17, baseline_loss = 2.5297, entropy_loss = -4.7669, learner_queue_size = 32, train_seconds = 2.1525e+04, _tick = 7989, _time = 1.6546e+09)
[2022-06-07 16:22:36,586][root][INFO] - Step 38650880 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 38650880, mean_episode_return = None, mean_episode_step = 2209.7, total_loss = -36.099, pg_loss = -76.598, baseline_loss = 45.25, entropy_loss = -4.7514, learner_queue_size = 32, train_seconds = 2.153e+04, _tick = 7990, _time = 1.6546e+09)
[2022-06-07 16:22:41,592][root][INFO] - Step 38661120 @ 2045.6 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 38661120, mean_episode_return = None, mean_episode_step = 1975.8, total_loss = -36.066, pg_loss = -59.914, baseline_loss = 28.517, entropy_loss = -4.6684, learner_queue_size = 32, train_seconds = 2.1535e+04, _tick = 7991, _time = 1.6546e+09)
[2022-06-07 16:22:46,598][root][INFO] - Step 38668800 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 38668800, mean_episode_return = 29.949, mean_episode_step = 2027.9, total_loss = 145.95, pg_loss = 45.023, baseline_loss = 105.6, entropy_loss = -4.6693, learner_queue_size = 32, train_seconds = 2.154e+04, _tick = 7993, _time = 1.6546e+09)
[2022-06-07 16:22:51,602][root][INFO] - Step 38679040 @ 2046.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 38679040, mean_episode_return = 36.675, mean_episode_step = 1970.7, total_loss = -129.39, pg_loss = -179.63, baseline_loss = 54.963, entropy_loss = -4.7267, learner_queue_size = 32, train_seconds = 2.1545e+04, _tick = 7995, _time = 1.6546e+09)
[2022-06-07 16:22:56,608][root][INFO] - Step 38686720 @ 1534.1 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 38686720, mean_episode_return = 43.243, mean_episode_step = 1829.6, total_loss = 72.042, pg_loss = -62.063, baseline_loss = 138.85, entropy_loss = -4.7473, learner_queue_size = 32, train_seconds = 2.155e+04, _tick = 7997, _time = 1.6546e+09)
[2022-06-07 16:23:01,614][root][INFO] - Step 38696960 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38696960, mean_episode_return = None, mean_episode_step = 1829.9, total_loss = 72.631, pg_loss = 38.367, baseline_loss = 39.016, entropy_loss = -4.752, learner_queue_size = 32, train_seconds = 2.1555e+04, _tick = 7998, _time = 1.6546e+09)
[2022-06-07 16:23:06,618][root][INFO] - Step 38704640 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 38704640, mean_episode_return = None, mean_episode_step = 2227.1, total_loss = -210.55, pg_loss = -208.42, baseline_loss = 2.659, entropy_loss = -4.7916, learner_queue_size = 32, train_seconds = 2.156e+04, _tick = 7998, _time = 1.6546e+09)
[2022-06-07 16:23:11,622][root][INFO] - Step 38714880 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 38714880, mean_episode_return = 40.88, mean_episode_step = 2172.9, total_loss = -205.49, pg_loss = -212.0, baseline_loss = 11.349, entropy_loss = -4.8406, learner_queue_size = 32, train_seconds = 2.1565e+04, _tick = 8001, _time = 1.6546e+09)
[2022-06-07 16:23:16,628][root][INFO] - Step 38722560 @ 1534.1 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 38722560, mean_episode_return = 75.71, mean_episode_step = 1790.1, total_loss = 8.5945, pg_loss = -65.429, baseline_loss = 78.903, entropy_loss = -4.88, learner_queue_size = 32, train_seconds = 2.157e+04, _tick = 8002, _time = 1.6546e+09)
[2022-06-07 16:23:21,634][root][INFO] - Step 38732800 @ 2045.5 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 38732800, mean_episode_return = 55.113, mean_episode_step = 2042.3, total_loss = 130.84, pg_loss = 35.325, baseline_loss = 100.4, entropy_loss = -4.8802, learner_queue_size = 32, train_seconds = 2.1575e+04, _tick = 8003, _time = 1.6546e+09)
[2022-06-07 16:23:26,640][root][INFO] - Step 38740480 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 38740480, mean_episode_return = 12.2, mean_episode_step = 1616.1, total_loss = 335.41, pg_loss = 139.41, baseline_loss = 200.82, entropy_loss = -4.8254, learner_queue_size = 32, train_seconds = 2.158e+04, _tick = 8004, _time = 1.6546e+09)
[2022-06-07 16:23:31,646][root][INFO] - Step 38750720 @ 2045.6 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 38750720, mean_episode_return = None, mean_episode_step = 2374.4, total_loss = 398.44, pg_loss = 257.94, baseline_loss = 145.32, entropy_loss = -4.8107, learner_queue_size = 32, train_seconds = 2.1585e+04, _tick = 8007, _time = 1.6546e+09)
[2022-06-07 16:23:36,652][root][INFO] - Step 38758400 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 38758400, mean_episode_return = None, mean_episode_step = 2010.2, total_loss = 369.48, pg_loss = 217.03, baseline_loss = 157.34, entropy_loss = -4.8846, learner_queue_size = 32, train_seconds = 2.159e+04, _tick = 8009, _time = 1.6546e+09)
[2022-06-07 16:23:41,666][root][INFO] - Step 38768640 @ 2042.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38768640, mean_episode_return = 30.05, mean_episode_step = 2395.2, total_loss = -92.118, pg_loss = -112.66, baseline_loss = 25.401, entropy_loss = -4.8565, learner_queue_size = 32, train_seconds = 2.1595e+04, _tick = 8012, _time = 1.6546e+09)
[2022-06-07 16:23:46,670][root][INFO] - Step 38778880 @ 2046.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 38778880, mean_episode_return = None, mean_episode_step = 2313.6, total_loss = -32.233, pg_loss = -36.391, baseline_loss = 9.022, entropy_loss = -4.864, learner_queue_size = 32, train_seconds = 2.16e+04, _tick = 8013, _time = 1.6546e+09)
[2022-06-07 16:23:51,674][root][INFO] - Step 38786560 @ 1534.8 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 38786560, mean_episode_return = -16.261, mean_episode_step = 1755.1, total_loss = -74.52, pg_loss = -116.94, baseline_loss = 47.283, entropy_loss = -4.8623, learner_queue_size = 32, train_seconds = 2.1605e+04, _tick = 8015, _time = 1.6546e+09)
[2022-06-07 16:23:56,680][root][INFO] - Step 38796800 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 38796800, mean_episode_return = None, mean_episode_step = 2244.2, total_loss = 15.757, pg_loss = -32.52, baseline_loss = 53.172, entropy_loss = -4.8951, learner_queue_size = 32, train_seconds = 2.161e+04, _tick = 8017, _time = 1.6546e+09)
[2022-06-07 16:24:01,686][root][INFO] - Step 38804480 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 38804480, mean_episode_return = None, mean_episode_step = 2026.7, total_loss = 93.298, pg_loss = 26.586, baseline_loss = 71.659, entropy_loss = -4.9481, learner_queue_size = 32, train_seconds = 2.1615e+04, _tick = 8017, _time = 1.6546e+09)
[2022-06-07 16:24:06,690][root][INFO] - Step 38814720 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 38814720, mean_episode_return = None, mean_episode_step = 1996.1, total_loss = -126.9, pg_loss = -125.75, baseline_loss = 3.8616, entropy_loss = -5.0053, learner_queue_size = 32, train_seconds = 2.162e+04, _tick = 8019, _time = 1.6546e+09)
[2022-06-07 16:24:11,694][root][INFO] - Step 38822400 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 38822400, mean_episode_return = None, mean_episode_step = 2647.3, total_loss = -2.8079, pg_loss = -8.58, baseline_loss = 10.842, entropy_loss = -5.0697, learner_queue_size = 32, train_seconds = 2.1625e+04, _tick = 8020, _time = 1.6546e+09)
[2022-06-07 16:24:16,734][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 16:24:16,963][root][INFO] - Step 38832640 @ 2031.6 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 38832640, mean_episode_return = 57.546, mean_episode_step = 2043.5, total_loss = 358.94, pg_loss = 216.54, baseline_loss = 147.51, entropy_loss = -5.1117, learner_queue_size = 32, train_seconds = 2.163e+04, _tick = 8023, _time = 1.6546e+09)
[2022-06-07 16:24:21,969][root][INFO] - Step 38840320 @ 1467.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38840320, mean_episode_return = 28.253, mean_episode_step = 2194.7, total_loss = 62.68, pg_loss = -1.7084, baseline_loss = 69.487, entropy_loss = -5.0985, learner_queue_size = 32, train_seconds = 2.1635e+04, _tick = 8025, _time = 1.6546e+09)
[2022-06-07 16:24:26,974][root][INFO] - Step 38850560 @ 2045.9 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 38850560, mean_episode_return = None, mean_episode_step = 2109.6, total_loss = -46.424, pg_loss = -49.574, baseline_loss = 8.2954, entropy_loss = -5.1456, learner_queue_size = 32, train_seconds = 2.164e+04, _tick = 8027, _time = 1.6546e+09)
[2022-06-07 16:24:31,978][root][INFO] - Step 38858240 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 38858240, mean_episode_return = None, mean_episode_step = 1920.2, total_loss = 11.2, pg_loss = -25.728, baseline_loss = 42.064, entropy_loss = -5.1352, learner_queue_size = 32, train_seconds = 2.1645e+04, _tick = 8029, _time = 1.6546e+09)
[2022-06-07 16:24:36,991][root][INFO] - Step 38868480 @ 2042.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 38868480, mean_episode_return = 102.38, mean_episode_step = 2443.1, total_loss = -155.57, pg_loss = -159.82, baseline_loss = 9.3359, entropy_loss = -5.0865, learner_queue_size = 32, train_seconds = 2.165e+04, _tick = 8031, _time = 1.6546e+09)
[2022-06-07 16:24:41,997][root][INFO] - Step 38876160 @ 1534.1 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 38876160, mean_episode_return = 57.001, mean_episode_step = 1949.6, total_loss = 240.25, pg_loss = 175.64, baseline_loss = 69.692, entropy_loss = -5.0865, learner_queue_size = 32, train_seconds = 2.1655e+04, _tick = 8034, _time = 1.6546e+09)
[2022-06-07 16:24:47,003][root][INFO] - Step 38886400 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 38886400, mean_episode_return = 44.313, mean_episode_step = 1768.5, total_loss = 160.0, pg_loss = 91.221, baseline_loss = 73.675, entropy_loss = -4.8983, learner_queue_size = 32, train_seconds = 2.166e+04, _tick = 8038, _time = 1.6546e+09)
[2022-06-07 16:24:52,010][root][INFO] - Step 38894080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38894080, mean_episode_return = None, mean_episode_step = 1626.6, total_loss = 315.22, pg_loss = 216.41, baseline_loss = 103.73, entropy_loss = -4.9232, learner_queue_size = 32, train_seconds = 2.1665e+04, _tick = 8039, _time = 1.6546e+09)
[2022-06-07 16:24:57,014][root][INFO] - Step 38904320 @ 2046.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 38904320, mean_episode_return = None, mean_episode_step = 2656.9, total_loss = -20.61, pg_loss = -61.095, baseline_loss = 45.468, entropy_loss = -4.9831, learner_queue_size = 32, train_seconds = 2.167e+04, _tick = 8041, _time = 1.6546e+09)
[2022-06-07 16:25:02,020][root][INFO] - Step 38912000 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 38912000, mean_episode_return = 57.623, mean_episode_step = 1797.2, total_loss = 83.944, pg_loss = -2.71, baseline_loss = 91.545, entropy_loss = -4.8912, learner_queue_size = 32, train_seconds = 2.1675e+04, _tick = 8044, _time = 1.6546e+09)
[2022-06-07 16:25:07,026][root][INFO] - Step 38922240 @ 2045.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38922240, mean_episode_return = 46.27, mean_episode_step = 1872.0, total_loss = 110.78, pg_loss = 23.823, baseline_loss = 91.893, entropy_loss = -4.932, learner_queue_size = 32, train_seconds = 2.168e+04, _tick = 8047, _time = 1.6546e+09)
[2022-06-07 16:25:12,028][root][INFO] - Step 38932480 @ 2047.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 38932480, mean_episode_return = None, mean_episode_step = 1847.8, total_loss = -161.91, pg_loss = -159.85, baseline_loss = 2.8804, entropy_loss = -4.9364, learner_queue_size = 32, train_seconds = 2.1685e+04, _tick = 8049, _time = 1.6546e+09)
[2022-06-07 16:25:17,032][root][INFO] - Step 38940160 @ 1534.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 38940160, mean_episode_return = 116.36, mean_episode_step = 1947.3, total_loss = 27.848, pg_loss = -16.73, baseline_loss = 49.47, entropy_loss = -4.892, learner_queue_size = 32, train_seconds = 2.169e+04, _tick = 8052, _time = 1.6546e+09)
[2022-06-07 16:25:22,038][root][INFO] - Step 38950400 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 38950400, mean_episode_return = 18.33, mean_episode_step = 2008.3, total_loss = -6.573, pg_loss = -31.234, baseline_loss = 29.479, entropy_loss = -4.8179, learner_queue_size = 32, train_seconds = 2.1695e+04, _tick = 8054, _time = 1.6546e+09)
[2022-06-07 16:25:27,044][root][INFO] - Step 38958080 @ 1534.2 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 38958080, mean_episode_return = 117.61, mean_episode_step = 2303.7, total_loss = 249.43, pg_loss = 164.4, baseline_loss = 89.729, entropy_loss = -4.6972, learner_queue_size = 32, train_seconds = 2.17e+04, _tick = 8056, _time = 1.6546e+09)
[2022-06-07 16:25:32,048][root][INFO] - Step 38965760 @ 1535.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 38965760, mean_episode_return = 57.359, mean_episode_step = 1762.9, total_loss = -187.69, pg_loss = -199.29, baseline_loss = 16.41, entropy_loss = -4.8118, learner_queue_size = 32, train_seconds = 2.1705e+04, _tick = 8058, _time = 1.6546e+09)
[2022-06-07 16:25:37,050][root][INFO] - Step 38976000 @ 2047.0 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 38976000, mean_episode_return = None, mean_episode_step = 2159.3, total_loss = 121.5, pg_loss = 58.247, baseline_loss = 68.058, entropy_loss = -4.8063, learner_queue_size = 32, train_seconds = 2.171e+04, _tick = 8060, _time = 1.6546e+09)
[2022-06-07 16:25:42,054][root][INFO] - Step 38986240 @ 2046.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 38986240, mean_episode_return = 77.94, mean_episode_step = 2157.5, total_loss = 299.14, pg_loss = 183.32, baseline_loss = 120.48, entropy_loss = -4.6644, learner_queue_size = 32, train_seconds = 2.1715e+04, _tick = 8062, _time = 1.6546e+09)
[2022-06-07 16:25:47,060][root][INFO] - Step 38993920 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 38993920, mean_episode_return = None, mean_episode_step = 1754.5, total_loss = 108.35, pg_loss = 68.555, baseline_loss = 44.483, entropy_loss = -4.6903, learner_queue_size = 32, train_seconds = 2.172e+04, _tick = 8063, _time = 1.6546e+09)
[2022-06-07 16:25:52,066][root][INFO] - Step 39001600 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39001600, mean_episode_return = None, mean_episode_step = 1821.7, total_loss = -183.16, pg_loss = -180.43, baseline_loss = 2.0323, entropy_loss = -4.7563, learner_queue_size = 32, train_seconds = 2.1725e+04, _tick = 8064, _time = 1.6546e+09)
[2022-06-07 16:25:57,072][root][INFO] - Step 39011840 @ 2045.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 39011840, mean_episode_return = None, mean_episode_step = 2114.1, total_loss = 300.79, pg_loss = 173.06, baseline_loss = 132.49, entropy_loss = -4.7604, learner_queue_size = 32, train_seconds = 2.173e+04, _tick = 8066, _time = 1.6546e+09)
[2022-06-07 16:26:02,078][root][INFO] - Step 39019520 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 39019520, mean_episode_return = 47.916, mean_episode_step = 2346.8, total_loss = -100.13, pg_loss = -159.32, baseline_loss = 63.996, entropy_loss = -4.8094, learner_queue_size = 32, train_seconds = 2.1735e+04, _tick = 8067, _time = 1.6546e+09)
[2022-06-07 16:26:07,082][root][INFO] - Step 39029760 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 39029760, mean_episode_return = None, mean_episode_step = 2574.2, total_loss = -40.928, pg_loss = -65.457, baseline_loss = 29.383, entropy_loss = -4.8541, learner_queue_size = 32, train_seconds = 2.174e+04, _tick = 8067, _time = 1.6546e+09)
[2022-06-07 16:26:12,086][root][INFO] - Step 39040000 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39040000, mean_episode_return = None, mean_episode_step = 2482.9, total_loss = -114.55, pg_loss = -117.53, baseline_loss = 7.8761, entropy_loss = -4.8917, learner_queue_size = 32, train_seconds = 2.1745e+04, _tick = 8069, _time = 1.6546e+09)
[2022-06-07 16:26:17,090][root][INFO] - Step 39047680 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39047680, mean_episode_return = None, mean_episode_step = 1973.6, total_loss = 154.12, pg_loss = 111.1, baseline_loss = 47.921, entropy_loss = -4.897, learner_queue_size = 32, train_seconds = 2.175e+04, _tick = 8071, _time = 1.6546e+09)
[2022-06-07 16:26:22,094][root][INFO] - Step 39057920 @ 2046.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 39057920, mean_episode_return = None, mean_episode_step = 2647.5, total_loss = -37.85, pg_loss = -75.729, baseline_loss = 42.706, entropy_loss = -4.8276, learner_queue_size = 32, train_seconds = 2.1755e+04, _tick = 8073, _time = 1.6546e+09)
[2022-06-07 16:26:27,098][root][INFO] - Step 39065600 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 39065600, mean_episode_return = 86.128, mean_episode_step = 2130.1, total_loss = -224.05, pg_loss = -251.48, baseline_loss = 32.268, entropy_loss = -4.8347, learner_queue_size = 32, train_seconds = 2.176e+04, _tick = 8074, _time = 1.6546e+09)
[2022-06-07 16:26:32,102][root][INFO] - Step 39075840 @ 2046.3 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 39075840, mean_episode_return = None, mean_episode_step = 2232.7, total_loss = -149.49, pg_loss = -156.47, baseline_loss = 11.818, entropy_loss = -4.837, learner_queue_size = 32, train_seconds = 2.1765e+04, _tick = 8077, _time = 1.6546e+09)
[2022-06-07 16:26:37,106][root][INFO] - Step 39083520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39083520, mean_episode_return = 91.976, mean_episode_step = 2329.5, total_loss = -104.54, pg_loss = -154.67, baseline_loss = 55.015, entropy_loss = -4.8855, learner_queue_size = 32, train_seconds = 2.177e+04, _tick = 8080, _time = 1.6546e+09)
[2022-06-07 16:26:42,112][root][INFO] - Step 39091200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39091200, mean_episode_return = None, mean_episode_step = 1706.1, total_loss = 326.43, pg_loss = 212.41, baseline_loss = 118.89, entropy_loss = -4.869, learner_queue_size = 32, train_seconds = 2.1775e+04, _tick = 8081, _time = 1.6546e+09)
[2022-06-07 16:26:47,118][root][INFO] - Step 39101440 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 39101440, mean_episode_return = 27.36, mean_episode_step = 2717.9, total_loss = 60.078, pg_loss = -11.162, baseline_loss = 76.115, entropy_loss = -4.8744, learner_queue_size = 32, train_seconds = 2.178e+04, _tick = 8083, _time = 1.6546e+09)
[2022-06-07 16:26:52,122][root][INFO] - Step 39111680 @ 2046.3 SPS. Inference batcher size: 54. Learner queue size: 32. Other stats: (step = 39111680, mean_episode_return = None, mean_episode_step = 1748.2, total_loss = 352.17, pg_loss = 223.88, baseline_loss = 133.21, entropy_loss = -4.9202, learner_queue_size = 32, train_seconds = 2.1785e+04, _tick = 8084, _time = 1.6546e+09)
[2022-06-07 16:26:57,126][root][INFO] - Step 39119360 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 39119360, mean_episode_return = None, mean_episode_step = 2137.9, total_loss = -95.644, pg_loss = -108.54, baseline_loss = 17.833, entropy_loss = -4.9332, learner_queue_size = 32, train_seconds = 2.179e+04, _tick = 8086, _time = 1.6546e+09)
[2022-06-07 16:27:02,138][root][INFO] - Step 39129600 @ 2042.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39129600, mean_episode_return = 99.329, mean_episode_step = 2051.1, total_loss = -162.77, pg_loss = -193.27, baseline_loss = 35.531, entropy_loss = -5.0287, learner_queue_size = 32, train_seconds = 2.1795e+04, _tick = 8089, _time = 1.6546e+09)
[2022-06-07 16:27:07,142][root][INFO] - Step 39137280 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39137280, mean_episode_return = 77.434, mean_episode_step = 2120.2, total_loss = 315.92, pg_loss = 132.69, baseline_loss = 188.32, entropy_loss = -5.0901, learner_queue_size = 32, train_seconds = 2.18e+04, _tick = 8092, _time = 1.6546e+09)
[2022-06-07 16:27:12,148][root][INFO] - Step 39147520 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39147520, mean_episode_return = None, mean_episode_step = 2212.1, total_loss = 154.63, pg_loss = 110.15, baseline_loss = 49.555, entropy_loss = -5.0716, learner_queue_size = 32, train_seconds = 2.1805e+04, _tick = 8093, _time = 1.6546e+09)
[2022-06-07 16:27:17,154][root][INFO] - Step 39155200 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39155200, mean_episode_return = 66.555, mean_episode_step = 2460.9, total_loss = 12.454, pg_loss = -49.031, baseline_loss = 66.677, entropy_loss = -5.1917, learner_queue_size = 32, train_seconds = 2.181e+04, _tick = 8095, _time = 1.6546e+09)
[2022-06-07 16:27:22,158][root][INFO] - Step 39162880 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 39162880, mean_episode_return = 30.7, mean_episode_step = 1640.5, total_loss = -101.65, pg_loss = -128.85, baseline_loss = 32.388, entropy_loss = -5.1898, learner_queue_size = 32, train_seconds = 2.1815e+04, _tick = 8097, _time = 1.6546e+09)
[2022-06-07 16:27:27,167][root][INFO] - Step 39173120 @ 2044.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 39173120, mean_episode_return = 118.09, mean_episode_step = 2418.3, total_loss = 223.56, pg_loss = 115.39, baseline_loss = 113.45, entropy_loss = -5.2848, learner_queue_size = 32, train_seconds = 2.182e+04, _tick = 8099, _time = 1.6546e+09)
[2022-06-07 16:27:32,170][root][INFO] - Step 39180800 @ 1535.2 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 39180800, mean_episode_return = None, mean_episode_step = 1983.6, total_loss = 46.507, pg_loss = 16.709, baseline_loss = 35.076, entropy_loss = -5.2782, learner_queue_size = 32, train_seconds = 2.1825e+04, _tick = 8099, _time = 1.6546e+09)
[2022-06-07 16:27:37,174][root][INFO] - Step 39191040 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 39191040, mean_episode_return = None, mean_episode_step = 2246.1, total_loss = 56.078, pg_loss = 8.0572, baseline_loss = 53.259, entropy_loss = -5.2389, learner_queue_size = 32, train_seconds = 2.183e+04, _tick = 8102, _time = 1.6546e+09)
[2022-06-07 16:27:42,178][root][INFO] - Step 39198720 @ 1534.7 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 39198720, mean_episode_return = 28.685, mean_episode_step = 1883.9, total_loss = 67.878, pg_loss = 17.78, baseline_loss = 55.318, entropy_loss = -5.2204, learner_queue_size = 32, train_seconds = 2.1835e+04, _tick = 8104, _time = 1.6546e+09)
[2022-06-07 16:27:47,182][root][INFO] - Step 39208960 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 39208960, mean_episode_return = 44.996, mean_episode_step = 2033.9, total_loss = -346.89, pg_loss = -400.33, baseline_loss = 58.647, entropy_loss = -5.2014, learner_queue_size = 32, train_seconds = 2.184e+04, _tick = 8108, _time = 1.6546e+09)
[2022-06-07 16:27:52,186][root][INFO] - Step 39216640 @ 1534.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 39216640, mean_episode_return = 66.92, mean_episode_step = 1958.4, total_loss = 226.93, pg_loss = 113.86, baseline_loss = 118.34, entropy_loss = -5.2627, learner_queue_size = 32, train_seconds = 2.1845e+04, _tick = 8110, _time = 1.6546e+09)
[2022-06-07 16:27:57,190][root][INFO] - Step 39226880 @ 2046.3 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 39226880, mean_episode_return = 40.097, mean_episode_step = 1400.2, total_loss = 289.81, pg_loss = 167.15, baseline_loss = 127.88, entropy_loss = -5.2151, learner_queue_size = 32, train_seconds = 2.185e+04, _tick = 8114, _time = 1.6546e+09)
[2022-06-07 16:28:02,194][root][INFO] - Step 39234560 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 39234560, mean_episode_return = 121.62, mean_episode_step = 1762.9, total_loss = 187.52, pg_loss = 112.48, baseline_loss = 80.269, entropy_loss = -5.2291, learner_queue_size = 32, train_seconds = 2.1855e+04, _tick = 8116, _time = 1.6546e+09)
[2022-06-07 16:28:07,198][root][INFO] - Step 39244800 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39244800, mean_episode_return = 111.71, mean_episode_step = 1757.5, total_loss = -116.28, pg_loss = -165.57, baseline_loss = 54.531, entropy_loss = -5.245, learner_queue_size = 32, train_seconds = 2.186e+04, _tick = 8119, _time = 1.6546e+09)
[2022-06-07 16:28:12,202][root][INFO] - Step 39252480 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39252480, mean_episode_return = None, mean_episode_step = 1540.4, total_loss = 173.09, pg_loss = 106.36, baseline_loss = 71.981, entropy_loss = -5.2546, learner_queue_size = 32, train_seconds = 2.1865e+04, _tick = 8121, _time = 1.6546e+09)
[2022-06-07 16:28:17,208][root][INFO] - Step 39262720 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39262720, mean_episode_return = None, mean_episode_step = 2357.2, total_loss = 252.99, pg_loss = 180.56, baseline_loss = 77.597, entropy_loss = -5.1649, learner_queue_size = 32, train_seconds = 2.187e+04, _tick = 8123, _time = 1.6546e+09)
[2022-06-07 16:28:22,214][root][INFO] - Step 39270400 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 39270400, mean_episode_return = 85.045, mean_episode_step = 1413.9, total_loss = 68.741, pg_loss = 2.8725, baseline_loss = 71.047, entropy_loss = -5.1787, learner_queue_size = 32, train_seconds = 2.1875e+04, _tick = 8126, _time = 1.6546e+09)
[2022-06-07 16:28:27,220][root][INFO] - Step 39280640 @ 2045.5 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 39280640, mean_episode_return = -4.0239, mean_episode_step = 2059.6, total_loss = 610.45, pg_loss = 423.58, baseline_loss = 191.94, entropy_loss = -5.0695, learner_queue_size = 32, train_seconds = 2.188e+04, _tick = 8129, _time = 1.6546e+09)
[2022-06-07 16:28:32,226][root][INFO] - Step 39288320 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 39288320, mean_episode_return = None, mean_episode_step = 2672.2, total_loss = 407.79, pg_loss = 266.66, baseline_loss = 146.22, entropy_loss = -5.0903, learner_queue_size = 32, train_seconds = 2.1885e+04, _tick = 8131, _time = 1.6546e+09)
[2022-06-07 16:28:37,230][root][INFO] - Step 39298560 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 39298560, mean_episode_return = None, mean_episode_step = 1489.8, total_loss = 708.83, pg_loss = 497.24, baseline_loss = 216.63, entropy_loss = -5.0391, learner_queue_size = 32, train_seconds = 2.189e+04, _tick = 8133, _time = 1.6546e+09)
[2022-06-07 16:28:42,236][root][INFO] - Step 39306240 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39306240, mean_episode_return = None, mean_episode_step = 1843.8, total_loss = 251.33, pg_loss = 130.68, baseline_loss = 125.68, entropy_loss = -5.0317, learner_queue_size = 32, train_seconds = 2.1895e+04, _tick = 8133, _time = 1.6546e+09)
[2022-06-07 16:28:47,242][root][INFO] - Step 39316480 @ 2045.6 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 39316480, mean_episode_return = None, mean_episode_step = 2076.5, total_loss = -56.102, pg_loss = -91.625, baseline_loss = 40.593, entropy_loss = -5.07, learner_queue_size = 32, train_seconds = 2.19e+04, _tick = 8135, _time = 1.6546e+09)
[2022-06-07 16:28:52,246][root][INFO] - Step 39324160 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 39324160, mean_episode_return = None, mean_episode_step = 1252.8, total_loss = 473.64, pg_loss = 326.76, baseline_loss = 151.97, entropy_loss = -5.0893, learner_queue_size = 32, train_seconds = 2.1905e+04, _tick = 8136, _time = 1.6546e+09)
[2022-06-07 16:28:57,252][root][INFO] - Step 39334400 @ 2045.6 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 39334400, mean_episode_return = None, mean_episode_step = 1503.2, total_loss = 414.98, pg_loss = 227.23, baseline_loss = 192.82, entropy_loss = -5.0759, learner_queue_size = 32, train_seconds = 2.191e+04, _tick = 8137, _time = 1.6546e+09)
[2022-06-07 16:29:02,258][root][INFO] - Step 39342080 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39342080, mean_episode_return = 86.914, mean_episode_step = 1776.9, total_loss = -158.59, pg_loss = -216.35, baseline_loss = 62.811, entropy_loss = -5.0514, learner_queue_size = 32, train_seconds = 2.1915e+04, _tick = 8140, _time = 1.6546e+09)
[2022-06-07 16:29:07,264][root][INFO] - Step 39352320 @ 2045.5 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39352320, mean_episode_return = 6.21, mean_episode_step = 1831.2, total_loss = -216.62, pg_loss = -250.72, baseline_loss = 39.226, entropy_loss = -5.1247, learner_queue_size = 32, train_seconds = 2.192e+04, _tick = 8144, _time = 1.6546e+09)
[2022-06-07 16:29:12,270][root][INFO] - Step 39360000 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 39360000, mean_episode_return = 36.065, mean_episode_step = 1498.3, total_loss = -36.059, pg_loss = -155.33, baseline_loss = 124.39, entropy_loss = -5.1265, learner_queue_size = 32, train_seconds = 2.1925e+04, _tick = 8146, _time = 1.6546e+09)
[2022-06-07 16:29:17,274][root][INFO] - Step 39370240 @ 2046.3 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 39370240, mean_episode_return = 72.93, mean_episode_step = 1624.1, total_loss = 58.441, pg_loss = -66.088, baseline_loss = 129.64, entropy_loss = -5.107, learner_queue_size = 32, train_seconds = 2.193e+04, _tick = 8148, _time = 1.6546e+09)
[2022-06-07 16:29:22,278][root][INFO] - Step 39377920 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39377920, mean_episode_return = 85.986, mean_episode_step = 1767.8, total_loss = -130.31, pg_loss = -200.96, baseline_loss = 75.708, entropy_loss = -5.0568, learner_queue_size = 32, train_seconds = 2.1935e+04, _tick = 8150, _time = 1.6546e+09)
[2022-06-07 16:29:27,284][root][INFO] - Step 39388160 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 39388160, mean_episode_return = 23.77, mean_episode_step = 1688.6, total_loss = 58.079, pg_loss = -25.587, baseline_loss = 88.688, entropy_loss = -5.022, learner_queue_size = 32, train_seconds = 2.194e+04, _tick = 8153, _time = 1.6546e+09)
[2022-06-07 16:29:32,290][root][INFO] - Step 39395840 @ 1534.2 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 39395840, mean_episode_return = None, mean_episode_step = 1932.4, total_loss = -10.77, pg_loss = -48.796, baseline_loss = 42.995, entropy_loss = -4.9688, learner_queue_size = 32, train_seconds = 2.1945e+04, _tick = 8154, _time = 1.6546e+09)
[2022-06-07 16:29:37,296][root][INFO] - Step 39406080 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 39406080, mean_episode_return = None, mean_episode_step = 1495.0, total_loss = 329.7, pg_loss = 213.64, baseline_loss = 121.01, entropy_loss = -4.948, learner_queue_size = 32, train_seconds = 2.195e+04, _tick = 8155, _time = 1.6546e+09)
[2022-06-07 16:29:42,302][root][INFO] - Step 39413760 @ 1534.2 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 39413760, mean_episode_return = 90.449, mean_episode_step = 1655.0, total_loss = 7.7918, pg_loss = -49.189, baseline_loss = 61.907, entropy_loss = -4.9265, learner_queue_size = 32, train_seconds = 2.1955e+04, _tick = 8157, _time = 1.6546e+09)
[2022-06-07 16:29:47,304][root][INFO] - Step 39424000 @ 2047.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 39424000, mean_episode_return = 29.8, mean_episode_step = 1849.3, total_loss = 67.898, pg_loss = -4.4874, baseline_loss = 77.326, entropy_loss = -4.9401, learner_queue_size = 32, train_seconds = 2.196e+04, _tick = 8160, _time = 1.6546e+09)
[2022-06-07 16:29:52,306][root][INFO] - Step 39431680 @ 1535.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 39431680, mean_episode_return = None, mean_episode_step = 2020.8, total_loss = 109.96, pg_loss = 58.853, baseline_loss = 56.018, entropy_loss = -4.9081, learner_queue_size = 32, train_seconds = 2.1965e+04, _tick = 8161, _time = 1.6546e+09)
[2022-06-07 16:29:57,319][root][INFO] - Step 39441920 @ 2042.8 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 39441920, mean_episode_return = None, mean_episode_step = 1904.7, total_loss = 25.894, pg_loss = -10.025, baseline_loss = 40.895, entropy_loss = -4.9754, learner_queue_size = 32, train_seconds = 2.197e+04, _tick = 8162, _time = 1.6546e+09)
[2022-06-07 16:30:02,322][root][INFO] - Step 39449600 @ 1535.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39449600, mean_episode_return = None, mean_episode_step = 2001.6, total_loss = -80.019, pg_loss = -119.47, baseline_loss = 44.507, entropy_loss = -5.06, learner_queue_size = 32, train_seconds = 2.1975e+04, _tick = 8162, _time = 1.6546e+09)
[2022-06-07 16:30:07,326][root][INFO] - Step 39459840 @ 2046.4 SPS. Inference batcher size: 58. Learner queue size: 32. Other stats: (step = 39459840, mean_episode_return = 58.471, mean_episode_step = 2043.4, total_loss = -65.262, pg_loss = -94.925, baseline_loss = 34.866, entropy_loss = -5.203, learner_queue_size = 32, train_seconds = 2.198e+04, _tick = 8164, _time = 1.6546e+09)
[2022-06-07 16:30:12,334][root][INFO] - Step 39467520 @ 1533.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39467520, mean_episode_return = 18.3, mean_episode_step = 1867.5, total_loss = 238.21, pg_loss = 135.0, baseline_loss = 108.44, entropy_loss = -5.2326, learner_queue_size = 32, train_seconds = 2.1985e+04, _tick = 8166, _time = 1.6546e+09)
[2022-06-07 16:30:17,340][root][INFO] - Step 39477760 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 39477760, mean_episode_return = None, mean_episode_step = 2087.9, total_loss = 43.334, pg_loss = 4.8505, baseline_loss = 43.695, entropy_loss = -5.2105, learner_queue_size = 32, train_seconds = 2.199e+04, _tick = 8166, _time = 1.6546e+09)
[2022-06-07 16:30:22,342][root][INFO] - Step 39485440 @ 1535.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39485440, mean_episode_return = None, mean_episode_step = 2067.2, total_loss = 26.641, pg_loss = -28.963, baseline_loss = 60.741, entropy_loss = -5.1369, learner_queue_size = 32, train_seconds = 2.1995e+04, _tick = 8167, _time = 1.6546e+09)
[2022-06-07 16:30:27,348][root][INFO] - Step 39493120 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 39493120, mean_episode_return = 105.73, mean_episode_step = 1842.2, total_loss = -69.846, pg_loss = -90.784, baseline_loss = 26.107, entropy_loss = -5.1688, learner_queue_size = 32, train_seconds = 2.2e+04, _tick = 8168, _time = 1.6546e+09)
[2022-06-07 16:30:32,354][root][INFO] - Step 39503360 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39503360, mean_episode_return = None, mean_episode_step = 2054.6, total_loss = 75.358, pg_loss = 45.363, baseline_loss = 35.197, entropy_loss = -5.2023, learner_queue_size = 32, train_seconds = 2.2005e+04, _tick = 8170, _time = 1.6546e+09)
[2022-06-07 16:30:37,366][root][INFO] - Step 39513600 @ 2043.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39513600, mean_episode_return = 20.72, mean_episode_step = 2736.1, total_loss = 19.454, pg_loss = -60.647, baseline_loss = 85.31, entropy_loss = -5.209, learner_queue_size = 32, train_seconds = 2.201e+04, _tick = 8172, _time = 1.6546e+09)
[2022-06-07 16:30:42,370][root][INFO] - Step 39521280 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 39521280, mean_episode_return = 23.006, mean_episode_step = 1760.3, total_loss = -303.58, pg_loss = -321.96, baseline_loss = 23.533, entropy_loss = -5.1513, learner_queue_size = 32, train_seconds = 2.2015e+04, _tick = 8175, _time = 1.6546e+09)
[2022-06-07 16:30:47,374][root][INFO] - Step 39531520 @ 2046.3 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 39531520, mean_episode_return = 25.87, mean_episode_step = 2138.9, total_loss = 169.85, pg_loss = 60.672, baseline_loss = 114.34, entropy_loss = -5.1522, learner_queue_size = 32, train_seconds = 2.202e+04, _tick = 8176, _time = 1.6546e+09)
[2022-06-07 16:30:52,380][root][INFO] - Step 39539200 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39539200, mean_episode_return = 104.22, mean_episode_step = 2129.9, total_loss = -224.05, pg_loss = -224.96, baseline_loss = 6.0091, entropy_loss = -5.104, learner_queue_size = 32, train_seconds = 2.2025e+04, _tick = 8177, _time = 1.6546e+09)
[2022-06-07 16:30:57,386][root][INFO] - Step 39549440 @ 2045.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 39549440, mean_episode_return = None, mean_episode_step = 2084.0, total_loss = -44.151, pg_loss = -68.584, baseline_loss = 29.546, entropy_loss = -5.1134, learner_queue_size = 32, train_seconds = 2.203e+04, _tick = 8178, _time = 1.6546e+09)
[2022-06-07 16:31:02,390][root][INFO] - Step 39557120 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 39557120, mean_episode_return = 45.919, mean_episode_step = 2071.8, total_loss = -167.99, pg_loss = -204.02, baseline_loss = 41.208, entropy_loss = -5.1705, learner_queue_size = 32, train_seconds = 2.2035e+04, _tick = 8180, _time = 1.6546e+09)
[2022-06-07 16:31:07,396][root][INFO] - Step 39567360 @ 2045.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (step = 39567360, mean_episode_return = 20.69, mean_episode_step = 1643.3, total_loss = -159.43, pg_loss = -167.87, baseline_loss = 13.649, entropy_loss = -5.2032, learner_queue_size = 32, train_seconds = 2.204e+04, _tick = 8183, _time = 1.6546e+09)
[2022-06-07 16:31:12,402][root][INFO] - Step 39575040 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39575040, mean_episode_return = None, mean_episode_step = 2064.1, total_loss = 530.75, pg_loss = 379.68, baseline_loss = 156.34, entropy_loss = -5.2668, learner_queue_size = 32, train_seconds = 2.2045e+04, _tick = 8184, _time = 1.6546e+09)
[2022-06-07 16:31:17,406][root][INFO] - Step 39585280 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 39585280, mean_episode_return = None, mean_episode_step = 2155.9, total_loss = -36.142, pg_loss = -61.874, baseline_loss = 30.914, entropy_loss = -5.1823, learner_queue_size = 32, train_seconds = 2.205e+04, _tick = 8186, _time = 1.6546e+09)
[2022-06-07 16:31:22,410][root][INFO] - Step 39592960 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 39592960, mean_episode_return = None, mean_episode_step = 1607.7, total_loss = 176.41, pg_loss = 122.47, baseline_loss = 59.118, entropy_loss = -5.1837, learner_queue_size = 32, train_seconds = 2.2055e+04, _tick = 8187, _time = 1.6546e+09)
[2022-06-07 16:31:27,414][root][INFO] - Step 39603200 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 39603200, mean_episode_return = 73.59, mean_episode_step = 2089.3, total_loss = 102.78, pg_loss = 39.892, baseline_loss = 68.116, entropy_loss = -5.2306, learner_queue_size = 32, train_seconds = 2.206e+04, _tick = 8190, _time = 1.6546e+09)
[2022-06-07 16:31:32,418][root][INFO] - Step 39610880 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 39610880, mean_episode_return = 80.297, mean_episode_step = 1962.8, total_loss = -113.36, pg_loss = -172.42, baseline_loss = 64.31, entropy_loss = -5.2523, learner_queue_size = 32, train_seconds = 2.2065e+04, _tick = 8191, _time = 1.6546e+09)
[2022-06-07 16:31:37,422][root][INFO] - Step 39621120 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39621120, mean_episode_return = 52.271, mean_episode_step = 2116.2, total_loss = 250.94, pg_loss = 163.36, baseline_loss = 92.84, entropy_loss = -5.261, learner_queue_size = 32, train_seconds = 2.207e+04, _tick = 8195, _time = 1.6546e+09)
[2022-06-07 16:31:42,426][root][INFO] - Step 39628800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39628800, mean_episode_return = None, mean_episode_step = 2313.6, total_loss = 617.32, pg_loss = 440.02, baseline_loss = 182.58, entropy_loss = -5.2761, learner_queue_size = 32, train_seconds = 2.2075e+04, _tick = 8196, _time = 1.6546e+09)
[2022-06-07 16:31:47,430][root][INFO] - Step 39639040 @ 2046.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 39639040, mean_episode_return = 36.217, mean_episode_step = 2127.3, total_loss = -30.065, pg_loss = -74.665, baseline_loss = 49.851, entropy_loss = -5.2501, learner_queue_size = 32, train_seconds = 2.208e+04, _tick = 8197, _time = 1.6546e+09)
[2022-06-07 16:31:52,438][root][INFO] - Step 39646720 @ 1533.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39646720, mean_episode_return = None, mean_episode_step = 2139.8, total_loss = 50.376, pg_loss = 6.0851, baseline_loss = 49.605, entropy_loss = -5.3136, learner_queue_size = 32, train_seconds = 2.2085e+04, _tick = 8198, _time = 1.6546e+09)
[2022-06-07 16:31:57,442][root][INFO] - Step 39656960 @ 2046.4 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 39656960, mean_episode_return = 52.431, mean_episode_step = 2135.4, total_loss = 76.781, pg_loss = 12.18, baseline_loss = 69.922, entropy_loss = -5.3218, learner_queue_size = 32, train_seconds = 2.209e+04, _tick = 8201, _time = 1.6546e+09)
[2022-06-07 16:32:02,446][root][INFO] - Step 39664640 @ 1534.8 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 39664640, mean_episode_return = 6.6555, mean_episode_step = 2208.9, total_loss = 103.46, pg_loss = 19.86, baseline_loss = 88.86, entropy_loss = -5.264, learner_queue_size = 32, train_seconds = 2.2095e+04, _tick = 8204, _time = 1.6546e+09)
[2022-06-07 16:32:07,450][root][INFO] - Step 39672320 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39672320, mean_episode_return = 47.787, mean_episode_step = 2218.0, total_loss = -36.779, pg_loss = -70.571, baseline_loss = 38.951, entropy_loss = -5.1601, learner_queue_size = 32, train_seconds = 2.21e+04, _tick = 8207, _time = 1.6546e+09)
[2022-06-07 16:32:12,454][root][INFO] - Step 39682560 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 39682560, mean_episode_return = 55.675, mean_episode_step = 2093.2, total_loss = 205.27, pg_loss = 131.11, baseline_loss = 79.309, entropy_loss = -5.1518, learner_queue_size = 32, train_seconds = 2.2105e+04, _tick = 8210, _time = 1.6546e+09)
[2022-06-07 16:32:17,458][root][INFO] - Step 39690240 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 39690240, mean_episode_return = 14.7, mean_episode_step = 1589.2, total_loss = 210.66, pg_loss = 151.58, baseline_loss = 64.321, entropy_loss = -5.2384, learner_queue_size = 32, train_seconds = 2.211e+04, _tick = 8211, _time = 1.6546e+09)
[2022-06-07 16:32:22,462][root][INFO] - Step 39700480 @ 2046.4 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 39700480, mean_episode_return = None, mean_episode_step = 2643.1, total_loss = 934.21, pg_loss = 384.67, baseline_loss = 554.77, entropy_loss = -5.2327, learner_queue_size = 32, train_seconds = 2.2116e+04, _tick = 8213, _time = 1.6546e+09)
[2022-06-07 16:32:27,466][root][INFO] - Step 39710720 @ 2046.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 39710720, mean_episode_return = 31.362, mean_episode_step = 1718.7, total_loss = -113.86, pg_loss = -141.67, baseline_loss = 33.006, entropy_loss = -5.1966, learner_queue_size = 32, train_seconds = 2.212e+04, _tick = 8214, _time = 1.6546e+09)
[2022-06-07 16:32:32,472][root][INFO] - Step 39718400 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 39718400, mean_episode_return = 47.94, mean_episode_step = 2079.5, total_loss = 545.13, pg_loss = 352.42, baseline_loss = 197.95, entropy_loss = -5.2362, learner_queue_size = 32, train_seconds = 2.2126e+04, _tick = 8217, _time = 1.6546e+09)
[2022-06-07 16:32:37,478][root][INFO] - Step 39728640 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 39728640, mean_episode_return = 15.64, mean_episode_step = 1960.9, total_loss = 250.15, pg_loss = 108.53, baseline_loss = 146.84, entropy_loss = -5.2219, learner_queue_size = 32, train_seconds = 2.213e+04, _tick = 8221, _time = 1.6546e+09)
[2022-06-07 16:32:42,484][root][INFO] - Step 39736320 @ 1534.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39736320, mean_episode_return = None, mean_episode_step = 1796.9, total_loss = -133.77, pg_loss = -133.07, baseline_loss = 4.4941, entropy_loss = -5.1893, learner_queue_size = 32, train_seconds = 2.2136e+04, _tick = 8221, _time = 1.6546e+09)
[2022-06-07 16:32:47,490][root][INFO] - Step 39746560 @ 2045.7 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 39746560, mean_episode_return = 84.913, mean_episode_step = 2126.1, total_loss = -110.79, pg_loss = -118.17, baseline_loss = 12.534, entropy_loss = -5.1535, learner_queue_size = 32, train_seconds = 2.214e+04, _tick = 8224, _time = 1.6546e+09)
[2022-06-07 16:32:52,494][root][INFO] - Step 39754240 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39754240, mean_episode_return = 60.281, mean_episode_step = 1761.4, total_loss = 173.37, pg_loss = 122.23, baseline_loss = 56.331, entropy_loss = -5.1912, learner_queue_size = 32, train_seconds = 2.2146e+04, _tick = 8227, _time = 1.6546e+09)
[2022-06-07 16:32:57,498][root][INFO] - Step 39764480 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39764480, mean_episode_return = 37.111, mean_episode_step = 2019.7, total_loss = -101.55, pg_loss = -141.28, baseline_loss = 44.841, entropy_loss = -5.1093, learner_queue_size = 32, train_seconds = 2.215e+04, _tick = 8230, _time = 1.6546e+09)
[2022-06-07 16:33:02,502][root][INFO] - Step 39772160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39772160, mean_episode_return = 26.07, mean_episode_step = 2257.3, total_loss = -249.24, pg_loss = -257.44, baseline_loss = 13.325, entropy_loss = -5.1238, learner_queue_size = 32, train_seconds = 2.2156e+04, _tick = 8232, _time = 1.6546e+09)
[2022-06-07 16:33:07,508][root][INFO] - Step 39782400 @ 2045.6 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 39782400, mean_episode_return = 77.704, mean_episode_step = 1618.9, total_loss = -142.24, pg_loss = -179.54, baseline_loss = 42.354, entropy_loss = -5.0602, learner_queue_size = 32, train_seconds = 2.216e+04, _tick = 8235, _time = 1.6546e+09)
[2022-06-07 16:33:12,514][root][INFO] - Step 39790080 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39790080, mean_episode_return = None, mean_episode_step = 2453.3, total_loss = 36.535, pg_loss = -12.761, baseline_loss = 54.415, entropy_loss = -5.1189, learner_queue_size = 32, train_seconds = 2.2166e+04, _tick = 8236, _time = 1.6546e+09)
[2022-06-07 16:33:17,518][root][INFO] - Step 39800320 @ 2046.3 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (step = 39800320, mean_episode_return = 50.57, mean_episode_step = 1962.2, total_loss = -20.425, pg_loss = -58.213, baseline_loss = 42.982, entropy_loss = -5.194, learner_queue_size = 32, train_seconds = 2.217e+04, _tick = 8238, _time = 1.6546e+09)
[2022-06-07 16:33:22,524][root][INFO] - Step 39808000 @ 1534.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39808000, mean_episode_return = None, mean_episode_step = 2000.1, total_loss = 43.804, pg_loss = 7.513, baseline_loss = 41.483, entropy_loss = -5.1912, learner_queue_size = 32, train_seconds = 2.2176e+04, _tick = 8240, _time = 1.6546e+09)
[2022-06-07 16:33:27,526][root][INFO] - Step 39818240 @ 2047.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39818240, mean_episode_return = None, mean_episode_step = 1595.2, total_loss = 96.866, pg_loss = 38.484, baseline_loss = 63.548, entropy_loss = -5.1662, learner_queue_size = 32, train_seconds = 2.218e+04, _tick = 8242, _time = 1.6546e+09)
[2022-06-07 16:33:32,530][root][INFO] - Step 39825920 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 39825920, mean_episode_return = None, mean_episode_step = 2430.8, total_loss = 252.57, pg_loss = 158.19, baseline_loss = 99.631, entropy_loss = -5.251, learner_queue_size = 32, train_seconds = 2.2186e+04, _tick = 8244, _time = 1.6546e+09)
[2022-06-07 16:33:37,534][root][INFO] - Step 39833600 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39833600, mean_episode_return = 42.213, mean_episode_step = 2337.5, total_loss = 568.07, pg_loss = 333.94, baseline_loss = 239.38, entropy_loss = -5.2502, learner_queue_size = 32, train_seconds = 2.219e+04, _tick = 8246, _time = 1.6546e+09)
[2022-06-07 16:33:42,538][root][INFO] - Step 39843840 @ 2046.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 39843840, mean_episode_return = 70.123, mean_episode_step = 1705.6, total_loss = 216.76, pg_loss = 103.76, baseline_loss = 118.24, entropy_loss = -5.2516, learner_queue_size = 32, train_seconds = 2.2196e+04, _tick = 8249, _time = 1.6546e+09)
[2022-06-07 16:33:47,544][root][INFO] - Step 39851520 @ 1534.1 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 39851520, mean_episode_return = None, mean_episode_step = 1529.9, total_loss = -82.311, pg_loss = -108.94, baseline_loss = 31.829, entropy_loss = -5.198, learner_queue_size = 32, train_seconds = 2.22e+04, _tick = 8250, _time = 1.6546e+09)
[2022-06-07 16:33:52,550][root][INFO] - Step 39861760 @ 2045.6 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (step = 39861760, mean_episode_return = None, mean_episode_step = 2261.5, total_loss = 152.62, pg_loss = 101.82, baseline_loss = 55.976, entropy_loss = -5.1842, learner_queue_size = 32, train_seconds = 2.2206e+04, _tick = 8253, _time = 1.6546e+09)
[2022-06-07 16:33:57,554][root][INFO] - Step 39869440 @ 1534.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 39869440, mean_episode_return = 12.511, mean_episode_step = 2653.0, total_loss = -57.39, pg_loss = -95.657, baseline_loss = 43.435, entropy_loss = -5.1692, learner_queue_size = 32, train_seconds = 2.221e+04, _tick = 8255, _time = 1.6546e+09)
[2022-06-07 16:34:02,558][root][INFO] - Step 39879680 @ 2046.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 39879680, mean_episode_return = 29.178, mean_episode_step = 2422.7, total_loss = 339.76, pg_loss = 236.11, baseline_loss = 108.84, entropy_loss = -5.1809, learner_queue_size = 32, train_seconds = 2.2216e+04, _tick = 8258, _time = 1.6546e+09)
[2022-06-07 16:34:07,564][root][INFO] - Step 39887360 @ 1534.1 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 39887360, mean_episode_return = 1.5397, mean_episode_step = 2057.5, total_loss = 363.48, pg_loss = 237.06, baseline_loss = 131.56, entropy_loss = -5.1386, learner_queue_size = 32, train_seconds = 2.2221e+04, _tick = 8260, _time = 1.6546e+09)
[2022-06-07 16:34:12,570][root][INFO] - Step 39897600 @ 2045.5 SPS. Inference batcher size: 45. Learner queue size: 32. Other stats: (step = 39897600, mean_episode_return = 23.623, mean_episode_step = 1613.5, total_loss = 106.02, pg_loss = -17.989, baseline_loss = 129.13, entropy_loss = -5.1218, learner_queue_size = 32, train_seconds = 2.2226e+04, _tick = 8263, _time = 1.6546e+09)
[2022-06-07 16:34:17,576][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 16:34:17,832][root][INFO] - Step 39905280 @ 1534.1 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 39907840, mean_episode_return = 132.92, mean_episode_step = 2405.1, total_loss = -32.339, pg_loss = -97.631, baseline_loss = 70.44, entropy_loss = -5.1484, learner_queue_size = 32, train_seconds = 2.2231e+04, _tick = 8265, _time = 1.6546e+09)
[2022-06-07 16:34:22,838][root][INFO] - Step 39915520 @ 1946.1 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 39915520, mean_episode_return = None, mean_episode_step = 2278.9, total_loss = 98.826, pg_loss = 31.506, baseline_loss = 72.476, entropy_loss = -5.1556, learner_queue_size = 32, train_seconds = 2.2236e+04, _tick = 8265, _time = 1.6546e+09)
[2022-06-07 16:34:27,842][root][INFO] - Step 39925760 @ 2046.3 SPS. Inference batcher size: 42. Learner queue size: 32. Other stats: (step = 39925760, mean_episode_return = None, mean_episode_step = 2621.1, total_loss = 87.332, pg_loss = 26.022, baseline_loss = 66.44, entropy_loss = -5.1292, learner_queue_size = 32, train_seconds = 2.2241e+04, _tick = 8265, _time = 1.6546e+09)
[2022-06-07 16:34:32,846][root][INFO] - Step 39933440 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39933440, mean_episode_return = None, mean_episode_step = 2176.6, total_loss = -178.75, pg_loss = -177.98, baseline_loss = 4.3625, entropy_loss = -5.1338, learner_queue_size = 32, train_seconds = 2.2246e+04, _tick = 8265, _time = 1.6546e+09)
[2022-06-07 16:34:37,852][root][INFO] - Step 39943680 @ 2045.6 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 39943680, mean_episode_return = None, mean_episode_step = 2349.5, total_loss = -8.1344, pg_loss = -37.638, baseline_loss = 34.671, entropy_loss = -5.1677, learner_queue_size = 32, train_seconds = 2.2251e+04, _tick = 8266, _time = 1.6546e+09)
[2022-06-07 16:34:42,858][root][INFO] - Step 39951360 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 39951360, mean_episode_return = 120.75, mean_episode_step = 1959.4, total_loss = 30.9, pg_loss = -55.667, baseline_loss = 91.765, entropy_loss = -5.198, learner_queue_size = 32, train_seconds = 2.2256e+04, _tick = 8269, _time = 1.6546e+09)
[2022-06-07 16:34:47,862][root][INFO] - Step 39961600 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 39961600, mean_episode_return = 53.271, mean_episode_step = 2185.0, total_loss = -201.28, pg_loss = -225.55, baseline_loss = 29.441, entropy_loss = -5.1759, learner_queue_size = 32, train_seconds = 2.2261e+04, _tick = 8271, _time = 1.6546e+09)
[2022-06-07 16:34:52,866][root][INFO] - Step 39969280 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 39969280, mean_episode_return = None, mean_episode_step = 2076.2, total_loss = -1.3729, pg_loss = -21.778, baseline_loss = 25.594, entropy_loss = -5.1889, learner_queue_size = 32, train_seconds = 2.2266e+04, _tick = 8273, _time = 1.6546e+09)
[2022-06-07 16:34:57,870][root][INFO] - Step 39979520 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 39979520, mean_episode_return = None, mean_episode_step = 2304.6, total_loss = 290.41, pg_loss = 175.34, baseline_loss = 120.28, entropy_loss = -5.2082, learner_queue_size = 32, train_seconds = 2.2271e+04, _tick = 8274, _time = 1.6546e+09)
[2022-06-07 16:35:02,882][root][INFO] - Step 39987200 @ 1532.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 39987200, mean_episode_return = 19.45, mean_episode_step = 2526.1, total_loss = -39.41, pg_loss = -60.773, baseline_loss = 26.532, entropy_loss = -5.1689, learner_queue_size = 32, train_seconds = 2.2276e+04, _tick = 8276, _time = 1.6546e+09)
[2022-06-07 16:35:07,886][root][INFO] - Step 39997440 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 39997440, mean_episode_return = None, mean_episode_step = 2053.9, total_loss = -14.901, pg_loss = -49.02, baseline_loss = 39.293, entropy_loss = -5.1742, learner_queue_size = 32, train_seconds = 2.2281e+04, _tick = 8278, _time = 1.6546e+09)
[2022-06-07 16:35:12,890][root][INFO] - Step 40005120 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 40005120, mean_episode_return = None, mean_episode_step = 1901.3, total_loss = 834.61, pg_loss = 573.88, baseline_loss = 265.79, entropy_loss = -5.0552, learner_queue_size = 32, train_seconds = 2.2286e+04, _tick = 8278, _time = 1.6546e+09)
[2022-06-07 16:35:17,894][root][INFO] - Step 40015360 @ 2046.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 40015360, mean_episode_return = 7.5897, mean_episode_step = 2374.9, total_loss = -149.31, pg_loss = -158.52, baseline_loss = 14.295, entropy_loss = -5.0842, learner_queue_size = 32, train_seconds = 2.2291e+04, _tick = 8279, _time = 1.6546e+09)
[2022-06-07 16:35:22,900][root][INFO] - Step 40023040 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40023040, mean_episode_return = 42.721, mean_episode_step = 2128.3, total_loss = -301.33, pg_loss = -315.32, baseline_loss = 19.112, entropy_loss = -5.1174, learner_queue_size = 32, train_seconds = 2.2296e+04, _tick = 8281, _time = 1.6546e+09)
[2022-06-07 16:35:27,906][root][INFO] - Step 40033280 @ 2045.6 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 40033280, mean_episode_return = None, mean_episode_step = 1801.8, total_loss = 494.35, pg_loss = 345.58, baseline_loss = 153.9, entropy_loss = -5.1206, learner_queue_size = 32, train_seconds = 2.2301e+04, _tick = 8284, _time = 1.6546e+09)
[2022-06-07 16:35:32,912][root][INFO] - Step 40040960 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40040960, mean_episode_return = 12.29, mean_episode_step = 1924.3, total_loss = -236.94, pg_loss = -248.55, baseline_loss = 16.774, entropy_loss = -5.1577, learner_queue_size = 32, train_seconds = 2.2306e+04, _tick = 8287, _time = 1.6546e+09)
[2022-06-07 16:35:37,918][root][INFO] - Step 40051200 @ 2045.6 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 40051200, mean_episode_return = 16.417, mean_episode_step = 1591.5, total_loss = -111.54, pg_loss = -147.69, baseline_loss = 41.295, entropy_loss = -5.1444, learner_queue_size = 32, train_seconds = 2.2311e+04, _tick = 8291, _time = 1.6546e+09)
[2022-06-07 16:35:42,922][root][INFO] - Step 40058880 @ 1534.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 40058880, mean_episode_return = 77.876, mean_episode_step = 2328.4, total_loss = 59.497, pg_loss = 14.649, baseline_loss = 49.97, entropy_loss = -5.1221, learner_queue_size = 32, train_seconds = 2.2316e+04, _tick = 8294, _time = 1.6546e+09)
[2022-06-07 16:35:47,926][root][INFO] - Step 40069120 @ 2046.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 40069120, mean_episode_return = 107.44, mean_episode_step = 2304.8, total_loss = 140.87, pg_loss = 58.933, baseline_loss = 87.096, entropy_loss = -5.1608, learner_queue_size = 32, train_seconds = 2.2321e+04, _tick = 8297, _time = 1.6546e+09)
[2022-06-07 16:35:52,932][root][INFO] - Step 40076800 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40076800, mean_episode_return = 63.314, mean_episode_step = 3191.1, total_loss = -82.287, pg_loss = -109.4, baseline_loss = 32.304, entropy_loss = -5.1948, learner_queue_size = 32, train_seconds = 2.2326e+04, _tick = 8300, _time = 1.6546e+09)
[2022-06-07 16:35:57,934][root][INFO] - Step 40087040 @ 2047.2 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 40087040, mean_episode_return = None, mean_episode_step = 2022.5, total_loss = 47.802, pg_loss = 3.6237, baseline_loss = 49.398, entropy_loss = -5.2197, learner_queue_size = 32, train_seconds = 2.2331e+04, _tick = 8302, _time = 1.6546e+09)
[2022-06-07 16:36:02,938][root][INFO] - Step 40094720 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40094720, mean_episode_return = 108.03, mean_episode_step = 2080.0, total_loss = 109.25, pg_loss = 31.703, baseline_loss = 82.772, entropy_loss = -5.2215, learner_queue_size = 32, train_seconds = 2.2336e+04, _tick = 8305, _time = 1.6546e+09)
[2022-06-07 16:36:07,942][root][INFO] - Step 40104960 @ 2046.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 40104960, mean_episode_return = 1.3347, mean_episode_step = 2056.3, total_loss = -141.99, pg_loss = -163.37, baseline_loss = 26.586, entropy_loss = -5.2019, learner_queue_size = 32, train_seconds = 2.2341e+04, _tick = 8308, _time = 1.6546e+09)
[2022-06-07 16:36:12,946][root][INFO] - Step 40112640 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 40112640, mean_episode_return = 52.778, mean_episode_step = 2001.7, total_loss = -14.489, pg_loss = -22.234, baseline_loss = 12.92, entropy_loss = -5.1758, learner_queue_size = 32, train_seconds = 2.2346e+04, _tick = 8309, _time = 1.6546e+09)
[2022-06-07 16:36:17,952][root][INFO] - Step 40122880 @ 2045.6 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 40122880, mean_episode_return = 24.059, mean_episode_step = 1648.3, total_loss = 50.906, pg_loss = -8.2133, baseline_loss = 64.281, entropy_loss = -5.1613, learner_queue_size = 32, train_seconds = 2.2351e+04, _tick = 8313, _time = 1.6546e+09)
[2022-06-07 16:36:22,954][root][INFO] - Step 40133120 @ 2047.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40133120, mean_episode_return = None, mean_episode_step = 1963.0, total_loss = 20.389, pg_loss = -14.456, baseline_loss = 39.948, entropy_loss = -5.1042, learner_queue_size = 32, train_seconds = 2.2356e+04, _tick = 8316, _time = 1.6546e+09)
[2022-06-07 16:36:27,958][root][INFO] - Step 40140800 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40140800, mean_episode_return = 91.911, mean_episode_step = 1432.3, total_loss = 71.031, pg_loss = 18.854, baseline_loss = 57.267, entropy_loss = -5.0894, learner_queue_size = 32, train_seconds = 2.2361e+04, _tick = 8318, _time = 1.6546e+09)
[2022-06-07 16:36:32,962][root][INFO] - Step 40151040 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 40151040, mean_episode_return = None, mean_episode_step = 2342.9, total_loss = 396.45, pg_loss = 277.99, baseline_loss = 123.4, entropy_loss = -4.9434, learner_queue_size = 32, train_seconds = 2.2366e+04, _tick = 8320, _time = 1.6546e+09)
[2022-06-07 16:36:37,968][root][INFO] - Step 40158720 @ 1534.1 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 40158720, mean_episode_return = None, mean_episode_step = 1785.5, total_loss = 157.55, pg_loss = 81.37, baseline_loss = 81.063, entropy_loss = -4.8812, learner_queue_size = 32, train_seconds = 2.2371e+04, _tick = 8321, _time = 1.6546e+09)
[2022-06-07 16:36:42,974][root][INFO] - Step 40168960 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 40168960, mean_episode_return = 105.83, mean_episode_step = 1780.8, total_loss = 444.97, pg_loss = 328.95, baseline_loss = 120.84, entropy_loss = -4.8184, learner_queue_size = 32, train_seconds = 2.2376e+04, _tick = 8325, _time = 1.6546e+09)
[2022-06-07 16:36:47,978][root][INFO] - Step 40176640 @ 1534.7 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40176640, mean_episode_return = 103.57, mean_episode_step = 2353.6, total_loss = 229.08, pg_loss = 125.73, baseline_loss = 108.17, entropy_loss = -4.8199, learner_queue_size = 32, train_seconds = 2.2381e+04, _tick = 8327, _time = 1.6546e+09)
[2022-06-07 16:36:52,982][root][INFO] - Step 40186880 @ 2046.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 40186880, mean_episode_return = 52.411, mean_episode_step = 1829.5, total_loss = 240.32, pg_loss = 166.18, baseline_loss = 79.007, entropy_loss = -4.8712, learner_queue_size = 32, train_seconds = 2.2386e+04, _tick = 8330, _time = 1.6546e+09)
[2022-06-07 16:36:57,987][root][INFO] - Step 40194560 @ 1534.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40194560, mean_episode_return = None, mean_episode_step = 1962.7, total_loss = -93.802, pg_loss = -94.624, baseline_loss = 5.6978, entropy_loss = -4.8756, learner_queue_size = 32, train_seconds = 2.2391e+04, _tick = 8332, _time = 1.6546e+09)
[2022-06-07 16:37:02,990][root][INFO] - Step 40204800 @ 2046.9 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 40204800, mean_episode_return = 36.14, mean_episode_step = 2006.9, total_loss = -61.848, pg_loss = -129.15, baseline_loss = 72.132, entropy_loss = -4.8327, learner_queue_size = 32, train_seconds = 2.2396e+04, _tick = 8334, _time = 1.6546e+09)
[2022-06-07 16:37:07,994][root][INFO] - Step 40212480 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 40212480, mean_episode_return = 36.31, mean_episode_step = 1747.7, total_loss = 10.218, pg_loss = -26.298, baseline_loss = 41.379, entropy_loss = -4.863, learner_queue_size = 32, train_seconds = 2.2401e+04, _tick = 8337, _time = 1.6546e+09)
[2022-06-07 16:37:13,000][root][INFO] - Step 40222720 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 40222720, mean_episode_return = None, mean_episode_step = 1698.3, total_loss = -81.021, pg_loss = -102.2, baseline_loss = 25.924, entropy_loss = -4.7403, learner_queue_size = 32, train_seconds = 2.2406e+04, _tick = 8339, _time = 1.6546e+09)
[2022-06-07 16:37:18,006][root][INFO] - Step 40230400 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 40230400, mean_episode_return = 19.494, mean_episode_step = 2104.9, total_loss = 201.73, pg_loss = 115.13, baseline_loss = 91.328, entropy_loss = -4.728, learner_queue_size = 32, train_seconds = 2.2411e+04, _tick = 8342, _time = 1.6546e+09)
[2022-06-07 16:37:23,012][root][INFO] - Step 40240640 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40240640, mean_episode_return = None, mean_episode_step = 1826.8, total_loss = -74.701, pg_loss = -82.204, baseline_loss = 12.126, entropy_loss = -4.6238, learner_queue_size = 32, train_seconds = 2.2416e+04, _tick = 8343, _time = 1.6546e+09)
[2022-06-07 16:37:28,018][root][INFO] - Step 40248320 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 40248320, mean_episode_return = 29.852, mean_episode_step = 2154.7, total_loss = -11.654, pg_loss = -46.527, baseline_loss = 39.519, entropy_loss = -4.6459, learner_queue_size = 32, train_seconds = 2.2421e+04, _tick = 8345, _time = 1.6546e+09)
[2022-06-07 16:37:33,024][root][INFO] - Step 40258560 @ 2045.6 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 40258560, mean_episode_return = None, mean_episode_step = 1829.5, total_loss = 43.276, pg_loss = -15.949, baseline_loss = 64.018, entropy_loss = -4.7924, learner_queue_size = 32, train_seconds = 2.2426e+04, _tick = 8345, _time = 1.6546e+09)
[2022-06-07 16:37:38,026][root][INFO] - Step 40266240 @ 1535.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40266240, mean_episode_return = None, mean_episode_step = 2058.5, total_loss = -61.141, pg_loss = -80.304, baseline_loss = 23.944, entropy_loss = -4.7808, learner_queue_size = 32, train_seconds = 2.2431e+04, _tick = 8345, _time = 1.6546e+09)
[2022-06-07 16:37:43,038][root][INFO] - Step 40276480 @ 2043.1 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 40276480, mean_episode_return = 82.834, mean_episode_step = 2491.7, total_loss = 139.62, pg_loss = 63.146, baseline_loss = 81.356, entropy_loss = -4.8851, learner_queue_size = 32, train_seconds = 2.2436e+04, _tick = 8346, _time = 1.6546e+09)
[2022-06-07 16:37:48,044][root][INFO] - Step 40284160 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40284160, mean_episode_return = None, mean_episode_step = 2563.3, total_loss = 310.04, pg_loss = 233.46, baseline_loss = 81.48, entropy_loss = -4.897, learner_queue_size = 32, train_seconds = 2.2441e+04, _tick = 8346, _time = 1.6546e+09)
[2022-06-07 16:37:53,050][root][INFO] - Step 40294400 @ 2045.5 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 40294400, mean_episode_return = 28.62, mean_episode_step = 1811.7, total_loss = -17.774, pg_loss = -75.425, baseline_loss = 62.638, entropy_loss = -4.9874, learner_queue_size = 32, train_seconds = 2.2446e+04, _tick = 8350, _time = 1.6546e+09)
[2022-06-07 16:37:58,054][root][INFO] - Step 40302080 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 40302080, mean_episode_return = None, mean_episode_step = 2112.8, total_loss = -12.814, pg_loss = -33.525, baseline_loss = 25.714, entropy_loss = -5.0039, learner_queue_size = 32, train_seconds = 2.2451e+04, _tick = 8351, _time = 1.6546e+09)
[2022-06-07 16:38:03,060][root][INFO] - Step 40312320 @ 2045.7 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 40312320, mean_episode_return = 32.064, mean_episode_step = 2655.5, total_loss = 210.46, pg_loss = 127.2, baseline_loss = 88.279, entropy_loss = -5.0204, learner_queue_size = 32, train_seconds = 2.2456e+04, _tick = 8353, _time = 1.6546e+09)
[2022-06-07 16:38:08,062][root][INFO] - Step 40320000 @ 1535.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 40320000, mean_episode_return = 45.951, mean_episode_step = 1459.1, total_loss = 70.243, pg_loss = 23.38, baseline_loss = 51.89, entropy_loss = -5.0268, learner_queue_size = 32, train_seconds = 2.2461e+04, _tick = 8354, _time = 1.6546e+09)
[2022-06-07 16:38:13,066][root][INFO] - Step 40330240 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 40330240, mean_episode_return = 68.219, mean_episode_step = 2093.5, total_loss = -0.20072, pg_loss = -42.608, baseline_loss = 47.462, entropy_loss = -5.055, learner_queue_size = 32, train_seconds = 2.2466e+04, _tick = 8356, _time = 1.6546e+09)
[2022-06-07 16:38:18,072][root][INFO] - Step 40337920 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40337920, mean_episode_return = 18.82, mean_episode_step = 1865.0, total_loss = 156.07, pg_loss = 92.45, baseline_loss = 68.732, entropy_loss = -5.114, learner_queue_size = 32, train_seconds = 2.2471e+04, _tick = 8357, _time = 1.6546e+09)
[2022-06-07 16:38:23,078][root][INFO] - Step 40348160 @ 2045.6 SPS. Inference batcher size: 36. Learner queue size: 32. Other stats: (step = 40348160, mean_episode_return = 25.104, mean_episode_step = 2692.0, total_loss = -252.23, pg_loss = -254.28, baseline_loss = 7.2526, entropy_loss = -5.2014, learner_queue_size = 32, train_seconds = 2.2476e+04, _tick = 8359, _time = 1.6546e+09)
[2022-06-07 16:38:28,082][root][INFO] - Step 40355840 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40355840, mean_episode_return = None, mean_episode_step = 1882.0, total_loss = -194.81, pg_loss = -192.72, baseline_loss = 3.0765, entropy_loss = -5.1633, learner_queue_size = 32, train_seconds = 2.2481e+04, _tick = 8360, _time = 1.6546e+09)
[2022-06-07 16:38:33,086][root][INFO] - Step 40366080 @ 2046.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 40366080, mean_episode_return = 43.951, mean_episode_step = 2823.5, total_loss = -161.59, pg_loss = -170.18, baseline_loss = 13.749, entropy_loss = -5.1631, learner_queue_size = 32, train_seconds = 2.2486e+04, _tick = 8362, _time = 1.6546e+09)
[2022-06-07 16:38:38,092][root][INFO] - Step 40373760 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40373760, mean_episode_return = 107.44, mean_episode_step = 1951.7, total_loss = 420.18, pg_loss = 262.21, baseline_loss = 163.21, entropy_loss = -5.2378, learner_queue_size = 32, train_seconds = 2.2491e+04, _tick = 8364, _time = 1.6546e+09)
[2022-06-07 16:38:43,098][root][INFO] - Step 40384000 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40384000, mean_episode_return = 26.379, mean_episode_step = 1967.5, total_loss = 382.53, pg_loss = 269.75, baseline_loss = 118.06, entropy_loss = -5.2759, learner_queue_size = 32, train_seconds = 2.2496e+04, _tick = 8365, _time = 1.6546e+09)
[2022-06-07 16:38:48,104][root][INFO] - Step 40394240 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 40394240, mean_episode_return = 107.55, mean_episode_step = 2055.3, total_loss = 190.58, pg_loss = 95.244, baseline_loss = 100.58, entropy_loss = -5.2388, learner_queue_size = 32, train_seconds = 2.2501e+04, _tick = 8368, _time = 1.6546e+09)
[2022-06-07 16:38:53,111][root][INFO] - Step 40401920 @ 1534.0 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40401920, mean_episode_return = 16.49, mean_episode_step = 2276.6, total_loss = 2.5896, pg_loss = -59.514, baseline_loss = 67.312, entropy_loss = -5.2088, learner_queue_size = 32, train_seconds = 2.2506e+04, _tick = 8371, _time = 1.6546e+09)
[2022-06-07 16:38:58,117][root][INFO] - Step 40412160 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 40412160, mean_episode_return = 37.622, mean_episode_step = 1821.9, total_loss = 207.88, pg_loss = 108.11, baseline_loss = 104.91, entropy_loss = -5.1481, learner_queue_size = 32, train_seconds = 2.2511e+04, _tick = 8373, _time = 1.6546e+09)
[2022-06-07 16:39:03,122][root][INFO] - Step 40419840 @ 1534.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 40419840, mean_episode_return = None, mean_episode_step = 2632.2, total_loss = 252.54, pg_loss = 175.84, baseline_loss = 81.852, entropy_loss = -5.1508, learner_queue_size = 32, train_seconds = 2.2516e+04, _tick = 8374, _time = 1.6546e+09)
[2022-06-07 16:39:08,126][root][INFO] - Step 40430080 @ 2046.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 40430080, mean_episode_return = None, mean_episode_step = 1764.0, total_loss = 308.92, pg_loss = 216.5, baseline_loss = 97.577, entropy_loss = -5.1589, learner_queue_size = 32, train_seconds = 2.2521e+04, _tick = 8376, _time = 1.6546e+09)
[2022-06-07 16:39:13,130][root][INFO] - Step 40437760 @ 1534.9 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 40437760, mean_episode_return = None, mean_episode_step = 2221.5, total_loss = 149.51, pg_loss = 58.615, baseline_loss = 96.071, entropy_loss = -5.1791, learner_queue_size = 32, train_seconds = 2.2526e+04, _tick = 8377, _time = 1.6546e+09)
[2022-06-07 16:39:18,136][root][INFO] - Step 40448000 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 40448000, mean_episode_return = None, mean_episode_step = 2288.2, total_loss = -185.26, pg_loss = -182.54, baseline_loss = 2.4951, entropy_loss = -5.2165, learner_queue_size = 32, train_seconds = 2.2531e+04, _tick = 8379, _time = 1.6546e+09)
[2022-06-07 16:39:23,142][root][INFO] - Step 40455680 @ 1534.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 40455680, mean_episode_return = None, mean_episode_step = 2535.5, total_loss = 82.785, pg_loss = 35.746, baseline_loss = 52.203, entropy_loss = -5.1631, learner_queue_size = 32, train_seconds = 2.2536e+04, _tick = 8380, _time = 1.6546e+09)
[2022-06-07 16:39:28,154][root][INFO] - Step 40465920 @ 2043.1 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 40465920, mean_episode_return = None, mean_episode_step = 2217.4, total_loss = 79.194, pg_loss = 37.765, baseline_loss = 46.621, entropy_loss = -5.1923, learner_queue_size = 32, train_seconds = 2.2541e+04, _tick = 8383, _time = 1.6546e+09)
[2022-06-07 16:39:33,158][root][INFO] - Step 40473600 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40473600, mean_episode_return = -6.5601, mean_episode_step = 2053.3, total_loss = 151.59, pg_loss = 91.177, baseline_loss = 65.657, entropy_loss = -5.2467, learner_queue_size = 32, train_seconds = 2.2546e+04, _tick = 8386, _time = 1.6546e+09)
[2022-06-07 16:39:38,162][root][INFO] - Step 40483840 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40483840, mean_episode_return = None, mean_episode_step = 2042.7, total_loss = 534.83, pg_loss = 380.15, baseline_loss = 159.93, entropy_loss = -5.2536, learner_queue_size = 32, train_seconds = 2.2551e+04, _tick = 8386, _time = 1.6546e+09)
[2022-06-07 16:39:43,166][root][INFO] - Step 40491520 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40491520, mean_episode_return = 38.636, mean_episode_step = 2830.6, total_loss = -259.85, pg_loss = -285.25, baseline_loss = 30.672, entropy_loss = -5.2733, learner_queue_size = 32, train_seconds = 2.2556e+04, _tick = 8387, _time = 1.6546e+09)
[2022-06-07 16:39:48,172][root][INFO] - Step 40501760 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 40501760, mean_episode_return = 37.631, mean_episode_step = 2391.1, total_loss = -60.916, pg_loss = -127.69, baseline_loss = 72.033, entropy_loss = -5.2547, learner_queue_size = 32, train_seconds = 2.2561e+04, _tick = 8391, _time = 1.6546e+09)
[2022-06-07 16:39:53,178][root][INFO] - Step 40509440 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40509440, mean_episode_return = 92.685, mean_episode_step = 2451.7, total_loss = 257.52, pg_loss = 144.73, baseline_loss = 118.02, entropy_loss = -5.2345, learner_queue_size = 32, train_seconds = 2.2566e+04, _tick = 8393, _time = 1.6546e+09)
[2022-06-07 16:39:58,182][root][INFO] - Step 40519680 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 40519680, mean_episode_return = 181.43, mean_episode_step = 1810.2, total_loss = 263.77, pg_loss = 125.98, baseline_loss = 142.99, entropy_loss = -5.2044, learner_queue_size = 32, train_seconds = 2.2571e+04, _tick = 8396, _time = 1.6546e+09)
[2022-06-07 16:40:03,188][root][INFO] - Step 40527360 @ 1534.2 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 40527360, mean_episode_return = 45.701, mean_episode_step = 1965.4, total_loss = -75.411, pg_loss = -135.83, baseline_loss = 65.626, entropy_loss = -5.2041, learner_queue_size = 32, train_seconds = 2.2576e+04, _tick = 8397, _time = 1.6546e+09)
[2022-06-07 16:40:08,194][root][INFO] - Step 40537600 @ 2045.5 SPS. Inference batcher size: 56. Learner queue size: 32. Other stats: (step = 40537600, mean_episode_return = 14.249, mean_episode_step = 1734.4, total_loss = 292.92, pg_loss = 160.73, baseline_loss = 137.44, entropy_loss = -5.2531, learner_queue_size = 32, train_seconds = 2.2581e+04, _tick = 8401, _time = 1.6546e+09)
[2022-06-07 16:40:13,198][root][INFO] - Step 40545280 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40545280, mean_episode_return = 7.6196, mean_episode_step = 2085.1, total_loss = -104.9, pg_loss = -137.38, baseline_loss = 37.737, entropy_loss = -5.2537, learner_queue_size = 32, train_seconds = 2.2586e+04, _tick = 8402, _time = 1.6546e+09)
[2022-06-07 16:40:18,204][root][INFO] - Step 40555520 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 40555520, mean_episode_return = None, mean_episode_step = 2156.5, total_loss = 109.62, pg_loss = 42.16, baseline_loss = 72.679, entropy_loss = -5.2198, learner_queue_size = 32, train_seconds = 2.2591e+04, _tick = 8405, _time = 1.6546e+09)
[2022-06-07 16:40:23,210][root][INFO] - Step 40563200 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 40563200, mean_episode_return = None, mean_episode_step = 1529.7, total_loss = 306.63, pg_loss = 119.82, baseline_loss = 192.03, entropy_loss = -5.2261, learner_queue_size = 32, train_seconds = 2.2596e+04, _tick = 8406, _time = 1.6546e+09)
[2022-06-07 16:40:28,216][root][INFO] - Step 40573440 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 40573440, mean_episode_return = 25.47, mean_episode_step = 1977.8, total_loss = 139.63, pg_loss = 66.48, baseline_loss = 78.358, entropy_loss = -5.2068, learner_queue_size = 32, train_seconds = 2.2601e+04, _tick = 8409, _time = 1.6546e+09)
[2022-06-07 16:40:33,222][root][INFO] - Step 40581120 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40581120, mean_episode_return = None, mean_episode_step = 1900.2, total_loss = -108.18, pg_loss = -108.89, baseline_loss = 5.833, entropy_loss = -5.1195, learner_queue_size = 32, train_seconds = 2.2606e+04, _tick = 8410, _time = 1.6546e+09)
[2022-06-07 16:40:38,228][root][INFO] - Step 40591360 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40591360, mean_episode_return = 21.16, mean_episode_step = 1703.4, total_loss = 210.38, pg_loss = 109.62, baseline_loss = 105.88, entropy_loss = -5.1204, learner_queue_size = 32, train_seconds = 2.2611e+04, _tick = 8412, _time = 1.6546e+09)
[2022-06-07 16:40:43,234][root][INFO] - Step 40599040 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 40599040, mean_episode_return = None, mean_episode_step = 1991.2, total_loss = -120.22, pg_loss = -127.63, baseline_loss = 12.493, entropy_loss = -5.0853, learner_queue_size = 32, train_seconds = 2.2616e+04, _tick = 8413, _time = 1.6546e+09)
[2022-06-07 16:40:48,238][root][INFO] - Step 40609280 @ 2046.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 40609280, mean_episode_return = None, mean_episode_step = 1886.6, total_loss = 157.49, pg_loss = 100.95, baseline_loss = 61.668, entropy_loss = -5.1278, learner_queue_size = 32, train_seconds = 2.2621e+04, _tick = 8415, _time = 1.6546e+09)
[2022-06-07 16:40:53,244][root][INFO] - Step 40616960 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40616960, mean_episode_return = None, mean_episode_step = 1977.4, total_loss = -25.389, pg_loss = -58.498, baseline_loss = 38.23, entropy_loss = -5.1221, learner_queue_size = 32, train_seconds = 2.2626e+04, _tick = 8416, _time = 1.6546e+09)
[2022-06-07 16:40:58,250][root][INFO] - Step 40627200 @ 2045.5 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 40627200, mean_episode_return = 39.015, mean_episode_step = 1888.9, total_loss = -119.25, pg_loss = -149.35, baseline_loss = 35.317, entropy_loss = -5.2202, learner_queue_size = 32, train_seconds = 2.2631e+04, _tick = 8420, _time = 1.6546e+09)
[2022-06-07 16:41:03,254][root][INFO] - Step 40634880 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40634880, mean_episode_return = 69.43, mean_episode_step = 2477.9, total_loss = 21.009, pg_loss = -48.777, baseline_loss = 75.042, entropy_loss = -5.2564, learner_queue_size = 32, train_seconds = 2.2636e+04, _tick = 8421, _time = 1.6546e+09)
[2022-06-07 16:41:08,260][root][INFO] - Step 40645120 @ 2045.6 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 40645120, mean_episode_return = 90.376, mean_episode_step = 1658.5, total_loss = 625.59, pg_loss = 66.062, baseline_loss = 564.71, entropy_loss = -5.1816, learner_queue_size = 32, train_seconds = 2.2641e+04, _tick = 8424, _time = 1.6546e+09)
[2022-06-07 16:41:13,266][root][INFO] - Step 40652800 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40652800, mean_episode_return = None, mean_episode_step = 2506.6, total_loss = -111.39, pg_loss = -123.09, baseline_loss = 16.87, entropy_loss = -5.1633, learner_queue_size = 32, train_seconds = 2.2646e+04, _tick = 8425, _time = 1.6546e+09)
[2022-06-07 16:41:18,270][root][INFO] - Step 40663040 @ 2046.4 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 40663040, mean_episode_return = None, mean_episode_step = 1589.1, total_loss = 523.95, pg_loss = 354.58, baseline_loss = 174.52, entropy_loss = -5.1492, learner_queue_size = 32, train_seconds = 2.2651e+04, _tick = 8426, _time = 1.6546e+09)
[2022-06-07 16:41:23,274][root][INFO] - Step 40670720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40670720, mean_episode_return = 59.741, mean_episode_step = 1718.7, total_loss = 14.758, pg_loss = -24.252, baseline_loss = 44.171, entropy_loss = -5.1603, learner_queue_size = 32, train_seconds = 2.2656e+04, _tick = 8429, _time = 1.6546e+09)
[2022-06-07 16:41:28,280][root][INFO] - Step 40680960 @ 2045.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 40680960, mean_episode_return = 38.51, mean_episode_step = 2094.6, total_loss = 75.983, pg_loss = 2.0267, baseline_loss = 79.128, entropy_loss = -5.172, learner_queue_size = 32, train_seconds = 2.2661e+04, _tick = 8432, _time = 1.6546e+09)
[2022-06-07 16:41:33,286][root][INFO] - Step 40688640 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40688640, mean_episode_return = None, mean_episode_step = 2473.3, total_loss = 199.59, pg_loss = 118.59, baseline_loss = 86.197, entropy_loss = -5.1904, learner_queue_size = 32, train_seconds = 2.2666e+04, _tick = 8433, _time = 1.6546e+09)
[2022-06-07 16:41:38,290][root][INFO] - Step 40698880 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 40698880, mean_episode_return = 48.974, mean_episode_step = 1934.2, total_loss = 30.611, pg_loss = -85.759, baseline_loss = 121.52, entropy_loss = -5.1452, learner_queue_size = 32, train_seconds = 2.2671e+04, _tick = 8436, _time = 1.6546e+09)
[2022-06-07 16:41:43,294][root][INFO] - Step 40706560 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40706560, mean_episode_return = None, mean_episode_step = 2477.6, total_loss = 268.23, pg_loss = 176.65, baseline_loss = 96.589, entropy_loss = -5.0125, learner_queue_size = 32, train_seconds = 2.2676e+04, _tick = 8436, _time = 1.6546e+09)
[2022-06-07 16:41:48,300][root][INFO] - Step 40716800 @ 2045.5 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 40716800, mean_episode_return = 80.354, mean_episode_step = 2021.5, total_loss = 57.205, pg_loss = -14.213, baseline_loss = 76.484, entropy_loss = -5.0669, learner_queue_size = 32, train_seconds = 2.2681e+04, _tick = 8439, _time = 1.6546e+09)
[2022-06-07 16:41:53,306][root][INFO] - Step 40724480 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40724480, mean_episode_return = None, mean_episode_step = 2466.1, total_loss = -18.981, pg_loss = -39.725, baseline_loss = 25.805, entropy_loss = -5.0614, learner_queue_size = 32, train_seconds = 2.2686e+04, _tick = 8440, _time = 1.6546e+09)
[2022-06-07 16:41:58,312][root][INFO] - Step 40734720 @ 2045.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 40734720, mean_episode_return = None, mean_episode_step = 1996.0, total_loss = 74.041, pg_loss = 15.887, baseline_loss = 63.317, entropy_loss = -5.1627, learner_queue_size = 32, train_seconds = 2.2691e+04, _tick = 8443, _time = 1.6546e+09)
[2022-06-07 16:42:03,318][root][INFO] - Step 40742400 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40742400, mean_episode_return = 110.07, mean_episode_step = 2070.0, total_loss = 256.53, pg_loss = 149.27, baseline_loss = 112.44, entropy_loss = -5.1809, learner_queue_size = 32, train_seconds = 2.2696e+04, _tick = 8444, _time = 1.6546e+09)
[2022-06-07 16:42:08,322][root][INFO] - Step 40752640 @ 2046.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40752640, mean_episode_return = 33.04, mean_episode_step = 2214.1, total_loss = -78.679, pg_loss = -85.08, baseline_loss = 11.625, entropy_loss = -5.2246, learner_queue_size = 32, train_seconds = 2.2701e+04, _tick = 8447, _time = 1.6546e+09)
[2022-06-07 16:42:13,326][root][INFO] - Step 40762880 @ 2046.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 40762880, mean_episode_return = 28.372, mean_episode_step = 1913.8, total_loss = 374.51, pg_loss = 229.77, baseline_loss = 149.98, entropy_loss = -5.2323, learner_queue_size = 32, train_seconds = 2.2706e+04, _tick = 8450, _time = 1.6546e+09)
[2022-06-07 16:42:18,347][root][INFO] - Step 40770560 @ 1529.5 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 40770560, mean_episode_return = 180.1, mean_episode_step = 1490.2, total_loss = 305.64, pg_loss = 191.62, baseline_loss = 119.24, entropy_loss = -5.2264, learner_queue_size = 32, train_seconds = 2.2711e+04, _tick = 8452, _time = 1.6546e+09)
[2022-06-07 16:42:23,353][root][INFO] - Step 40778240 @ 1534.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 40778240, mean_episode_return = -11.32, mean_episode_step = 2103.7, total_loss = -100.67, pg_loss = -136.18, baseline_loss = 40.728, entropy_loss = -5.2169, learner_queue_size = 32, train_seconds = 2.2716e+04, _tick = 8453, _time = 1.6546e+09)
[2022-06-07 16:42:28,358][root][INFO] - Step 40788480 @ 2046.0 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 40788480, mean_episode_return = None, mean_episode_step = 2544.4, total_loss = 117.08, pg_loss = 67.539, baseline_loss = 54.784, entropy_loss = -5.2394, learner_queue_size = 32, train_seconds = 2.2721e+04, _tick = 8455, _time = 1.6546e+09)
[2022-06-07 16:42:33,364][root][INFO] - Step 40796160 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 40796160, mean_episode_return = 102.26, mean_episode_step = 1948.5, total_loss = -281.73, pg_loss = -282.02, baseline_loss = 5.5343, entropy_loss = -5.253, learner_queue_size = 32, train_seconds = 2.2726e+04, _tick = 8458, _time = 1.6546e+09)
[2022-06-07 16:42:38,370][root][INFO] - Step 40806400 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 40806400, mean_episode_return = None, mean_episode_step = 2644.5, total_loss = -196.67, pg_loss = -194.46, baseline_loss = 3.0425, entropy_loss = -5.2598, learner_queue_size = 32, train_seconds = 2.2731e+04, _tick = 8459, _time = 1.6546e+09)
[2022-06-07 16:42:43,376][root][INFO] - Step 40816640 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40816640, mean_episode_return = None, mean_episode_step = 2095.0, total_loss = 195.33, pg_loss = 42.202, baseline_loss = 158.4, entropy_loss = -5.2775, learner_queue_size = 32, train_seconds = 2.2736e+04, _tick = 8461, _time = 1.6546e+09)
[2022-06-07 16:42:48,383][root][INFO] - Step 40824320 @ 1533.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40824320, mean_episode_return = 98.79, mean_episode_step = 2085.8, total_loss = 31.911, pg_loss = -27.725, baseline_loss = 64.908, entropy_loss = -5.2715, learner_queue_size = 32, train_seconds = 2.2741e+04, _tick = 8463, _time = 1.6546e+09)
[2022-06-07 16:42:53,389][root][INFO] - Step 40832000 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40832000, mean_episode_return = 16.83, mean_episode_step = 1716.9, total_loss = -74.697, pg_loss = -133.17, baseline_loss = 63.701, entropy_loss = -5.229, learner_queue_size = 32, train_seconds = 2.2746e+04, _tick = 8465, _time = 1.6546e+09)
[2022-06-07 16:42:58,394][root][INFO] - Step 40842240 @ 2046.1 SPS. Inference batcher size: 38. Learner queue size: 32. Other stats: (step = 40842240, mean_episode_return = None, mean_episode_step = 2442.0, total_loss = -201.74, pg_loss = -198.9, baseline_loss = 2.3594, entropy_loss = -5.1959, learner_queue_size = 32, train_seconds = 2.2751e+04, _tick = 8468, _time = 1.6546e+09)
[2022-06-07 16:43:03,398][root][INFO] - Step 40849920 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 40849920, mean_episode_return = 47.042, mean_episode_step = 2097.1, total_loss = 297.18, pg_loss = 130.97, baseline_loss = 171.38, entropy_loss = -5.1729, learner_queue_size = 32, train_seconds = 2.2756e+04, _tick = 8470, _time = 1.6546e+09)
[2022-06-07 16:43:08,402][root][INFO] - Step 40860160 @ 2046.3 SPS. Inference batcher size: 46. Learner queue size: 32. Other stats: (step = 40860160, mean_episode_return = 128.53, mean_episode_step = 3209.7, total_loss = -138.92, pg_loss = -156.86, baseline_loss = 23.035, entropy_loss = -5.1002, learner_queue_size = 32, train_seconds = 2.2761e+04, _tick = 8473, _time = 1.6546e+09)
[2022-06-07 16:43:13,406][root][INFO] - Step 40867840 @ 1534.8 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 40867840, mean_episode_return = -35.38, mean_episode_step = 2369.5, total_loss = 386.4, pg_loss = 245.22, baseline_loss = 146.27, entropy_loss = -5.0909, learner_queue_size = 32, train_seconds = 2.2766e+04, _tick = 8475, _time = 1.6546e+09)
[2022-06-07 16:43:18,410][root][INFO] - Step 40878080 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 40878080, mean_episode_return = None, mean_episode_step = 2713.6, total_loss = -160.25, pg_loss = -158.2, baseline_loss = 3.0532, entropy_loss = -5.1098, learner_queue_size = 32, train_seconds = 2.2771e+04, _tick = 8477, _time = 1.6546e+09)
[2022-06-07 16:43:23,414][root][INFO] - Step 40885760 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40885760, mean_episode_return = 79.455, mean_episode_step = 1992.1, total_loss = -93.153, pg_loss = -122.56, baseline_loss = 34.518, entropy_loss = -5.1121, learner_queue_size = 32, train_seconds = 2.2776e+04, _tick = 8479, _time = 1.6546e+09)
[2022-06-07 16:43:28,420][root][INFO] - Step 40896000 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 40896000, mean_episode_return = None, mean_episode_step = 1899.3, total_loss = 63.285, pg_loss = 20.045, baseline_loss = 48.291, entropy_loss = -5.0513, learner_queue_size = 32, train_seconds = 2.2781e+04, _tick = 8481, _time = 1.6546e+09)
[2022-06-07 16:43:33,426][root][INFO] - Step 40906240 @ 2045.5 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 40906240, mean_episode_return = None, mean_episode_step = 2014.2, total_loss = 232.38, pg_loss = 150.2, baseline_loss = 87.115, entropy_loss = -4.931, learner_queue_size = 32, train_seconds = 2.2786e+04, _tick = 8482, _time = 1.6546e+09)
[2022-06-07 16:43:38,432][root][INFO] - Step 40913920 @ 1534.2 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (step = 40913920, mean_episode_return = None, mean_episode_step = 1626.1, total_loss = 9.1616, pg_loss = -6.7684, baseline_loss = 20.859, entropy_loss = -4.9286, learner_queue_size = 32, train_seconds = 2.2791e+04, _tick = 8483, _time = 1.6546e+09)
[2022-06-07 16:43:43,438][root][INFO] - Step 40924160 @ 2045.6 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (step = 40924160, mean_episode_return = None, mean_episode_step = 2234.8, total_loss = -20.64, pg_loss = -53.41, baseline_loss = 37.739, entropy_loss = -4.9686, learner_queue_size = 32, train_seconds = 2.2796e+04, _tick = 8485, _time = 1.6546e+09)
[2022-06-07 16:43:48,444][root][INFO] - Step 40931840 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40931840, mean_episode_return = -9.5056, mean_episode_step = 2652.4, total_loss = -205.81, pg_loss = -220.15, baseline_loss = 19.337, entropy_loss = -5.0006, learner_queue_size = 32, train_seconds = 2.2801e+04, _tick = 8488, _time = 1.6546e+09)
[2022-06-07 16:43:53,450][root][INFO] - Step 40942080 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 40942080, mean_episode_return = None, mean_episode_step = 1766.0, total_loss = 332.9, pg_loss = 237.44, baseline_loss = 100.43, entropy_loss = -4.9689, learner_queue_size = 32, train_seconds = 2.2806e+04, _tick = 8490, _time = 1.6546e+09)
[2022-06-07 16:43:58,454][root][INFO] - Step 40949760 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 40949760, mean_episode_return = 50.411, mean_episode_step = 2305.3, total_loss = 269.44, pg_loss = 175.46, baseline_loss = 98.921, entropy_loss = -4.9422, learner_queue_size = 32, train_seconds = 2.2811e+04, _tick = 8493, _time = 1.6546e+09)
[2022-06-07 16:44:03,460][root][INFO] - Step 40957440 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40957440, mean_episode_return = 84.154, mean_episode_step = 2017.9, total_loss = -92.26, pg_loss = -114.6, baseline_loss = 27.209, entropy_loss = -4.8689, learner_queue_size = 32, train_seconds = 2.2816e+04, _tick = 8495, _time = 1.6546e+09)
[2022-06-07 16:44:08,466][root][INFO] - Step 40967680 @ 2045.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 40967680, mean_episode_return = 64.129, mean_episode_step = 1180.7, total_loss = 246.65, pg_loss = 140.12, baseline_loss = 111.37, entropy_loss = -4.8393, learner_queue_size = 32, train_seconds = 2.2822e+04, _tick = 8497, _time = 1.6546e+09)
[2022-06-07 16:44:13,470][root][INFO] - Step 40975360 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 40975360, mean_episode_return = 13.78, mean_episode_step = 2680.4, total_loss = 37.524, pg_loss = -6.2792, baseline_loss = 48.63, entropy_loss = -4.8266, learner_queue_size = 32, train_seconds = 2.2826e+04, _tick = 8498, _time = 1.6546e+09)
[2022-06-07 16:44:18,474][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 16:44:18,660][root][INFO] - Step 40985600 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 40985600, mean_episode_return = 46.339, mean_episode_step = 2098.5, total_loss = 160.01, pg_loss = 83.292, baseline_loss = 81.524, entropy_loss = -4.8089, learner_queue_size = 32, train_seconds = 2.2832e+04, _tick = 8501, _time = 1.6546e+09)
[2022-06-07 16:44:23,666][root][INFO] - Step 40993280 @ 1479.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 40993280, mean_episode_return = None, mean_episode_step = 2305.2, total_loss = -169.85, pg_loss = -167.91, baseline_loss = 2.8692, entropy_loss = -4.8128, learner_queue_size = 32, train_seconds = 2.2837e+04, _tick = 8503, _time = 1.6546e+09)
[2022-06-07 16:44:28,670][root][INFO] - Step 41003520 @ 2046.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 41003520, mean_episode_return = -6.1472, mean_episode_step = 2321.1, total_loss = 103.25, pg_loss = 24.179, baseline_loss = 83.94, entropy_loss = -4.8733, learner_queue_size = 32, train_seconds = 2.2842e+04, _tick = 8506, _time = 1.6546e+09)
[2022-06-07 16:44:33,674][root][INFO] - Step 41011200 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 41011200, mean_episode_return = 25.879, mean_episode_step = 3143.6, total_loss = -170.74, pg_loss = -219.49, baseline_loss = 53.616, entropy_loss = -4.8726, learner_queue_size = 32, train_seconds = 2.2847e+04, _tick = 8508, _time = 1.6546e+09)
[2022-06-07 16:44:38,678][root][INFO] - Step 41021440 @ 2046.3 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (step = 41021440, mean_episode_return = None, mean_episode_step = 1795.9, total_loss = 69.422, pg_loss = 10.835, baseline_loss = 63.482, entropy_loss = -4.8958, learner_queue_size = 32, train_seconds = 2.2852e+04, _tick = 8510, _time = 1.6546e+09)
[2022-06-07 16:44:43,682][root][INFO] - Step 41029120 @ 1534.8 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 41029120, mean_episode_return = None, mean_episode_step = 2559.2, total_loss = 95.002, pg_loss = 55.826, baseline_loss = 43.998, entropy_loss = -4.8215, learner_queue_size = 32, train_seconds = 2.2857e+04, _tick = 8512, _time = 1.6546e+09)
[2022-06-07 16:44:48,686][root][INFO] - Step 41039360 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 41039360, mean_episode_return = 64.292, mean_episode_step = 1726.5, total_loss = -0.34505, pg_loss = -83.083, baseline_loss = 87.654, entropy_loss = -4.9162, learner_queue_size = 32, train_seconds = 2.2862e+04, _tick = 8514, _time = 1.6546e+09)
[2022-06-07 16:44:53,690][root][INFO] - Step 41047040 @ 1534.7 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41047040, mean_episode_return = 4.8896, mean_episode_step = 1957.0, total_loss = -104.46, pg_loss = -110.73, baseline_loss = 11.146, entropy_loss = -4.8701, learner_queue_size = 32, train_seconds = 2.2867e+04, _tick = 8517, _time = 1.6546e+09)
[2022-06-07 16:44:58,696][root][INFO] - Step 41057280 @ 2045.5 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (step = 41057280, mean_episode_return = 160.34, mean_episode_step = 2218.8, total_loss = 146.23, pg_loss = 88.962, baseline_loss = 62.175, entropy_loss = -4.909, learner_queue_size = 32, train_seconds = 2.2872e+04, _tick = 8519, _time = 1.6546e+09)
[2022-06-07 16:45:03,702][root][INFO] - Step 41064960 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41064960, mean_episode_return = 120.69, mean_episode_step = 1973.5, total_loss = 445.28, pg_loss = 315.98, baseline_loss = 134.23, entropy_loss = -4.9278, learner_queue_size = 32, train_seconds = 2.2877e+04, _tick = 8522, _time = 1.6546e+09)
[2022-06-07 16:45:08,708][root][INFO] - Step 41075200 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41075200, mean_episode_return = 28.752, mean_episode_step = 2100.3, total_loss = 2524.1, pg_loss = 989.12, baseline_loss = 1539.8, entropy_loss = -4.7956, learner_queue_size = 32, train_seconds = 2.2882e+04, _tick = 8525, _time = 1.6546e+09)
[2022-06-07 16:45:13,714][root][INFO] - Step 41085440 @ 2045.6 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 41085440, mean_episode_return = None, mean_episode_step = 2158.8, total_loss = 614.72, pg_loss = 440.7, baseline_loss = 178.92, entropy_loss = -4.8944, learner_queue_size = 32, train_seconds = 2.2887e+04, _tick = 8527, _time = 1.6546e+09)
[2022-06-07 16:45:18,718][root][INFO] - Step 41093120 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41093120, mean_episode_return = 62.225, mean_episode_step = 2533.4, total_loss = 39.675, pg_loss = -17.377, baseline_loss = 62.037, entropy_loss = -4.9848, learner_queue_size = 32, train_seconds = 2.2892e+04, _tick = 8529, _time = 1.6546e+09)
[2022-06-07 16:45:23,722][root][INFO] - Step 41103360 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41103360, mean_episode_return = 137.86, mean_episode_step = 2343.5, total_loss = 33.99, pg_loss = -3.9187, baseline_loss = 42.821, entropy_loss = -4.9124, learner_queue_size = 32, train_seconds = 2.2897e+04, _tick = 8531, _time = 1.6546e+09)
[2022-06-07 16:45:28,726][root][INFO] - Step 41111040 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41111040, mean_episode_return = 35.347, mean_episode_step = 1955.8, total_loss = -173.51, pg_loss = -187.4, baseline_loss = 18.874, entropy_loss = -4.9831, learner_queue_size = 32, train_seconds = 2.2902e+04, _tick = 8532, _time = 1.6546e+09)
[2022-06-07 16:45:33,730][root][INFO] - Step 41118720 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41118720, mean_episode_return = None, mean_episode_step = 1652.4, total_loss = 143.48, pg_loss = 112.5, baseline_loss = 35.978, entropy_loss = -4.9954, learner_queue_size = 32, train_seconds = 2.2907e+04, _tick = 8533, _time = 1.6546e+09)
[2022-06-07 16:45:38,734][root][INFO] - Step 41128960 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 41128960, mean_episode_return = 23.318, mean_episode_step = 2014.1, total_loss = -11.703, pg_loss = -62.695, baseline_loss = 55.939, entropy_loss = -4.9481, learner_queue_size = 32, train_seconds = 2.2912e+04, _tick = 8536, _time = 1.6546e+09)
[2022-06-07 16:45:43,738][root][INFO] - Step 41136640 @ 1534.7 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 41136640, mean_episode_return = 68.165, mean_episode_step = 2260.9, total_loss = 407.02, pg_loss = 257.97, baseline_loss = 153.97, entropy_loss = -4.9307, learner_queue_size = 32, train_seconds = 2.2917e+04, _tick = 8538, _time = 1.6546e+09)
[2022-06-07 16:45:48,744][root][INFO] - Step 41146880 @ 2045.5 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 41146880, mean_episode_return = 49.985, mean_episode_step = 1706.9, total_loss = -68.88, pg_loss = -106.56, baseline_loss = 42.667, entropy_loss = -4.983, learner_queue_size = 32, train_seconds = 2.2922e+04, _tick = 8542, _time = 1.6546e+09)
[2022-06-07 16:45:53,750][root][INFO] - Step 41157120 @ 2045.6 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 41157120, mean_episode_return = None, mean_episode_step = 2334.0, total_loss = 233.75, pg_loss = 154.92, baseline_loss = 83.855, entropy_loss = -5.0225, learner_queue_size = 32, train_seconds = 2.2927e+04, _tick = 8544, _time = 1.6546e+09)
[2022-06-07 16:45:58,756][root][INFO] - Step 41164800 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41164800, mean_episode_return = 6.2098, mean_episode_step = 1912.7, total_loss = -24.42, pg_loss = -67.076, baseline_loss = 47.669, entropy_loss = -5.0132, learner_queue_size = 32, train_seconds = 2.2932e+04, _tick = 8547, _time = 1.6546e+09)
[2022-06-07 16:46:03,762][root][INFO] - Step 41175040 @ 2045.5 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 41175040, mean_episode_return = 28.096, mean_episode_step = 2462.6, total_loss = 71.371, pg_loss = -6.445, baseline_loss = 82.762, entropy_loss = -4.9454, learner_queue_size = 32, train_seconds = 2.2937e+04, _tick = 8551, _time = 1.6546e+09)
[2022-06-07 16:46:08,766][root][INFO] - Step 41182720 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41182720, mean_episode_return = 23.249, mean_episode_step = 1774.8, total_loss = 60.448, pg_loss = 10.146, baseline_loss = 55.137, entropy_loss = -4.8348, learner_queue_size = 32, train_seconds = 2.2942e+04, _tick = 8554, _time = 1.6546e+09)
[2022-06-07 16:46:13,770][root][INFO] - Step 41192960 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 41192960, mean_episode_return = 164.18, mean_episode_step = 2269.7, total_loss = 177.35, pg_loss = 95.894, baseline_loss = 86.229, entropy_loss = -4.777, learner_queue_size = 32, train_seconds = 2.2947e+04, _tick = 8556, _time = 1.6546e+09)
[2022-06-07 16:46:18,774][root][INFO] - Step 41200640 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 41200640, mean_episode_return = None, mean_episode_step = 1791.5, total_loss = -1.9105, pg_loss = -30.333, baseline_loss = 33.173, entropy_loss = -4.7503, learner_queue_size = 32, train_seconds = 2.2952e+04, _tick = 8556, _time = 1.6546e+09)
[2022-06-07 16:46:23,778][root][INFO] - Step 41210880 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41210880, mean_episode_return = None, mean_episode_step = 1558.0, total_loss = 651.06, pg_loss = 467.37, baseline_loss = 188.47, entropy_loss = -4.7826, learner_queue_size = 32, train_seconds = 2.2957e+04, _tick = 8557, _time = 1.6546e+09)
[2022-06-07 16:46:28,782][root][INFO] - Step 41221120 @ 2046.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 41221120, mean_episode_return = None, mean_episode_step = 2412.2, total_loss = 109.87, pg_loss = 47.882, baseline_loss = 66.639, entropy_loss = -4.6524, learner_queue_size = 32, train_seconds = 2.2962e+04, _tick = 8559, _time = 1.6546e+09)
[2022-06-07 16:46:33,786][root][INFO] - Step 41228800 @ 1534.8 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (step = 41228800, mean_episode_return = 42.481, mean_episode_step = 2461.5, total_loss = -104.34, pg_loss = -121.03, baseline_loss = 21.339, entropy_loss = -4.6445, learner_queue_size = 32, train_seconds = 2.2967e+04, _tick = 8560, _time = 1.6546e+09)
[2022-06-07 16:46:38,792][root][INFO] - Step 41239040 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 41239040, mean_episode_return = None, mean_episode_step = 2515.6, total_loss = -130.53, pg_loss = -130.23, baseline_loss = 4.409, entropy_loss = -4.7048, learner_queue_size = 32, train_seconds = 2.2972e+04, _tick = 8562, _time = 1.6546e+09)
[2022-06-07 16:46:43,798][root][INFO] - Step 41246720 @ 1534.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 41246720, mean_episode_return = None, mean_episode_step = 2047.3, total_loss = -73.452, pg_loss = -99.982, baseline_loss = 31.228, entropy_loss = -4.6986, learner_queue_size = 32, train_seconds = 2.2977e+04, _tick = 8563, _time = 1.6546e+09)
[2022-06-07 16:46:48,802][root][INFO] - Step 41256960 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 41256960, mean_episode_return = None, mean_episode_step = 2018.8, total_loss = 462.43, pg_loss = 328.28, baseline_loss = 138.84, entropy_loss = -4.6974, learner_queue_size = 32, train_seconds = 2.2982e+04, _tick = 8563, _time = 1.6546e+09)
[2022-06-07 16:46:53,806][root][INFO] - Step 41264640 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41264640, mean_episode_return = 61.591, mean_episode_step = 2106.4, total_loss = 186.93, pg_loss = 93.603, baseline_loss = 98.085, entropy_loss = -4.7578, learner_queue_size = 32, train_seconds = 2.2987e+04, _tick = 8565, _time = 1.6546e+09)
[2022-06-07 16:46:58,810][root][INFO] - Step 41274880 @ 2046.4 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 41274880, mean_episode_return = 93.973, mean_episode_step = 1672.6, total_loss = -44.349, pg_loss = -77.615, baseline_loss = 38.025, entropy_loss = -4.7587, learner_queue_size = 32, train_seconds = 2.2992e+04, _tick = 8569, _time = 1.6546e+09)
[2022-06-07 16:47:03,814][root][INFO] - Step 41282560 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41282560, mean_episode_return = 8.7797, mean_episode_step = 2705.1, total_loss = -108.64, pg_loss = -128.65, baseline_loss = 24.814, entropy_loss = -4.8031, learner_queue_size = 32, train_seconds = 2.2997e+04, _tick = 8571, _time = 1.6546e+09)
[2022-06-07 16:47:08,820][root][INFO] - Step 41292800 @ 2045.6 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 41292800, mean_episode_return = None, mean_episode_step = 2014.0, total_loss = -15.903, pg_loss = -40.448, baseline_loss = 29.432, entropy_loss = -4.8868, learner_queue_size = 32, train_seconds = 2.3002e+04, _tick = 8572, _time = 1.6546e+09)
[2022-06-07 16:47:13,826][root][INFO] - Step 41300480 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41300480, mean_episode_return = None, mean_episode_step = 1938.7, total_loss = 281.43, pg_loss = 188.22, baseline_loss = 98.093, entropy_loss = -4.8814, learner_queue_size = 32, train_seconds = 2.3007e+04, _tick = 8572, _time = 1.6546e+09)
[2022-06-07 16:47:18,832][root][INFO] - Step 41310720 @ 2045.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 41310720, mean_episode_return = 34.501, mean_episode_step = 1741.0, total_loss = -115.06, pg_loss = -136.67, baseline_loss = 26.417, entropy_loss = -4.8124, learner_queue_size = 32, train_seconds = 2.3012e+04, _tick = 8573, _time = 1.6546e+09)
[2022-06-07 16:47:23,838][root][INFO] - Step 41318400 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41318400, mean_episode_return = None, mean_episode_step = 2206.2, total_loss = 4.4898, pg_loss = -39.533, baseline_loss = 48.925, entropy_loss = -4.9018, learner_queue_size = 32, train_seconds = 2.3017e+04, _tick = 8574, _time = 1.6546e+09)
[2022-06-07 16:47:28,842][root][INFO] - Step 41328640 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41328640, mean_episode_return = None, mean_episode_step = 1723.1, total_loss = -33.399, pg_loss = -57.749, baseline_loss = 29.237, entropy_loss = -4.8869, learner_queue_size = 32, train_seconds = 2.3022e+04, _tick = 8575, _time = 1.6546e+09)
[2022-06-07 16:47:33,846][root][INFO] - Step 41336320 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41336320, mean_episode_return = 40.35, mean_episode_step = 2548.3, total_loss = -57.012, pg_loss = -94.516, baseline_loss = 42.37, entropy_loss = -4.8661, learner_queue_size = 32, train_seconds = 2.3027e+04, _tick = 8576, _time = 1.6546e+09)
[2022-06-07 16:47:38,850][root][INFO] - Step 41346560 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41346560, mean_episode_return = 87.355, mean_episode_step = 2822.7, total_loss = 548.44, pg_loss = 359.71, baseline_loss = 193.66, entropy_loss = -4.9316, learner_queue_size = 32, train_seconds = 2.3032e+04, _tick = 8578, _time = 1.6546e+09)
[2022-06-07 16:47:43,854][root][INFO] - Step 41354240 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 41354240, mean_episode_return = None, mean_episode_step = 1706.7, total_loss = 19.665, pg_loss = -15.602, baseline_loss = 40.222, entropy_loss = -4.9541, learner_queue_size = 32, train_seconds = 2.3037e+04, _tick = 8579, _time = 1.6546e+09)
[2022-06-07 16:47:48,858][root][INFO] - Step 41364480 @ 2046.4 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 41364480, mean_episode_return = None, mean_episode_step = 2275.2, total_loss = -70.79, pg_loss = -90.021, baseline_loss = 24.167, entropy_loss = -4.9357, learner_queue_size = 32, train_seconds = 2.3042e+04, _tick = 8581, _time = 1.6546e+09)
[2022-06-07 16:47:53,864][root][INFO] - Step 41372160 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41372160, mean_episode_return = 76.999, mean_episode_step = 2382.0, total_loss = -226.99, pg_loss = -228.93, baseline_loss = 6.8322, entropy_loss = -4.8972, learner_queue_size = 32, train_seconds = 2.3047e+04, _tick = 8583, _time = 1.6546e+09)
[2022-06-07 16:47:58,870][root][INFO] - Step 41382400 @ 2045.6 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 41382400, mean_episode_return = None, mean_episode_step = 1568.8, total_loss = 342.57, pg_loss = 237.37, baseline_loss = 110.04, entropy_loss = -4.8405, learner_queue_size = 32, train_seconds = 2.3052e+04, _tick = 8586, _time = 1.6546e+09)
[2022-06-07 16:48:03,876][root][INFO] - Step 41390080 @ 1534.2 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 41390080, mean_episode_return = None, mean_episode_step = 1860.5, total_loss = -224.84, pg_loss = -222.61, baseline_loss = 2.6161, entropy_loss = -4.8452, learner_queue_size = 32, train_seconds = 2.3057e+04, _tick = 8588, _time = 1.6546e+09)
[2022-06-07 16:48:08,882][root][INFO] - Step 41400320 @ 2045.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 41400320, mean_episode_return = 38.934, mean_episode_step = 2496.0, total_loss = 111.88, pg_loss = 52.521, baseline_loss = 64.232, entropy_loss = -4.875, learner_queue_size = 32, train_seconds = 2.3062e+04, _tick = 8592, _time = 1.6546e+09)
[2022-06-07 16:48:13,886][root][INFO] - Step 41410560 @ 2046.4 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 41410560, mean_episode_return = 77.159, mean_episode_step = 1974.2, total_loss = 181.94, pg_loss = 100.36, baseline_loss = 86.383, entropy_loss = -4.795, learner_queue_size = 32, train_seconds = 2.3067e+04, _tick = 8596, _time = 1.6546e+09)
[2022-06-07 16:48:18,890][root][INFO] - Step 41418240 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41418240, mean_episode_return = 11.5, mean_episode_step = 3089.8, total_loss = 63.052, pg_loss = -40.871, baseline_loss = 108.6, entropy_loss = -4.6786, learner_queue_size = 32, train_seconds = 2.3072e+04, _tick = 8598, _time = 1.6546e+09)
[2022-06-07 16:48:23,894][root][INFO] - Step 41428480 @ 2046.3 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 41428480, mean_episode_return = None, mean_episode_step = 2058.4, total_loss = 315.07, pg_loss = 227.89, baseline_loss = 91.938, entropy_loss = -4.7593, learner_queue_size = 32, train_seconds = 2.3077e+04, _tick = 8599, _time = 1.6546e+09)
[2022-06-07 16:48:28,898][root][INFO] - Step 41436160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41436160, mean_episode_return = 100.59, mean_episode_step = 2325.2, total_loss = -2.2764, pg_loss = -49.103, baseline_loss = 51.603, entropy_loss = -4.7768, learner_queue_size = 32, train_seconds = 2.3082e+04, _tick = 8602, _time = 1.6546e+09)
[2022-06-07 16:48:33,902][root][INFO] - Step 41446400 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 41446400, mean_episode_return = 26.19, mean_episode_step = 2963.6, total_loss = 52.132, pg_loss = -26.871, baseline_loss = 83.842, entropy_loss = -4.8382, learner_queue_size = 32, train_seconds = 2.3087e+04, _tick = 8605, _time = 1.6546e+09)
[2022-06-07 16:48:38,906][root][INFO] - Step 41454080 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41454080, mean_episode_return = None, mean_episode_step = 1949.5, total_loss = 220.81, pg_loss = 147.59, baseline_loss = 77.963, entropy_loss = -4.7397, learner_queue_size = 32, train_seconds = 2.3092e+04, _tick = 8606, _time = 1.6546e+09)
[2022-06-07 16:48:43,911][root][INFO] - Step 41464320 @ 2046.1 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 41464320, mean_episode_return = None, mean_episode_step = 2557.6, total_loss = -5.3148, pg_loss = -30.453, baseline_loss = 29.906, entropy_loss = -4.7683, learner_queue_size = 32, train_seconds = 2.3097e+04, _tick = 8607, _time = 1.6546e+09)
[2022-06-07 16:48:48,914][root][INFO] - Step 41472000 @ 1535.0 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (step = 41472000, mean_episode_return = 42.738, mean_episode_step = 1892.6, total_loss = 22.967, pg_loss = -36.994, baseline_loss = 64.719, entropy_loss = -4.7583, learner_queue_size = 32, train_seconds = 2.3102e+04, _tick = 8608, _time = 1.6546e+09)
[2022-06-07 16:48:53,918][root][INFO] - Step 41482240 @ 2046.3 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41482240, mean_episode_return = None, mean_episode_step = 1912.7, total_loss = 65.634, pg_loss = 20.327, baseline_loss = 50.116, entropy_loss = -4.8094, learner_queue_size = 32, train_seconds = 2.3107e+04, _tick = 8610, _time = 1.6546e+09)
[2022-06-07 16:48:58,922][root][INFO] - Step 41489920 @ 1534.8 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 41489920, mean_episode_return = None, mean_episode_step = 2357.5, total_loss = 374.23, pg_loss = 264.21, baseline_loss = 114.93, entropy_loss = -4.9065, learner_queue_size = 32, train_seconds = 2.3112e+04, _tick = 8611, _time = 1.6546e+09)
[2022-06-07 16:49:03,927][root][INFO] - Step 41500160 @ 2046.1 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 41500160, mean_episode_return = None, mean_episode_step = 2161.9, total_loss = 108.2, pg_loss = 55.696, baseline_loss = 57.491, entropy_loss = -4.9845, learner_queue_size = 32, train_seconds = 2.3117e+04, _tick = 8614, _time = 1.6546e+09)
[2022-06-07 16:49:08,933][root][INFO] - Step 41507840 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41507840, mean_episode_return = 93.479, mean_episode_step = 1795.3, total_loss = 120.71, pg_loss = 32.407, baseline_loss = 93.282, entropy_loss = -4.9755, learner_queue_size = 32, train_seconds = 2.3122e+04, _tick = 8616, _time = 1.6546e+09)
[2022-06-07 16:49:13,938][root][INFO] - Step 41518080 @ 2045.8 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (step = 41518080, mean_episode_return = 16.99, mean_episode_step = 2263.6, total_loss = -208.4, pg_loss = -229.21, baseline_loss = 25.725, entropy_loss = -4.9122, learner_queue_size = 32, train_seconds = 2.3127e+04, _tick = 8619, _time = 1.6546e+09)
[2022-06-07 16:49:18,942][root][INFO] - Step 41525760 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41525760, mean_episode_return = 32.86, mean_episode_step = 2211.7, total_loss = 320.32, pg_loss = 214.5, baseline_loss = 110.81, entropy_loss = -4.9803, learner_queue_size = 32, train_seconds = 2.3132e+04, _tick = 8622, _time = 1.6546e+09)
[2022-06-07 16:49:23,946][root][INFO] - Step 41536000 @ 2046.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 41536000, mean_episode_return = 107.6, mean_episode_step = 2742.9, total_loss = 226.55, pg_loss = 159.44, baseline_loss = 72.05, entropy_loss = -4.9386, learner_queue_size = 32, train_seconds = 2.3137e+04, _tick = 8626, _time = 1.6546e+09)
[2022-06-07 16:49:28,952][root][INFO] - Step 41543680 @ 1534.2 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 41543680, mean_episode_return = None, mean_episode_step = 2275.4, total_loss = 316.53, pg_loss = 203.02, baseline_loss = 118.45, entropy_loss = -4.9428, learner_queue_size = 32, train_seconds = 2.3142e+04, _tick = 8627, _time = 1.6546e+09)
[2022-06-07 16:49:33,958][root][INFO] - Step 41553920 @ 2045.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41553920, mean_episode_return = 92.371, mean_episode_step = 1879.8, total_loss = 26.595, pg_loss = -48.125, baseline_loss = 79.609, entropy_loss = -4.8888, learner_queue_size = 32, train_seconds = 2.3147e+04, _tick = 8631, _time = 1.6546e+09)
[2022-06-07 16:49:38,962][root][INFO] - Step 41561600 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 41561600, mean_episode_return = 114.29, mean_episode_step = 2115.5, total_loss = 59.736, pg_loss = 15.211, baseline_loss = 49.464, entropy_loss = -4.9389, learner_queue_size = 32, train_seconds = 2.3152e+04, _tick = 8633, _time = 1.6546e+09)
[2022-06-07 16:49:43,966][root][INFO] - Step 41571840 @ 2046.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 41571840, mean_episode_return = 23.655, mean_episode_step = 2303.4, total_loss = 392.09, pg_loss = 260.05, baseline_loss = 137.04, entropy_loss = -4.9993, learner_queue_size = 32, train_seconds = 2.3157e+04, _tick = 8636, _time = 1.6546e+09)
[2022-06-07 16:49:48,972][root][INFO] - Step 41582080 @ 2045.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 41582080, mean_episode_return = 1.21, mean_episode_step = 2215.8, total_loss = 123.56, pg_loss = 72.31, baseline_loss = 56.316, entropy_loss = -5.0697, learner_queue_size = 32, train_seconds = 2.3162e+04, _tick = 8637, _time = 1.6546e+09)
[2022-06-07 16:49:53,978][root][INFO] - Step 41589760 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41589760, mean_episode_return = 28.626, mean_episode_step = 2791.8, total_loss = -232.81, pg_loss = -235.5, baseline_loss = 7.7929, entropy_loss = -5.1001, learner_queue_size = 32, train_seconds = 2.3167e+04, _tick = 8640, _time = 1.6546e+09)
[2022-06-07 16:49:58,984][root][INFO] - Step 41600000 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41600000, mean_episode_return = 20.587, mean_episode_step = 1886.9, total_loss = -189.17, pg_loss = -210.28, baseline_loss = 26.168, entropy_loss = -5.0534, learner_queue_size = 32, train_seconds = 2.3172e+04, _tick = 8642, _time = 1.6546e+09)
[2022-06-07 16:50:03,991][root][INFO] - Step 41607680 @ 1533.9 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41607680, mean_episode_return = 29.27, mean_episode_step = 1713.4, total_loss = -177.56, pg_loss = -184.02, baseline_loss = 11.534, entropy_loss = -5.0706, learner_queue_size = 32, train_seconds = 2.3177e+04, _tick = 8644, _time = 1.6546e+09)
[2022-06-07 16:50:08,997][root][INFO] - Step 41617920 @ 2045.6 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 41617920, mean_episode_return = 45.99, mean_episode_step = 2566.3, total_loss = 272.93, pg_loss = 159.02, baseline_loss = 119.01, entropy_loss = -5.1063, learner_queue_size = 32, train_seconds = 2.3182e+04, _tick = 8647, _time = 1.6546e+09)
[2022-06-07 16:50:14,002][root][INFO] - Step 41625600 @ 1534.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 41625600, mean_episode_return = None, mean_episode_step = 1876.9, total_loss = 74.026, pg_loss = 33.746, baseline_loss = 45.402, entropy_loss = -5.1228, learner_queue_size = 32, train_seconds = 2.3187e+04, _tick = 8648, _time = 1.6546e+09)
[2022-06-07 16:50:19,006][root][INFO] - Step 41635840 @ 2046.3 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 41635840, mean_episode_return = 50.354, mean_episode_step = 1946.9, total_loss = -11.665, pg_loss = -35.884, baseline_loss = 29.429, entropy_loss = -5.2107, learner_queue_size = 32, train_seconds = 2.3192e+04, _tick = 8651, _time = 1.6546e+09)
[2022-06-07 16:50:24,010][root][INFO] - Step 41643520 @ 1534.8 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 41643520, mean_episode_return = 98.053, mean_episode_step = 2718.0, total_loss = -90.835, pg_loss = -119.98, baseline_loss = 34.356, entropy_loss = -5.2086, learner_queue_size = 32, train_seconds = 2.3197e+04, _tick = 8654, _time = 1.6546e+09)
[2022-06-07 16:50:29,014][root][INFO] - Step 41653760 @ 2046.3 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 41653760, mean_episode_return = None, mean_episode_step = 2300.8, total_loss = 88.392, pg_loss = 56.638, baseline_loss = 36.934, entropy_loss = -5.18, learner_queue_size = 32, train_seconds = 2.3202e+04, _tick = 8656, _time = 1.6546e+09)
[2022-06-07 16:50:34,018][root][INFO] - Step 41661440 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41661440, mean_episode_return = 94.957, mean_episode_step = 1846.7, total_loss = -178.68, pg_loss = -182.53, baseline_loss = 8.9969, entropy_loss = -5.1416, learner_queue_size = 32, train_seconds = 2.3207e+04, _tick = 8659, _time = 1.6546e+09)
[2022-06-07 16:50:39,024][root][INFO] - Step 41671680 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 41671680, mean_episode_return = 75.108, mean_episode_step = 2072.7, total_loss = -213.1, pg_loss = -214.55, baseline_loss = 6.5653, entropy_loss = -5.1179, learner_queue_size = 32, train_seconds = 2.3212e+04, _tick = 8662, _time = 1.6546e+09)
[2022-06-07 16:50:44,030][root][INFO] - Step 41679360 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41679360, mean_episode_return = None, mean_episode_step = 1509.5, total_loss = -204.83, pg_loss = -201.97, baseline_loss = 2.3154, entropy_loss = -5.1756, learner_queue_size = 32, train_seconds = 2.3217e+04, _tick = 8664, _time = 1.6546e+09)
[2022-06-07 16:50:49,034][root][INFO] - Step 41689600 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41689600, mean_episode_return = -7.2702, mean_episode_step = 1746.1, total_loss = -69.251, pg_loss = -96.569, baseline_loss = 32.493, entropy_loss = -5.174, learner_queue_size = 32, train_seconds = 2.3222e+04, _tick = 8666, _time = 1.6546e+09)
[2022-06-07 16:50:54,040][root][INFO] - Step 41697280 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41697280, mean_episode_return = 39.79, mean_episode_step = 2417.9, total_loss = 24.902, pg_loss = -24.429, baseline_loss = 54.517, entropy_loss = -5.1861, learner_queue_size = 32, train_seconds = 2.3227e+04, _tick = 8669, _time = 1.6546e+09)
[2022-06-07 16:50:59,046][root][INFO] - Step 41707520 @ 2045.5 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 41707520, mean_episode_return = 45.49, mean_episode_step = 1860.9, total_loss = 135.41, pg_loss = 75.334, baseline_loss = 65.263, entropy_loss = -5.1919, learner_queue_size = 32, train_seconds = 2.3232e+04, _tick = 8671, _time = 1.6546e+09)
[2022-06-07 16:51:04,052][root][INFO] - Step 41715200 @ 1534.1 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 41715200, mean_episode_return = 64.079, mean_episode_step = 2144.3, total_loss = 233.71, pg_loss = 137.32, baseline_loss = 101.62, entropy_loss = -5.2244, learner_queue_size = 32, train_seconds = 2.3237e+04, _tick = 8673, _time = 1.6546e+09)
[2022-06-07 16:51:09,058][root][INFO] - Step 41722880 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41722880, mean_episode_return = 47.615, mean_episode_step = 2459.1, total_loss = -148.15, pg_loss = -177.14, baseline_loss = 34.125, entropy_loss = -5.1356, learner_queue_size = 32, train_seconds = 2.3242e+04, _tick = 8676, _time = 1.6546e+09)
[2022-06-07 16:51:14,062][root][INFO] - Step 41733120 @ 2046.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (step = 41733120, mean_episode_return = None, mean_episode_step = 2149.6, total_loss = 36.047, pg_loss = 5.8629, baseline_loss = 35.366, entropy_loss = -5.1819, learner_queue_size = 32, train_seconds = 2.3247e+04, _tick = 8678, _time = 1.6546e+09)
[2022-06-07 16:51:19,068][root][INFO] - Step 41740800 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41740800, mean_episode_return = 93.599, mean_episode_step = 2467.6, total_loss = 57.857, pg_loss = -13.818, baseline_loss = 76.804, entropy_loss = -5.1295, learner_queue_size = 32, train_seconds = 2.3252e+04, _tick = 8680, _time = 1.6546e+09)
[2022-06-07 16:51:24,104][root][INFO] - Step 41751040 @ 2033.3 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 41751040, mean_episode_return = 66.524, mean_episode_step = 2127.2, total_loss = -7.0207, pg_loss = -67.442, baseline_loss = 65.611, entropy_loss = -5.1893, learner_queue_size = 32, train_seconds = 2.3257e+04, _tick = 8684, _time = 1.6546e+09)
[2022-06-07 16:51:29,110][root][INFO] - Step 41758720 @ 1534.2 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41758720, mean_episode_return = None, mean_episode_step = 2565.3, total_loss = 17.803, pg_loss = -6.9847, baseline_loss = 29.964, entropy_loss = -5.1759, learner_queue_size = 32, train_seconds = 2.3262e+04, _tick = 8685, _time = 1.6546e+09)
[2022-06-07 16:51:34,114][root][INFO] - Step 41768960 @ 2046.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 41768960, mean_episode_return = 135.57, mean_episode_step = 1610.1, total_loss = 294.06, pg_loss = 198.45, baseline_loss = 100.7, entropy_loss = -5.0947, learner_queue_size = 32, train_seconds = 2.3267e+04, _tick = 8689, _time = 1.6546e+09)
[2022-06-07 16:51:39,118][root][INFO] - Step 41776640 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 41776640, mean_episode_return = 50.975, mean_episode_step = 2601.9, total_loss = 183.68, pg_loss = 105.89, baseline_loss = 82.908, entropy_loss = -5.1215, learner_queue_size = 32, train_seconds = 2.3272e+04, _tick = 8692, _time = 1.6546e+09)
[2022-06-07 16:51:44,122][root][INFO] - Step 41786880 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 41786880, mean_episode_return = 12.22, mean_episode_step = 1540.1, total_loss = 238.84, pg_loss = 157.44, baseline_loss = 86.425, entropy_loss = -5.032, learner_queue_size = 32, train_seconds = 2.3277e+04, _tick = 8695, _time = 1.6546e+09)
[2022-06-07 16:51:49,126][root][INFO] - Step 41794560 @ 1534.8 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (step = 41794560, mean_episode_return = 77.923, mean_episode_step = 1833.7, total_loss = 5.5749, pg_loss = -58.613, baseline_loss = 69.173, entropy_loss = -4.9843, learner_queue_size = 32, train_seconds = 2.3282e+04, _tick = 8698, _time = 1.6546e+09)
[2022-06-07 16:51:54,132][root][INFO] - Step 41804800 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 41804800, mean_episode_return = 48.843, mean_episode_step = 1909.6, total_loss = -27.619, pg_loss = -69.701, baseline_loss = 46.946, entropy_loss = -4.8635, learner_queue_size = 32, train_seconds = 2.3287e+04, _tick = 8701, _time = 1.6546e+09)
[2022-06-07 16:51:59,138][root][INFO] - Step 41812480 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41812480, mean_episode_return = None, mean_episode_step = 2209.2, total_loss = 4.091, pg_loss = -9.4453, baseline_loss = 18.429, entropy_loss = -4.8928, learner_queue_size = 32, train_seconds = 2.3292e+04, _tick = 8703, _time = 1.6546e+09)
[2022-06-07 16:52:04,142][root][INFO] - Step 41822720 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41822720, mean_episode_return = 47.444, mean_episode_step = 1557.5, total_loss = 484.32, pg_loss = 291.16, baseline_loss = 198.07, entropy_loss = -4.9097, learner_queue_size = 32, train_seconds = 2.3297e+04, _tick = 8706, _time = 1.6546e+09)
[2022-06-07 16:52:09,146][root][INFO] - Step 41830400 @ 1534.8 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 41830400, mean_episode_return = None, mean_episode_step = 2413.5, total_loss = 262.12, pg_loss = 151.71, baseline_loss = 115.38, entropy_loss = -4.9682, learner_queue_size = 32, train_seconds = 2.3302e+04, _tick = 8706, _time = 1.6546e+09)
[2022-06-07 16:52:14,150][root][INFO] - Step 41840640 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41840640, mean_episode_return = 44.701, mean_episode_step = 1529.6, total_loss = 390.54, pg_loss = 260.9, baseline_loss = 134.71, entropy_loss = -5.0695, learner_queue_size = 32, train_seconds = 2.3307e+04, _tick = 8708, _time = 1.6546e+09)
[2022-06-07 16:52:19,156][root][INFO] - Step 41848320 @ 1534.1 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 41848320, mean_episode_return = None, mean_episode_step = 2233.0, total_loss = 480.51, pg_loss = 335.53, baseline_loss = 150.08, entropy_loss = -5.0996, learner_queue_size = 32, train_seconds = 2.3312e+04, _tick = 8708, _time = 1.6546e+09)
[2022-06-07 16:52:24,162][root][INFO] - Step 41858560 @ 2045.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41858560, mean_episode_return = None, mean_episode_step = 2352.2, total_loss = 371.99, pg_loss = 252.13, baseline_loss = 124.89, entropy_loss = -5.0315, learner_queue_size = 32, train_seconds = 2.3317e+04, _tick = 8708, _time = 1.6546e+09)
[2022-06-07 16:52:29,166][root][INFO] - Step 41866240 @ 1534.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41866240, mean_episode_return = 105.75, mean_episode_step = 2041.9, total_loss = 201.96, pg_loss = 127.08, baseline_loss = 79.958, entropy_loss = -5.0874, learner_queue_size = 32, train_seconds = 2.3322e+04, _tick = 8711, _time = 1.6546e+09)
[2022-06-07 16:52:34,170][root][INFO] - Step 41876480 @ 2046.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (step = 41876480, mean_episode_return = None, mean_episode_step = 2000.6, total_loss = 34.175, pg_loss = -1.7125, baseline_loss = 40.937, entropy_loss = -5.0496, learner_queue_size = 32, train_seconds = 2.3327e+04, _tick = 8713, _time = 1.6546e+09)
[2022-06-07 16:52:39,174][root][INFO] - Step 41886720 @ 2046.4 SPS. Inference batcher size: 53. Learner queue size: 32. Other stats: (step = 41886720, mean_episode_return = None, mean_episode_step = 2066.7, total_loss = -103.08, pg_loss = -125.67, baseline_loss = 27.66, entropy_loss = -5.0675, learner_queue_size = 32, train_seconds = 2.3332e+04, _tick = 8715, _time = 1.6546e+09)
[2022-06-07 16:52:44,180][root][INFO] - Step 41894400 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 41894400, mean_episode_return = None, mean_episode_step = 1623.6, total_loss = 291.23, pg_loss = 211.53, baseline_loss = 84.779, entropy_loss = -5.0812, learner_queue_size = 32, train_seconds = 2.3337e+04, _tick = 8715, _time = 1.6546e+09)
[2022-06-07 16:52:49,186][root][INFO] - Step 41904640 @ 2045.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 41904640, mean_episode_return = -3.19, mean_episode_step = 1737.2, total_loss = 338.64, pg_loss = 251.46, baseline_loss = 92.267, entropy_loss = -5.0898, learner_queue_size = 32, train_seconds = 2.3342e+04, _tick = 8718, _time = 1.6546e+09)
[2022-06-07 16:52:54,192][root][INFO] - Step 41912320 @ 1534.1 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 41912320, mean_episode_return = 45.911, mean_episode_step = 2052.2, total_loss = 165.36, pg_loss = 96.081, baseline_loss = 74.358, entropy_loss = -5.0823, learner_queue_size = 32, train_seconds = 2.3347e+04, _tick = 8721, _time = 1.6546e+09)
[2022-06-07 16:52:59,198][root][INFO] - Step 41922560 @ 2045.6 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 41922560, mean_episode_return = None, mean_episode_step = 1800.1, total_loss = 6.4783, pg_loss = -13.627, baseline_loss = 25.109, entropy_loss = -5.0028, learner_queue_size = 32, train_seconds = 2.3352e+04, _tick = 8723, _time = 1.6546e+09)
[2022-06-07 16:53:04,202][root][INFO] - Step 41930240 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 41930240, mean_episode_return = -0.97027, mean_episode_step = 1994.1, total_loss = 224.93, pg_loss = 125.87, baseline_loss = 104.1, entropy_loss = -5.0442, learner_queue_size = 32, train_seconds = 2.3357e+04, _tick = 8725, _time = 1.6546e+09)
[2022-06-07 16:53:09,206][root][INFO] - Step 41940480 @ 2046.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 41940480, mean_episode_return = 94.31, mean_episode_step = 2006.0, total_loss = 105.69, pg_loss = 32.045, baseline_loss = 78.635, entropy_loss = -4.9858, learner_queue_size = 32, train_seconds = 2.3362e+04, _tick = 8729, _time = 1.6546e+09)
[2022-06-07 16:53:14,210][root][INFO] - Step 41948160 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 41948160, mean_episode_return = 70.701, mean_episode_step = 2250.6, total_loss = 0.028881, pg_loss = -39.184, baseline_loss = 44.201, entropy_loss = -4.9881, learner_queue_size = 32, train_seconds = 2.3367e+04, _tick = 8731, _time = 1.6546e+09)
[2022-06-07 16:53:19,214][root][INFO] - Step 41958400 @ 2046.4 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 41958400, mean_episode_return = 57.333, mean_episode_step = 1653.4, total_loss = -140.58, pg_loss = -172.82, baseline_loss = 37.325, entropy_loss = -5.0797, learner_queue_size = 32, train_seconds = 2.3372e+04, _tick = 8733, _time = 1.6546e+09)
[2022-06-07 16:53:24,218][root][INFO] - Step 41968640 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 41968640, mean_episode_return = None, mean_episode_step = 1760.0, total_loss = 524.55, pg_loss = 355.57, baseline_loss = 174.09, entropy_loss = -5.1145, learner_queue_size = 32, train_seconds = 2.3377e+04, _tick = 8734, _time = 1.6546e+09)
[2022-06-07 16:53:29,222][root][INFO] - Step 41976320 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 41976320, mean_episode_return = None, mean_episode_step = 1950.1, total_loss = 3.8276, pg_loss = -34.96, baseline_loss = 43.954, entropy_loss = -5.1662, learner_queue_size = 32, train_seconds = 2.3382e+04, _tick = 8734, _time = 1.6546e+09)
[2022-06-07 16:53:34,228][root][INFO] - Step 41984000 @ 1534.1 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 41984000, mean_episode_return = 58.082, mean_episode_step = 1882.2, total_loss = -84.57, pg_loss = -107.15, baseline_loss = 27.781, entropy_loss = -5.202, learner_queue_size = 32, train_seconds = 2.3387e+04, _tick = 8737, _time = 1.6546e+09)
[2022-06-07 16:53:39,234][root][INFO] - Step 41994240 @ 2045.6 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 41994240, mean_episode_return = 17.13, mean_episode_step = 1820.4, total_loss = 290.52, pg_loss = 151.71, baseline_loss = 144.07, entropy_loss = -5.2512, learner_queue_size = 32, train_seconds = 2.3392e+04, _tick = 8741, _time = 1.6546e+09)
[2022-06-07 16:53:44,238][root][INFO] - Step 42001920 @ 1534.8 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (step = 42001920, mean_episode_return = 48.581, mean_episode_step = 2235.7, total_loss = -34.951, pg_loss = -70.554, baseline_loss = 40.831, entropy_loss = -5.2278, learner_queue_size = 32, train_seconds = 2.3397e+04, _tick = 8743, _time = 1.6546e+09)
[2022-06-07 16:53:49,242][root][INFO] - Step 42012160 @ 2046.4 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (step = 42012160, mean_episode_return = 117.12, mean_episode_step = 2398.8, total_loss = -45.182, pg_loss = -95.473, baseline_loss = 55.464, entropy_loss = -5.1735, learner_queue_size = 32, train_seconds = 2.3402e+04, _tick = 8745, _time = 1.6546e+09)
[2022-06-07 16:53:54,246][root][INFO] - Step 42022400 @ 2046.4 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 42022400, mean_episode_return = 51.551, mean_episode_step = 2060.5, total_loss = 215.69, pg_loss = 118.15, baseline_loss = 102.64, entropy_loss = -5.0949, learner_queue_size = 32, train_seconds = 2.3407e+04, _tick = 8748, _time = 1.6546e+09)
[2022-06-07 16:53:59,258][root][INFO] - Step 42030080 @ 1532.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42030080, mean_episode_return = 33.74, mean_episode_step = 1906.9, total_loss = 479.09, pg_loss = 331.42, baseline_loss = 152.75, entropy_loss = -5.0688, learner_queue_size = 32, train_seconds = 2.3412e+04, _tick = 8751, _time = 1.6546e+09)
[2022-06-07 16:54:04,264][root][INFO] - Step 42037760 @ 1534.2 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42037760, mean_episode_return = 31.166, mean_episode_step = 1949.4, total_loss = 165.09, pg_loss = 73.986, baseline_loss = 96.155, entropy_loss = -5.0546, learner_queue_size = 32, train_seconds = 2.3417e+04, _tick = 8753, _time = 1.6546e+09)
[2022-06-07 16:54:09,269][root][INFO] - Step 42048000 @ 2045.9 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42048000, mean_episode_return = 18.74, mean_episode_step = 2068.5, total_loss = -157.53, pg_loss = -186.26, baseline_loss = 33.71, entropy_loss = -4.9741, learner_queue_size = 32, train_seconds = 2.3422e+04, _tick = 8757, _time = 1.6546e+09)
[2022-06-07 16:54:14,275][root][INFO] - Step 42055680 @ 1534.2 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (step = 42055680, mean_episode_return = None, mean_episode_step = 1962.6, total_loss = 143.49, pg_loss = 73.856, baseline_loss = 74.558, entropy_loss = -4.9251, learner_queue_size = 32, train_seconds = 2.3427e+04, _tick = 8759, _time = 1.6546e+09)
[2022-06-07 16:54:19,281][root][INFO] - Saving checkpoint to /home/jaelee/nethack/version_euiyulsong/neurips-2021-the-nethack-challenge2/nethack_baselines/torchbeast/outputs/2022-06-07/10-23-43/checkpoint.tar
[2022-06-07 16:54:19,479][root][INFO] - Step 42065920 @ 2045.5 SPS. Inference batcher size: 41. Learner queue size: 32. Other stats: (step = 42065920, mean_episode_return = None, mean_episode_step = 1677.7, total_loss = -89.712, pg_loss = -100.66, baseline_loss = 15.696, entropy_loss = -4.7448, learner_queue_size = 32, train_seconds = 2.3432e+04, _tick = 8761, _time = 1.6546e+09)
[2022-06-07 16:54:24,485][root][INFO] - Step 42073600 @ 1475.7 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42073600, mean_episode_return = 34.77, mean_episode_step = 1741.7, total_loss = 0.55056, pg_loss = -32.892, baseline_loss = 38.175, entropy_loss = -4.7325, learner_queue_size = 32, train_seconds = 2.3438e+04, _tick = 8763, _time = 1.6546e+09)
[2022-06-07 16:54:29,491][root][INFO] - Step 42083840 @ 2045.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42083840, mean_episode_return = None, mean_episode_step = 2284.0, total_loss = -109.02, pg_loss = -119.13, baseline_loss = 14.911, entropy_loss = -4.8025, learner_queue_size = 32, train_seconds = 2.3442e+04, _tick = 8763, _time = 1.6546e+09)
[2022-06-07 16:54:34,497][root][INFO] - Step 42091520 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42091520, mean_episode_return = None, mean_episode_step = 2120.4, total_loss = 94.73, pg_loss = 46.137, baseline_loss = 53.475, entropy_loss = -4.8815, learner_queue_size = 32, train_seconds = 2.3448e+04, _tick = 8765, _time = 1.6546e+09)
[2022-06-07 16:54:39,503][root][INFO] - Step 42101760 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42101760, mean_episode_return = 18.355, mean_episode_step = 1665.5, total_loss = 5.0655, pg_loss = -49.837, baseline_loss = 59.742, entropy_loss = -4.8395, learner_queue_size = 32, train_seconds = 2.3452e+04, _tick = 8769, _time = 1.6546e+09)
[2022-06-07 16:54:44,506][root][INFO] - Step 42109440 @ 1535.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 42109440, mean_episode_return = None, mean_episode_step = 2287.3, total_loss = -50.823, pg_loss = -99.175, baseline_loss = 53.2, entropy_loss = -4.8472, learner_queue_size = 32, train_seconds = 2.3458e+04, _tick = 8771, _time = 1.6546e+09)
[2022-06-07 16:54:49,512][root][INFO] - Step 42119680 @ 2045.4 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (step = 42119680, mean_episode_return = None, mean_episode_step = 1837.1, total_loss = 338.71, pg_loss = 207.54, baseline_loss = 135.98, entropy_loss = -4.8088, learner_queue_size = 32, train_seconds = 2.3462e+04, _tick = 8771, _time = 1.6546e+09)
[2022-06-07 16:54:54,518][root][INFO] - Step 42127360 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42127360, mean_episode_return = 46.256, mean_episode_step = 2629.0, total_loss = 68.706, pg_loss = -14.546, baseline_loss = 88.07, entropy_loss = -4.819, learner_queue_size = 32, train_seconds = 2.3468e+04, _tick = 8772, _time = 1.6546e+09)
[2022-06-07 16:54:59,522][root][INFO] - Step 42137600 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42137600, mean_episode_return = -45.049, mean_episode_step = 2297.5, total_loss = 263.04, pg_loss = 175.56, baseline_loss = 92.458, entropy_loss = -4.9773, learner_queue_size = 32, train_seconds = 2.3472e+04, _tick = 8776, _time = 1.6546e+09)
[2022-06-07 16:55:04,526][root][INFO] - Step 42145280 @ 1534.8 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 42145280, mean_episode_return = 36.004, mean_episode_step = 1991.3, total_loss = 34.473, pg_loss = -6.3952, baseline_loss = 45.875, entropy_loss = -5.0074, learner_queue_size = 32, train_seconds = 2.3478e+04, _tick = 8779, _time = 1.6546e+09)
[2022-06-07 16:55:09,530][root][INFO] - Step 42155520 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42155520, mean_episode_return = 55.421, mean_episode_step = 1778.9, total_loss = 308.37, pg_loss = 205.56, baseline_loss = 107.86, entropy_loss = -5.0529, learner_queue_size = 32, train_seconds = 2.3482e+04, _tick = 8782, _time = 1.6546e+09)
[2022-06-07 16:55:14,534][root][INFO] - Step 42163200 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42163200, mean_episode_return = 81.369, mean_episode_step = 1971.4, total_loss = 114.5, pg_loss = 12.397, baseline_loss = 107.17, entropy_loss = -5.0647, learner_queue_size = 32, train_seconds = 2.3488e+04, _tick = 8784, _time = 1.6546e+09)
[2022-06-07 16:55:19,538][root][INFO] - Step 42173440 @ 2046.4 SPS. Inference batcher size: 62. Learner queue size: 32. Other stats: (step = 42173440, mean_episode_return = None, mean_episode_step = 2416.8, total_loss = 345.72, pg_loss = 221.54, baseline_loss = 129.2, entropy_loss = -5.015, learner_queue_size = 32, train_seconds = 2.3492e+04, _tick = 8785, _time = 1.6546e+09)
[2022-06-07 16:55:24,542][root][INFO] - Step 42181120 @ 1534.7 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 42181120, mean_episode_return = 35.194, mean_episode_step = 2393.0, total_loss = 360.15, pg_loss = 232.35, baseline_loss = 132.82, entropy_loss = -5.0151, learner_queue_size = 32, train_seconds = 2.3498e+04, _tick = 8787, _time = 1.6546e+09)
[2022-06-07 16:55:29,548][root][INFO] - Step 42191360 @ 2045.6 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (step = 42191360, mean_episode_return = None, mean_episode_step = 2031.0, total_loss = 26.401, pg_loss = -10.15, baseline_loss = 41.537, entropy_loss = -4.9859, learner_queue_size = 32, train_seconds = 2.3502e+04, _tick = 8788, _time = 1.6546e+09)
[2022-06-07 16:55:34,554][root][INFO] - Step 42199040 @ 1534.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42199040, mean_episode_return = 60.415, mean_episode_step = 2405.5, total_loss = 95.115, pg_loss = 43.788, baseline_loss = 56.205, entropy_loss = -4.8788, learner_queue_size = 32, train_seconds = 2.3508e+04, _tick = 8790, _time = 1.6546e+09)
[2022-06-07 16:55:39,558][root][INFO] - Step 42209280 @ 2046.4 SPS. Inference batcher size: 37. Learner queue size: 32. Other stats: (step = 42209280, mean_episode_return = None, mean_episode_step = 1838.4, total_loss = 195.41, pg_loss = 117.32, baseline_loss = 82.926, entropy_loss = -4.8406, learner_queue_size = 32, train_seconds = 2.3513e+04, _tick = 8792, _time = 1.6546e+09)
[2022-06-07 16:55:44,562][root][INFO] - Step 42216960 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42216960, mean_episode_return = 33.428, mean_episode_step = 2200.3, total_loss = -108.96, pg_loss = -169.66, baseline_loss = 65.57, entropy_loss = -4.8699, learner_queue_size = 32, train_seconds = 2.3518e+04, _tick = 8793, _time = 1.6546e+09)
[2022-06-07 16:55:49,567][root][INFO] - Step 42227200 @ 2045.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42227200, mean_episode_return = -5.28, mean_episode_step = 2408.3, total_loss = -113.86, pg_loss = -149.2, baseline_loss = 40.242, entropy_loss = -4.8997, learner_queue_size = 32, train_seconds = 2.3523e+04, _tick = 8797, _time = 1.6546e+09)
[2022-06-07 16:55:54,570][root][INFO] - Step 42234880 @ 1535.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 42234880, mean_episode_return = 32.961, mean_episode_step = 1826.0, total_loss = -60.039, pg_loss = -90.856, baseline_loss = 35.684, entropy_loss = -4.8673, learner_queue_size = 32, train_seconds = 2.3528e+04, _tick = 8799, _time = 1.6546e+09)
[2022-06-07 16:55:59,574][root][INFO] - Step 42245120 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 42245120, mean_episode_return = -2.3703, mean_episode_step = 1725.2, total_loss = 19.095, pg_loss = -4.305, baseline_loss = 28.335, entropy_loss = -4.9354, learner_queue_size = 32, train_seconds = 2.3533e+04, _tick = 8802, _time = 1.6546e+09)
[2022-06-07 16:56:04,578][root][INFO] - Step 42252800 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42252800, mean_episode_return = None, mean_episode_step = 1841.9, total_loss = 140.72, pg_loss = 67.426, baseline_loss = 78.287, entropy_loss = -4.9915, learner_queue_size = 32, train_seconds = 2.3538e+04, _tick = 8803, _time = 1.6546e+09)
[2022-06-07 16:56:09,582][root][INFO] - Step 42263040 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 42263040, mean_episode_return = None, mean_episode_step = 1428.0, total_loss = 209.12, pg_loss = 133.67, baseline_loss = 80.372, entropy_loss = -4.9251, learner_queue_size = 32, train_seconds = 2.3543e+04, _tick = 8805, _time = 1.6546e+09)
[2022-06-07 16:56:14,586][root][INFO] - Step 42270720 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42270720, mean_episode_return = 43.313, mean_episode_step = 2611.7, total_loss = 17.904, pg_loss = -7.4392, baseline_loss = 30.343, entropy_loss = -4.9994, learner_queue_size = 32, train_seconds = 2.3548e+04, _tick = 8807, _time = 1.6546e+09)
[2022-06-07 16:56:19,590][root][INFO] - Step 42280960 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42280960, mean_episode_return = None, mean_episode_step = 1717.1, total_loss = -58.094, pg_loss = -65.228, baseline_loss = 12.248, entropy_loss = -5.1138, learner_queue_size = 32, train_seconds = 2.3553e+04, _tick = 8808, _time = 1.6546e+09)
[2022-06-07 16:56:24,594][root][INFO] - Step 42288640 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42288640, mean_episode_return = 109.73, mean_episode_step = 2012.6, total_loss = -40.333, pg_loss = -66.856, baseline_loss = 31.722, entropy_loss = -5.1994, learner_queue_size = 32, train_seconds = 2.3558e+04, _tick = 8810, _time = 1.6546e+09)
[2022-06-07 16:56:29,598][root][INFO] - Step 42298880 @ 2046.3 SPS. Inference batcher size: 55. Learner queue size: 32. Other stats: (step = 42298880, mean_episode_return = 57.631, mean_episode_step = 2325.7, total_loss = 138.28, pg_loss = 76.217, baseline_loss = 67.24, entropy_loss = -5.1767, learner_queue_size = 32, train_seconds = 2.3563e+04, _tick = 8812, _time = 1.6546e+09)
[2022-06-07 16:56:34,602][root][INFO] - Step 42306560 @ 1534.8 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42306560, mean_episode_return = 68.958, mean_episode_step = 1725.1, total_loss = 673.47, pg_loss = 481.98, baseline_loss = 196.64, entropy_loss = -5.1433, learner_queue_size = 32, train_seconds = 2.3568e+04, _tick = 8813, _time = 1.6546e+09)
[2022-06-07 16:56:39,606][root][INFO] - Step 42316800 @ 2046.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 42316800, mean_episode_return = 53.328, mean_episode_step = 1946.3, total_loss = 106.75, pg_loss = 79.051, baseline_loss = 32.891, entropy_loss = -5.1888, learner_queue_size = 32, train_seconds = 2.3573e+04, _tick = 8817, _time = 1.6546e+09)
[2022-06-07 16:56:44,610][root][INFO] - Step 42324480 @ 1534.8 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (step = 42324480, mean_episode_return = None, mean_episode_step = 2076.4, total_loss = 114.33, pg_loss = 89.656, baseline_loss = 29.871, entropy_loss = -5.1981, learner_queue_size = 32, train_seconds = 2.3578e+04, _tick = 8819, _time = 1.6546e+09)
[2022-06-07 16:56:49,614][root][INFO] - Step 42334720 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 42334720, mean_episode_return = 97.156, mean_episode_step = 2590.5, total_loss = -80.476, pg_loss = -100.77, baseline_loss = 25.415, entropy_loss = -5.1227, learner_queue_size = 32, train_seconds = 2.3583e+04, _tick = 8822, _time = 1.6546e+09)
[2022-06-07 16:56:54,618][root][INFO] - Step 42342400 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 42342400, mean_episode_return = None, mean_episode_step = 1886.0, total_loss = 180.81, pg_loss = 119.95, baseline_loss = 66.013, entropy_loss = -5.1553, learner_queue_size = 32, train_seconds = 2.3588e+04, _tick = 8823, _time = 1.6546e+09)
[2022-06-07 16:56:59,622][root][INFO] - Step 42352640 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 42352640, mean_episode_return = None, mean_episode_step = 1756.6, total_loss = -44.421, pg_loss = -57.203, baseline_loss = 17.944, entropy_loss = -5.1615, learner_queue_size = 32, train_seconds = 2.3593e+04, _tick = 8825, _time = 1.6546e+09)
[2022-06-07 16:57:04,626][root][INFO] - Step 42360320 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42360320, mean_episode_return = None, mean_episode_step = 2091.2, total_loss = 673.45, pg_loss = 504.53, baseline_loss = 174.11, entropy_loss = -5.1991, learner_queue_size = 32, train_seconds = 2.3598e+04, _tick = 8826, _time = 1.6546e+09)
[2022-06-07 16:57:09,630][root][INFO] - Step 42370560 @ 2046.4 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42370560, mean_episode_return = None, mean_episode_step = 2422.6, total_loss = -147.96, pg_loss = -146.46, baseline_loss = 3.6972, entropy_loss = -5.192, learner_queue_size = 32, train_seconds = 2.3603e+04, _tick = 8827, _time = 1.6546e+09)
[2022-06-07 16:57:14,634][root][INFO] - Step 42378240 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 42378240, mean_episode_return = 51.281, mean_episode_step = 2030.1, total_loss = 326.26, pg_loss = 215.58, baseline_loss = 115.88, entropy_loss = -5.2023, learner_queue_size = 32, train_seconds = 2.3608e+04, _tick = 8829, _time = 1.6546e+09)
[2022-06-07 16:57:19,638][root][INFO] - Step 42388480 @ 2046.3 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 42388480, mean_episode_return = 81.527, mean_episode_step = 2082.4, total_loss = -23.424, pg_loss = -86.41, baseline_loss = 68.156, entropy_loss = -5.1706, learner_queue_size = 32, train_seconds = 2.3613e+04, _tick = 8833, _time = 1.6546e+09)
[2022-06-07 16:57:24,644][root][INFO] - Step 42396160 @ 1534.2 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42396160, mean_episode_return = 6.1802, mean_episode_step = 1974.4, total_loss = -175.0, pg_loss = -183.34, baseline_loss = 13.466, entropy_loss = -5.1265, learner_queue_size = 32, train_seconds = 2.3618e+04, _tick = 8835, _time = 1.6546e+09)
[2022-06-07 16:57:29,650][root][INFO] - Step 42406400 @ 2045.6 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 42406400, mean_episode_return = 52.56, mean_episode_step = 1725.1, total_loss = 497.25, pg_loss = 366.58, baseline_loss = 135.8, entropy_loss = -5.1318, learner_queue_size = 32, train_seconds = 2.3623e+04, _tick = 8838, _time = 1.6546e+09)
[2022-06-07 16:57:34,656][root][INFO] - Step 42414080 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42414080, mean_episode_return = None, mean_episode_step = 1780.2, total_loss = -84.523, pg_loss = -94.121, baseline_loss = 14.709, entropy_loss = -5.1114, learner_queue_size = 32, train_seconds = 2.3628e+04, _tick = 8839, _time = 1.6546e+09)
[2022-06-07 16:57:39,662][root][INFO] - Step 42424320 @ 2045.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 42424320, mean_episode_return = None, mean_episode_step = 2343.4, total_loss = 70.596, pg_loss = 44.55, baseline_loss = 31.109, entropy_loss = -5.0628, learner_queue_size = 32, train_seconds = 2.3633e+04, _tick = 8840, _time = 1.6546e+09)
[2022-06-07 16:57:44,666][root][INFO] - Step 42434560 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42434560, mean_episode_return = None, mean_episode_step = 2325.7, total_loss = 59.583, pg_loss = 24.615, baseline_loss = 40.043, entropy_loss = -5.0744, learner_queue_size = 32, train_seconds = 2.3638e+04, _tick = 8842, _time = 1.6546e+09)
[2022-06-07 16:57:49,678][root][INFO] - Step 42442240 @ 1532.3 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (step = 42442240, mean_episode_return = 56.724, mean_episode_step = 1997.7, total_loss = 140.28, pg_loss = 58.54, baseline_loss = 86.779, entropy_loss = -5.0402, learner_queue_size = 32, train_seconds = 2.3643e+04, _tick = 8845, _time = 1.6546e+09)
[2022-06-07 16:57:54,682][root][INFO] - Step 42452480 @ 2046.4 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 42452480, mean_episode_return = 23.41, mean_episode_step = 2353.7, total_loss = -12.127, pg_loss = -35.321, baseline_loss = 28.122, entropy_loss = -4.9288, learner_queue_size = 32, train_seconds = 2.3648e+04, _tick = 8848, _time = 1.6546e+09)
[2022-06-07 16:57:59,686][root][INFO] - Step 42460160 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42460160, mean_episode_return = 102.3, mean_episode_step = 2081.2, total_loss = 59.396, pg_loss = 7.0372, baseline_loss = 57.201, entropy_loss = -4.8422, learner_queue_size = 32, train_seconds = 2.3653e+04, _tick = 8850, _time = 1.6546e+09)
[2022-06-07 16:58:04,690][root][INFO] - Step 42470400 @ 2046.4 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 42470400, mean_episode_return = None, mean_episode_step = 2049.3, total_loss = 616.44, pg_loss = 445.36, baseline_loss = 175.92, entropy_loss = -4.838, learner_queue_size = 32, train_seconds = 2.3658e+04, _tick = 8851, _time = 1.6546e+09)
[2022-06-07 16:58:09,696][root][INFO] - Step 42478080 @ 1534.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 42478080, mean_episode_return = 125.01, mean_episode_step = 1950.0, total_loss = 120.75, pg_loss = 69.27, baseline_loss = 56.307, entropy_loss = -4.8275, learner_queue_size = 32, train_seconds = 2.3663e+04, _tick = 8853, _time = 1.6546e+09)
[2022-06-07 16:58:14,702][root][INFO] - Step 42488320 @ 2045.6 SPS. Inference batcher size: 39. Learner queue size: 32. Other stats: (step = 42488320, mean_episode_return = 6.1796, mean_episode_step = 1944.0, total_loss = -107.16, pg_loss = -140.09, baseline_loss = 37.746, entropy_loss = -4.8194, learner_queue_size = 32, train_seconds = 2.3668e+04, _tick = 8855, _time = 1.6546e+09)
[2022-06-07 16:58:19,708][root][INFO] - Step 42496000 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42496000, mean_episode_return = 51.174, mean_episode_step = 1860.1, total_loss = 572.07, pg_loss = 323.15, baseline_loss = 253.74, entropy_loss = -4.8173, learner_queue_size = 32, train_seconds = 2.3673e+04, _tick = 8856, _time = 1.6546e+09)
[2022-06-07 16:58:24,714][root][INFO] - Step 42506240 @ 2045.6 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42506240, mean_episode_return = 62.499, mean_episode_step = 2389.3, total_loss = -170.63, pg_loss = -195.83, baseline_loss = 30.021, entropy_loss = -4.8145, learner_queue_size = 32, train_seconds = 2.3678e+04, _tick = 8859, _time = 1.6546e+09)
[2022-06-07 16:58:29,720][root][INFO] - Step 42513920 @ 1534.1 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42513920, mean_episode_return = None, mean_episode_step = 2067.0, total_loss = 220.87, pg_loss = 135.05, baseline_loss = 90.638, entropy_loss = -4.8126, learner_queue_size = 32, train_seconds = 2.3683e+04, _tick = 8860, _time = 1.6546e+09)
[2022-06-07 16:58:34,726][root][INFO] - Step 42524160 @ 2045.6 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 42524160, mean_episode_return = 87.568, mean_episode_step = 1468.8, total_loss = 105.52, pg_loss = 17.649, baseline_loss = 92.707, entropy_loss = -4.8316, learner_queue_size = 32, train_seconds = 2.3688e+04, _tick = 8863, _time = 1.6546e+09)
[2022-06-07 16:58:39,730][root][INFO] - Step 42531840 @ 1534.8 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (step = 42531840, mean_episode_return = 82.279, mean_episode_step = 2258.8, total_loss = 163.82, pg_loss = 97.005, baseline_loss = 71.569, entropy_loss = -4.7567, learner_queue_size = 32, train_seconds = 2.3693e+04, _tick = 8866, _time = 1.6546e+09)
[2022-06-07 16:58:44,734][root][INFO] - Step 42542080 @ 2046.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42542080, mean_episode_return = 5.9493, mean_episode_step = 1880.0, total_loss = 544.88, pg_loss = 370.66, baseline_loss = 178.97, entropy_loss = -4.7486, learner_queue_size = 32, train_seconds = 2.3698e+04, _tick = 8869, _time = 1.6546e+09)
[2022-06-07 16:58:49,738][root][INFO] - Step 42549760 @ 1534.8 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (step = 42549760, mean_episode_return = 176.09, mean_episode_step = 1827.2, total_loss = -195.64, pg_loss = -201.17, baseline_loss = 10.33, entropy_loss = -4.8042, learner_queue_size = 32, train_seconds = 2.3703e+04, _tick = 8871, _time = 1.6546e+09)
[2022-06-07 16:58:54,742][root][INFO] - Step 42560000 @ 2046.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 42560000, mean_episode_return = 61.742, mean_episode_step = 1646.9, total_loss = -94.805, pg_loss = -123.12, baseline_loss = 33.082, entropy_loss = -4.7683, learner_queue_size = 32, train_seconds = 2.3708e+04, _tick = 8875, _time = 1.6546e+09)
[2022-06-07 16:58:59,746][root][INFO] - Step 42567680 @ 1534.8 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (step = 42567680, mean_episode_return = 76.105, mean_episode_step = 2182.4, total_loss = -272.45, pg_loss = -282.93, baseline_loss = 15.195, entropy_loss = -4.7149, learner_queue_size = 32, train_seconds = 2.3713e+04, _tick = 8877, _time = 1.6546e+09)
[2022-06-07 16:59:04,759][root][INFO] - Step 42577920 @ 2042.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 42577920, mean_episode_return = None, mean_episode_step = 2369.6, total_loss = -231.02, pg_loss = -229.72, baseline_loss = 3.4391, entropy_loss = -4.7407, learner_queue_size = 32, train_seconds = 2.3718e+04, _tick = 8879, _time = 1.6546e+09)
[2022-06-07 16:59:09,762][root][INFO] - Step 42585600 @ 1535.0 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 42585600, mean_episode_return = None, mean_episode_step = 1838.2, total_loss = 220.28, pg_loss = 133.44, baseline_loss = 91.502, entropy_loss = -4.6661, learner_queue_size = 32, train_seconds = 2.3723e+04, _tick = 8880, _time = 1.6546e+09)
[2022-06-07 16:59:14,764][root][INFO] - Step 42595840 @ 2047.1 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 42595840, mean_episode_return = 19.247, mean_episode_step = 1601.8, total_loss = 218.78, pg_loss = 121.46, baseline_loss = 101.99, entropy_loss = -4.6686, learner_queue_size = 32, train_seconds = 2.3728e+04, _tick = 8884, _time = 1.6546e+09)
[2022-06-07 16:59:19,770][root][INFO] - Step 42603520 @ 1534.1 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 42603520, mean_episode_return = None, mean_episode_step = 2011.6, total_loss = 465.75, pg_loss = 326.91, baseline_loss = 143.45, entropy_loss = -4.6144, learner_queue_size = 32, train_seconds = 2.3733e+04, _tick = 8886, _time = 1.6546e+09)
[2022-06-07 16:59:24,774][root][INFO] - Step 42613760 @ 2046.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (step = 42613760, mean_episode_return = -38.17, mean_episode_step = 2090.2, total_loss = 322.86, pg_loss = 219.71, baseline_loss = 107.84, entropy_loss = -4.6898, learner_queue_size = 32, train_seconds = 2.3738e+04, _tick = 8887, _time = 1.6546e+09)
[2022-06-07 16:59:29,778][root][INFO] - Step 42621440 @ 1534.8 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 42621440, mean_episode_return = None, mean_episode_step = 1850.2, total_loss = 63.293, pg_loss = 7.4487, baseline_loss = 60.55, entropy_loss = -4.7062, learner_queue_size = 32, train_seconds = 2.3743e+04, _tick = 8888, _time = 1.6546e+09)
[2022-06-07 16:59:34,782][root][INFO] - Step 42631680 @ 2046.4 SPS. Inference batcher size: 57. Learner queue size: 32. Other stats: (step = 42631680, mean_episode_return = None, mean_episode_step = 2261.7, total_loss = -71.453, pg_loss = -88.272, baseline_loss = 21.515, entropy_loss = -4.6965, learner_queue_size = 32, train_seconds = 2.3748e+04, _tick = 8890, _time = 1.6546e+09)
[2022-06-07 16:59:39,788][root][INFO] - Step 42639360 @ 1534.2 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 42639360, mean_episode_return = None, mean_episode_step = 2391.7, total_loss = 227.31, pg_loss = 129.12, baseline_loss = 102.94, entropy_loss = -4.7501, learner_queue_size = 32, train_seconds = 2.3753e+04, _tick = 8891, _time = 1.6546e+09)
[2022-06-07 16:59:44,794][root][INFO] - Step 42649600 @ 2045.6 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (step = 42649600, mean_episode_return = None, mean_episode_step = 2174.6, total_loss = 318.3, pg_loss = 218.4, baseline_loss = 104.58, entropy_loss = -4.6844, learner_queue_size = 32, train_seconds = 2.3758e+04, _tick = 8891, _time = 1.6546e+09)
[2022-06-07 16:59:49,798][root][INFO] - Step 42657280 @ 1534.8 SPS. Inference batcher size: 28. Learner queue size: 32. Other stats: (step = 42657280, mean_episode_return = None, mean_episode_step = 2271.7, total_loss = 434.47, pg_loss = 282.0, baseline_loss = 157.15, entropy_loss = -4.6834, learner_queue_size = 32, train_seconds = 2.3763e+04, _tick = 8893, _time = 1.6546e+09)
[2022-06-07 16:59:54,802][root][INFO] - Step 42667520 @ 2046.3 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42667520, mean_episode_return = 44.974, mean_episode_step = 1857.3, total_loss = -17.601, pg_loss = -64.691, baseline_loss = 51.786, entropy_loss = -4.6959, learner_queue_size = 32, train_seconds = 2.3768e+04, _tick = 8895, _time = 1.6546e+09)
[2022-06-07 16:59:59,806][root][INFO] - Step 42677760 @ 2046.3 SPS. Inference batcher size: 32. Learner queue size: 32. Other stats: (step = 42677760, mean_episode_return = None, mean_episode_step = 1940.1, total_loss = 271.34, pg_loss = 147.03, baseline_loss = 129.01, entropy_loss = -4.7044, learner_queue_size = 32, train_seconds = 2.3773e+04, _tick = 8897, _time = 1.6546e+09)
[2022-06-07 17:00:04,810][root][INFO] - Step 42685440 @ 1534.8 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (step = 42685440, mean_episode_return = None, mean_episode_step = 2485.3, total_loss = 159.1, pg_loss = 31.938, baseline_loss = 131.83, entropy_loss = -4.6673, learner_queue_size = 32, train_seconds = 2.3778e+04, _tick = 8898, _time = 1.6546e+09)
[2022-06-07 17:00:09,816][root][INFO] - Step 42695680 @ 2045.5 SPS. Inference batcher size: 35. Learner queue size: 32. Other stats: (step = 42695680, mean_episode_return = 24.47, mean_episode_step = 1936.5, total_loss = -134.2, pg_loss = -158.14, baseline_loss = 28.682, entropy_loss = -4.7413, learner_queue_size = 32, train_seconds = 2.3783e+04, _tick = 8901, _time = 1.6546e+09)
[2022-06-07 17:00:14,822][root][INFO] - Step 42703360 @ 1534.2 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 42703360, mean_episode_return = 7.7797, mean_episode_step = 1801.2, total_loss = 50.391, pg_loss = -4.4304, baseline_loss = 59.562, entropy_loss = -4.7403, learner_queue_size = 32, train_seconds = 2.3788e+04, _tick = 8903, _time = 1.6546e+09)
[2022-06-07 17:00:19,826][root][INFO] - Step 42713600 @ 2046.3 SPS. Inference batcher size: 33. Learner queue size: 32. Other stats: (step = 42713600, mean_episode_return = None, mean_episode_step = 2140.0, total_loss = 14.388, pg_loss = -30.636, baseline_loss = 49.649, entropy_loss = -4.6244, learner_queue_size = 32, train_seconds = 2.3793e+04, _tick = 8906, _time = 1.6546e+09)
[2022-06-07 17:00:24,830][root][INFO] - Step 42721280 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42721280, mean_episode_return = 4.6397, mean_episode_step = 1884.2, total_loss = 192.74, pg_loss = 98.073, baseline_loss = 99.307, entropy_loss = -4.6446, learner_queue_size = 32, train_seconds = 2.3798e+04, _tick = 8909, _time = 1.6546e+09)
[2022-06-07 17:00:29,834][root][INFO] - Step 42731520 @ 2046.3 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 42731520, mean_episode_return = 58.62, mean_episode_step = 2323.4, total_loss = -144.34, pg_loss = -159.4, baseline_loss = 19.728, entropy_loss = -4.6667, learner_queue_size = 32, train_seconds = 2.3803e+04, _tick = 8910, _time = 1.6546e+09)
[2022-06-07 17:00:34,838][root][INFO] - Step 42739200 @ 1534.8 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (step = 42739200, mean_episode_return = 116.97, mean_episode_step = 1678.1, total_loss = 127.22, pg_loss = 43.584, baseline_loss = 88.396, entropy_loss = -4.7582, learner_queue_size = 32, train_seconds = 2.3808e+04, _tick = 8912, _time = 1.6546e+09)
[2022-06-07 17:00:39,842][root][INFO] - Step 42749440 @ 2046.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (step = 42749440, mean_episode_return = 8.6695, mean_episode_step = 2379.4, total_loss = 551.49, pg_loss = 387.35, baseline_loss = 168.88, entropy_loss = -4.7429, learner_queue_size = 32, train_seconds = 2.3813e+04, _tick = 8915, _time = 1.6546e+09)
[2022-06-07 17:00:44,848][root][INFO] - Step 42757120 @ 1534.1 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42757120, mean_episode_return = 74.227, mean_episode_step = 2747.2, total_loss = -28.073, pg_loss = -102.47, baseline_loss = 79.147, entropy_loss = -4.7524, learner_queue_size = 32, train_seconds = 2.3818e+04, _tick = 8917, _time = 1.6546e+09)
[2022-06-07 17:00:49,854][root][INFO] - Step 42767360 @ 2045.6 SPS. Inference batcher size: 29. Learner queue size: 32. Other stats: (step = 42767360, mean_episode_return = None, mean_episode_step = 1598.7, total_loss = 192.71, pg_loss = 129.6, baseline_loss = 67.904, entropy_loss = -4.8002, learner_queue_size = 32, train_seconds = 2.3823e+04, _tick = 8919, _time = 1.6546e+09)
[2022-06-07 17:00:54,860][root][INFO] - Step 42775040 @ 1534.1 SPS. Inference batcher size: 30. Learner queue size: 32. Other stats: (step = 42775040, mean_episode_return = 89.117, mean_episode_step = 2174.7, total_loss = 132.22, pg_loss = 56.798, baseline_loss = 80.19, entropy_loss = -4.7639, learner_queue_size = 32, train_seconds = 2.3828e+04, _tick = 8922, _time = 1.6546e+09)
[2022-06-07 17:00:59,866][root][INFO] - Step 42785280 @ 2045.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (step = 42785280, mean_episode_return = -0.76542, mean_episode_step = 1527.2, total_loss = -71.152, pg_loss = -106.18, baseline_loss = 39.782, entropy_loss = -4.7546, learner_queue_size = 32, train_seconds = 2.3833e+04, _tick = 8924, _time = 1.6546e+09)
[2022-06-07 17:01:04,872][root][INFO] - Step 42792960 @ 1534.2 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 42792960, mean_episode_return = 48.821, mean_episode_step = 2058.8, total_loss = 239.52, pg_loss = 118.63, baseline_loss = 125.67, entropy_loss = -4.7769, learner_queue_size = 32, train_seconds = 2.3838e+04, _tick = 8927, _time = 1.6546e+09)
[2022-06-07 17:01:09,878][root][INFO] - Step 42803200 @ 2045.6 SPS. Inference batcher size: 43. Learner queue size: 32. Other stats: (step = 42803200, mean_episode_return = None, mean_episode_step = 1649.5, total_loss = 36.856, pg_loss = -0.59626, baseline_loss = 42.139, entropy_loss = -4.6863, learner_queue_size = 32, train_seconds = 2.3843e+04, _tick = 8929, _time = 1.6546e+09)
[2022-06-07 17:01:14,884][root][INFO] - Step 42810880 @ 1534.2 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42810880, mean_episode_return = None, mean_episode_step = 2288.5, total_loss = -171.02, pg_loss = -168.47, baseline_loss = 2.1188, entropy_loss = -4.6633, learner_queue_size = 32, train_seconds = 2.3848e+04, _tick = 8930, _time = 1.6546e+09)
[2022-06-07 17:01:19,890][root][INFO] - Step 42821120 @ 2045.5 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 42821120, mean_episode_return = 64.86, mean_episode_step = 1885.8, total_loss = -40.458, pg_loss = -85.264, baseline_loss = 49.458, entropy_loss = -4.6515, learner_queue_size = 32, train_seconds = 2.3853e+04, _tick = 8932, _time = 1.6546e+09)
[2022-06-07 17:01:24,894][root][INFO] - Step 42828800 @ 1534.8 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42828800, mean_episode_return = None, mean_episode_step = 2336.6, total_loss = 450.24, pg_loss = 309.34, baseline_loss = 145.59, entropy_loss = -4.6954, learner_queue_size = 32, train_seconds = 2.3858e+04, _tick = 8933, _time = 1.6546e+09)
[2022-06-07 17:01:29,898][root][INFO] - Step 42839040 @ 2046.4 SPS. Inference batcher size: 63. Learner queue size: 32. Other stats: (step = 42839040, mean_episode_return = 63.539, mean_episode_step = 1933.7, total_loss = -10.634, pg_loss = -66.271, baseline_loss = 60.314, entropy_loss = -4.6764, learner_queue_size = 32, train_seconds = 2.3863e+04, _tick = 8936, _time = 1.6546e+09)
[2022-06-07 17:01:34,902][root][INFO] - Step 42846720 @ 1534.7 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42846720, mean_episode_return = None, mean_episode_step = 2053.6, total_loss = -88.17, pg_loss = -110.82, baseline_loss = 27.333, entropy_loss = -4.6794, learner_queue_size = 32, train_seconds = 2.3868e+04, _tick = 8937, _time = 1.6546e+09)
[2022-06-07 17:01:39,906][root][INFO] - Step 42856960 @ 2046.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 42856960, mean_episode_return = 32.892, mean_episode_step = 1950.4, total_loss = 203.17, pg_loss = 101.76, baseline_loss = 106.16, entropy_loss = -4.7453, learner_queue_size = 32, train_seconds = 2.3873e+04, _tick = 8938, _time = 1.6546e+09)
[2022-06-07 17:01:44,912][root][INFO] - Step 42864640 @ 1534.1 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (step = 42864640, mean_episode_return = None, mean_episode_step = 2767.9, total_loss = -73.443, pg_loss = -97.331, baseline_loss = 28.66, entropy_loss = -4.7715, learner_queue_size = 32, train_seconds = 2.3878e+04, _tick = 8939, _time = 1.6546e+09)
[2022-06-07 17:01:49,918][root][INFO] - Step 42874880 @ 2045.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42874880, mean_episode_return = None, mean_episode_step = 2169.8, total_loss = 24.178, pg_loss = -6.9309, baseline_loss = 35.987, entropy_loss = -4.878, learner_queue_size = 32, train_seconds = 2.3883e+04, _tick = 8940, _time = 1.6546e+09)
[2022-06-07 17:01:54,922][root][INFO] - Step 42882560 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42882560, mean_episode_return = None, mean_episode_step = 2402.5, total_loss = 143.87, pg_loss = 73.611, baseline_loss = 75.121, entropy_loss = -4.8597, learner_queue_size = 32, train_seconds = 2.3888e+04, _tick = 8940, _time = 1.6546e+09)
[2022-06-07 17:01:59,926][root][INFO] - Step 42892800 @ 2046.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (step = 42892800, mean_episode_return = 20.48, mean_episode_step = 2504.3, total_loss = -11.218, pg_loss = -71.096, baseline_loss = 64.744, entropy_loss = -4.8661, learner_queue_size = 32, train_seconds = 2.3893e+04, _tick = 8943, _time = 1.6546e+09)
[2022-06-07 17:02:04,930][root][INFO] - Step 42903040 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42903040, mean_episode_return = 50.504, mean_episode_step = 1895.8, total_loss = -129.28, pg_loss = -160.14, baseline_loss = 35.753, entropy_loss = -4.8908, learner_queue_size = 32, train_seconds = 2.3898e+04, _tick = 8945, _time = 1.6546e+09)
[2022-06-07 17:02:09,934][root][INFO] - Step 42910720 @ 1534.8 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (step = 42910720, mean_episode_return = None, mean_episode_step = 1883.3, total_loss = 354.55, pg_loss = 231.72, baseline_loss = 127.72, entropy_loss = -4.8907, learner_queue_size = 32, train_seconds = 2.3903e+04, _tick = 8945, _time = 1.6546e+09)
[2022-06-07 17:02:14,938][root][INFO] - Step 42920960 @ 2046.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (step = 42920960, mean_episode_return = None, mean_episode_step = 3572.4, total_loss = -3.9354, pg_loss = -29.638, baseline_loss = 30.609, entropy_loss = -4.9061, learner_queue_size = 32, train_seconds = 2.3908e+04, _tick = 8946, _time = 1.6546e+09)
[2022-06-07 17:02:19,944][root][INFO] - Step 42928640 @ 1534.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (step = 42928640, mean_episode_return = None, mean_episode_step = 2245.0, total_loss = -120.42, pg_loss = -120.9, baseline_loss = 5.3855, entropy_loss = -4.9011, learner_queue_size = 32, train_seconds = 2.3913e+04, _tick = 8947, _time = 1.6546e+09)
[2022-06-07 17:02:24,950][root][INFO] - Step 42938880 @ 2045.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (step = 42938880, mean_episode_return = None, mean_episode_step = 2213.5, total_loss = 145.94, pg_loss = 50.534, baseline_loss = 100.3, entropy_loss = -4.8975, learner_queue_size = 32, train_seconds = 2.3918e+04, _tick = 8948, _time = 1.6546e+09)
[2022-06-07 17:02:29,953][root][INFO] - Step 42946560 @ 1535.1 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (step = 42946560, mean_episode_return = None, mean_episode_step = 2835.7, total_loss = 102.63, pg_loss = 67.318, baseline_loss = 40.212, entropy_loss = -4.899, learner_queue_size = 32, train_seconds = 2.3923e+04, _tick = 8950, _time = 1.6546e+09)
[2022-06-07 17:02:34,959][root][INFO] - Step 42956800 @ 2045.6 SPS. Inference batcher size: 62. Learner queue size: 32. Othe